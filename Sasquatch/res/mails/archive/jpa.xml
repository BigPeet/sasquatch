<?xml version="1.0" encoding="UTF-8"?>
<mails>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Schema generation feedback</header>
    <date>Wed Jan 02 16:51:36 CET 2013</date>
    <body>Hello everyone and happy new year to all, I have been discussing the schema generation feature with Max in our tooling team. We have some minor feedback We probably should offer a default value for create-script-target /  drop-script-target. Also being able to specify relative paths would be useful to not have to change these from one environment to another. What kind of URLs are accepted for destinations? Only file based URLs or any thing? Not sure we want to be forced to write support for every URL protocol on the planet. Does create-database-schemas also cover the (non) creation of catalogs for completeness (CREATE CATALOG command)? Is the *-script-source considered mutual exclusive from the generated script execution or is it considered something that is run in *addition* to these  scripts ? Should generateSchema be renamed prepareSchema to better reflect what actually happens? It's not necessary just schema generation but can involve creation / update / drop etc. Emmanuel</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Schema generation feedback</header>
    <date>Tue Jan 08 00:47:30 CET 2013</date>
    <body>Hi Emmanuel, Thanks for the feedback.  Some comments and further questions for you and the  group below.... -Linda What did you have in mind here? I'm assuming we should support file-based URLs.  I'd like to hear from the  group as to what other protocols should be supported. If catalog is functionally equivalent to schema, then yes.  If catalog were functionally equivalent to database, then I would say not. What do you think? I'm not sure I'm understanding the question, but I'll give it a try :-) ... The way the schema generation section is currently written (although perhaps  not explicitly enough), it is assumed that schema-generation-target and schema-generation-action must be specified.  *-script-source wouldn't be mutually exclusive then, but rather complementary. So, does your question then reduce to whether specification of the  *-script-source properties, by themselves, should be sufficient to cause tables to be  created, dropped, or both? I'm generally not too wedded to particular names, but "schema generation" as a generic term seems to have become fairly established.   If we were to change  it though, what did you have in mind for the related property names?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Clarification of PersistenceUnitUtils.getIdentifier(…)</header>
    <date>Thu Jan 10 10:18:26 CET 2013</date>
    <body>Hi all, the JavaDoc of  PersistenceUnitUtils.getIdentifier(…) states the following:  Return the id of the entity.  A generated id is not guaranteed to be available until after  the database insert has occurred.  Returns null if the entity does not yet have an id. I had a discussion with Romain Manni-Bucau (involved with the Apache  Foundation and OpenJPA, AFAIK) about the detailed semantics of this in case  you are using primitive identifiers like this: @Entity Romain argued that the spec defines that getIdentifier(…) would/should return  null for a new Person(), as it theoretically did not get any ID assigned yet  but effectively has a value. Personally, I didn't read this meaning into the  specification of the method. This impression seems to be backed by the fact  that I discovered different persistence providers handling this differently: - OpenJPA 2.2.0 -&amp;gt; returns null for new Person(), but a non-null value once I  called em.persist(…) - Hibernate 3.10.0 -&amp;gt; return a non-null value even for new Person() What is the actual intent of the method? As it is right now, one cannot use  the method to determine whether the entity already had been persisted  reliably across persistence providers. What do the TCK tests of this method  look like? Regards, Ollie --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification of PersistenceUnitUtils.getIdentifier(…)</header>
    <date>Fri Jan 11 01:16:27 CET 2013</date>
    <body>Yes, this is a hole in the spec from JPA 2.0. I'd like to get opinions from the group as to how we should close it,  especially if you have knowledge of developers depending on the current behaviors of  your respective implementations. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification of PersistenceUnitUtils.getIdentifier(…)</header>
    <date>Fri Jan 11 17:23:53 CET 2013</date>
    <body>Granted that this is an "implementation detail", but the problem here is that returning null in this case effectively requires bytecode enhancement.  As Hibernate does not work on bytecode enhancement we have to simply return the entity instance's current attribute state. So as I said, I realize that the above is an "implementation detail", but given that the spec specifically does not require bytecode enhancement I don't see how we can then effectively require bytecode enhancement</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Mon Jan 14 09:01:55 CET 2013</date>
    <body>Hi, After using Hibernate for a while, I had assumed that insertable = false and  updatable = false where useful for supporting reuse of columns across fields.   But I got into an argument this week end with someone saying that the JPA  spec does not define how/if this is mandated. Example: @Entity     @Id     @GeneratedValue(strategy = GenerationType.IDENTITY)     @OneToMany(cascade = CascadeType.ALL, mappedBy = "company") [..] @Entity     @Id     @GeneratedValue(strategy = GenerationType.IDENTITY)     @ManyToOne()     @JoinColumn(name = "COMPANY_ID")      // Reuse of the field for the FK id     @Column(name = "COMPANY_ID", insertable = false, updatable = false) I went through the JPA 2.1 draft, can could not find anything specific to  this topic. Should this be clarified? Or was it discussed before I joined? Best regards, Nicolas Seyvet</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Clarification: BindableType</header>
    <date>Thu Jan 17 13:07:44 CET 2013</date>
    <body>Hi all, the metamodel API exposes a Bindable interface which in turn exposes a  BindableType. Unfortunately it doesn't seem to be defined in which cases  which BindableType should be returned. E.g. I have the following types: @Entity @Entity   … Using the Criteria API I now try to find out that a Path instance is starting  at Person is pointing to an entity effectively: I was expecting to get ENTITY_TYPE returned but I get SINGULAR_ATTRIBUTE from  a recent Hibernate. Trying to come up with a bug report for Hibernate I tried  to find out what the specified behavior is but couldn't find anything except  the rather brief JavaDoc. Cheers, Ollie --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification: BindableType</header>
    <date>Thu Jan 17 13:19:56 CET 2013</date>
    <body>As a follow up: I am now falling back on checking whether BindableType is a  SingluarAttribute and invoking ….isAssociation() on it. If that fails, I  inspect the persistentAttributeType for being one of the association  annotations. Quite a wild ride actually but there doesn't seem to be an easier way without  having direct access to the Metamodel. Cheers, Ollie Am 17.01.2013 um 13:07 schrieb Oliver Gierke &amp;lt;ogierke@...&amp;gt;:  Hi all,    the metamodel API exposes a Bindable interface which in turn exposes a   BindableType. Unfortunately it doesn't seem to be defined in which cases   which BindableType should be returned. E.g. I have the following types:    @Entity    @Entity   …    Using the Criteria API I now try to find out that a Path instance is   starting at Person is pointing to an entity effectively:      I was expecting to get ENTITY_TYPE returned but I get SINGULAR_ATTRIBUTE   from a recent Hibernate. Trying to come up with a bug report for Hibernate   I tried to find out what the specified behavior is but couldn't find   anything except the rather brief JavaDoc.    Cheers,  Ollie  --   /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */ --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification: BindableType</header>
    <date>Thu Jan 17 14:16:20 CET 2013</date>
    <body>The path in your example is a SINGULAR_ATTRIBUTE, the only time you should expect an ENTITY_TYPE is if the Path&amp;lt;&amp;gt; represents an actual entity like: from.getModel().getBinableType() == ENTITY_TYPE I do see how this question arises as the specification does not clearly call out when each type should be returned.  One has to search the interfaces to see what interfaces extend from Bindable then it becomes clearer. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Clarification: BindableType</header>
    <date>Thu Jan 17 14:46:46 CET 2013</date>
    <body>Hm, from reading the spec naively, would have expected a SingluarAttribute  instance for from.get(…).getModel() which then returns a BindableType of  ENTITY_TYPE as the path is pointing to an entity. I was under the impression that the separation between Attribute interfaces  and BindableType actually exists to allow a flexible mapping. Otherwise I  could just do instanceof checks on the actual Attribute type. Also, according to chapter 2.9 of the Spec an attribute with the …ToOne or  …ToMany annotations is considered an association. Thus, am I wrong  considering the call to ….isAssociation() on my Path pointing to "address"  returning false to be a bug then? Cheers, Ollie   Am 17.01.2013 um 14:16 schrieb Gordon Yorke &amp;lt;gordon.yorke@...&amp;gt;:  The path in your example is a SINGULAR_ATTRIBUTE, the only time you should   expect an ENTITY_TYPE is if the Path&amp;lt;&amp;gt; represents an actual entity like:    from.getModel().getBinableType() == ENTITY_TYPE    I do see how this question arises as the specification does not clearly   call out when each type should be returned.  One has to search the   interfaces to see what interfaces extend from Bindable then it becomes   clearer.    --Gordon --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Clarification: BindableType</header>
    <date>Thu Jan 17 15:10:56 CET 2013</date>
    <body>The issue is that the BindableType is the actual type of the Bindable and has nothing to do with state of the attribute/EntityType. "getBindableJavaType" on Bindable likely extends the confusion and should not be there or renamed to something like getTargetJavaType() but it was added as a shortcut to get the target type without requiring a cast to PluralAttribute. in your example : ((Attribute)path.getModel()).isAssociation() should return true.  You cannot always cast to Attribute without checking instanceof or BindableType first though. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Clarification: BindableType</header>
    <date>Thu Jan 17 15:16:31 CET 2013</date>
    <body>I am not sure I get the special semantics of Bindable. I digged a bit deeper  into what different implementations return for the calls, here's what I found  out:             | BindableType       | ….isAssociation() | ------------+--------------------+-------------------+ Hibernate   | SINGULAR_ATTRIBUTE | false             | OpenJpa     | ENTITY_TYPE        | true              | EclipseLink | SINGULAR_ATTRIBUTE | true              | ------------+--------------------+-------------------+ This quite a mixed bag (if you wanted to be negative you could call it a  mess). At least I think I can reasonably argue that ….isAssociation() is not  implemented correctly given the definition of an association in 2.9 of the  spec. I'll refrain from looking into BindableType at all then and just shoot for  ….isAssociation() falling back on the persistence attribute type. Cheers, Ollie Am 17.01.2013 um 15:10 schrieb Gordon Yorke &amp;lt;gordon.yorke@...&amp;gt;:  The issue is that the BindableType is the actual type of the Bindable and   has nothing to do with state of the attribute/EntityType.    "getBindableJavaType" on Bindable likely extends the confusion and should   not be there or renamed to something like getTargetJavaType() but it was   added as a shortcut to get the target type without requiring a cast to   PluralAttribute.    in your example : ((Attribute)path.getModel()).isAssociation() should   return true.  You cannot always cast to Attribute without checking   instanceof or BindableType first though.    --Gordon   --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Feature request: allow type level annotations to be used as meta-annotations</header>
    <date>Thu Jan 17 16:50:57 CET 2013</date>
    <body>Hi all, I repeatedly find myself annotating my JPA entities with the very same set of  annotations: @Entity @EntityListeners(AuditingEntityListener.class) If both @Entity and @EntityListener were allowed to be used as  meta-annotations I could collapse them into: @Target(TYPE) @Retention(RUNTIME) @Entity @EntityListeners(AuditingEntityListener.class) Resulting in: @AuditedEntity This is in line with the meta-annotation handling CDI exposes to introduce  annotation with richer semantics in annotation code [0]. The following  changes would be required. - Add ElementType.ANNOTATION_TYPE to the relevant annotations - Specify that persistence providers have to evaluate the annotations from  the meta-level as well using the first one found, so that locally defined  annotations would be considered first. Cheers, Ollie [0]  https://github.com/dblevins/metatypes/ --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Feature request: allow type level annotations to be used as meta-annotations</header>
    <date>Thu Jan 17 20:53:33 CET 2013</date>
    <body>Hi Oliver, I agree with your suggestion.  We tried to bring in a  Metatype/Stereotype-like functionality into Java EE 7, but ran out of runway for this release.  It is definitely on  our roadmap for Java EE 8 however.  You can view the discussions on the  javaee-spec.java.net project archives. Could you please log this in the JPA JIRA as an RFE for JPA.next? thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] EntityGraph's AttributeNode and Type</header>
    <date>Thu Jan 17 21:06:18 CET 2013</date>
    <body>Building on Ollie's feedback with respect to BindableType I was reviewing AttributeNode.getType() and see this method as having very little value and unnecessarily crowds the interface.  Given the stage we are at with delivering the specifiation I suggest we simply remove this method for now and provide other methods that add value to the interface in the future.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EntityGraph's AttributeNode and Type</header>
    <date>Thu Jan 17 21:14:44 CET 2013</date>
    <body>Hi Gordon, Would it be useful to simply return the Java type rather than Type?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Feature request: allow type level annotations to be used as meta-annotations</header>
    <date>Thu Jan 17 21:15:11 CET 2013</date>
    <body>There you go:  http://java.net/jira/browse/JPA_SPEC-43 : ) Am 17.01.2013 um 20:53 schrieb Linda DeMichiel &amp;lt;linda.demichiel@...&amp;gt;:  Hi Oliver,    I agree with your suggestion.  We tried to bring in a   Metatype/Stereotype-like functionality  into Java EE 7, but ran out of runway for this release.  It is definitely   on our roadmap  for Java EE 8 however.  You can view the discussions on the   javaee-spec.java.net project archives.    Could you please log this in the JPA JIRA as an RFE for JPA.next?    thanks,    -Linda     https://github.com/dblevins/metatypes/ --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Thu Jan 17 21:21:40 CET 2013</date>
    <body>Hi Nicolas, This was discussed back in the JPA 1.0 days.  The semantics of insertable /  updatable are defined in the Column annotation and are consistent with your use of them  in the example above. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EntityGraph's AttributeNode and Type</header>
    <date>Thu Jan 17 21:44:36 CET 2013</date>
    <body>Perhaps but they are practically the same thing in this case and I do not see a clear use case for this information especially since there is no way to inspect the related subgraphs of the attribute node.  It would be far more helpful to add getSubgraphs() getKeySubgraphs() as a starting point for EntityGraph inspection. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Query.setLockMode</header>
    <date>Thu Jan 17 23:29:05 CET 2013</date>
    <body>Query.setLockMode currently states: "@throws IllegalStateException if the query is found not to be * a Java Persistence query language SELECT query * or a Criteria API query" To be consistent with the JPQL semantics, this should be restated as "@throws IllegalStateException if the query is found not to be * a Java Persistence query language SELECT query * or a CriteriaQuery query" I.e., we disallow setting query lock modes for update and delete "queries" I plan to make this change, so yell now if you disagree. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EntityGraph's AttributeNode and Type</header>
    <date>Thu Jan 17 23:43:54 CET 2013</date>
    <body>Thanks, Gordon. Folks, I plan to remove AttributeNode.getType().  Yell now if you disagree. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Clarification: FETCH JOINs and ORDER BY clauses</header>
    <date>Sun Jan 20 10:07:18 CET 2013</date>
    <body>Hi all, I have a user asking why a query he builds using a fetch clause gets rejected  as soon as he's referring to the association declared in the fetch clause: @Entity @Entity select p from Person p left join fetch p.address where p.firstname =  :firstname works, whereas select p from Person p left join fetch p.address where p.firstname =  :firstname order by p.address.city fails on Hibernate with the following error message: "query specified join fetching, but the owner of the fetched association was  not present in the select list" This definitely makes some sense to me as Section 4.4.5.3 of the spec  explicitly states:  The association referenced by the right side of the FETCH JOIN clause must   be an association or element collection that is referenced from an entity   or embeddable that is returned as a result of the query. It is not   permitted to specify an identification variable for the objects referenced   by the right side of the FETCH JOIN clause, and hence references to the   implicitly fetched entities or elements *cannot appear elsewhere in the   query.* I browsed the spec intensively but couldn't find any discussion of how to  combine order by clauses with fetch joins as one obviously has to violate the  latter of the two statements if one wants to sort by a property referenced in  the fetch clause. What's the intended way of combining ORDER BY clauses and FETCH JOINs? Cheers, Ollie --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Typo in spec</header>
    <date>Mon Jan 21 08:37:30 CET 2013</date>
    <body>Typo Section 3.3 Container-managed persistence contexts are described further in section 7.6. Persistence contexts managed by the application are described futher in section 7.7.   futher -&amp;gt; further.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Limited mapping for Enum Types</header>
    <date>Mon Jan 21 09:15:18 CET 2013</date>
    <body>One key issue that is to my feeling is not addressed sufficiently is how we map an Enum. 1./ An enum is not allowed to have a Persistent Counterpart 2./ An enum has the limited choice of  - @Enumerated(EnumType.STRING)  - @Enumerated(EnumType.ORDINAL) Which is in real case scenario's too limiting.  The limitations seem to originate from the idea that "enums that have state are not supported". I explain both. Consider the Following Table representing a Male/Female Choice in the DB Table: SEX SEX_ID  SEX_NAME M       Male F       Female N/A     Not Established Two Issues come up, matching with the earlier listed limitations.  1./ Sex has also a Descriptive Field, the Name. It is common to have a fixed set of Categories defined on the DB, and have additional attributes associated with them. Although Constant, those attributes can be specific to the environment. One might decide on the Database end to just rename 'Not Established' to 'Unknown' without affecting the integrity of the Database, because the PK is based on SEX_ID, not SEX_NAME. It would be much more flexible to have an Enum, as a READ-ONLY entity do a one-time SYNC/FETCH to sync up additional Attributes once the ID is established (the Enum constants are defined). 2./ Even though on the Database the SEX_ID is keyed by M/F, Most likely, one would like this to be defined as MALE / FEMALE Enum Constants. In Addition, The Syntax of an Enum does not allow for Special Characters (forward slashes, spaces, etc). So 'N/A' cannot be represented by an Enum Constant with the same name. Also instead of using a Alphabetic ID, one might decide on the Database end to key to the Enum Constants based on a numeric Key. A numeric Key can also not be represented as an Enum Constant, and the ORDINAL will rarely match the database generated keys. Or even a worse scenario for 2./ but typical: Everywhere the ENUM is being keyed using a numeric value, but the value of the key is different from Environment to Environment (Production versus QA). So the Key associated with the ENUM Constants first need to be fetched/synced from Database. Kind of a combination of 1./ and 2./ together. These two issues makes it extremely difficult to use Enum's in a pre-existing Database, with a schema that was not designed for use with JPA specifically. This is how the Enum could look like with such features enabled: @Enumerated(EnumType.CUSTOM) @OneTimeSync  MALE,  FEMALE,  @Id  @Column('sex_id')    @MapEntity('M',Sex.MALE),    @MapEntity('F',Sex.FEMALE),    @MapEntity('N/A',Sex.UNKNOWN)  @ImmutableAttribute  @Column(sex_name) or for a Numerically Keyed value, but keys fetched from Database: @Enumerated(EnumType.CUSTOM) @OneTimeSync  MALE,  FEMALE,  @SyncId  // Id/Key to be used for a one-time-sync.  @Column('sex_id')    @MapEntity('M',Sex.MALE),    @MapEntity('F',Sex.FEMALE),    @MapEntity('N/A',Sex.UNKNOWN)  @Id  @ImmutableAttribute  @Column('sex_key')  @ImmutableAttribute  @Column(sex_name)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Mon Jan 21 11:02:51 CET 2013</date>
    <body>Ok that is good. But this is a typical instance of a table field being re-used within an  entity. Let's say, there is a table A with three columns id, col1 and col2.    A | id |col1 | col2| Is the following legal according to the spec? @Entity      @Id      @GeneratedValue(strategy = GenerationType.IDENTITY)   @Column(name = "col1 ")     @Column(name = "col1 ", insertable = false, updatable = false)         [..] If not, should it be covered in the spec? -----Original Message----- From: Linda DeMichiel [ mailto:linda.demichiel@...]  Sent: Thursday, January 17, 2013 9:22 PM To: jsr338-experts@... Subject: [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field  re-use Hi Nicolas,  Hi,  After using Hibernate for a while, I had assumed that insertable = false   and updatable = false where useful for supporting reuse of columns across   fields.  But I got into an argument this week end with someone saying that   the JPA spec does not define how/if this is mandated.  Example:  @Entity       @Id       @GeneratedValue(strategy = GenerationType.IDENTITY)       @OneToMany(cascade = CascadeType.ALL, mappedBy = "company")  [..]  @Entity       @Id       @GeneratedValue(strategy = GenerationType.IDENTITY)       @ManyToOne()       @JoinColumn(name = "COMPANY_ID")        // Reuse of the field for the FK id       @Column(name = "COMPANY_ID", insertable = false, updatable = false)  I went through the JPA 2.1 draft, can could not find anything specific to   this topic.  Should this be clarified? Or was it discussed before I joined? This was discussed back in the JPA 1.0 days.  The semantics of insertable /  updatable are defined in the Column annotation and are consistent with your  use of them in the example above. -Linda  Best regards,  Nicolas Seyvet</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Mon Jan 21 14:28:48 CET 2013</date>
    <body>In JPA 1.0 we did not get around to specifying a read-only mapping (in fact we still haven't). Because of that, using the insertable=false, updatable=false combination in the @Column mapping became the de facto way of setting a mapping to be read-only. It is not ideal since it requires setting multiple physical column/join column attributes when a logical read-only option would be the more appropriate option, but for the most part it has stuck and is what most vendors support since it at least uses standard options instead of a vendor-specific read-only annotation. -Mike</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Tue Jan 22 13:06:36 CET 2013</date>
    <body>" and is what most vendors support since it at least uses standard options  instead of a vendor-specific read-only annotation." Ok, but this is still not a specification. I guess the point is that this specification will not clarify the re-use of a  table column within the same entity. Is that correct? /Nicolas -----Original Message----- From: michael keith [ mailto:michael.keith@...]  Sent: Monday, January 21, 2013 2:29 PM To: jsr338-experts@... Cc: Linda DeMichiel Subject: [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field  re-use In JPA 1.0 we did not get around to specifying a read-only mapping (in fact  we still haven't). Because of that, using the insertable=false, updatable=false combination in  the @Column mapping became the de facto way of setting a mapping to be  read-only. It is not ideal since it requires setting multiple physical  column/join column attributes when a logical read-only option would be the  more appropriate option, but for the most part it has stuck and is what most  vendors support since it at least uses standard options instead of a  vendor-specific read-only annotation. -Mike  Hi Nicolas,  This was discussed back in the JPA 1.0 days.  The semantics of   insertable / updatable are defined in the Column annotation and are   consistent with your use of them in the example above.  -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Tue Jan 22 14:23:59 CET 2013</date>
    <body>No, I never said the spec "will not clarify the re-use of a table column". I just filled in some of the history and how we got to the current state. There is nothing in the spec that disallows reusing a column in this way, and this is how the vendors support it, so you shouldn't encounter any problems in your application. -Mike</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Tue Jan 22 14:50:32 CET 2013</date>
    <body>I mean to say that it was not the intent of the spec to detail this.  Let me  try to ask this question differently: Would it be good to standardize how the  vendors support this as part of jsr-338 scope? /Nicolas -----Original Message----- From: michael keith [ mailto:michael.keith@...]  Sent: Tuesday, January 22, 2013 2:24 PM To: jsr338-experts@... Cc: Nicolas Seyvet Subject: Re: [jsr338-experts] Re: insertable = false, updatable = false &amp;amp;  field re-use No, I never said the spec "will not clarify the re-use of a table column". I  just filled in some of the history and how we got to the current state. There  is nothing in the spec that disallows reusing a column in this way, and this  is how the vendors support it, so you shouldn't encounter any problems in  your application. -Mike  " and is what most vendors support since it at least uses standard options   instead of a vendor-specific read-only annotation."  Ok, but this is still not a specification.  I guess the point is that this specification will not clarify the re-use of   a table column within the same entity. Is that correct?  /Nicolas  -----Original Message-----  From: michael keith [ mailto:michael.keith@ ...]  Sent: Monday, January 21, 2013 2:29 PM  To: jsr338-experts@...  Cc: Linda DeMichiel  Subject: [jsr338-experts] Re: insertable = false, updatable = false&amp;amp;    field re-use  In JPA 1.0 we did not get around to specifying a read-only mapping (in fact   we still haven't).  Because of that, using the insertable=false, updatable=false combination in   the @Column mapping became the de facto way of setting a mapping to be   read-only. It is not ideal since it requires setting multiple physical   column/join column attributes when a logical read-only option would be the   more appropriate option, but for the most part it has stuck and is what   most vendors support since it at least uses standard options instead of a   vendor-specific read-only annotation.  -Mike</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Tue Jan 22 16:00:24 CET 2013</date>
    <body>Ideally we would have some kind of annotation that just pointed to the other attribute to signify that the column was defined by it and that only one of the mappings should write to the column or contribute to schema gen (a bit like the @MapsId situation, but in the opposite direction). Since in practice something already works, though, and there's not really any time left to add new features to the spec in jsr 338 it won't likely happen in this round. Maybe in the future if enough people ask for it?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Typo in spec</header>
    <date>Tue Jan 22 19:11:14 CET 2013</date>
    <body>Thanks!!</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Tue Jan 22 19:17:49 CET 2013</date>
    <body>Right. BTW, IIRC, in past we ran up against the issue that vendors differed in their  use of this technique for overlapping PK/FK mappings -- which was a problem that we solved with the  introduction of @MapsId.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification: FETCH JOINs and ORDER BY clauses</header>
    <date>Tue Jan 22 19:34:44 CET 2013</date>
    <body>Hi Oliver, I guess I don't understand your question.  Are you asking how you can order  query results by the target of the join fetch?  If so, the intent of the spec is that you can not do that. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification: FETCH JOINs and ORDER BY clauses</header>
    <date>Tue Jan 22 20:04:20 CET 2013</date>
    <body>Okay, what's the reason to forbid that? -- Sent while on the run... Am 22.01.2013 um 19:34 schrieb Linda DeMichiel &amp;lt;linda.demichiel@...&amp;gt;:  Hi Oliver,      I guess I don't understand your question.  Are you asking how you can order   query results by the target  of the join fetch?  If so, the intent of the spec is that you can not do   that.    regards,    -Linda      </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification: FETCH JOINs and ORDER BY clauses</header>
    <date>Tue Jan 22 20:34:36 CET 2013</date>
    <body>The orderby rules were written to allow direct mapping to SQL.  If an  implementation were to support ordering by something that were incidentally fetched but not  in the SELECT list that would be one thing -- but I don't think we should require  support for this.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] criteria api missing where(List&#xD;
                          &lt;predicate&gt;&#xD;
                            )&#xD;
                          &lt;/predicate&gt;</header>
    <date>Wed Jan 23 07:47:14 CET 2013</date>
    <body>Hello, I opened JPA_SPEC-44, which was closed ( http://java.net/jira/browse/JPA_SPEC-44 ). It was about the criteria query API missing a where() method with a List parameter: Although groupBy, having, orderBy etc. all have both array (varargs) and List parameter versions. The issue was closed with this comment: "It was felt that CriteriaQuery&amp;lt;T&amp;gt; where(List&amp;lt;Predicate&amp;gt; restrictions) was not needed, as unlike in the other cases, there are operations to combine expressions using the logical operators." However, I think it would be a good idea to include it. I'm using the criteria API like this: I then have to do this: I think it would be easier, and the API would be more consistent, if we could just do this: What do you think? Best regards, Yannick Majoros</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: criteria api missing where(List&#xD;
                          &lt;predicate&gt;&#xD;
                            )&#xD;
                          &lt;/predicate&gt;</header>
    <date>Wed Jan 23 13:23:34 CET 2013</date>
    <body>+1 from me as well.  This is often how I build my where clauses in criteria api. On Wed, Jan 23, 2013 at 1:47 AM, Yannick Majoros yannick.majoros@... Hello, I opened JPA_SPEC-44, which was closed ( http://java.net/jira/browse/ JPA_SPEC-44 ). It was about the criteria query API missing a where() method with a List parameter: Although groupBy, having, orderBy etc. all have both array (varargs) and List parameter versions. The issue was closed with this comment: "It was felt that CriteriaQuery&amp;lt;T&amp;gt; where(List&amp;lt;Predicate&amp;gt; restrictions) was not needed, as unlike in the other cases, there are operations to combine expressions using the logical operators." However, I think it would be a good idea to include it. I'm using the criteria API like this: ClientStatus searchedClientStatus = clientSearch.getClientStatus()     predicates.add( I then have to do this:         clientQuery.where( I think it would be easier, and the API would be more consistent, if we could just do this: What do you think? Best regards, Yannick Majoros</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: criteria api missing where(List&#xD;
                          &lt;predicate&gt;&#xD;
                            )&#xD;
                          &lt;/predicate&gt;</header>
    <date>Wed Jan 23 17:52:17 CET 2013</date>
    <body>Have you considered using the CriteriaBuilder and method to additively build  up your predicates? e.g. ...</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: criteria api missing where(List&#xD;
                          &lt;predicate&gt;&#xD;
                            )&#xD;
                          &lt;/predicate&gt;</header>
    <date>Wed Jan 23 18:00:07 CET 2013</date>
    <body>Linda, Atleast in my case, that assumes that you have one predicate already defined.  In my setup I have no predicates guaranteed defined.  This would end up happening: p = cb.and(p,cityPredicate) Should that work? On Wed, Jan 23, 2013 at 11:52 AM, Linda DeMichiel linda.demichiel@... Hello, I opened JPA_SPEC-44, which was closed ( http://java.net/jira/browse/ JPA_SPEC-44 ). It was about the criteria query API missing a where() method with a List parameter: Although groupBy, having, orderBy etc. all have both array (varargs) and List parameter versions. The issue was closed with this comment: "It was felt that CriteriaQuery&amp;lt;T&amp;gt; where(List&amp;lt;Predicate&amp;gt; restrictions) was not needed, as unlike in the other cases, there are operations to combine expressions using the logical operators." However, I think it would be a good idea to include it. I'm using the criteria API like this: ClientStatus searchedClientStatus = clientSearch.getClientStatus() predicates.add( I then have to do this: clientQuery.where( I think it would be easier, and the API would be more consistent, if we could just do this: What do you think?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Early access to the TCK</header>
    <date>Thu Jan 24 20:49:05 CET 2013</date>
    <body>A few months ago there was a discussion about early access to the TCK as being a generally good thing and there seemed to be a general consensus for doing that in any later revisions of the spec.  The Cliff Notes (tm) version of the pros were mainly: 1) the ability to identify as early as possible problems in the TCK itself 2) to help providers get started on certifying 3) allow early feedback from the group as to missing portability coverage I was just curious whether we could possibly start getting access to the 2.1 TCK, especially now that the spec is pretty solidified.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Early access to the TCK</header>
    <date>Thu Jan 24 21:13:02 CET 2013</date>
    <body>+1 I am not a lawyer, but I assume, as EC Member, I'd be among "Qualified Individuals" to have access to the TCK without a license fee For Qualified Not-for-Profits and Qualified Individuals: $0. Werner On Thu, Jan 24, 2013 at 8:49 PM, Steve Ebersole steve.ebersole@... A few months ago there was a discussion about early access to the TCK as being a generally good thing and there seemed to be a general consensus for doing that in any later revisions of the spec.  The Cliff Notes (tm) version of the pros were mainly: 1) the ability to identify as early as possible problems in the TCK itself 2) to help providers get started on certifying 3) allow early feedback from the group as to missing portability coverage I was just curious whether we could possibly start getting access to the 2.1 TCK, especially now that the spec is pretty solidified.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Early access to the TCK</header>
    <date>Thu Jan 24 21:39:28 CET 2013</date>
    <body>Hi Steve, The team tells me that we are still a number of weeks away on the TCK being  ready for this. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Early access to the TCK</header>
    <date>Thu Jan 24 22:09:37 CET 2013</date>
    <body>To be honest, that reads like: we write the tests right before we ship the  software. How can the expert group make sure the test we will see eventually  are in line with what the spec defines? This essentially leaves us as author  of a spec that we will potentially see implementations of that do/can not  follow the spec because the TCK we have no influence on didn't catch serious  glitches. Where's the value in voting about a spec where the actual rules  that govern implementations are not under the control of those who vote? Beside Steve's issue with implementing the new features of the spec what is  the plan for the general work on the TCK going forward? To be honest I think  it's close to ridiculous to ask members of the EG to hand in ideas and  suggestions for improvements to the TCK which will then be implemented  however by whoever somewhere. Especially in the light of the ambiguities discovered recently I think one of  the top priorities going forward has to be that we (the EG) can make sure  we'll see the the new features and behavior tested adequately. This not only  will help us creating a good/better spec as ambiguities will be found before  it is to late but also a much better usability as the lack of defined *and  enforced* semantics prevents functionality from being rendered useless as  pretty much every persistence provider behaves slightly different as they  essentially have no real means to test their implementations. So I suggest that going forward... 1. the TCK has to developed alongside the spec by the EG. Newly defined API  and functionality has to be backed by test cases on submission of the feature  to the spec. 2. This requires EG members having access to the TCK during spec development  (requires appropriate licensing of the spec) 3. If I conclude wishful thinking here I wonder why even the community  shouldn't have access to the TCK to maybe help out discovering potential  issues. @Werner: does any of the current JCP versions already define TCK rules like  this? The closest JSR I can remember is CDI that always developed an Apache  licensed TCK fully in the open IIRC. Or is there any general rules within the  JCP/Oracle that should prevent us from going down that route? Cheers, Ollie -- Sent while on the run... Am 24.01.2013 um 21:39 schrieb Linda DeMichiel &amp;lt;linda.demichiel@...&amp;gt;:  Hi Steve,    The team tells me that we are still a number of weeks away on the TCK being   ready for this.    thanks,    -Linda    </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Early access to the TCK</header>
    <date>Thu Jan 24 22:23:51 CET 2013</date>
    <body>Oli/all, In an ideal world, that's how TDD works  And based on the session Patrick and others from the PMO held a while ago on TCKs, a TCK must not be tailored towards one implementation or the RI only, so if a Spec Lead company is large enough like in this case, ideally the part that writes the TCK should not necessarily be the ones who write the Spec or RI, though in some situations of course they may interact. Regarding the license, I believe the Spec page  http://jcp.org/en/jsr/detail?id=338 section 2.18 makes it very clear, that neither RI nor TCK for 338 are licensed under Apache or any other Open Source license. Nor are JSON or WebSockets, just to mention 2 other parts of the Java EE 7 Umbrella. In those cases the Spec isn't even in a public repository, while RI is, though certain licenses may also apply for commercial purposes beyond R&amp;amp;D, other Open Source Projects that go as Non-Profits or those "Qualified Individuals".  JCP.next is not about "Make everything Open Source", it is about a transparent way of designing and developing the JSRs with as little "deals behind closed doors" as possible, or none HTH, Werner On Thu, Jan 24, 2013 at 10:09 PM, Oliver Gierke ogierke@... To be honest, that reads like: we write the tests right before we ship the software. How can the expert group make sure the test we will see eventually are in line with what the spec defines? This essentially leaves us as author of a spec that we will potentially see implementations of that do/can not follow the spec because the TCK we have no influence on didn't catch serious glitches. Where's the value in voting about a spec where the actual rules that govern implementations are not under the control of those who vote? Beside Steve's issue with implementing the new features of the spec what is the plan for the general work on the TCK going forward? To be honest I think it's close to ridiculous to ask members of the EG to hand in ideas and suggestions for improvements to the TCK which will then be implemented however by whoever somewhere. Especially in the light of the ambiguities discovered recently I think one of the top priorities going forward has to be that we (the EG) can make sure we'll see the the new features and behavior tested adequately. This not only will help us creating a good/better spec as ambiguities will be found before it is to late but also a much better usability as the lack of defined *and enforced* semantics prevents functionality from being rendered useless as pretty much every persistence provider behaves slightly different as they essentially have no real means to test their implementations. So I suggest that going forward... 1. the TCK has to developed alongside the spec by the EG. Newly defined API and functionality has to be backed by test cases on submission of the feature to the spec. 2. This requires EG members having access to the TCK during spec development (requires appropriate licensing of the spec) 3. If I conclude wishful thinking here I wonder why even the community shouldn't have access to the TCK to maybe help out discovering potential issues. @Werner: does any of the current JCP versions already define TCK rules like this? The closest JSR I can remember is CDI that always developed an Apache licensed TCK fully in the open IIRC. Or is there any general rules within the JCP/Oracle that should prevent us from going down that route? Cheers, Ollie -- Sent while on the run... linda.demichiel@...</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Early access to the TCK</header>
    <date>Thu Jan 24 23:24:05 CET 2013</date>
    <body>Ollie, Thanks for your enthusiasm in offering to contribute here. To put things into perspective, I need to note that in over 10 years of creating TCKs we've found essentially no one who was interested in reviewing our TCKs, including our TCK licensees.  Because of this, our processes and practices are working on the assumption that such review will not happen, and we're still in the transition period of changing over to a model that better supports such review. Note also that we don't have a dedicated team for the JPA TCK so our TCK engineers may have been working on the TCKs for other technologies before the TCK for JPA, thus increasing the lead time here. Our team is doing their best to provide something as soon as possible. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification: FETCH JOINs and ORDER BY clauses</header>
    <date>Thu Jan 24 20:53:45 CET 2013</date>
    <body>What do you mean by "incidentally fetched". It's explicitly fetched I'd argue. So just to make sure I get this right: 1. If you want to order on something you want to make sure it gets loaded  eagerly, you have to equip the SELECT clause appropriately. 2. Only if you're not ordering on something you want to load eagerly you can  use ….fetch(…). Correct? Cheers, Ollie Am 22.01.2013 um 20:34 schrieb Linda DeMichiel &amp;lt;linda.demichiel@...&amp;gt;:        The orderby rules were written to allow direct mapping to SQL.  If an   implementation  were to support ordering by something that were incidentally fetched but   not in the  SELECT list that would be one thing -- but I don't think we should require   support for this.       --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Early access to the TCK</header>
    <date>Fri Jan 25 15:30:11 CET 2013</date>
    <body>To chime in on this, Oliver never said Open Source.  Obviously those of us that work in that realm believe it to be  superior way to develop software, and I personally at least would love to see the JPA TCK get to that point as many other TCKs are doing.  However, the point Oliver made was about making it *open*.  If I read Oliver correctly, I took his use of "open" to mean transparent as in open to within this group which you then elaborated on beautifully. Just to add to Oliver's list of reasons.. Over the years I have submitted *many* accepted challenges to the JPA TCK.  But the problem there is that providers who have already certified do not have to re-certify with the updated TCK, even though they passed flawed tests (perhaps even passed *because of* flawed tests).  Overall it brings into question the integrity of being "certified" at least to some extent, in my opinion.  It also directly leads to a fragmented JPA community with major concerns over provider migration.  Granted there will always be provider migration concerns, but I do not think that should happen over spec mandated (and certified!) features/behavior. On 01/24/2013 03:23 PM, Werner Keil Oli/all, In an ideal world, that's how TDD works &lt;img src="msg00046/gifWc5PRAti5g.gif" style="margin: 0px 0.2ex; vertical-align: middle;" goomoji="329"/&gt;  And based on the session Patrick and others from the PMO held a while ago on TCKs, a TCK must not be tailored towards one implementation or the RI only, so if a Spec Lead company is large enough like in this case, ideally the part that writes the TCK should not necessarily be the ones who write the Spec or RI, though in some situations of course they may interact. Regarding the license, I believe the Spec page  http://jcp.org/en/jsr/detail?id=338 section 2.18 makes it very clear, that neither RI nor TCK for 338 are licensed under Apache or any other Open Source license. Nor are JSON or WebSockets, just to mention 2 other parts of the Java EE 7 Umbrella. In those cases the Spec isn't even in a public repository, while RI is, though certain licenses may also apply for commercial purposes beyond R&amp;amp;D, other Open Source Projects that go as Non-Profits or those "Qualified Individuals".  JCP.next is not about "Make everything Open Source", it is about a transparent way of designing and developing the JSRs with as little "deals behind closed doors" as possible, or none &lt;img src="msg00046/gifqSE2s99eym.gif" style="margin: 0px 0.2ex; vertical-align: middle;" goomoji="347"/&gt; HTH, Werner On Thu, Jan 24, 2013 at 10:09 PM, Oliver Gierke ogierke@... &lt;blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"&gt; To be honest, that reads like: we write the tests right before we ship the software. How can the expert group make sure the test we will see eventually are in line with what the spec defines? This essentially leaves us as author of a spec that we will potentially see implementations of that do/can not follow the spec because the TCK we have no influence on didn't catch serious glitches. Where's the value in voting about a spec where the actual rules that govern implementations are not under the control of those who vote? Beside Steve's issue with implementing the new features of the spec what is the plan for the general work on the TCK going forward? To be honest I think it's close to ridiculous to ask members of the EG to hand in ideas and suggestions for improvements to the TCK which will then be implemented however by whoever somewhere. Especially in the light of the ambiguities discovered recently I think one of the top priorities going forward has to be that we (the EG) can make sure we'll see the the new features and behavior tested adequately. This not only will help us creating a good/better spec as ambiguities will be found before it is to late but also a much better usability as the lack of defined *and enforced* semantics prevents functionality from being rendered useless as pretty much every persistence provider behaves slightly different as they essentially have no real means to test their implementations. So I suggest that going forward... 1. the TCK has to developed alongside the spec by the EG. Newly defined API and functionality has to be backed by test cases on submission of the feature to the spec. 2. This requires EG members having access to the TCK during spec development (requires appropriate licensing of the spec) 3. If I conclude wishful thinking here I wonder why even the community shouldn't have access to the TCK to maybe help out discovering potential issues. @Werner: does any of the current JCP versions already define TCK rules like this? The closest JSR I can remember is CDI that always developed an Apache licensed TCK fully in the open IIRC. Or is there any general rules within the JCP/Oracle that should prevent us from going down that route? Cheers, Ollie -- Sent while on the run... linda.demichiel@... weeks away on the TCK being ready for this. early access to the TCK as being a generally good thing and there seemed later revisions of the spec. The Cliff Notes (tm) version of the pros problems in the TCK itself missing portability coverage start getting access to the 2.1 TCK, especially now that the spec is pretty</body>
  </mail>
  <mail>
    <header>[jpa-spec users] persistence XSD version in persistence.xml</header>
    <date>Sun Jan 27 17:52:08 CET 2013</date>
    <body>Hi All, In persistence.xml , version attribute and XSD version should still be specified as 2.0, else I am getting error when I deployed in teh glassfish server version glassfish-4.0-b72. In public draft document, I find below "The container must validate the persistence.xml file against the persistence_2_1.xsd, persistence_2_0.xsd, or persistence_1_0.xsd schema in accordance with the version specified by the persistence.xml file and report any validation errors." Hence , as per it , shouldnt version 2.1 version value be allowed in the persistence.xml, or am I missing something? The exception Caused by: org.xml.sax.SAXParseException; lineNumber: 4; columnNumber: 121; cvc-complex-type.3.1: Value '2.1' of attribute 'version' of element 'persistence' is not valid with respect to the corresponding attribute use. Attribute 'version' has a fixed value of '2.0'.     at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)     at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:134) |SEVERE|glassfish 4.0|javax.enterprise.system.core|_ThreadID=45;_ThreadName=AutoDeployer;_TimeMillis=1359302785077;_LevelValue=1000;_MessageID=NCLS-CORE-0026;|Exception during lifecycle processing java.io.IOException: org.xml.sax.SAXParseExceptionpublicId: http://www.oracle.com/technetwork/java/index.html    var g_HttpRelativeWebRoot = "/ocom/";'. Thanks, Nivedita</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: persistence XSD version in persistence.xml</header>
    <date>Sun Jan 27 21:04:38 CET 2013</date>
    <body>Hi Nivedita, The persistence_2_1.xsd is not available yet.  Hopefully it will be shortly. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification: FETCH JOINs and ORDER BY clauses</header>
    <date>Mon Jan 28 22:45:52 CET 2013</date>
    <body>Yes. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] schema generation open issues</header>
    <date>Tue Jan 29 00:14:46 CET 2013</date>
    <body>There are currently a few open issues in schema generation that need clarification.  These pertain to sections 9.4 ("Schema generation") and 8.2.1.9 ("properties") in the spec. I've made some clarifications and proposed some additions to the metadata for schema generation as described below.  The draft I've just uploaded reflects these changes  ( http://java.net/projects/jpa-spec/downloads/download/JavaPersistence01282013.pdf ). Please review these sections and post any feedback.   Changes are summarized  below. ---- Section 9.4 This section isn't sufficiently clear about requirements for the specification of schema-generation-action and schema-generation-target. I've clarified that schema-generation-action must be specified or no actions will be taken.  This was an assumed default in the current public draft, but IMO not sufficiently explicit. Given the clarification to schema-generation-action, I think it now makes sense for schema-generation-target to have default values. That is, if script targets are specified by the properties for scripts, then generation actions should default into scripts.  If not, actions should be directly into the database. The current public draft assumes that if script sources are specified, generation should occur from scripts exclusively; and conversely, if script sources are not specified, generation should occur exclusively from the ORM metadata.   I've gotten feedback that this wasn't sufficiently flexible, and that we should support a combination of ORM and script sources.  (For example, consider the case where generation should occur from scripts, but it is desirable to add stored procedures to the database.) I therefore propose that we add a schema-generation-source property that will support such combination.  Again, if script sources are specified by the properties for scripts, then generation will default to use of only script sources; if script sources aren't specified, then generation will default from the ORM metadata. I've also received feedback (from our JPA/EE integration engineer) that we need to specify that in Java EE environments that strings corresponding to file URLs must use absolute (and not relative) paths.  I've added that clarification as well. Section 8.2.1.9 In the current public draft, this section lacks information as to how script locations are specified.  I've added a clarification that scripts packaged as part of the application must be specified relative to the root of the persistence unit, as we do for other packaged artifacts.  I've also added a clarification about the expectations for use of file URLs in Java EE environments as above. Finally, when I wrote this up initially, I was reluctant to add metadata for schema generation actions (including targets) because I believed that such metadata should not be specific to the persistence unit definition itself.  It seems however that people expect that the use of these properties (as specified in section 9.4) will be available anyway, whether they are specified in the spec document or not, and that some vendors will support these anyway as well. Further, this additional metadata may facilitate ease of use in prototyping/development environments.  I've therefore added these properties into 8.2.1.9.  If you disagree, please speak up. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 10:56:16 CET 2013</date>
    <body>Dear JPA experts, Could you please clarify, whether it is allowed for stateful session bean having container-managed extended persistence context to return  its EntityManager to clients (and for clients to perform operations on this EntityManager), or is it forbidden? Currently, GlassFish allows returning the extended EntityManager to outside (e.g.  http://java.net/jira/browse/GLASSFISH-11805 ), Apache OpenEJB does not, and there is currently ongoing discussion about issue https://issues.apache.org/jira/browse/TOMEE-509 .  JPA 2.0 specification had following text (stars added by me): ----------------------------------------------- 3.3 Persistence Context Lifetime ... When an extended persistence context is used, the extended persistence context exists from the time the EntityManager instance is created until it is closed. This persistence context might span multiple transactions and non-transactional invocations of the EntityManager. A container-managed extended persistence context is enlisted in the current transaction when *the EntityManager is invoked in the scope of that transaction* or when the stateful session bean to which the extended persistence context is bound is invoked in the scope of that transaction. ----------------------------------------------- The part between “*” seems to suggest, that EntityManager could be accessed by some external entity directly, not necessarily through  stateful session bean method.  I cannot find this text in JPA 2.1 public draft though.   Question: can reference to extended EntityManager be provided to outside, and methods called on this reference? Regards, Donatas</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 11:02:48 CET 2013</date>
    <body>Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb "donatas.ciuksys@..." unter Dear JPA experts, Could you please clarify, whether it is allowed for stateful session bean having container-managed extended persistence context to return its EntityManager to clients (and for clients to perform operations on this EntityManager), or is it forbidden? Currently, GlassFish allows returning the extended EntityManager to outside (e.g.  http://java.net/jira/browse/GLASSFISH-11805 ), Apache OpenEJB does not, and there is currently ongoing discussion about issue https://issues.apache.org/jira/browse/TOMEE-509  ;. JPA 2.0 specification had following text (stars added by me): ----------------------------------------------- 3.3 Persistence Context Lifetime ... When an extended persistence context is used, the extended persistence context exists from the time the EntityManager instance is created until it is closed. This persistence context might span multiple transactions and non-transactional invocations of the EntityManager. A container-managed extended persistence context is enlisted in the current transaction when *the EntityManager is invoked in the scope of that transaction* or when the stateful session bean to which the extended persistence context is bound is invoked in the scope of that transaction. ----------------------------------------------- The part between ³*² seems to suggest, that EntityManager could be accessed by some external entity directly, not necessarily through stateful session bean method. I cannot find this text in JPA 2.1 public draft though.   Question: can reference to extended EntityManager be provided to outside, and methods called on this reference? Regards, Donatas</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 12:49:58 CET 2013</date>
    <body>Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers On Tue, Jan 29, 2013 at 11:02 AM, Arne Limburg arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 12:54:40 CET 2013</date>
    <body>Hi Christian, I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans. Regards, Arne Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients? Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers On Tue, Jan 29, 2013 at 11:02 AM, Arne Limburg arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 13:08:40 CET 2013</date>
    <body>Hi Arne, For a container-managed persistence context, it is expected that the container injects a proxy EntityManager instance. This proxy might contain application-server specific logic (e.g. keeping track of the "close" events, if various proxies target the same real EM instance). Because this is not forbidden by the EJB or JPA spec and up to the app-server vendor, I would presume, that this is of course not supported, because manipulating the proxy outside the app-server could break the logic assumed by the app-server. Christian On Tue, Jan 29, 2013 at 12:54 PM, Arne Limburg arne.limburg@... Hi Christian, I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans. Regards, Arne Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients? Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers On Tue, Jan 29, 2013 at 11:02 AM, Arne Limburg arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 13:57:56 CET 2013</date>
    <body>Hi Christian,   Original question is about single VM, single EJB container, multiple EJB beans implementing singe use-case.   Imagine, we’d like to implement somewhat complex use-case (maybe even conversation alike), and single EJB component is too coarse-grained (low cohesion). We would naturally like to split it to several components. One of them is SFSB with extended persistence context, others just want to access this entity manager. Of course, there is a way to implement this using persistence context propagation. But the limitation is that this SFSB must always be the first in the call chain, so it must contain all the methods of use-case, though most of them will just delegate to other components. This does not sound lean. CDI allows us to inject “things”, and provides opportunity to simplify this call chain. If it would be legal for SFSB to have a CDI producer returning extended EntityManager, use-case implementation could be simplified. JSF page could directly invoke stateless business components. Glassfish supports such a producers, the questions is, is it legal per EJB/JPA specification, or is it explicitly forbidden.   Just for comparison: it seems that transactional EntityManager can be provided to other EJB/CDI components (CDI spec even has examples), and all EJB implementations support it. Why extended EntityManager is different?   Donatas   From: Christian von Kutzleben [mailto:cvkutzleben@...] Sent: Tuesday, January 29, 2013 2:09 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne, For a container-managed persistence context, it is expected that the container injects a proxy EntityManager instance. This proxy might contain application-server specific logic (e.g. keeping track of the "close" events, if various proxies target the same real EM instance). Because this is not forbidden by the EJB or JPA spec and up to the app-server vendor, I would presume, that this is of course not supported, because manipulating the proxy outside the app-server could break the logic assumed by the app-server. Christian arne.limburg@... Hi Christian,   I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans.   Regards, Arne   Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 14:21:31 CET 2013</date>
    <body>Hi Donatas, Although I can not actually answer your question, because I'm just looking through the JPA and not the EJB or CDI goggles, I have the feeling - maybe I'm wrong - that there is a slight misconception about the nature of "extended" vs. "non-extended" persistence contexts: As a JPA/EJB user you might easily get the impression, that those are two different things, but actually, they are not: It is how the container handles a persistence context, which makes the difference, it's not inherent in the actually created EntityManager instance of the JPA provider. This means, we, as the JPA vendor, don't know at all whether a persistence context we create and which is used by the container and the application is used as extended or non-extended persistence context. This in turn means, there is no custom way to create an extended persistence context, except through the container itself. (Not sure, whether this would be a requirement for the CDI producer you mentioned or not.) Christian On Tue, Jan 29, 2013 at 1:57 PM, Donatas Čiukšys donatas.ciuksys@... Hi Christian,   Original question is about single VM, single EJB container, multiple EJB beans implementing singe use-case.   Imagine, we’d like to implement somewhat complex use-case (maybe even conversation alike), and single EJB component is too coarse-grained (low cohesion). We would naturally like to split it to several components. One of them is SFSB with extended persistence context, others just want to access this entity manager. Of course, there is a way to implement this using persistence context propagation. But the limitation is that this SFSB must always be the first in the call chain, so it must contain all the methods of use-case, though most of them will just delegate to other components. This does not sound lean. CDI allows us to inject “things”, and provides opportunity to simplify this call chain. If it would be legal for SFSB to have a CDI producer returning extended EntityManager, use-case implementation could be simplified. JSF page could directly invoke stateless business components. Glassfish supports such a producers, the questions is, is it legal per EJB/JPA specification, or is it explicitly forbidden.   Just for comparison: it seems that transactional EntityManager can be provided to other EJB/CDI components (CDI spec even has examples), and all EJB implementations support it. Why extended EntityManager is different?   Donatas   From: Christian von Kutzleben [mailto: cvkutzleben@... ] Sent: Tuesday, January 29, 2013 2:09 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne, For a container-managed persistence context, it is expected that the container injects a proxy EntityManager instance. This proxy might contain application-server specific logic (e.g. keeping track of the "close" events, if various proxies target the same real EM instance). Because this is not forbidden by the EJB or JPA spec and up to the app-server vendor, I would presume, that this is of course not supported, because manipulating the proxy outside the app-server could break the logic assumed by the app-server. Christian arne.limburg@... Hi Christian,   I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans.   Regards, Arne   Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 16:12:56 CET 2013</date>
    <body>Hi Christian, What you write is not right: You write that "there is no custom way to create an extended persistence context, except through the container itself." But Chapter 7.7 states about application-managed persistence context that they are "extended in scope". So in fact EVERY EntityManager we create manually is extended. But that is not the point of Donatas and me: Donatas point is: You can always take an EntityManager that was injected by the Container and pass it around (to another EJB or even outside an EJB). The question is: Is it supposed to work outside that EJB it was injected into? And my additional question is: Is lazy loading of entities from an extended EntityManager supposed to work outside that EJB. Regards, Arne Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 14:21 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients? Hi Donatas, Although I can not actually answer your question, because I'm just looking through the JPA and not the EJB or CDI goggles, I have the feeling - maybe I'm wrong - that there is a slight misconception about the nature of "extended" vs. "non-extended" persistence contexts: As a JPA/EJB user you might easily get the impression, that those are two different things, but actually, they are not: It is how the container handles a persistence context, which makes the difference, it's not inherent in the actually created EntityManager instance of the JPA provider. This means, we, as the JPA vendor, don't know at all whether a persistence context we create and which is used by the container and the application is used as extended or non-extended persistence context. This in turn means, there is no custom way to create an extended persistence context, except through the container itself. (Not sure, whether this would be a requirement for the CDI producer you mentioned or not.) Christian On Tue, Jan 29, 2013 at 1:57 PM, Donatas Čiukšys donatas.ciuksys@... Hi Christian,   Original question is about single VM, single EJB container, multiple EJB beans implementing singe use-case.   Imagine, we’d like to implement somewhat complex use-case (maybe even conversation alike), and single EJB component is too coarse-grained (low cohesion). We would naturally like to split it to several components. One of them is SFSB with extended persistence context, others just want to access this entity manager. Of course, there is a way to implement this using persistence context propagation. But the limitation is that this SFSB must always be the first in the call chain, so it must contain all the methods of use-case, though most of them will just delegate to other components. This does not sound lean. CDI allows us to inject “things”, and provides opportunity to simplify this call chain. If it would be legal for SFSB to have a CDI producer returning extended EntityManager, use-case implementation could be simplified. JSF page could directly invoke stateless business components. Glassfish supports such a producers, the questions is, is it legal per EJB/JPA specification, or is it explicitly forbidden.   Just for comparison: it seems that transactional EntityManager can be provided to other EJB/CDI components (CDI spec even has examples), and all EJB implementations support it. Why extended EntityManager is different?   Donatas   From: Christian von Kutzleben [mailto: cvkutzleben@... ] Sent: Tuesday, January 29, 2013 2:09 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne, For a container-managed persistence context, it is expected that the container injects a proxy EntityManager instance. This proxy might contain application-server specific logic (e.g. keeping track of the "close" events, if various proxies target the same real EM instance). Because this is not forbidden by the EJB or JPA spec and up to the app-server vendor, I would presume, that this is of course not supported, because manipulating the proxy outside the app-server could break the logic assumed by the app-server. Christian arne.limburg@... Hi Christian,   I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans.   Regards, Arne   Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 16:16:37 CET 2013</date>
    <body>Ø   As a JPA/EJB user you might easily get the impression, that those are two different things, but actually, they are not   Well I wish they would be the same :) I actually encountered a case when EJB container treats them differently. My EJB component (with @PersistenceContext annotation) has method: getEntityManager(). Other EJB components use this method successfully unless the PersistenceContext is of extended type (Apache OpenEJB case).   The question could be put this way: must SFSB component (governing extended EM) be present in call stack if that EntityManager is accessed? Could the answer make it to JPA specificaition?   Donatas   From: Christian von Kutzleben [mailto:cvkutzleben@...] Sent: Tuesday, January 29, 2013 3:22 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Donatas, Although I can not actually answer your question, because I'm just looking through the JPA and not the EJB or CDI goggles, I have the feeling - maybe I'm wrong - that there is a slight misconception about the nature of "extended" vs. "non-extended" persistence contexts: As a JPA/EJB user you might easily get the impression, that those are two different things, but actually, they are not: It is how the container handles a persistence context, which makes the difference, it's not inherent in the actually created EntityManager instance of the JPA provider. This means, we, as the JPA vendor, don't know at all whether a persistence context we create and which is used by the container and the application is used as extended or non-extended persistence context. This in turn means, there is no custom way to create an extended persistence context, except through the container itself. (Not sure, whether this would be a requirement for the CDI producer you mentioned or not.) Christian donatas.ciuksys@... Hi Christian,   Original question is about single VM, single EJB container, multiple EJB beans implementing singe use-case.   Imagine, we’d like to implement somewhat complex use-case (maybe even conversation alike), and single EJB component is too coarse-grained (low cohesion). We would naturally like to split it to several components. One of them is SFSB with extended persistence context, others just want to access this entity manager. Of course, there is a way to implement this using persistence context propagation. But the limitation is that this SFSB must always be the first in the call chain, so it must contain all the methods of use-case, though most of them will just delegate to other components. This does not sound lean. CDI allows us to inject “things”, and provides opportunity to simplify this call chain. If it would be legal for SFSB to have a CDI producer returning extended EntityManager, use-case implementation could be simplified. JSF page could directly invoke stateless business components. Glassfish supports such a producers, the questions is, is it legal per EJB/JPA specification, or is it explicitly forbidden.   Just for comparison: it seems that transactional EntityManager can be provided to other EJB/CDI components (CDI spec even has examples), and all EJB implementations support it. Why extended EntityManager is different?   Donatas   From: Christian von Kutzleben [mailto: cvkutzleben@... ] Sent: Tuesday, January 29, 2013 2:09 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne, For a container-managed persistence context, it is expected that the container injects a proxy EntityManager instance. This proxy might contain application-server specific logic (e.g. keeping track of the "close" events, if various proxies target the same real EM instance). Because this is not forbidden by the EJB or JPA spec and up to the app-server vendor, I would presume, that this is of course not supported, because manipulating the proxy outside the app-server could break the logic assumed by the app-server. Christian arne.limburg@... Hi Christian,   I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans.   Regards, Arne   Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 16:21:31 CET 2013</date>
    <body>Hi Arne, He was asking about "container-managed" persistence contexts, not "application-managed" persistence contexts. it was injected into? For a portable application I would not assume that, but I can't give a definite answer on that. Assuming your same-VM scenario: of course this should work, why shouldn't work? Christian On Tue, Jan 29, 2013 at 4:12 PM, Arne Limburg arne.limburg@... Hi Christian, What you write is not right: You write that "there is no custom way to create an extended persistence context, except through the container itself." But Chapter 7.7 states about application-managed persistence context that they are "extended in scope". So in fact EVERY EntityManager we create manually is extended. But that is not the point of Donatas and me: Donatas point is: You can always take an EntityManager that was injected by the Container and pass it around (to another EJB or even outside an EJB). The question is: Is it supposed to work outside that EJB it was injected into? And my additional question is: Is lazy loading of entities from an extended EntityManager supposed to work outside that EJB. Regards, Arne Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 14:21 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients? Hi Donatas, Although I can not actually answer your question, because I'm just looking through the JPA and not the EJB or CDI goggles, I have the feeling - maybe I'm wrong - that there is a slight misconception about the nature of "extended" vs. "non-extended" persistence contexts: As a JPA/EJB user you might easily get the impression, that those are two different things, but actually, they are not: It is how the container handles a persistence context, which makes the difference, it's not inherent in the actually created EntityManager instance of the JPA provider. This means, we, as the JPA vendor, don't know at all whether a persistence context we create and which is used by the container and the application is used as extended or non-extended persistence context. This in turn means, there is no custom way to create an extended persistence context, except through the container itself. (Not sure, whether this would be a requirement for the CDI producer you mentioned or not.) Christian On Tue, Jan 29, 2013 at 1:57 PM, Donatas Čiukšys donatas.ciuksys@... Hi Christian,   Original question is about single VM, single EJB container, multiple EJB beans implementing singe use-case.   Imagine, we’d like to implement somewhat complex use-case (maybe even conversation alike), and single EJB component is too coarse-grained (low cohesion). We would naturally like to split it to several components. One of them is SFSB with extended persistence context, others just want to access this entity manager. Of course, there is a way to implement this using persistence context propagation. But the limitation is that this SFSB must always be the first in the call chain, so it must contain all the methods of use-case, though most of them will just delegate to other components. This does not sound lean. CDI allows us to inject “things”, and provides opportunity to simplify this call chain. If it would be legal for SFSB to have a CDI producer returning extended EntityManager, use-case implementation could be simplified. JSF page could directly invoke stateless business components. Glassfish supports such a producers, the questions is, is it legal per EJB/JPA specification, or is it explicitly forbidden.   Just for comparison: it seems that transactional EntityManager can be provided to other EJB/CDI components (CDI spec even has examples), and all EJB implementations support it. Why extended EntityManager is different?   Donatas   From: Christian von Kutzleben [mailto: cvkutzleben@... ] Sent: Tuesday, January 29, 2013 2:09 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne, For a container-managed persistence context, it is expected that the container injects a proxy EntityManager instance. This proxy might contain application-server specific logic (e.g. keeping track of the "close" events, if various proxies target the same real EM instance). Because this is not forbidden by the EJB or JPA spec and up to the app-server vendor, I would presume, that this is of course not supported, because manipulating the proxy outside the app-server could break the logic assumed by the app-server. Christian arne.limburg@... Hi Christian,   I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans.   Regards, Arne   Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 16:26:36 CET 2013</date>
    <body>Hi Christian, OK, I agree with you that one cannot create a container-managed persistence context, except through the container itself ;-) About the lazy-loading: I expected that to work, too, but I had a scenario where it didn't work in JBoss AS 7 with Hibernate… Btw. Both points (Donatas and mine) should be clarified by the spec (at least there should be a sentence that it is not portable). Regards, Arne Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 16:21 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients? Hi Arne, He was asking about "container-managed" persistence contexts, not "application-managed" persistence contexts. For a portable application I would not assume that, but I can't give a definite answer on that. Assuming your same-VM scenario: of course this should work, why shouldn't work? Christian On Tue, Jan 29, 2013 at 4:12 PM, Arne Limburg arne.limburg@... Hi Christian, What you write is not right: You write that "there is no custom way to create an extended persistence context, except through the container itself." But Chapter 7.7 states about application-managed persistence context that they are "extended in scope". So in fact EVERY EntityManager we create manually is extended. But that is not the point of Donatas and me: Donatas point is: You can always take an EntityManager that was injected by the Container and pass it around (to another EJB or even outside an EJB). The question is: Is it supposed to work outside that EJB it was injected into? And my additional question is: Is lazy loading of entities from an extended EntityManager supposed to work outside that EJB. Regards, Arne Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 14:21 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients? Hi Donatas, Although I can not actually answer your question, because I'm just looking through the JPA and not the EJB or CDI goggles, I have the feeling - maybe I'm wrong - that there is a slight misconception about the nature of "extended" vs. "non-extended" persistence contexts: As a JPA/EJB user you might easily get the impression, that those are two different things, but actually, they are not: It is how the container handles a persistence context, which makes the difference, it's not inherent in the actually created EntityManager instance of the JPA provider. This means, we, as the JPA vendor, don't know at all whether a persistence context we create and which is used by the container and the application is used as extended or non-extended persistence context. This in turn means, there is no custom way to create an extended persistence context, except through the container itself. (Not sure, whether this would be a requirement for the CDI producer you mentioned or not.) Christian On Tue, Jan 29, 2013 at 1:57 PM, Donatas Čiukšys donatas.ciuksys@... Hi Christian,   Original question is about single VM, single EJB container, multiple EJB beans implementing singe use-case.   Imagine, we’d like to implement somewhat complex use-case (maybe even conversation alike), and single EJB component is too coarse-grained (low cohesion). We would naturally like to split it to several components. One of them is SFSB with extended persistence context, others just want to access this entity manager. Of course, there is a way to implement this using persistence context propagation. But the limitation is that this SFSB must always be the first in the call chain, so it must contain all the methods of use-case, though most of them will just delegate to other components. This does not sound lean. CDI allows us to inject “things”, and provides opportunity to simplify this call chain. If it would be legal for SFSB to have a CDI producer returning extended EntityManager, use-case implementation could be simplified. JSF page could directly invoke stateless business components. Glassfish supports such a producers, the questions is, is it legal per EJB/JPA specification, or is it explicitly forbidden.   Just for comparison: it seems that transactional EntityManager can be provided to other EJB/CDI components (CDI spec even has examples), and all EJB implementations support it. Why extended EntityManager is different?   Donatas   From: Christian von Kutzleben [mailto: cvkutzleben@... ] Sent: Tuesday, January 29, 2013 2:09 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne, For a container-managed persistence context, it is expected that the container injects a proxy EntityManager instance. This proxy might contain application-server specific logic (e.g. keeping track of the "close" events, if various proxies target the same real EM instance). Because this is not forbidden by the EJB or JPA spec and up to the app-server vendor, I would presume, that this is of course not supported, because manipulating the proxy outside the app-server could break the logic assumed by the app-server. Christian arne.limburg@... Hi Christian,   I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans.   Regards, Arne   Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 16:28:43 CET 2013</date>
    <body>Hi Donatas Well I wish they would be the same :) I actually encountered a case when EJB container treats them differently. My EJB component (with @PersistenceContext annotation) has method: getEntityManager(). Other EJB components use this method successfully unless the PersistenceContext is of extended type (Apache OpenEJB case).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 16:41:45 CET 2013</date>
    <body>Hi Arne, It suprises me, that it did not work: the JPA entities naturally should only know their persistence context and not any proxies. I'm not sure, whether JBoss disables anything for the (Hibernate) persistence context after the bean invocation. If you use JBoss AS 7 with our JPA implementation (or probably any other 3rd party JPA implementation), then JBoss can only use JPA API/SPI to communicate with us. And there is no way to disable anything regarding lazy loading directly. Did you get a specific exception? Christian On Tue, Jan 29, 2013 at 4:26 PM, Arne Limburg arne.limburg@... Hi Christian, OK, I agree with you that one cannot create a container-managed persistence context, except through the container itself ;-) About the lazy-loading: I expected that to work, too, but I had a scenario where it didn't work in JBoss AS 7 with Hibernate… Btw. Both points (Donatas and mine) should be clarified by the spec (at least there should be a sentence that it is not portable). Regards, Arne Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 16:21 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients? Hi Arne, He was asking about "container-managed" persistence contexts, not "application-managed" persistence contexts. For a portable application I would not assume that, but I can't give a definite answer on that. Assuming your same-VM scenario: of course this should work, why shouldn't work? Christian On Tue, Jan 29, 2013 at 4:12 PM, Arne Limburg arne.limburg@... Hi Christian, What you write is not right: You write that "there is no custom way to create an extended persistence context, except through the container itself." But Chapter 7.7 states about application-managed persistence context that they are "extended in scope". So in fact EVERY EntityManager we create manually is extended. But that is not the point of Donatas and me: Donatas point is: You can always take an EntityManager that was injected by the Container and pass it around (to another EJB or even outside an EJB). The question is: Is it supposed to work outside that EJB it was injected into? And my additional question is: Is lazy loading of entities from an extended EntityManager supposed to work outside that EJB. Regards, Arne Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 14:21 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients? Hi Donatas, Although I can not actually answer your question, because I'm just looking through the JPA and not the EJB or CDI goggles, I have the feeling - maybe I'm wrong - that there is a slight misconception about the nature of "extended" vs. "non-extended" persistence contexts: As a JPA/EJB user you might easily get the impression, that those are two different things, but actually, they are not: It is how the container handles a persistence context, which makes the difference, it's not inherent in the actually created EntityManager instance of the JPA provider. This means, we, as the JPA vendor, don't know at all whether a persistence context we create and which is used by the container and the application is used as extended or non-extended persistence context. This in turn means, there is no custom way to create an extended persistence context, except through the container itself. (Not sure, whether this would be a requirement for the CDI producer you mentioned or not.) Christian On Tue, Jan 29, 2013 at 1:57 PM, Donatas Čiukšys donatas.ciuksys@... Hi Christian,   Original question is about single VM, single EJB container, multiple EJB beans implementing singe use-case.   Imagine, we’d like to implement somewhat complex use-case (maybe even conversation alike), and single EJB component is too coarse-grained (low cohesion). We would naturally like to split it to several components. One of them is SFSB with extended persistence context, others just want to access this entity manager. Of course, there is a way to implement this using persistence context propagation. But the limitation is that this SFSB must always be the first in the call chain, so it must contain all the methods of use-case, though most of them will just delegate to other components. This does not sound lean. CDI allows us to inject “things”, and provides opportunity to simplify this call chain. If it would be legal for SFSB to have a CDI producer returning extended EntityManager, use-case implementation could be simplified. JSF page could directly invoke stateless business components. Glassfish supports such a producers, the questions is, is it legal per EJB/JPA specification, or is it explicitly forbidden.   Just for comparison: it seems that transactional EntityManager can be provided to other EJB/CDI components (CDI spec even has examples), and all EJB implementations support it. Why extended EntityManager is different?   Donatas   From: Christian von Kutzleben [mailto: cvkutzleben@... ] Sent: Tuesday, January 29, 2013 2:09 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne, For a container-managed persistence context, it is expected that the container injects a proxy EntityManager instance. This proxy might contain application-server specific logic (e.g. keeping track of the "close" events, if various proxies target the same real EM instance). Because this is not forbidden by the EJB or JPA spec and up to the app-server vendor, I would presume, that this is of course not supported, because manipulating the proxy outside the app-server could break the logic assumed by the app-server. Christian arne.limburg@... Hi Christian,   I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans.   Regards, Arne   Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 17:47:19 CET 2013</date>
    <body>Hi Christian, Arne,   Small code example to illustrate the issue:   ------------------------------------------------------------ @Stateful       @Stateless         @PostConstruct             entityManagerHolder.getEntityManager().persist(...); // &amp;lt;-- WORKING OK         em.persist(...); // &amp;lt;-- NOT WORKING     ------------------------------------------------------------   As noted in method testEMaccess() comments, one line is working, other is not (in OpenEJB; in GlassFish both lines are working).   Note that both lines are very similar, and most programmers would be tended to get rid of repeated “entityManagerHolder.getEntityManager()” calls by storing em in class field (as shown in the second sentence).   Donatas   From: Christian von Kutzleben [mailto:cvkutzleben@...] Sent: Tuesday, January 29, 2013 5:29 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Donatas Well I wish they would be the same :) I actually encountered a case when EJB container treats them differently. My EJB component (with @PersistenceContext annotation) has method: getEntityManager(). Other EJB components use this method successfully unless the PersistenceContext is of extended type (Apache OpenEJB case).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 18:48:11 CET 2013</date>
    <body>Is class A stateful in your app? Which version of AS7 exactly are you using? Have you tried to recreate against the latest JBoss AS nightly build http://community.jboss.org/thread/167590 ? Also, general questions about JBoss AS can also be raised on https://community.jboss.org/en/jbossas7 . Regards, Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Tue Jan 29 23:14:13 CET 2013</date>
    <body>What is the proposed behavior of EntityGraph or Subgraph when addAttributeNodes is called with non-basic attribute types (a ManyToOne for example)?  Is that considered an exception condition?  And if so, what exception (IllegalArgumentException)?  Or do providers simply interpret it as a call to addSubgraph? Also, a minor edit to report in the spec, at least as of the Public Review Draft.  In the definition of the NamedEntityGraph annotation, the type of subclassSubgraphs is defined as NamedSubGraph[] rather than NamedSubgraph[]</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Tue Jan 29 23:42:54 CET 2013</date>
    <body>Also related... Do EntityGraph.getAttributeNodes / Subgraph.getAttributeNodes return Subgraphs in the AttributeNode list?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EntityGraph's AttributeNode and Type</header>
    <date>Wed Jan 30 00:13:04 CET 2013</date>
    <body>Ok, I think I was missing this part in my earlier questions. So AttributeNode.getType() is going away; got it.  Will there be separate getSubgraphs() and getKeySubgraphs() methods (in addition to getAttributeNodes()) as mentioned by Gordon? Still does not clear up my overall confusion of addAttributeNodes when passed non-basic types in terms of spec-defined behavior.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EntityGraph's AttributeNode and Type</header>
    <date>Wed Jan 30 02:27:25 CET 2013</date>
    <body>Not in this release, but perhaps later if needed.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 02:27:55 CET 2013</date>
    <body>It just adds the attibute node for the attribute, but doesn't give the  opportunity to expand on it as a subgraph. BTW, the documentation for EntityGraph is messed up.  The last paragraph of  the getName method docs should really have been applied to EntityGraph instead (i.e., at top level). thanks!</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 02:30:58 CET 2013</date>
    <body>Yes</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 03:34:04 CET 2013</date>
    <body>Is adding the same attribute (by name) using both addAttributeNode and addSubgraph allowed? ...</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Wed Jan 30 07:00:56 CET 2013</date>
    <body>Hi Scott, Class A is annotated with @Stateless in the example, though I do not see any  difference it being @Stateful. This is not the case of persistence context  propagation/inheritance where it would matter. Real life implementation would  of course use CDI scopes like @ConversationScoped (on EntityManagerHolder)  and @RequestScoped (on class A). Since this is not CDI specific (but JPA/EJB  integration), I skipped CDI annotations. Also this is not the question whether Hibernate/JBoss support this scenario  (I stumbled upon OpenEJB not supporting this), but question whether this is  not portable per JPA specification, and where in specification it is told so. Regards, Donatas -----Original Message----- From: Scott Marlow [ mailto:smarlow@...]  Sent: Tuesday, January 29, 2013 7:48 PM To: users@... Cc: Donatas Čiukšys Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended  EntityManager to clients?  Hi Christian, Arne,  Small code example to illustrate the issue:  ------------------------------------------------------------  @Stateful       @PersistenceContext(type=PersistenceContextType.EXTENDED) private Is class A stateful in your app? Which version of AS7 exactly are you using? Have you tried to recreate against the latest JBoss AS nightly build  http://community.jboss.org/thread/167590 ? Also, general questions about JBoss AS can also be raised on  https://community.jboss.org/en/jbossas7 . Regards, Scott  @Stateless       @PostConstruct           entityManagerHolder.getEntityManager().persist(...); // &amp;lt;--  WORKING OK           em.persist(...); // &amp;lt;-- NOT WORKING  ------------------------------------------------------------  As noted in method testEMaccess() comments, one line is working, other  is not (in OpenEJB; in GlassFish both lines are working).  Note that both lines are very similar, and most programmers would be  tended to get rid of repeated “entityManagerHolder.getEntityManager()”  calls by storing em in class field (as shown in the second sentence).  Donatas  *From:*Christian von Kutzleben [ mailto:cvkutzleben@ ...]  *Sent:* Tuesday, January 29, 2013 5:29 PM  *To:* users@...  *Subject:* [jpa-spec users] Re: Is it allowed for SFSB to return  extended EntityManager to clients?  Hi Donatas      Well I wish they would be the same :) I actually encountered a case      when EJB container treats them differently. My EJB component (with      @PersistenceContext annotation) has method: getEntityManager().      Other EJB components use this method successfully unless the      PersistenceContext is of extended type (Apache OpenEJB case).  The injected proxy is not exactly the same as the real persistence  context; the injected proxy might be different (that's up to the  container), but  eventually the persistence context implementation is the same.      The question could be put this way: must SFSB component (governing      extended EM) be present in call stack if that EntityManager is      accessed? Could the answer make it to JPA specificaition?  I would assume so for a portable application.  Christian      Donatas      *From:*Christian von Kutzleben [ mailto:cvkutzleben@ ... mailto:cvkutzleben@ ...&amp;gt;]      *Sent:* Tuesday, January 29, 2013 3:22 PM mailto:users@      *Subject:* [jpa-spec users] Re: Is it allowed for SFSB to return      extended EntityManager to clients?      Hi Donatas,      Although I can not actually answer your question, because I'm just      looking through the JPA and not the EJB or CDI goggles,      I have the feeling - maybe I'm wrong - that there is a slight      misconception about the nature of "extended" vs. "non-extended"      persistence contexts:      As a JPA/EJB user you might easily get the impression, that those      are two different things, but actually, they are not:      It is how the container handles a persistence context, which makes      the difference, it's not inherent in the actually created      EntityManager instance of the JPA provider. This means, we, as the      JPA vendor, don't know at all whether a persistence      context we create and which is used by the container and the      application is used as extended or non-extended persistence context.      This in turn means, there is no custom way to create an extended      persistence context, except through the container itself.      (Not sure, whether this would be a requirement for the CDI producer      you mentioned or not.)      Christian      On Tue, Jan 29, 2013 at 1:57 PM, Donatas Čiukšys mailto:donatas.ciuksys@          Hi Christian,          Original question is about single VM, single EJB container,          multiple EJB beans implementing singe use-case.          Imagine, we’d like to implement somewhat complex use-case (maybe          even conversation alike), and single EJB component is too          coarse-grained (low cohesion). We would naturally like to split          it to several components. One of them is SFSB with extended          persistence context, others just want to access this entity          manager.          Of course, there is a way to implement this using persistence          context propagation. But the limitation is that this SFSB must          always be the first in the call chain, so it must contain all          the methods of use-case, though most of them will just delegate          to other components. This does not sound lean.          CDI allows us to inject “things”, and provides opportunity to          simplify this call chain. If it would be legal for SFSB to have          a CDI producer returning extended EntityManager, use-case          implementation could be simplified. JSF page could directly          invoke stateless business components. Glassfish supports such a          producers, the questions is, is it legal per EJB/JPA          specification, or is it explicitly forbidden.          Just for comparison: it seems that transactional EntityManager          can be provided to other EJB/CDI components (CDI spec even has          examples), and all EJB implementations support it. Why extended          EntityManager is different?          Donatas          *From:*Christian von Kutzleben [ mailto:cvkutzleben@ ... mailto:cvkutzleben@ ...&amp;gt;]          *Sent:* Tuesday, January 29, 2013 2:09 PM mailto:users@          *Subject:* [jpa-spec users] Re: Is it allowed for SFSB to return          extended EntityManager to clients?          Hi Arne,          For a container-managed persistence context, it is expected that          the container injects a proxy EntityManager instance.          This proxy might contain application-server specific logic (e.g.          keeping track of the "close" events, if various          proxies target the same real EM instance).          Because this is not forbidden by the EJB or JPA spec and up to          the app-server vendor,          I would presume, that this is of course not supported, because          manipulating the proxy outside          the app-server could break the logic assumed by the app-server.          Christian          On Tue, Jan 29, 2013 at 12:54 PM, Arne Limburg          &amp;lt;arne.limburg@... mailto:arne.limburg@              Hi Christian,              I am not talking about clients in another VM, of course this              is not supported since the EntityManager is not defined to              be Serializable.              We are just talking about local clients here, like JSF              managed beans or CDI beans.              Regards,              Arne              *Von: *Christian von Kutzleben &amp;lt;cvkutzleben@... mailto:cvkutzleben@              *Antworten an: *"users@... mailto:users@ ...&amp;gt;" &amp;lt;users@... mailto:users@              *Datum: *Dienstag, 29. Januar 2013 12:49              *An: *"users@... mailto:users@ ...&amp;gt;" &amp;lt;users@... mailto:users@              *Betreff: *[jpa-spec users] Re: Is it allowed for SFSB to              return extended EntityManager to clients?              Hi Arne and Donatas,              Just my 2 cents regardings this:              The general case is, that the client is in another VM.              This in turn would required remoting concepts wrt. to the              EntityManager, as e.g. an RMI proxy instance would be needed,              so that the actual requests really happen in the same context.              It seems not to be viable, to actually serialize the whole              context to any client,              because the context would exist twice then, and some              resources hold by the context might not be transferable at all.              (e.g. a JDBC connection). This leaves a remote proxy as only              workable approach, and giving the lack of              any discussion of this aspect in the spec, I doubt whether              this scenario was ever intended.              Personally I do also dislike this approach because it tends              to break the layering of an application.              Christian              P.S. in our JDO product we had a feature once - which was              dropped later - to have remote PersistenceManagers              On Tue, Jan 29, 2013 at 11:02 AM, Arne Limburg              &amp;lt;arne.limburg@... mailto:arne.limburg@                  Hi all,                  Good issue from Donatas. I stumbled over this, too, some                  time ago and I                  would like to extend the question:                  When an entity was loaded from an extended persistence                  context that is                  bound to a stateful session bean. Is lazy loading                  supposed to work outside                  of the call stack of that stateful session bean, i.e. in                  a JSF managed                  bean or a cdi bean that is directly accessed via EL?                  I think it was Jboss AS 7 with Hibernate where this did                  not work.                  Regards,                  Arne                  Am 29.01.13 10:56 schrieb "donatas.ciuksys@... mailto:donatas.ciuksys@ ...&amp;gt;" unter                  &amp;lt;donatas.ciuksys@... mailto:donatas.ciuksys@ ...&amp;gt;&amp;gt;:                  &amp;gt;Dear JPA experts,                  &amp;gt;Could you please clarify, whether it is allowed for   stateful session                  &amp;gt;bean having container-managed extended persistence context   to return                  &amp;gt;its EntityManager to clients (and for clients to perform   operations on                  &amp;gt;this EntityManager), or is it forbidden?                  &amp;gt;Currently, GlassFish allows returning the extended   EntityManager to                  &amp;gt;outside (e.g. http://java.net/jira/browse/GLASSFISH-11805 ),   Apache                  &amp;gt;OpenEJB does not, and there is currently ongoing   discussion about issue https://issues.apache.org/jira/browse/TOMEE-509  ;.                  &amp;gt;JPA 2.0 specification had following text (stars added by   me):                  &amp;gt;-----------------------------------------------                  &amp;gt;3.3 Persistence Context Lifetime                  &amp;gt;...                  &amp;gt;When an extended persistence context is used, the extended   persistence                  &amp;gt;context exists from the time the                  &amp;gt;EntityManager instance is created until it is closed. This   persistence                  &amp;gt;context might span multiple transactions                  &amp;gt;and non-transactional invocations of the EntityManager. A                  &amp;gt;container-managed extended persistence                  &amp;gt;context is enlisted in the current transaction when *the   EntityManager                  &amp;gt;is invoked in the scope of                  &amp;gt;that transaction* or when the stateful session bean to   which the                  &amp;gt;extended persistence context is bound is                  &amp;gt;invoked in the scope of that transaction.                  &amp;gt;-----------------------------------------------                  &amp;gt;The part between ³*² seems to suggest, that EntityManager   could be                  &amp;gt;accessed by some external entity directly, not necessarily   through                  &amp;gt;stateful session bean method.                  &amp;gt;I cannot find this text in JPA 2.1 public draft though.                  &amp;gt;Question: can reference to extended EntityManager be   provided to                  &amp;gt;outside, and methods called on this reference?                  &amp;gt;Regards,                  &amp;gt;Donatas              --              Christian von Kutzleben              Chief Engineer | Versant GmbH mailto:cromberg@ http://www.versant.com http://www.db4o.com              --              Versant              GmbH is incorporated in Germany. Company registration              number: HRB              54723, Amtsgericht Hamburg. Registered Office: Halenreie 42,              22359              Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John              CONFIDENTIALITY              NOTICE: This e-mail message, including any attachments, is              for the sole              use of the intended recipient(s) and may contain confidential or              proprietary information. Any unauthorized review, use,              disclosure or              distribution is prohibited. If you are not the intended              recipient,              immediately contact the sender by reply e-mail and destroy              all copies of              the original message.          --          Christian von Kutzleben          Chief Engineer | Versant GmbH mailto:cromberg@ http://www.versant.com http://www.db4o.com          --          Versant          GmbH is incorporated in Germany. Company registration number: HRB          54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359          Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John          CONFIDENTIALITY          NOTICE: This e-mail message, including any attachments, is for          the sole          use of the intended recipient(s) and may contain confidential or          proprietary information. Any unauthorized review, use, disclosure or          distribution is prohibited. If you are not the intended recipient,          immediately contact the sender by reply e-mail and destroy all          copies of          the original message.      --      Christian von Kutzleben      Chief Engineer | Versant GmbH mailto:cromberg@ http://www.versant.com http://www.db4o.com      --      Versant      GmbH is incorporated in Germany. Company registration number: HRB      54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359      Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John      CONFIDENTIALITY      NOTICE: This e-mail message, including any attachments, is for the sole      use of the intended recipient(s) and may contain confidential or      proprietary information. Any unauthorized review, use, disclosure or      distribution is prohibited. If you are not the intended recipient,      immediately contact the sender by reply e-mail and destroy all copies of      the original message.  --  Christian von Kutzleben  Chief Engineer | Versant GmbH  (T) +49 40 60990-0  (F) +49 40 60990-113 mailto:cromberg@ http://www.versant.com http://www.db4o.com  --  Versant  GmbH is incorporated in Germany. Company registration number: HRB  54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359  Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John  CONFIDENTIALITY  NOTICE: This e-mail message, including any attachments, is for the sole  use of the intended recipient(s) and may contain confidential or  proprietary information. Any unauthorized review, use, disclosure or  distribution is prohibited. If you are not the intended recipient,  immediately contact the sender by reply e-mail and destroy all copies of  the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 15:21:01 CET 2013</date>
    <body>Hello Steve, Calling addAttributeNodes with a non-basic type is not an exception condition.  It covers the simple case where a user wishes to include the attribute in the entity graph but has no requirement to further specify a subgraph for the attribute target class.  The operations specify behaviour for this case specifically for instance with fetch graph the default fetch graph is used for the target entity when no subgraph is specified otherwise the user would need to recreate the default fetch graph for all referenced attributes. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 15:29:13 CET 2013</date>
    <body>Yes, they should although it may be easier to support more inspection in the future if AttributeNode.getSubgraps() AttributeNode.getKeySubgraphs() were to exist or planned to exist. The alternate pattern that will be needed of isKeySubgraph() is not a great pattern.  I think we should add this simple methods to this release if possible. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 15:32:04 CET 2013</date>
    <body>Yes it should be allowed, the subsequent addition would replace the existing definition.  Otherwise there is no way for users to modify existing entity graphs. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 15:46:05 CET 2013</date>
    <body>Based on on your responses then, I think its is much more feasible to include the 2 new methods (getSubgraphs and getKeySubgraphs) in addition to getAttributeNodes in this release then.  Otherwise the API, IMHO is very confusing in this regard. Also, I'd like to suggest that this notion of "key subgraphs" be renamed to "map key subgraphs".  I see key here and continually think of database keys (as in maybe references to composite primary key embeddables).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 15:51:11 CET 2013</date>
    <body>In regards to getAttributeNodes returning a List, why a List exactly? What is the intending ordering of this List?  Insertion order? The reason I ask is that obviously the behavior you lay out (replacing) would be most easily served by usnig a Map.  But using a Map for the storage makes it awkward for returning Lists. And conversely using Lists for the storage makes it awkward to test for the replacement condition.  True, this is a implementation detail/difficulty, but its caused by using a List in the API when I am not sure that is necessary.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Partial rollback of entities/transactions</header>
    <date>Wed Jan 30 16:00:48 CET 2013</date>
    <body>Hello everyone, I stumbled upon a problem that I think deserves a standard solution in JPA. Imagine an application that allows a user to update an entity (or a set of entities) in several successive steps (e.g. in separate dialogs) before committing to the database. Within each step the entity can be updated (one or several times) and the user has to see its current state. The user can choose to cancel a step (dialog) at any time and proceed with another, or restart the same step again. As far as I know (maybe I'm wrong) there no way to cancel only selected changes made to an uncommitted entity. So far I heard/read about two workarounds: 1. Before each step (dialog) you create a copy of the entity and work with it. If the step is finished gracefully the state of the copy has to be written to the entity. If the step is canceled the entity remains unchanged. 2. One needs to track all the changes within a step and manually undo the changes on the entity. In my opinion this use case should be quite common and deserves an easy high-level API. There are two similar mechanism I know about: A) Savepoints: JDBC offers the concept of savepoints within an Transaction to do a partial rollbacks.  B) Nested transactions: Allows to create sub-transactions which can be rolled back separately. A similar construct in JPA would help a lot to avoid cumbersome, handmade, solutions. - Sebastian</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 17:42:51 CET 2013</date>
    <body>The list order is undefined.   Having a Map as the return type adds the complication of specifying a key as users of the untyped String API are not going to expect or want the keys to be Attribute types and vice versa for other users. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 17:45:23 CET 2013</date>
    <body>I never said to have a Map as the return type.   I am talking about internally (impl detail) using a Map.  Honestly, to me the return ought</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 18:35:15 CET 2013</date>
    <body>Hi Gordon, Could you propose the specific methods that you think should be added in this  release? thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 20:29:17 CET 2013</date>
    <body>The original plan was to simplify the interfaces and eliminate the AttributeNode as an artifact the user needed, however as the structure of the Subgraph became more complex to support the more complex scenarios I no longer think this is an option as we should support inspection in the future.  I am proposing that Subgraph interface no longer extends from AttributeNode and that AttributeNode would be updated to be :     /**      * Return the name of the attribute corresponding to the      * attribute node.      * @return name of the attribute      */     /** * Return the Map&amp;lt;Class, Subgraph&amp;gt; of Subgraphs associated with this AttributeNode      * @return Map of Subgraphs associated with this AttribureNode      */     /** * Return the Map&amp;lt;Class, Subgraph&amp;gt; of Subgraphs associated with this AttributeNode's map key * @return Map of Subgraphs associated with this AttribureNode's map key      */ This would allow for easier inspection of the EntityGraph and easily allow the user to differentiate between map key subgraphs and element subgraphs. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 21:02:29 CET 2013</date>
    <body>Oh, I misunderstood.  I thought you were suggesting that getSubgraphs and getKeySubgraphs would become part of EntityGraph and Subgraph. Putting them on AttributeNode does not make sense to me.   At least not as I was understanding the intention/breakdown between AttributeNode and Subgraph from your and Linda's replies from yesterday.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 21:51:46 CET 2013</date>
    <body>The issue is multiple key subgraphs and multiple subgraphs can exist for any attribute.  If we do not organize the subgraphs under an AttributeNode just like the annotations are structured it gets very difficult to convey the contents of an entity graph to a user in a manageable way.  if we were to add getKeySubgraphs to EntityManager the return would have to be List&amp;lt;Map&amp;lt;Class,Subgraph&amp;gt;&amp;gt; at the least.  Any other structure(ie. List&amp;lt;Subgraph&amp;gt;) and the user would need to organize the results to make the information meaningful.  Also if get*Subgraphs methods were added to the EntityGraph the user would need to check all three collections to determine what attributes were present in the entity graph. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 22:38:14 CET 2013</date>
    <body>I think this is an improvement. Unless there is objection, I will plan to make this change. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Early access to the TCK</header>
    <date>Wed Jan 30 17:56:01 CET 2013</date>
    <body>If that can help someone to vent frustrations, you can come and help Hardy, Gunnar and I write the Bean Validation TCK :D We can definitively use spare hands as the Feb 20th deadline is approaching fast ;) Emmanuel  To be honest, that reads like: we write the tests right before we ship the   software. How can the expert group make sure the test we will see   eventually are in line with what the spec defines? This essentially leaves   us as author of a spec that we will potentially see implementations of that   do/can not follow the spec because the TCK we have no influence on didn't   catch serious glitches. Where's the value in voting about a spec where the   actual rules that govern implementations are not under the control of those   who vote?    Beside Steve's issue with implementing the new features of the spec what is   the plan for the general work on the TCK going forward? To be honest I   think it's close to ridiculous to ask members of the EG to hand in ideas   and suggestions for improvements to the TCK which will then be implemented   however by whoever somewhere.    Especially in the light of the ambiguities discovered recently I think one   of the top priorities going forward has to be that we (the EG) can make   sure we'll see the the new features and behavior tested adequately. This   not only will help us creating a good/better spec as ambiguities will be   found before it is to late but also a much better usability as the lack of   defined *and enforced* semantics prevents functionality from being rendered   useless as pretty much every persistence provider behaves slightly   different as they essentially have no real means to test their   implementations.    So I suggest that going forward...    1. the TCK has to developed alongside the spec by the EG. Newly defined API   and functionality has to be backed by test cases on submission of the   feature to the spec.  2. This requires EG members having access to the TCK during spec   development (requires appropriate licensing of the spec)  3. If I conclude wishful thinking here I wonder why even the community   shouldn't have access to the TCK to maybe help out discovering potential   issues.    @Werner: does any of the current JCP versions already define TCK rules like   this? The closest JSR I can remember is CDI that always developed an Apache   licensed TCK fully in the open IIRC. Or is there any general rules within   the JCP/Oracle that should prevent us from going down that route?    Cheers,  Ollie    --  Sent while on the run...    Am 24.01.2013 um 21:39 schrieb Linda DeMichiel &amp;lt;linda.demichiel@...&amp;gt;:    &amp;gt; Hi Steve,  &amp;gt;   &amp;gt; The team tells me that we are still a number of weeks away on the TCK   &amp;gt; being ready for this.  &amp;gt;   &amp;gt; thanks,  &amp;gt;   &amp;gt; -Linda  &amp;gt;   &amp;gt;   &amp;gt;&amp;gt; A few months ago there was a discussion about early access to the TCK as   &amp;gt;&amp;gt; being a generally good thing and there seemed  &amp;gt;&amp;gt; to be a general consensus for doing that in any later revisions of the   &amp;gt;&amp;gt; spec. The Cliff Notes (tm) version of the pros  &amp;gt;&amp;gt; were mainly:  &amp;gt;&amp;gt; 1) the ability to identify as early as possible problems in the TCK   &amp;gt;&amp;gt; itself  &amp;gt;&amp;gt; 2) to help providers get started on certifying  &amp;gt;&amp;gt; 3) allow early feedback from the group as to missing portability coverage  &amp;gt;&amp;gt;   &amp;gt;&amp;gt; I was just curious whether we could possibly start getting access to the   &amp;gt;&amp;gt; 2.1 TCK, especially now that the spec is pretty  &amp;gt;&amp;gt; solidified</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Thu Jan 31 15:10:43 CET 2013</date>
    <body>Sorry for being dense about this (but honestly I have asked others in the group and they are just as confused by this graph stuff)... Can you explain the "multiple key subgraphs and multiple subgraphs can exist for any attribute" part? An attribute would be something like the "customer" attribute on an Account entity.  So we'd have EntityGraph&amp;lt;Account&amp;gt;, with AttributeNode&amp;lt;Customer&amp;gt; as part of that EntityGraph&amp;lt;Account&amp;gt;'s getAttributeNodes() collection.  The mere presence of AttributeNode&amp;lt;Customer&amp;gt; would indicate that Account.customer is supposed to be fetched, yes? As I understood it from yesterday's replies from yourself and Linda the intent in the above case would be that Account.customer could be depending on whether the user intended to further qualify the fetch graph "below" Customer.  Your new suggestion seems to indicate a fundamentally different view of this.  Now, an attribute reference is aways going to be an AttributeNode.  Under that we'd just have further possible qualifications of that fetch subgraph.  Yes?  If so, OK I can buy that as an improvement (the old deal was VERY counter-intuitive IMHO). Going back to the quote "multiple key subgraphs and multiple subgraphs can exist for any attribute", especially confusing to me is the "multiple key subgraphs" portion.  How can an attribute (a representation of the Map attribute) have multiple subgraphs *at that level* representing the map key?  There is something fundamental I am missing here...</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Thu Jan 31 18:36:03 CET 2013</date>
    <body>Hi Steve, I don't know exactly what Gordon had in mind when he wrote this, but here is the problem that I see. Since an entity graph may involve many entities, there may be multiple entities that have the same attribute (i.e., attribute name/type combination).  If you just retrieve all the subgraphs in an entity graph, that doesn't give you enough information as to what subgraph corresponds to which entity's attribute. Traversing the graph downwards (and recursively) from the root, via the attribute nodes for any given entity allows the structure of the graph to be discerned. HTH, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Thu Jan 31 19:11:27 CET 2013</date>
    <body>I get that part.   I would assume you can only get the AttributeNodes relative to the Graph that contains it. So if I have:    ...    ...    ... If I wanted to get a graph of Account+Customer+(billing)Address you would do: EntityGraph&amp;lt;Account&amp;gt; accountGraph = em.createEntityGraph( Account.class Subgraph&amp;lt;Customer&amp;gt; customerGraph = accountGraph.addSubgraph( // though as I understand it calling addAttributeNodes( Customer_.billngAddress ) would be the same :/ So the question is all in the proposed API for understanding an EntityGraph.  In the current proposal (public review draft), as I understood it you'd end up with EntityGraph&amp;lt;Account&amp;gt; returning 2 AttributeNodes from getAttributeNodes(): 1) AttributeNode&amp;lt;String&amp;gt;(referenceNumber) 2) Subgraph&amp;lt;Customer&amp;gt;(customer) In turn, Subgraph&amp;lt;Customer&amp;gt;(customer) would return 2 AtributeNodes from its getAttributeNodes(): 1) AttributeNode&amp;lt;String&amp;gt;(name) 2) Subgraph&amp;lt;Address&amp;gt;(billingAddress) I understand that EntityGraph&amp;lt;Account&amp;gt;.getAttributeNodes() above returning *all* AttributeNodes for the entire graph would be badness. So now in the new proposal, if I understand correctly, we'd instead have EntityGraph&amp;lt;Account&amp;gt; returning 2 AttributeNodes from getAttributeNodes() like so: 1) AttributeNode&amp;lt;String&amp;gt;(referenceNumber) 1.a) getSubgraphs() -&amp;gt; none 1.b) getKeySubgraphs() -&amp;gt; none 2) AttributeNode&amp;lt;Customer&amp;gt;(customer) 2.a) getSubgraphs() -&amp;gt; [Subgraph&amp;lt;Customer&amp;gt;] 2.b) getKeySubgraphs() -&amp;gt; none and so on. Tbh, its six-in-one to me.  They each give you the same information. Yes, in one you need to do an explicit type check which is always nice to avoid. And still, I am not getting how you possibly end up with multiple "key subgraphs" for a given AttributeNode.  That part makes no sense to me. In my understanding the Subgraph there would refer to the Map key of the Map-valued collection represented by the containing AttributeNode. How can a Map have multiple keys?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Thu Jan 31 19:22:17 CET 2013</date>
    <body>You wouldn't.  I had assumed the confusion was over whether EntityGraph would  have methods getSubgraphs and getKeySubgraphs (which would result in the badness I  described in my last msg) In my understanding the Subgraph there would refer to the Map key of the  Map-valued collection</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Thu Jan 31 19:24:42 CET 2013</date>
    <body>Hello Steve,   Comments below: --Gordon Because for a polymorphic relationship there may be additional subgraphs defined for a particular attribute to include attributes of the target's subclass types.  This functionality is the reason "type" is on Subgraph. Yes Yes, in order to keep the interface model as simple as possible. Yes, the interfaces need to be expanded to capture the complicated nature of the AttributeNode. See first comment.  It is because entity graphs support subclass specific definitions for each type within a polymorphic attribute.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Thu Jan 31 19:26:35 CET 2013</date>
    <body>I understand that EntityGraph&amp;lt;Account&amp;gt;.getAttributeNodes() above returning * all * AttributeNodes for the entire graph would be badness.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Thu Jan 31 19:32:37 CET 2013</date>
    <body>It would  because entity graphs support subclass specific definitions for each type within a polymorphic attribute. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Thu Jan 31 19:49:12 CET 2013</date>
    <body>These look great.  Thanks Linda.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Thu Jan 31 20:13:12 CET 2013</date>
    <body>One quick question. For 'javax.persistence.schema-generation-source' values of metadata-then-scripts and scripts-then-metadata is the order the same for both create and drop? I can envision a situation, for example, with metadata-then-scripts where the metadata creates a table and then the script creates a procedure.  I am not sure that all databases will allow you to drop the table if other database objects refer to it.  Would it be better to say that the defined order (x-then-y) applies to create, but the reverse order (y-then-x) applies to drop?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Thu Jan 31 20:50:59 CET 2013</date>
    <body>Good point. Actually, I'm wondering whether this will be sufficient. Perhaps a safer approach is that we define a schema-drop-source property (that looks exactly like schema-generation-source, with the same defaults) to cover this. What do you all think? -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Thu Jan 31 20:52:12 CET 2013</date>
    <body>I also am not sure about this passage in discussing 'javax.persistence.ddl-create-script-target' and 'javax.persistence.ddl-drop-script-target': The persistence provider must produce both create and drop scripts if the corresponding DDL targets are specified. This is independent of whether a drop action is included in the value passed for the javax.persistence.schema-generation-action property. To me, this sounds like you are saying that the following config will generate both create and drop scripts: javax.persistence.schema-generation-action=create even though the config explicitly specified just create scripts.  Is that your intent? Same for: javax.persistence.schema-generation-action=drop that is supposed to generate both create and drop scripts? Perhaps the reasoning is that javax.persistence.schema-generation-action is really just geared towards javax.persistence.schema-generation-action=database.  But if that is the case, maybe we ought to rename the setting to be more explicit of that fact or at least document it in the discussion of 'javax.persistence.schema-generation-action'</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Thu Jan 31 21:13:20 CET 2013</date>
    <body>yes yes partially.  also it was intended to control whether any action would be taken. I'm thinking that all this might be clearer if we required  schema-generation-target to be specified. But if that is the case, maybe we ought to rename the setting to be</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Thu Jan 31 22:03:43 CET 2013</date>
    <body>Well the confusion IMO as a user is that I specifically asked for only creation scripts to be generated, but got both creation and drop scripts.  Thats not going to change by making schema-generation-target required.  Even this is still confusing: javax.persistence.schema-generation-action=create javax.persistence.schema-generation-target=scripts because i would not expect to get a drop script.  But if I read the other parts of the spec correctly, the following actually results in an exception: javax.persistence.schema-generation-action=create javax.persistence.schema-generation-target=scripts The exception is because 'javax.persistence.ddl-drop-script-target' is not specified, again even though i just said I only want create scripts generated.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Fri Feb 01 16:23:07 CET 2013</date>
    <body>I guess what I was getting at yesterday is that it *seems* as if the intent is that 'javax.persistence.ddl-create-script-target' and 'javax.persistence.ddl-drop-script-target': 1) need to be specified either both together or neither 2) Control: 2.a) whether scripts are generated 2.b) what types of scripts are generated. So if I have: javax.persistence.schema-generation-action=create javax.persistence.schema-generation-target=database-and-scripts the "action=create" bit really only describes the database target side of things.  On the script target side, both create and drop are created. To me at least that is not the most intuitive set up.  If I say "action=create", why are the drop scripts being generated?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Fri Feb 01 17:08:26 CET 2013</date>
    <body>Another option is another boolean property (javax.persistence.schema-generation-source-reverse-drop?) that says to reverse the order for drop.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Fri Feb 01 20:17:53 CET 2013</date>
    <body>Since there seems to be agreement on this, I have proceeded to update the  spec. I'll be posting the updated javadocs shortly. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Fri Feb 01 20:32:08 CET 2013</date>
    <body>Available now on the project Downloads area.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Non-entity class: legal as second argument to createNamedQuery()?</header>
    <date>Sat Feb 02 00:07:48 CET 2013</date>
    <body>Is it legal to use a non-entity class as the second argument to EntityManager#createNamedQuery(String, Class) ? For example, I'd like to do:</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Sat Feb 02 17:13:26 CET 2013</date>
    <body>WRT various source scripts (DDL create/drop sources, import script sources) what are we saying is the standard for delimiters?  Or is this up to each provider? In Hibernate we actually hide this behind an interface so that it is pluggable.  The interface, as you would imagine, is very simple.  Is that something we want to explore for the spec?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Arbitrary precsion value in JPA</header>
    <date>Mon Feb 04 19:16:51 CET 2013</date>
    <body>Hello, The spec should mention some more details about the value of a persistent property of arbitrary-precision numbers such as BigDecimal type. For example, if the application defines a BigDecimal field with scale = 2 and sets a value of 1234.56789, then a) will the value be stored in database as 1234.56 (i.e. losing precision)? b) If the field were declared without a scale parameter, then would the column value be the same as the in-memory value of 1234.56789? c) If the provider is responsible for rounding the in-memory value, then what is the rounding mode? I have few more questions, but as a starter if some clarifications are available for the above questions, it will be helpful. Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Mon Feb 04 22:49:36 CET 2013</date>
    <body>Steve, all, You have raised good questions.  To clarify all this up, I am proposing that  we refactor the metadata a bit. To summarize, I am proposing that we do the following: 1) modify javax.persistence.schema-generation-action to be javax.persistence.schema-generation.database-action.  The property values and semantics are the same, with the exception that if this property is not specified, no schema generation actions must be taken *on the database*. (2) remove javax.persistence.schema-generation-target and replace with javax.persistence.schema-generation.script-action.  The values of this property and semantics would be the same as javax.persistence.schema-generation.database-action, except that this pertains to scripts only.  If this property is not specified, no scripts will be generated. (3) To accommodate the feedback about drop execution order, I think we should add another property, javax.persistence.schema-generation.drop-source that is just like javax.persistence.schema-generation-source, but allows control over dropping the database artifacts and the order in which this is to occur. (4) make some minor naming changes to make things more consistent/clearer. More formally, the resulting spec section would look like this: Section 8.2.1.9  would be updated similarly, as appropriate. [Section 9.4] *  javax.persistence.schema-generation.database-action The javax.persistence.schema-generation.database-action property specifies the action to be taken by the persistence provider with regard to the database artifacts.  The values for this property are "none", "create", "drop-and-create", "drop". If the javax.persistence.schema-generation.database-action property is not specified, no schema generation actions must be taken on the database. *  javax.persistence.schema-generation.script-action The javax.persistence.schema-generation.script-action property specifies which scripts are to be generated by the persistence provider. The values for this property are "none", "create", "drop-and-create", "drop". Scripts will only be generated if script targets are specified. If this property is not specified, no scripts will be generated. *  javax.persistence.schema-generation.database-source The javax.persistence.schema-generation.database-source property specifies whether the creation of database artifacts is to occur on the basis of the object/relational mapping metadata, DDL script, or a combination of the two. The values for this property are "metadata", "script", "metadata-then-script", "script-then-metadata".  If this property is not specified, and a script is specified by the javax.persistence.schema-generation.create-script-source property, the script (only) will be used for schema generation; otherwise if this property is not specified, schema generation will occur on the basis of the object/relational mapping metadata (only). The "metadata-then-script" and "script-then-metadata" values specify that a combination of metadata and script is to be used and the order in which this use is to occur. If either of these values is specified and the resulting database actions are not disjoint, the results are undefined and schema generation may fail. *  javax.persistence.schema-generation.drop-source The javax.persistence.schema-generation.drop-source property specifies whether the dropping of database artifacts is to occur on the basis of the object/relational mapping metadata, DDL script, or a combination of the two. The values for this property are "metadata", "script", "metadata-then-script", "script-then-metadata".  If this property is not specified, and a script is specified by the javax.persistence.schema-generation.drop-script-source property, otherwise if this property is not specified, the dropping of database artifacts will occur on the basis of the object/relational mapping metadata (only). The "metadata-then-script" and "script-then-metadata" values specify that a combination of metadata and script is to be used and the order in which this use is to occur. If either of these values is specified and the resulting database actions are not disjoint, the results are undefined and the dropping of database artifacts may fail. *  javax.persistence.schema-generation.create-database-schemas In Java EE environments, it is anticipated that the Java EE platform provider may wish to control the creation of database schemas rather than delegate this task to the persistence provider.  The javax.persistence.schema-generation.create-database-schemas property specifies whether the persistence provider is to create the database schema(s) in addition to creating database objects such as tables, sequences, constraints, etc. The value of this boolean property should be set to true if the persistence provider is to create schemas in the database or to generate DDL that contains "CREATE SCHEMA" commands. If this property is not supplied, the provider should not attempt to create database schemas. This property may also be specified in Java SE environments. *  javax.persistence.schema-generation.create-script-target,    javax.persistence.schema-generation.drop-script-target If scripts are to be generated, the target locations for the writing of these scripts must be specified. The javax.persistence.schema-generation.create-script-target property specifies a java.IO.Writer configured for use by the persistence provider for output of the DDL script or a string specifying the file URL for the DDL script. This property should only be specified if scripts are to be generated. The javax.persistence.schema-generation.drop-script-target property specifies a java.IO.Writer configured for use by the persistence provider for output of the DDL script or a string specifying the file URL for the DDL script. This property should only be specified if scripts are to be generated. *  javax.persistence.database-product-name,    javax.persistence.database-major-version,    javax.persistence.database-minor-version If scripts are to be generated by the persistence provider and a connection to the target database is not supplied, the javax.persistence.database-product-name property must be specified. The value of this property should be the value returned for the target database by the JDBC DatabaseMetaData method getDatabaseProductName. If sufficient database version information is not included in the result of this method, the javax.persistence. database-major-version and javax.persistence.database-minor-version properties should be specified as needed. These should contain the values returned by the JDBC getDatabaseMajorVersion and getDatabaseMinorVersion methods respectively. *  javax.persistence.schema-generation.create-script-source, *  javax.persistence.schema-generation.drop-script-source The javax.persistence.schema-generation.create-script-source and javax.persistence.schema-generation.drop-script-source properties are used for script execution. In Java EE container environments, it is generally expected that the container will be responsible for executing DDL scripts, although the container is permitted to delegate this task to the persistence provider. If DDL scripts are to be used in Java SE environments or if the Java EE container delegates the execution of scripts to the persistence provider, these properties must be specified.  The  javax.persistence.schema-generation.create-script-source property specifies a java.IO.Reader configured for reading of the DDL script  or a string designating a file URL for the DDL script. The javax.persistence.schema-generation.drop-script-source property specifies a java.IO.Reader configured for reading of the DDL script or a string designating a file URL for the DDL script. *  javax.persistence.schema-generation.connection The javax.persistence.schema-generation.connection property specifies the JDBC connection to be used for schema generation. This is intended for use in Java EE environments, where the platform provider may want to control the database privileges that are available to the persistence provider. This connection is provided by the container, and should be closed by the container when the schema generation request or entity manager factory creation completes. The connection provided must have credentials sufficient for the persistence provider to carry out the requested actions. If this property is not specified, the persistence provider should use the DataSource that has otherwise been provided. [subsection 9.4.1] Data Loading Data loading, by means of the use of SQL scripts, may occur as part of the schema generation process after the creation of the database artifacts or independently of schema generation. The specification of the javax.persistence.sql-load-script-source controls whether data loading will occur. *  javax.persistence.sql-load-script-source In Java EE container environments, it is generally expected that the container will be responsible for executing data load scripts, although the container is permitted to delegate this task to the persistence provider. If a load script is to be used in Java SE environments or if the Java EE container delegates the execution of the load script to the persistence provider, this property must be specified.  The javax.persistence.sql-load-script-source property specifies a java.IO.Reader configured for reading of the SQL load script for database initialization or a string designating a file URL for the script.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 18:13:06 CET 2013</date>
    <body>1) I think the split into database-action and script-action plays much nicer.  Nice thought. 2) You wrote '* javax.persistence.schema-generation.database-source' in the updated spec section, as a replacement for javax.persistence.schema-generation-source.  IMO this should be named 'javax.persistence.schema-generation.create-source' instead to pair with the new 'javax.persistence.schema-generation.drop-source' 3) I wonder if a better consistent naming scheme would be to use javax.persistence.schema-generation.database.&amp;lt;setting&amp;gt; and javax.persistence.schema-generation.scripts.&amp;lt;setting&amp;gt;?  To me that plays much nicer: javax.persistence.database-product-name javax.persistence.database-major-version javax.persistence.database-minor-version javax.persistence.schema-generation.create-source javax.persistence.schema-generation.drop-source javax.persistence.schema-generation.create-script-source javax.persistence.schema-generation.drop-script-source javax.persistence.schema-generation.connection javax.persistence.schema-generation.create-database-schemas* javax.persistence.schema-generation.database.action javax.persistence.schema-generation.scripts.action javax.persistence.schema-generation.scripts.create-target javax.persistence.schema-generation.scripts.drop-target * - I was not clear whether create-database-schemas is meant to describe just database target or scripts target or both.. 4) WRT "data loading", the question of delimiter is still open.  I think this ought to be spelled out in the spec, otherwise provider portability will be a major problem.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 20:20:21 CET 2013</date>
    <body>thanks, Steve I am fine with this.  See the below for minor tweaks/questions. Otherwise, I plan to commit these changes to the spec. This would be javax.persistence.schema-generation.scripts.create-source and  drop-source, right? Should the preceding be  javax.persistence.schema-generation.database.create-source and drop-source ? Or is that obvious? Should cover both</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 20:22:00 CET 2013</date>
    <body>[snip] [I want to separate this item out, since the rest of the changes seem stable.] I'm not sure I understand what you mean by this?  Are you referring to a  property to cover the SQL terminator character in scripts?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 20:55:33 CET 2013</date>
    <body>In the initial suggestion, create-source and drop-source are meant to identify the metadata/scripts/metadata-then-scripts/scripts-then-metadata selections.  create-script-source and drop-script-source are meant to identify the location (Reader/file-url) of the input scripts There are multiple axises, if you will, at play here.  My suggestion was to focus the setting naming based on what used to be called targets: scripts, database.  So really for target-specific settings we have: For non-target-specific settings, we have just: As I read it initially, I had thought that create-script-source and drop-script-source were both meant for database and scripts targets.  Is that accurate?  If not, then by all means javax.persistence.schema-generation.scripts.create-source and drop-source make more sense.  But in that case I think we need to clarify the wording about when settings are supposed to bear on just database target or just scripts target.  The renaming I suggested is a step in that direction. To be honest, I did not like the multiple meanings for the term "source".  It gets confusing trying to explain the settings.  But, as I had no better sugestion, I let it go.  "source" can mean either: 1) what "sources" should we consider to obtain create/drop DDL commands 2) for the scripts source (see 1), what are the source/location/reader for that script.  My gut tells me this is the usage that needs a new term, I just dont know what that term is.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 21:07:05 CET 2013</date>
    <body>Well I just mean that commands in a script file might be delimited in a number of ways (eol-delimited, semicolon-delimited, keyword-delimited, etc). It is usually not feasible to handle the entire contents of such a "data loading script" as one big string, especially for sending to the database directly via JDBC.  Typically you want to be able to handle command-by-command.  Hence the command delimiter. As an exmaple, if Provider-A assumes end-of-line as the command delimiters and Provider-B assumes semicolon users will not be able to feed the same script to both providers. I just think it makes sense to be up front about expectations in this regard in the spec.  Are providers expected to parse the import script in the same fashion?  Are we saying this is just totally provider defined (and therefore completely non-portable)? This is where I was saying that Hibernate actually uses an interface to basically it takes a Reader and breaks it down into String[] of the commands.  That is obviously extremely flexible.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 21:10:46 CET 2013</date>
    <body>I had assumed that the appropriate database SQL delimiters would be used.  By  Provider-A and B are you referring to persistence providers or database providers?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 21:30:03 CET 2013</date>
    <body>I think it would be confusing to call these javax.persistence.schema-generation.scripts.create-source when they reference the actual scripts and are not actions.  We could call them javax.persistence.schema-generation.database.create-script-source if you need to qualify them but I believe they are fine as they are.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 21:37:39 CET 2013</date>
    <body>Yes, persistence providers.  I also don't personally like requiring DB specific delimiters.  Another tenant of JPA is database portability.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 21:44:11 CET 2013</date>
    <body>However, if this isn't database delimiters, then why aren't newline  characters enough?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 21:51:46 CET 2013</date>
    <body>Because experience tells us (Hibernate team) that users do not like writing scripts that are line delimited.  Also, our interface allows handling of comments, etc that might need to be ignored. I guess as long as the language we use does not exclude persistence providers from using more user-friendly options in parsing the import script, its fine.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 22:06:15 CET 2013</date>
    <body>OK, thanks.  Will keep as in Steve's initial proposal.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] updated spec draft and xsds</header>
    <date>Wed Feb 06 01:48:12 CET 2013</date>
    <body>I've posted a new spec draft with the updated schema generation metadata to the project Downloads area.   This draft also contains the updated persistence and orm schemas.  I have also uploaded these xsd files to the Downloads area. Notice that the namespace in the schemas has changed, as we will no longer be using java.sun.com for new work.   There are no other modifications to the schema for the persistence.xml, but since we have changed the orm schema, I figured it would be less confusing if both xsd files were updated. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Extensibility of schema generation</header>
    <date>Thu Feb 07 17:26:08 CET 2013</date>
    <body>Hello all, first let me emphasize that I'm really happy to see that JPA 2.1 will support database schema generation. In my opinion this is a very important and useful feature for everyone working with JPA. According to the current draft of the spec, JPA will support automatic creation of the database schema from mapping metadata and custom DDL scripts. The problem I'm seeing here is that these methods typically only work well if there is no initial database at all. But this is rarely the case. A very frequent case is that the database schema is not created but migrated . New versions of existing applications often require the database schema to be modified. In these cases the schema creation methods described in the current draft are not really useful. Of cause the JPA provider is able to automatically create missing tables and columns, but typically developers need more control over the migration process. Especially when doing real schema refactorings like column data type changes, modified constraints, etc. There are very good tools out there that focus on database schema migration like Liquibase [1] or Flyway [2]. But unfortunately there is no really easy portable way to integrate these tools into a JEE application.  So I was thinking that it may be a good addition to the spec to provide an extension point for the schema creation process. This would be a great feature because it would allow users to customize the schema creation in any way they want. They could integrate 3rd party tools specialized in database schema migration or they could build their own way of working with schema versions and their migration. I could think of an API like this:   void generateSchema(java.sql.Connection connection, java.util.Properties map) One option to specify an implementation for custom schema generation would be to set a property for the persistent unit like this: javax.persistence.schema-generation.provider-class=com.example.myapp.MySchemaGenerationProvider Another option would be to use the standard JDK ServiceLoader mechanism. What do you think? Does it make sense to provided portable extensibility for the schema generation process? Is it worth adding it to the spec? I would love to get feedback on this. Best regards Christian Kaltepoth [1]  http://www.liquibase.org/ [2]  http://flywaydb.org/ -- Christian Kaltepoth Blog: http://blog.kaltepoth.de/ Twitter: http://twitter.com/chkal GitHub: https://github.com/chkal</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft and xsds</header>
    <date>Fri Feb 08 18:07:44 CET 2013</date>
    <body>There is a discrepency between orm.xsd and the spec. orm.xsd defines that join-table lists that foreign-key should be a sub-element, however @JoinTable does not define a foreignKeys attribute.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft and xsds</header>
    <date>Fri Feb 08 18:33:20 CET 2013</date>
    <body>Sigh.  This is a bug (and with the other @XXXTable mappings that allow  joinColumns to be specified).  I intended compound FKs to be handled in @JoinColumns, but these  mapping types don't have a @JoinColumns subelement by rather a JoinColumn[] element. Will need to reevaluate. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Partial rollback of entities/transactions</header>
    <date>Sat Feb 09 08:29:02 CET 2013</date>
    <body>Hey Sebastian, in my opinion a JPA entity is a one-to-one representation of data in the database. Therefore you should only modify properties of the entity, when you want to change the values in the database. Therefore it is IMHO not a very good practice to modify properties of an attached entity although the user is still able to press a "cancel" button. So in my opinion you should really go with "workaround #1" and simply create copy of object to work on. That's a much cleaner solution and you have more control over how changes are handled when the user either clicks "OK" or "Cancel". Just my 2 cents on this. :) Christian 2013/1/30 sheba.public+javanet@... Hello everyone, I stumbled upon a problem that I think deserves a standard solution in JPA. Imagine an application that allows a user to update an entity (or a set of entities) in several successive steps (e.g. in separate dialogs) before committing to the database. Within each step the entity can be updated (one or several times) and the user has to see its current state. The user can choose to cancel a step (dialog) at any time and proceed with another, or restart the same step again. As far as I know (maybe I'm wrong) there no way to cancel only selected changes made to an uncommitted entity. So far I heard/read about two workarounds: 1. Before each step (dialog) you create a copy of the entity and work with it. If the step is finished gracefully the state of the copy has to be written to the entity. If the step is canceled the entity remains unchanged. 2. One needs to track all the changes within a step and manually undo the changes on the entity. In my opinion this use case should be quite common and deserves an easy high-level API. There are two similar mechanism I know about: A) Savepoints: JDBC offers the concept of savepoints within an Transaction to do a partial rollbacks. B) Nested transactions: Allows to create sub-transactions which can be rolled back separately. A similar construct in JPA would help a lot to avoid cumbersome, handmade, solutions. - Sebastian</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] ForeignKey annotation [Re: Re: updated spec draft and xsds]</header>
    <date>Mon Feb 11 20:48:31 CET 2013</date>
    <body>Here's my assessment of the issue. Ideally, I think foreign-key metadata should be kept in the JoinColumn(s) annotations.   However, for compound primary keys we have JoinColumn[] elements (or related elements) in the JoinTable, CollectionTable, and SecondaryTable annotations, which do not support this cleanly. I see several options (none unfortunately ideal): 1)  Move @ForeignKey to top level.  I think this gets messy because     it allows for a lot of overlap in where it might be specified. 2)  Add @ForeignKey elements to JoinTable, CollectionTable, and SecondaryTable     and clarify that they are to be used for compound FKs. 3)  Clarify that for compound foreign keys, in JoinTable, CollectionTable,     and SecondaryTable that at most one of the joinColumn elements should     be used to hold the FK metadata for the compound FK. Am I missing anything? Do any of you see a better way? So far, I am leaning toward #3 as the least objectionable. Opinions?? thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Thu Feb 14 23:17:46 CET 2013</date>
    <body>Quick clarification question... In regards to entity graphs the spec says that an entity's identifier and version attributes are automatically considered to be part of the fetch graph.  Does that mean, specifically, that: em.createEntityGraph( SomeEntity.class ).getAttributeNodes() should contain AttributeNodes for SomeEntity's identifier and version (if any) attributes?  Or is the "automatically" more considered implicit?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Fri Feb 15 15:32:35 CET 2013</date>
    <body>I guess the larger question here is about the intent of getAttributeNodes().  Is the expectation for it to return just the attribute nodes explicitly added by the user, or nodes for all the attributes that the persistence provider will actually be fetching? That distinction goes beyond just id and version attributes.  As the spec says, a "persistence provider is permitted to fetch additional entity state beyond that specified by a fetch graph or load graph" as long as the provider at the least fetches any explicitly added attribute nodes.  So given a call like: em.createEntityGraph( SomeEntity.class ) a provider is free to fetch all attributes of SomeEntity. So which truth should getAttributeNodes() return?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Fri Feb 15 15:55:31 CET 2013</date>
    <body>getAttributeNodes() should only return the nodes explicitly added by the user. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] PagSeguro; Pagamento em analise..</header>
    <date>Fri Feb 15 16:03:25 CET 2013</date>
    <body>Title: Nova pagina 1 Ola, Seu pagamento de R$ 143,70 para   Groupon   esta em analise. Status:   Em analise Código:   340C1-AG79-4BC1-9AA1- 093DSAG Site:   http://www.groupon.com.br E-mail:   groupon@... Telefone:   11 3233-4012       Comprovante da transacao.   ITENS DO PEDIDO QUANTIDADE VALOR (R$) TOTAL (R$) Desconto imperdivel em delicioso Rodizio na Vento Aragano! 3 47,90 143,70 Total geral: R$ 143,70 Voce recebera um e-mail assim que o pagamento for confirmado. Importante:   Este pagamento sera registrado na fatura do seu cartao de credito como PagSeguro Panfleteria .   Facilite sua vida: o PagSeguro e a melhor maneira de fazer pagamentos e receber valores na internet com a seguranca dos seus dados garantida.   Aproveite para criar sua conta usando o seu email. . Atenciosamente, Equipe PagSeguro. PagSeguro.   Sua compra protegida. DUVIDAS? Acesse   http://www.pagseguro.com.br/ atendimento Este e um e-mail automatico disparado pelo sistema. Favor nao responde-lo, pois esta conta nao e monitorada.   Código: GCQ09438</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] ForeignKey annotation [Re: Re: updated spec draft and xsds]</header>
    <date>Fri Feb 15 19:21:06 CET 2013</date>
    <body>After weighing the options here, I've concluded that to accommodate the ability to specify composite foreign keys, we should add a ForeignKey element to the CollectionTable, SecondaryTable, and AssociationOverride annotations, and a pair of ForeignKey elements to the JoinTable annotation. If both such a ForeignKey element and the foreignKey element within the JoinColumn element are specified in one of these annotations, the behavior would be undefined. I also think that we need to make more intuitive what specifying @ForeignKey() as a value of such an element means--i.e., specifying @ForeignKey() when all of its member values are defaulted-- since the user's expectation will probably be that this means that foreign key constraints will be generated when schema generation is in effect.  To clarify this, we should replace the current disableForeignKey with a 3-valued enum (PROVIDER_DEFAULT, CONSTRAINT, NO_CONSTRAINT).  By default, if @ForeignKey is not specified, the default provider behavior will apply and @ForeignKey() will be equivalent to @ForeignKey(CONSTRAINT). I've uploaded javadocs that capture all of this. ( http://java.net/projects/jpa-spec/downloads/download/JPAjavadocs02142013.zip ) See especially the following: ForeignKey, ConstraintMode, JoinColumn, JoinTable, CollectionTable, SecondaryTable. Please let me know if you find anything amiss. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] new spec draft (PFD candidate) and orm.xsd</header>
    <date>Mon Feb 18 22:59:57 CET 2013</date>
    <body>I've just uploaded a new spec draft and orm.xsd file to our project downloads area:  http://java.net/projects/jpa-spec/downloads These reflect the recent updates to the foreign key metadata. I would like to submit this draft (or something very close to it) to the JCP at the end of the week as the JPA 2.1 Proposed Final Draft. Please let me know if you find any issues. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new spec draft (PFD candidate) and orm.xsd</header>
    <date>Wed Feb 20 14:25:53 CET 2013</date>
    <body>Hi Linda, Linda DeMichiel, am 18 Feb 2013 hast Du um 13:59 zum Thema "[jsr338- experts] new spec draft (PFD candidate) a"  geschrieben :  I've just uploaded a new spec draft and orm.xsd file to our project  downloads area:  http://java.net/projects/jpa-spec/downloads    These reflect the recent updates to the foreign key metadata.    I would like to submit this draft (or something very close to it)  to the JCP at the end of the week as the JPA 2.1 Proposed Final Draft.    Please let me know if you find any issues. thanks a lot for the tremendous work that you have put into this draft.  Please allow me for some minor comments: 3.7 3rd paragraph p. 108 construst -&amp;gt; construct 3.7.1 javadoc of addKeySubgraph 1st paragraph p. 110 "Use of ... to the graph". Given the general remark in the 2nd  paragraph of the javadoc of the interface on p. 109, and in consistence  with the other javadocs of addKeySubgraph this sentence should be  removed, shouldn't it? 3.7.3 javadoc of getClassType p.116 Would expect something like "Return the java class of the managed type  for which ...". 3.7.4.2 pp. 119 - 121 Would it make sense to consistently replace "fetch" by "load"  throughout this section? 3.10.2 javadoc of setLockMode throws clause p. 138 Criteria API query -&amp;gt; CriteriaQuery query 7.4 javadoc of addNamedEntityGraph p. 338 A remark on the (non-)effect on already created mutable copies (similar  to the addNamedQuery javadoc) might be helpful. 9.4 p. 384 2nd bullet last sentence I believe to remember a discussion on whether both script targets need  to be set even if the action (either create or drop) triggers only one  script to be created, but the current wording of the spec is not clear  enough to me to understand what the outcome of the discussion has been. 11.2.1.1 2nd paragraph 2nd sentence p. 491 in -&amp;gt; is   Best regards Rainer --- Rainer Schweigkoffer               SAP AG Walldorf Regulatory Compliance              TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, F.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...       fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Lars  Dalgaard, Luisa Deplazes Delgado, Bill McDermott (Co-CEO),  Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP  Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse  oder sonstige vertrauliche Informationen enthalten.  Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist  Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung  oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte  benachrichtigen Sie uns und vernichten Sie die empfangene  E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged,  undisclosed, or otherwise confidential information. If you  have received this e-mail in error, you are hereby  notified that any review, copying, or distribution of it  is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your  cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new spec draft (PFD candidate) and orm.xsd</header>
    <date>Wed Feb 20 22:35:07 CET 2013</date>
    <body>Hi Rainer, Thanks for the helpful comments.  More inline below.... fixed yes -- gone fixed fetch is used generically here, so I changed consistently to that. fixed I was going to add something here, but decided not to, as I think the existing docs are sufficiently clear about the (im)mutability of what is stored/returned Changed to: "A script will only be generated if the script target is  specified." Which begs the question of what happens if drop-and-create is specified, but only one script supplied.  I've left that as undefined.  It might result in a deployment error, for example. fixed Thanks again best regards -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] JPA TCK</header>
    <date>Sat Feb 23 01:18:31 CET 2013</date>
    <body>We expect to have a preliminary version of the JPA 2.1 TCK available for expert group members by the end of next week.  Since the TCK is too large to be emailed, it will be made available on our JLE drop site. If you are not a licensee, a temporary account will need to be set up for you.  In this case, if you need access to the TCK, you should send me a note requesting such access and providing the following contact information, which I will then pass on to the people managing the site to have you set up:     Expert name     Company Name (if not an individual)     Address     Phone Number     Email Address -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA TCK</header>
    <date>Sat Feb 23 12:00:33 CET 2013</date>
    <body>That's great news, Linda. Thank you very much. I'll send you the details  off-list. Cheers, Ollie Am 23.02.2013 um 01:18 schrieb Linda DeMichiel &amp;lt;linda.demichiel@...&amp;gt;:  We expect to have a preliminary version of the JPA 2.1 TCK available  for expert group members by the end of next week.  Since the TCK is  too large to be emailed, it will be made available on our JLE drop  site.    If you are not a licensee, a temporary account will need to be set up  for you.  In this case, if you need access to the TCK, you should send  me a note requesting such access and providing the following contact  information, which I will then pass on to the people managing the site  to have you set up:     Expert name     Company Name (if not an individual)     Address     Phone Number     Email Address    -Linda --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] JPA 2.1 Proposed Final Draft</header>
    <date>Mon Feb 25 21:24:49 CET 2013</date>
    <body>I've just submitted the JPA 2.1 PFD to the JCP.  This very closely matches the draft I uploaded a week ago, and incorporates the editorial feedback I received since then. The only changes to the javadocs were to the comments for the following  methods: TypedQuery.setLockMode EntityGraph.addKeySubgraph Subgraph.getClassType The updated files are available on  http://java.net/projects/jpa-spec/downloads -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Wed Mar 06 23:16:18 CET 2013</date>
    <body>[1] requires that at Transaction completion, the container closes (or returns to cache), the transaction-scoped persistence context.  What is supposed to happen when the JTA transaction completes in a different thread than the application thread?  For example, if a background thread calls the Synchronization.afterCompletion() because the tx timeout period has been exceeded (as some Transaction Managers may do), its not exactly thread-safe to call EntityManager.close() (see [2]). Specifically, the application could be in the middle of a persist or some other EntityManager method, when EntityManager.close() is called. Related to the above, if a JTA transaction rollback occurs in a background thread [3], how are the managed entities expected to be detached without violating the EntityManager thread-safety [2]? There may be vendor specific solutions but shouldn't we (JPA spec eg) account for the interaction of thread-unsafe persistence contexts and the JTA Synchronization.afterCompletion that may be invoked in non-application (background) threads? Scott [1] 7.9.1 Container Responsibilities - After the JTA transaction has completed (either by transaction commit or rollback), the container closes the entity manager calling EntityManager.close. [2] 7.2 Obtaining an EntityManager - An entity manager must not be shared among multiple concurrently executing threads, as the entity manager and persistence context are not required to be threadsafe. Entity managers must only be accessed in a single-threaded manner. [3] 7.9.2 Provider Responsibilities - When the JTA transaction rolls back, the provider must detach all managed entities if the persistence context is of type SynchronizationType.SYNCHRONIZED or has otherwise been joined to the transaction.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 07 23:41:57 CET 2013</date>
    <body>The team here tells me that this should not be happening, and that the  transaction managers they are familiar with will just mark the transaction for rollback rather than rolling  it back at the point of timeout. Nevertheless, if the container were working with a TM where a timeout did  result in immediate rollback and invocation of afterCompletion, the container should note this,  and at the point at which the transaction would normally be completed then do the actual close as  it normally would. What do your transaction manager and container do?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Fri Mar 08 01:48:43 CET 2013</date>
    <body>Currently, we roll the transaction back from the background timer thread.  The JTA spec [4] does allow different threads to start/end the transaction. Should we include a form of the above text in the JPA 2.1 spec (section 7.9.1 [1])? How would we word what the provider side has to do when detaching entities after rollback [3]?  I'm not sure that the persistence provider will have the same chance to make a note for the container side to take action on (if there is an EE container involved).  There is also the expectation that any JPA provider will work, with any EE container to consider. [4] JTA 1.1 spec 3.4.3 Thread of Control: " The X/Open XA interface specifies that the transaction association related xa calls must be invoked from the same thread context. This thread-of-control requirement is not applicable to the object-oriented component-based application run-time environment, in which application threads are dispatched dynamically at method invocation time.  Different Java threads may be using the same connection resource to access the resource manager if the connection spans multiple method invocation. Depending on the implementation of the application server, different Java threads may be involved with the same XAResource object. The resource context and the transaction context may be operated independent of thread context. This means, for example, that it’s possible for different threads to be invoking the XAResource.start and XAResource.end methods. If the application server allows multiple threads to use a single XAResource object and the associated connection to the resource manager, it is the responsibility of the application server to ensure that there is only one transaction context associated with the resource at any point of time. Thus the XAResource interface specified in this document requires that the resource managers be able to support the two-phase commit protocol from any thread context. " Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Sat Mar 09 02:31:13 CET 2013</date>
    <body>Yes, I realize this is permitted. Unfortunately, I don't think this may always work, because the container may be relying on synchronization notifications at the normally expected tx end to know when it should be calling close (i.e., it may not know when the tx was started).  If EJB CMT were used, the container would know when a tx was started and could use a business method boundary as the interpositioning point.  If a container wrapped UserTransaction, I suppose it could use that point as well, but it is not obvious to me how this would be handled otherwise. How does your implementation handle this? I'd also like to hear from the other implementations here as to what they do and how their transaction manager implementations handle timeout.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Regularize sua divida.</header>
    <date>Sat Mar 09 15:28:40 CET 2013</date>
    <body>___________________________________________________________________________________________     Prezado(a) Cliente:   Temos uma otima oportunidade para Voce!  </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Mon Mar 11 17:32:02 CET 2013</date>
    <body>Yes, it would be easier if containers could queue (or hand-off) the action (closing EntityManager or returning it to a EM pool) to happen at the tx boundary as you suggest (if known). One concern that I have is if we were to hand-off the closing of the EntityManager to the top level component, that would probably be too late (e.g. we could exhaust memory/resources before that is reached). I'm not saying that we would, just pointing out that the action needs to be taken as close as possible to the transaction ending. How deeply do we want to go up the EE stack to deal with this?  Is there a non-JPA mechanism that we need to help with the container and provider side? At a minimum, I think that we should add a requirement for all providers to deal with this situation.  And, text about how containers should also deal with this if needed. On the container side we close the EntityManager in the background thread and the application thread cleans up when the container level deals with the transaction getting cancelled.  Depending on where this discussion goes, we will follow the recommendations made here.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Sua ocorr�ncia de FURTO(OUTROS) foi analisada..</header>
    <date>Tue Mar 12 15:01:51 CET 2013</date>
    <body>Title: Nova pagina 1 Sua ocorrência de FURTO(OUTROS) foi analisada. Caso não consiga visualizar o Boletim de Ocorrência (B.O.) em anexo, acesse o seguinte endereço: http://www.delegaciaeletronica.sp.gov.br/bo/Visualizacao.do?numProtocolo=09f8dg70ffs8d7f6s8 Protocolo:  3840293480242 Senha:  RT1 94803945803912R Mantenha em mãos o seu CPF, o número do protocolo e a senha, pois será preciso informá-los para visualização do B.O. Ao imprimir o B.O. assine-o no espaço "Responsável pela Informação". Para maiores informações, você deve entrar em contato com a Delegacia Eletrônica Grato por usar nossos serviços, Delegacia Eletrônica Polícia Civil do Estado de São Paulo. Secretaria da Segurança Pública e Defesa Social</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Current standalone TCK drop</header>
    <date>Wed Mar 13 16:16:39 CET 2013</date>
    <body>As I understand it the current TCK drop is not supposed to be final, but I have found serious problems with it.  To the point where it is essentially unusable. First, the signature tests are wrong.  Running against the JPA API jar I created for Hibernate testing using the latest Proposed Final Draft I get errors that "javax.persistence.criteria.CommonAbstractQuery" is missing from my jar.  But javax.persistence.criteria.CommonAbstractQuery was removed from the spec way back in Draft 6. Second, I am completely unable to run any of the actual persistence tests.  Every single one of them fails in perfoming set up.  The error is always the same: 03-13-2013 01:22:19:  ERROR: Map returned from emf.getProperties() was null,this should never occur However, it is impossible for the Hibernate EMF to return a null Map there.  So, since I can't actually debug the tests (or please I'd love to have details on how to actually accomplish that) I fell back to caveman debugging and put throwing exceptions as the first thing I do inside (1) the Hibernate PersistenceProvider#createEntityManagerFactory call and the constructor to the Hibernate EMF impl.  Neither gets triggered.  So that tells me that the TCK is not even attempting to call out to the configured "alternate provider".</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 16:43:00 CET 2013</date>
    <body>This is a bug in our APIs.  It looks like I neglected to remove it after our  having gone back and forth on the factorization so many times.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 16:55:09 CET 2013</date>
    <body>I haven't looked as deeply into the code as Steve yet, but I'd like to get a  general idea of how the feedback process is expected to work. Assuming I'd  like to add additional test cases to the TCK or have comments on existing  tests, what's the preferred way to communicate those? For the latter emailing  the EG mailing list seems to be fine. The former - through Git patches maybe? Cheers, Ollie Am 13.03.2013 um 16:16 schrieb Steve Ebersole &amp;lt;steve.ebersole@...&amp;gt;:  As I understand it the current TCK drop is not supposed to be final, but I   have found serious problems with it.  To the point where it is essentially   unusable.    First, the signature tests are wrong.  Running against the JPA API jar I   created for Hibernate testing using the latest Proposed Final Draft I get   errors that "javax.persistence.criteria.CommonAbstractQuery" is missing   from my jar.  But javax.persistence.criteria.CommonAbstractQuery was   removed from the spec way back in Draft 6.    Second, I am completely unable to run any of the actual persistence tests.    Every single one of them fails in perfoming set up.  The error is always   the same:    03-13-2013 01:22:19:  ERROR: Map returned from emf.getProperties() was   null,this should never occur    However, it is impossible for the Hibernate EMF to return a null Map there.    So, since I can't actually debug the tests (or please I'd love to have   details on how to actually accomplish that) I fell back to caveman   debugging and put throwing exceptions as the first thing I do inside (1)   the Hibernate PersistenceProvider#createEntityManagerFactory call and the   constructor to the Hibernate EMF impl.  Neither gets triggered.  So that   tells me that the TCK is not even attempting to call out to the configured   "alternate provider" --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-151-50465477  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 17:03:37 CET 2013</date>
    <body>Any idea on the second issue?  Its by far the more obstructive of the 2.  Hard to test a provider if you cant tell the TCK to use said provider :)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 17:16:01 CET 2013</date>
    <body>This sounds to me like a configuration issue.  I'll let our TCK folks try to  help....</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 17:22:47 CET 2013</date>
    <body>Well I did follow all documented instructions... ;)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 18:09:33 CET 2013</date>
    <body>Are the "TCK folks" on this list?  Or how do I proceed with that?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 18:26:27 CET 2013</date>
    <body>Yes</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 18:59:41 CET 2013</date>
    <body>Hi Ollie, You should send your suggestions to me. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 19:01:15 CET 2013</date>
    <body>Hi Steve, Since you are a licensee, I recommend that you use your direct licensee  channels for this. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Wed Mar 13 19:04:33 CET 2013</date>
    <body>Are others responding privately perhaps?  :) At a minimum, I would like to state that the JTA transaction could be rolled back from an external thread in the following sections: The current wording is: " 7.9.1 Container Responsibilities ... *  After the JTA transaction has completed (either by transaction commit or rollback), the container closes the entity manager calling EntityManager.close. ... 7.9.2 Provider Responsibilities ... *  When the JTA transaction rolls back, the provider must detach all managed entities if the persistence context is of type SynchronizationType.SYNCHRONIZED or has otherwise been joined to the transaction. ... "</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Wed Mar 13 19:15:19 CET 2013</date>
    <body>Hi Scott, No.  I wish they were responding in *any* manner! As you point out, the JTA spec already allows this, so if that is all we were  to do, I'm not sure I see the point.   In case I am being dense though, can you tell me what words  you would like to see added to the spec. To me, the real issue seems to be whether we can/should provide any guidance  as to how to handle such situations.   I'd like to get the benefit of hearing from the vendors here as  to what their implementations do. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Updated javadocs</header>
    <date>Wed Mar 13 19:47:11 CET 2013</date>
    <body>I've uploaded new javadocs which reflect this fix.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Wed Mar 13 20:10:10 CET 2013</date>
    <body>For 7.9.1, how about something like: " After the JTA transaction has completed (either by transaction commit or rollback), the container closes the entity manager by calling EntityManager.close.  The JTA transaction may rollback in a background thread (e.g. transaction timeout), in which case, the container should arrange for the entity manager to be closed but the EntityManager.close() should not be concurrently called while the application is in an EntityManager invocation. " The 7.9.2 wording can be similar I think: " When the JTA transaction rolls back, the provider must detach all managed entities if the persistence context is of type SynchronizationType.SYNCHRONIZED or has otherwise been joined to the transaction.  The JTA transaction may rollback in a background thread (e.g. transaction timeout), in which case, the provider should arrange for the managed entities to be detached from the persistence context but not concurrently while the application is in an EntityManager invocation. "</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Wed Mar 13 20:53:47 CET 2013</date>
    <body>If anyone is interested, the background and discussions for this can be found at: https://hibernate.onjira.com/browse/HHH-7910 and https://github.com/hibernate/hibernate-orm/pull/476 As for what various providers do, we take an approach in Hibernate where: 1) While handling a sync callback, we check to see if the thread of execution is the same as the last known "main line" call into the EM. If it is not, we set a flag and stop processing the sync callback (for the SUCCESS case we just let the execution proceed regardless because for Hibernate there is no danger in that case). 2) Upon entry and exit from every public facing EM method we check that flag and process the "after completion" processing if needed. Why entry and exit?  Because we like to be through :)  The exit checks explicitly handle *concurrent calls* to the EM through the sync (while a call is in flight within the EM back on the application thread); the entry calls handle a slightly different, though related, case mainly in in-container execution.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Wed Mar 13 22:55:00 CET 2013</date>
    <body>I think this is reasonable.   Do any of you object to such language being  added to the spec?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 01:57:04 CET 2013</date>
    <body>Hi Evan, I understand your point about throwing the PersistenceException.  However,  when do you expect the container to call EntityManager.close()?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 02:13:44 CET 2013</date>
    <body>I do not object as this is an improvement on what we have today. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 02:14:04 CET 2013</date>
    <body>There is nothing in the spec now preventing a container from closing an EntityManager before the end of the bean method invocation or user transaction commit so IllegalStateExceptions could happen now. If preventing the close() is needed then that would need additional language likely something similar to what you propose. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 02:22:07 CET 2013</date>
    <body>Well, FWIW that was my initial thought.  However, this imposes more  complexity on the container implementation, so I'm not sure that it should be made a requirement.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 02:32:57 CET 2013</date>
    <body>Yes.  Interestingly enough, however, this particularly issue hasn't been  raised here in the 7 years since the release of JPA 1.0 :-) Given that the Java EE 7 release is set to close very shortly, I don't think  we can realistically impose this requirement on containers at this point.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 02:39:08 CET 2013</date>
    <body>At the point at which close is called, or other?  I.e., by the container or  the persistence provider?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 15:38:23 CET 2013</date>
    <body>Are you proposing that the JPA spec be changed, to throw a PersistenceException subclass instead of the IllegalStateException during all calls that can currently throw an IllegalStateException (as per javadoc comments)?  I didn't understand what you meant last night when I read your message but today *I think* that I am getting it better. I don't think that we can prevent IllegalStateException exceptions from being thrown but as to your proposal to use a wrapped exception for the documented known cases, I'm not against changing that at the right time. Rather than throw a PersistenceException, I would rather throw something like a IllegalPersistenceStateException or PersistenceIllegalStateException that is a subclass of IllegalStateException, so that application code doesn't need to change for this improvement. If this is a long term change for next time, I'd like to do a sweep of other exceptions as well (EJBException is one that I'd like to replace with a PersistenceException someday). Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 16:27:45 CET 2013</date>
    <body>This was my first thought also (you can see some of that earlier discussion in the jira that Steve Ebersole mentioned earlier). The short answer, is there is no current way to allow that to happen in a standard way. Currently, as a user, if you want the EntityManager.close to be called from the application thread (your same time requirement), the transaction should also end from the application thread as well.  The fact that your application depends on the EntityManager.close happening in the application thread, means you should use a transaction manager that handles transaction timeout lazily via Transaction.setRollbackOnly. I'm not sure why an application would/could have a dependency on the EntityManager.close occurring in the application thread though.  Could you give an example of how applications could be dependent on that exactly?  Please mention how that application will suffer, if the EntityManager.close occurs in a different thread but not concurrently while the application is using it. Currently, with the proposed spec change, I don't see how we are causing additional pain for either way of handling transaction timeout (immediate tx cancellation from background thread, tx.setRollbackOnly).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new sub-thread about handing "deadlock detected" error from the database...</header>
    <date>Thu Mar 14 16:47:36 CET 2013</date>
    <body>When the DBMS forces rollback, there is no concurrency problem today that I know of.  I'm not sure how clear I was before, probably not enough.  The concurrency issue is about a background thread calling EntityManager.close() while the application thread is in the middle of another EntityManager.close(). We all know that EnitityManager is not required to be thread safe, although some implementations offer thread safety as an option at the persistence unit definition level (which could slow application performance if enabled from the javadoc comments :). My "stop the presses" cry about this situation, is to bring awareness about this potential problem.  I didn't know if others were aware or not.  Now that we are all more aware of the issue, I think (everyone's) customers will expect us to fix the concurrency issue in our implementations whether we come to an agreement or not, to how we can avoid concurrency errors. Having said that, I'm very interested in hearing about the pain that your users are seeing and how we might be able to help when the DBMS returns a "deadlock detected error". If I understand your proposal, the fix would be to throw a specific error instead of the IllegalStateException.  Do you have a link pointing to a case that is accessible to us, that shows an example of an application getting a "deadlock detected" error and the exception call stack that goes with it? Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Stored Procedure Query updates</header>
    <date>Thu Mar 14 21:40:57 CET 2013</date>
    <body>Hello All,    Working through the TCK testing has indicated that the behaviour of the stored procedure queries is really not clear and could use some clarifications.  I propose the following updates to section 3.10.17.3 and the related JavaDocs (attached) on StoredProcedureQuery.java.    Below removed text has been styles with strikethrough and new text is in red.  For those with plain text email clients I have attached a ODF document as well The setParameter methods are used to set the values of all required IN and INOUT parameters. It is not required to set the values of stored procedure parameters for  which default values have been defined by the stored procedure. The case where there is only a single result set (or a single result) plus any results passed back via INOUT and OUT parameters is supported using the getResultList and getSingleResult methods.     When calling getResultList and getSingleResult on a stored procedure query the provider will call execute() on an unexecuted stored procedure query before processing the getResultList and getSingleResult. The case where there is only an update count plus any results passed back via INOUT and OUT parameters is supported using the executeUpdate method.     When calling executeUpdate on a stored procedure query the provider will call execute() on an unexecuted stored procedure query followed by a getUpdateCount.  The results of an executeUpdate will be those of getUpdateCount. The getOutputParameterValue methods are used to retrieve the values passed back from the procedure through INOUT and OUT parameters. The execute method supports both the simple case where scalar results are passed back only via INOUT and OUT parameters as well as the most general case (multiple result sets and/or update counts, possibly also in combination with output parameter values). The execute method returns true if the first result is a result set, and false if it is an update count or there are no results other than through INOUT and OUT parameters, if any. If the execute method returns true, the pending result set can be obtained by calling getResultList and getSingleResult .  The hasMoreResults method can then be used to test for further results. If execute or hasMoreResults returns false, the getUpdateCount method can be called to obtain the pending result if it is an update count. The getUpdateCount method will return either the update count (zero or greater) or -1 if there is no update count (i.e., either the next result is a result set or there is no next update count ). For portability, results that correspond to JDBC result sets and update counts need to be processed before the values of any INOUT or OUT parameters are extracted. After results returned through getResultList and getUpdateCount have been exhausted, results returned through INOUT and OUT parameters can be retrieved. The getOutputParameterValue methods are used to retrieve the values passed back from the procedure through INOUT and OUT parameters. For portability, results that correspond to JDBC result sets and update counts need to be processed before the values of any INOUT or OUT parameters are extracted. When using REF_CURSOR parameters for results sets the updates counts should be exhausted before calling getResultList to retrieve the result set.  Alternatively the REF_CURSOR result set can be retrieved through  getOutputParameterValue.  Result set mappings will be applied to REF_CURSOR results in the order the REF_CURSOR parameters were registered with the query In the simplest case, where results are returned only via INOUT and OUT parameters, execute can be followed immediately by calls to getOutputParameterValue. Attachment: storedprocwriteup.odt Description: application/vnd.oasis.opendocument.text Attachment: StoredProcedureQuery.pdf Description: Adobe PDF document Attachment: StoredProcedureQuery.java Description: Text document</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 22:03:57 CET 2013</date>
    <body>I have added the proposed language to the spec.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] new spec draft (Final Release candidate)</header>
    <date>Tue Mar 19 20:40:20 CET 2013</date>
    <body>I've just uploaded a new spec draft and javadocs to our project downloads area:  http://java.net/projects/jpa-spec/downloads These reflect the recent updates to the StoredProcedureQuery interface, clarifications with regard to transaction rollback on a background thread, and minor clarification to the result of the EMF.getCache method.  Changebars reflect changes since the Proposed Final Draft. We are closing in on the Java EE 7 target release date. Please therefore let me know if you find any issues with this draft, or any reason that it should not be submitted for the JPA 2.1 Final Release. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new spec draft (Final Release candidate)</header>
    <date>Tue Mar 19 20:53:24 CET 2013</date>
    <body>Hi Linda, In 7.9.2, we should change "container" to "provider": " When the JTA transaction rolls back, the provider must detach all managed entities if the persistence context is of type SynchronizationType.SYNCHRONIZED or has otherwise been joined to the transaction. Note that the JTA transaction may rollback in a background thread (e.g., as a result of transaction timeout), in which case the *provider* should arrange for the managed entities to be detached from the persistence context but not concurrently while the application is in an EntityManager invocation. " Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new spec draft (Final Release candidate)</header>
    <date>Tue Mar 19 22:36:31 CET 2013</date>
    <body>Hi Scott, Yes,  I will make the change. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new sub-thread about handing "deadlock detected" error from the database...</header>
    <date>Wed Mar 20 13:53:27 CET 2013</date>
    <body>Evan, From my point of view, there are multiple aspects to this discussion. Some of the aspects could be handled by non-JPA technologies that may not exist yet.  I think that the problem of avoiding concurrent EntityManager invocations has been around for a long time but just didn't become a priority until now. To meet the new JPA 2.1 requirements, the thread-unsafe EntityManager needs to be protected from concurrent access, however that can be accomplished by the container/provider respectfully. I think that some of the related aspects are: 1.  Informing the application that the tx timed out so that the *state of the world* may be collected (this is more of a management aspect than JPA IMO).  Currently, this is already possible (in vendor specific ways) and I'm not sure if we would want to standardize the notification of this event occurring somewhere (in some EE.future version).  In a previous (pre EE) Java application server that I'm familiar with, I liked how application code could receive notifications of events like this (e.g. applications could be notified if a database connection went bad so that the application could help notify someone to restart the db server). 2.  Having a consistent way for the application thread to learn that a tx timeout occurred.  I think that this was the point of your use case below but I don't believe the application will always see an IllegalStateException, although it could see an ISE.  The application will eventually see that the transaction ended but that might not happen until the CMT method returns (or BMT logic tries to end the TX that no longer exists).  Previous to the new 2.1 requirement for handling concurrency, applications could of seen various IllegalState like exceptions (e.g. NullPointerException).  Can we depend on the application *eventually* seeing that the tx disappeared when the application tries to end the TX?  Do we need more than that (in some EE.future version?) Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] minor bug in spec</header>
    <date>Wed Mar 20 19:50:33 CET 2013</date>
    <body>When I was reviewing the spec last night, I noticed the following problem. The restriction against propagating persistence contexts of type SynchronizationType.UNSYNCHRONIZED into PCs of type SYNCHRONIZED currently states: "If there is a persistence context of type SynchronizationType.UNSYNCHRONIZED associated with the JTA transaction and the target component specifies a persistence context of type SynchronizationType.SYNCHRONIZED, an EJBException is thrown by the container." This is inappropriate, since the components involved may not be EJBs. I plan to change it to state that the IllegalStateException is thrown. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] EMF.getProperties() and EM.getProperties()</header>
    <date>Thu Mar 21 14:02:44 CET 2013</date>
    <body>Working with the new TCK I ran across 2 situations I'd like to discuss with the group. First, the new TCK is now including assertions that EntityManager.getProperties() contains properties used to build the EntityManagerFactory that was used to create the EntityManager.  In my opinion the spec does not say anywhere that this should be the case. Can anyone point me to where it does if I am wrong (or else the TCK needs to be fixed for that)? Secondly, the types of properties the TCK is checking for got me thinking.  It checks for some sensitive information like JDBC url and user and password.  These are things we expose and are kind of forced to expose at the moment.  I'd like to suggest that we update the spec to explicitly state that some of these properties are not available period, at the very least javax.persistence.jdbc.password though I'd argue for javax.persistence.jdbc.url and javax.persistence.jdbc.user as well.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EMF.getProperties() and EM.getProperties()</header>
    <date>Thu Mar 21 17:06:07 CET 2013</date>
    <body>I don't believe it does.  This sounds like a bug. Are you referring to EMF.getProperties?  I wouldn't expect these to be  available from EM.getProperties. I think it is reasonable that an environment be able to restrict the  availability of such information. I think that adding something like the following to the discussion of the EMF  interface in the spec would be reasonable:    Note that the policies of the installation environment may restrict some  information from being made available through the getProperties method (for example,  JDBC user,  password, URL).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EMF.getProperties() and EM.getProperties()</header>
    <date>Thu Mar 21 18:59:28 CET 2013</date>
    <body>Thanks! Personally I'd like to filter those out in Hibernate implementation, but nervous about the TCK continuing to test for that information if this is not discussed in the spec.  So that would be great.  Thanks!</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] updated spec draft (Final Release candidate)</header>
    <date>Fri Mar 22 23:59:49 CET 2013</date>
    <body>This reflects the outcome of the discussion of the past week, including the changes to the exception raised by illegal propagation of persistence contexts.  I've also made a number of what I believe to be purely editorial changes.  The change bars flag the changes since the last draft (i.e., from the draft of March 19). The updated javadocs contain only changes to the documentation. The modifications were the following:   EntityManager.close:  added StoredProcedureQuery to the list   EntityManagerFactory.addNamedQuery: added references to  createNamedStoredProcedureQuery   SqlResultSetMapping:  name element:  added reference to StoredProcedureQuery JoinColumns, MapKeyJoinColumns, PrimaryKeyJoinColumn, PrimaryKeyJoinColumns:   foreignKey element: made documentation more uniform</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Schema generation with existing EMF</header>
    <date>Sun Mar 24 08:49:34 CET 2013</date>
    <body>I've been running into a conceptual issue with schema generation in JPA 2.1  and how unit tests are typically organized. Let's say you want a clean database for every unit test method. I prefer to  create the schema in the database before every test method, and then drop it  after the test method completes. I have a single EntityManagerFactory for the whole test class, it can be  shared by every test method. This is possible with the Hibernate API, the  schema generator uses an existing EMF configuration to produce the  create/drop scripts. With the static Persistence.generateSchema() in 2.1 this approach doesn't  work. Calling generateSchema() internally builds a new EMF every time. One of the problems is that you get new automatically generated foreign key  names every time you call generateSchema(). The "drop" action therefore  always fails, unless you define all foreign key names in your metadata. There  could be other automatically generated artifact names, so the "drop" action  in general, if called from a static context, is not useful during development. The current solution then is to specify "drop-and-create" and to build and  close an EMF for every unit test method. This means starting and stopping the  persistence provider for every test method, slowing down test runs  significantly. A better solution would be an additional Persistence.generateSchema(EMF,  properties) method that accepts an existing EMF and some "override"  properties.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Schema generation with existing EMF</header>
    <date>Mon Mar 25 19:23:04 CET 2013</date>
    <body>Hi Christian, This is an interesting proposal, but too much to consider in the remaining  couple of days that we have left in this release cycle. Could you please log this as a JIRA item RFE so we can track for future. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Why no AutoCloseable for EntityManager?</header>
    <date>Fri Mar 29 23:59:18 CET 2013</date>
    <body>Has the possibility of EntityManager extending AutoCloseable been considered? Surely EntityManager instances *should* be able to work with try-with-resources.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] AUTO: RICK CURTIS is out of the office (returning 04/08/2013)</header>
    <date>Sat Mar 30 04:02:27 CET 2013</date>
    <body>I am out of the office until 04/08/2013. While I am out of the office please contact my manager Nate Ziemann with any urgent issues. For technical issues contact Kevin Sutter. Note: This is an automated response to your message  "[jpa-spec users] Why no AutoCloseable for EntityManager?" sent on 03/29/2013 5:59:18 PM. This is the only notification you will receive while this person is away.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] modifying fields of a removed instance</header>
    <date>Tue Apr 02 17:42:30 CEST 2013</date>
    <body>Dear group, Am I right in the assumption, that reading or writing fields/properties of a removed entity (prior to commit) is allowed? I.e. the following sequence is valid: Thank you! Christian -- Christian von Kutzleben Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cvkutzleben@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: modifying fields of a removed instance</header>
    <date>Tue Apr 02 20:00:54 CEST 2013</date>
    <body>Hi Christian, Correct.  That is allowed.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: modifying fields of a removed instance</header>
    <date>Tue Apr 02 20:23:16 CEST 2013</date>
    <body>Well, more correctly... the spec is silent on the matter.  It does not say it is allowed, nor does it say it is disallowed.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: modifying fields of a removed instance</header>
    <date>Wed Apr 03 08:58:04 CEST 2013</date>
    <body>Hi Steve, Thanks for your reply! That was also our interpretation. We assumed, that this particular case should be explicitly disallowed in the spec, if the intention was, that this is not supported. So we do support this. Christian On Tue, Apr 2, 2013 at 8:23 PM, Steve Ebersole steve.ebersole@... Well, more correctly... the spec is silent on the matter.  It does not say it is allowed, nor does it say it is disallowed. Hi Christian, Correct.  That is allowed. Dear group, Am I right in the assumption, that reading or writing fields/properties of a removed entity (prior to commit) is allowed? I.e. the following sequence is valid: Thank you! Christian -- Christian von Kutzleben Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cvkutzleben@... cromberg@... www.versant.com http://www.versant.com www.db4o.com http://www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] updated javadocs</header>
    <date>Thu Apr 04 03:23:49 CEST 2013</date>
    <body>We caught a small glitch in the use of generics in the result of EntityManager.getEntityGraph. Instead of the signature of this method should be I've just uploaded new javadocs that reflect this to  http://java.net/projects/jpa-spec/downloads . There are no other changes to APIs or spec from what I uploaded on March 22. I will be submitting all required materials to the JCP shortly, in preparation for the final approval ballot. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Why no AutoCloseable for EntityManager?</header>
    <date>Thu Apr 04 14:25:06 CEST 2013</date>
    <body>Sorry to insist about this, and I know it's very late in the day for this version, but unless I've failed to see something this does look like a stupid oversight that is very easily fixed. EntityManager contains a close() method which has to be called on anything returned by emf.createEntityManager().  If the words "implements AutoCloseable" were added to the declaration of interface EntityManager then, without making any other changes, the try-with-resources statement could be used to ensure closure in all circumstances, thus:   // use em This is now the preferred idiom for anything that requires a close call. The only down side I can see to this change is that it would tie JPA to java 1.7 or later, since java.lang.AutoCloseable was only introduced in this version, but that doesn't seem much of a problem. Is there a good reason this hasn't been done?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Why no AutoCloseable for EntityManager?</header>
    <date>Thu Apr 04 14:52:35 CEST 2013</date>
    <body>Hi, Just my 2 cents regarding this, why I don't see this as useful: - EntityManager.close() has a special semantics, if a transaction is active, it does not free resources then - our implementation keeps resources while a transaction is active, so having a "transaction-scope" with a try could make sense*, but for "close" it does not bring us any advantage (might be different for other vendors) * also see the .NET System.Transactions API with using and the TransactionScope Regards, Christian On Thu, Apr 4, 2013 at 2:25 PM, jonty_lawson@... Sorry to insist about this, and I know it's very late in the day for this version, but unless I've failed to see something this does look like a stupid oversight that is very easily fixed. EntityManager contains a close() method which has to be called on anything returned by emf.createEntityManager(). If the words "implements AutoCloseable" were added to the declaration of interface EntityManager then, without making any other changes, the try-with-resources statement could be used to ensure closure in all circumstances, thus:   // use em This is now the preferred idiom for anything that requires a close call. The only down side I can see to this change is that it would tie JPA to java 1.7 or later, since java.lang.AutoCloseable was only introduced in this version, but that doesn't seem much of a problem. Is there a good reason this hasn't been done?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Why no AutoCloseable for EntityManager?</header>
    <date>Fri Apr 05 15:37:32 CEST 2013</date>
    <body> Just my 2 cents regarding this, why I don't see this as useful: Wow. So are you saying that calling close() on the EM is not important (at least for your implementation)? If this is the case in general then you are right and adding AutoCloseable to EntityManager would be a mistake. The javadoc for AutoCloseable states: "A resource that must be closed when it is no longer needed", and if this doesn't apply to EM instances then it's appropriate that it not be implemented. However, good citizen that I am, I had assumed that it was important that the close method be called on instances of EM, and that use of a try/finally was a good way of achieving this. This belief was fostered by: 1) The general usage of close methods within the java libraries.      Usually these require that close be called to free system resources      and it is effectively a programming error to fail to do so (eg in java.io,      java.net, java.sql, java.nio, javax.management and elsewhere). 2) The examples in the JPA spec that show close being called on an EM      in a number of places. Admittedly the JPA spec is silent on the *necessity* of calling close on application-managed EMs. I summary, I would say that either the JPA spec should be updated to state that calling close on an EM is optional (and explain when it should be called as is done for, eg, javax.naming.NamingEnumeration), or it should be changed to indicate that it is necessary and EM should implement AutoCloseable.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Why no AutoCloseable for EntityManager?</header>
    <date>Fri Apr 05 16:05:13 CEST 2013</date>
    <body>I summary, I would say that either the JPA spec should be updated to state that calling close on an EM is optional (and explain when it should be called as is done for, eg, javax.naming.NamingEnumeration), or it should be changed to indicate that it is necessary and EM should implement AutoCloseable.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] @Index annotation</header>
    <date>Sat Apr 06 07:20:20 CEST 2013</date>
    <body>I'm sure this has been covered before, but it's hard to search the mailing list archives for a word as generic as "index". I currently see no way to add an index for a column provided by a @MappedSuperclass (the @Table annotation cannot be used with @MappedSuperclass). Is this correct? And if so, what is the reason behind this restriction? Is there a way to specify indexes in a superclass without resorting to using @Entity with an inheritance strategy? MappedSuperclass lets you have your cake and eat it, too--it basically emulates "table per class" inheritance while you can still set the strategy to something else. Thanks, Alvin</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] EntityManagerFactory.addNamedQuery() stores a template?</header>
    <date>Tue Apr 16 00:22:15 CEST 2013</date>
    <body>Hello Linda,     From the JavaDoc of the newly introduced EntityManagerFactory.addNamedQuery(String name, Query q), I am assuming that the supplied query q is treated as a template as opposed to an instance as far as binding parameters are concerned. Let us consider a simple case  // we create a query      // and configure it     // and bind its parameter to a specific value      // let us declare this query to the persistence unit     // after a while we recall it         EntityManager em2 = ...;      // Then according to the JavaDoc the following assertions should succeed        assertTrue(query != query2); // the query we declared is a template and hence what is recalled is essentially a new one        assertEquals(51, query2.getFirstResult());    // but the new one  retains the first result position and other configuration of the template           // but the binding parameters are not memorized i.e   Will you please confirm if my reading of the spec in this regard is correct or not?         Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] scope of EntityGraphs</header>
    <date>Tue Apr 16 00:57:31 CEST 2013</date>
    <body>The scope of EntityGraph should be persistence unit, not persistence context. The method such as EntityManager.createEntityGraph(String name) etc. looks non-intuitive. If an application wants to create a graph at runtime,  it should call EntityManagerFactory.createEntityGraph(String name). As the API currently stands, it seems to allow EntityGraph of the same name be defined by two separate persistence contexts -- opening up a pandora's box of confusion. It is much safer to define these graphs scoped at EntityManagerFactory level and imposing uniqueness on the name of the graphs. Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: scope of EntityGraphs</header>
    <date>Tue Apr 16 01:37:51 CEST 2013</date>
    <body>It is.  However, an EntityGraph that is a mutable copy of a named entity  graph can be retrieved via the createEntityGraph method, modified, and then added back  to the EMF to replace the original one.   Given the use cases for entity graphs, it  seemed reasonable that they should be able to be retrieved via the EntityManager.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EntityManagerFactory.addNamedQuery() stores a template?</header>
    <date>Tue Apr 16 01:41:57 CEST 2013</date>
    <body>Yes</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: scope of EntityGraphs</header>
    <date>Tue Apr 16 04:24:17 CEST 2013</date>
    <body>Hello Linda,   Thanks for explaining the usage. But I am afraid it looks confusing/non-intuitive to me. 1. If the graphs are in EMF scope, why the following method is on EM?   /** * Return all named EntityGraphs that have been defined for the provided class type. * @param entityClass entity class * @return list of all entity graphs defined for the entity * @throws IllegalArgumentException if the class is not an entity */ 2. If two entitymanagers separately call createEntityGraph(String egName) but with the same input argument, they get two separate mutable copies -- right? Now if they mutate their respective copies differently and replace back to the EMF, which copy of the original graph wins? The other entitymanagers later when gets the graph by the same name will now receive a graph with new structure -- right?   So then what does immutability will signify in such case? 3. And above all, why all these needless complexity? Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Linda DeMichiel ---04/15/2013 04:38:18 PM---On 4/15/2013 3:57 PM, Pinaki Poddar wrote: &amp;gt; The scope of EntityGraph should be persistence unit, no From: To: jsr338-experts@... Date: 04/15/2013 04:38 PM Subject: [jsr338-experts] Re: scope of EntityGraphs It is.  However, an EntityGraph that is a mutable copy of a named entity graph can be retrieved via the createEntityGraph method, modified, and then added back to the EMF to replace the original one.   Given the use cases for entity graphs, it seemed reasonable that they should be able to be retrieved via the EntityManager. http://openjpa.apache.org/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Method signature in EntityGraph.addSubclassSubgraph()</header>
    <date>Wed Apr 24 21:04:40 CEST 2013</date>
    <body>Following method signature is defined in EntityGraph&amp;lt;T&amp;gt; interface     /**      * Add additional attributes to this entity graph that      * correspond to attributes of subclasses of this EntityGraph's      * entity type.  ...     **/     Should the correct signature be    public  &amp;lt;S extends Otherwise, the current signature &amp;lt;T&amp;gt; will hide Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Uniqueness of names of NamedQuery/StoredProcedure</header>
    <date>Wed May 01 22:09:33 CEST 2013</date>
    <body>The spec API uses name as the identifying moniker for named queries and stored procedures as in       public or               public While the name of a @NamedQuery is scoped within a persistence unit (Section 3.10.14, pp 150), does this scope also include the named stored procedures?   For example, can we define a NamedQuery and a StoredProedure with the same name? Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Uniqueness of names of NamedQuery/StoredProcedure</header>
    <date>Wed May 01 22:42:39 CEST 2013</date>
    <body>Is JPA still subject to change after the Final Draft? Werner On Wed, May 1, 2013 at 10:09 PM, Pinaki Poddar ppoddar@... The spec API uses name as the identifying moniker for named queries and stored procedures as in       public or               public While the name of a @NamedQuery is scoped within a persistence unit (Section 3.10.14, pp 150), does this scope also include the named stored procedures?   For example, can we define a NamedQuery and a StoredProedure with the same name? Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Uniqueness of names of NamedQuery/StoredProcedure</header>
    <date>Wed May 01 23:48:52 CEST 2013</date>
    <body>The spec doesn't say so explicitly (although it should), but I would not  expect this to work (or at any rate be portable). We'll need to clarify in future. What do you think the behavior should be?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Uniqueness of names of NamedQuery/StoredProcedure</header>
    <date>Wed May 01 23:49:54 CEST 2013</date>
    <body>Certainly!  But that would be in an MR or JPA 2.2 :-)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Uniqueness of names of NamedQuery/StoredProcedure</header>
    <date>Thu May 02 00:33:59 CEST 2013</date>
    <body>Hello Linda, Each of the query variants that can be recalled by a name must have a name that is unique in a persistence unit scope.   Effectively, my answer to the earlier question: "can we define a NamedQuery and a StoredProedure with the same name?" is "no". Nor can we define a NamedQuery(name="findAll") in two entity classes A and B. But spec already prohibits that. It is a common (or at least good) practice to qualify such common names by their class scope e.g. "Person.findAll" and "Order.findAll". So extending the uniqueness across all type of queries will not be difficult for the developers and will not invite unnecessary complexity for the providers. Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Linda DeMichiel ---05/01/2013 02:51:25 PM---On 5/1/2013 1:09 PM, Pinaki Poddar wrote: &amp;gt; The spec API uses name as the identifying moniker for na From: To: jsr338-experts@... Date: 05/01/2013 02:51 PM Subject: [jsr338-experts] Re: Uniqueness of names of NamedQuery/StoredProcedure The spec doesn't say so explicitly (although it should), but I would not expect this to work (or at any rate be portable). We'll need to clarify in future. What do you think the behavior should be? http://openjpa.apache.org/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] find and removed objects</header>
    <date>Wed May 08 16:30:14 CEST 2013</date>
    <body>Dear group, Is "find" supposed to be consistent with "contains", i.e. removed objects won't be found, even if the context is not yet synchronized to the database? Thank you! Christian</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Faces Flow Definition</header>
    <date>Mon May 13 21:10:49 CEST 2013</date>
    <body>I have been following the JSF 2.2 specification closely, nice work.  I have a question regarding the final implementation of Faces Flow.  It seems that the flow definition may be added to the faces-config.xml file, within the &amp;lt;flow-definition&amp;gt; elements as follows: .... This works well under the current builds.  However, as mentioned in some previous articles and within the spec itself, the flow definition can also reside within a file that is named /WEB-INF/&amp;lt;flowName&amp;gt;-flow.xml OR /&amp;lt;flowName&amp;gt;/&amp;lt;flowName&amp;gt;-flow.xml, where the flow definition XML file still follow the following format under the final release of the specification? xmlns=" http://www.w3.org/1999/xhtml"       xmlns:h=" http://xmlns.jcp.org/jsf/html"       xmlns:f=" http://xmlns.jcp.org/jsf/core"       xmlns:j=" http://xmlns.jcp.org/jsf/flow" ;&amp;gt;                 &amp;lt;j:flow-return id="returnFromB"&amp;gt;                .... If so, I am unable to get my examples working when specifying a flow definition within an XML file named after the flow, such as exampleFlow2-flow.xml.  Also, the latest builds of Netbeans 7.3 for EE 7 do not seem to recognize the  http://xmlns.jcp.org/jsf/flow URI . Thanks in advance for your help.  I am trying to document the proper usage of the faces flow feature and I want to ensure that I am not incorrect. Best</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Faces Flow Definition</header>
    <date>Mon May 13 21:30:17 CEST 2013</date>
    <body>Hi there, I think you meant to send this to the JSF users group. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Faces Flow Definition</header>
    <date>Mon May 13 22:28:06 CEST 2013</date>
    <body>Hi Linda, Indeed, a case of having too many tabs open.  Sorry for posting to the  incorrect group. Keep up the good work on JPA. Best Regards Josh Juneau http://jj-blogger.blogspot.com https://www.apress.com/index.php/author/author/view/id/1866  Hi there,    I think you meant to send this to the JSF users group.    regards,    -Linda       http://www.w3.org/1999/xhtml" http://xmlns.jcp.org/jsf/html" http://xmlns.jcp.org/jsf/core" http://xmlns.jcp.org/jsf/flow" ;&amp;gt;     http://xmlns.jcp.org/jsf/flow URI .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: find and removed objects</header>
    <date>Sat Jun 01 00:44:57 CEST 2013</date>
    <body>Hi Christian, Yes -- that is correct. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Fri Jun 07 17:40:57 CEST 2013</date>
    <body>Perhaps we should start a new thread if there is more interest in switching to a two phase approach to creating the container entity manager factory (for post 2.1).  I'm just now learning of a conflict between implicit CDI bean manager support and using ClassFileTransformers to enhance/rewrite entity classes.  I'd like to allow both but the features seem to conflict with each other. Knowing whether there is an explicit CDI bean manager, is easy to detect (beans.xml is found) but for the implicit support, we assume there is a bean manager if there is an EJB module.  When we create the container entity manager factory, if we pass in an implicit CDI bean manager, the CDI bean manager creation will scan application classes, which means ClassFileTransformers will register to late. Thoughts?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Tue Jun 11 16:18:13 CEST 2013</date>
    <body>I would like to introduce the least pain, when/if we switch to allowing a two phase bootstream of the container EMF.  I propose that we introduce a TwoPhaseBootstrap interface, if the persistence provider implements the TwoPhaseBootstrap interface, the methods can be used by the caller (e.g. EE container looking to create the container entity manager factory). Bootstrap IMO, this is a good way to allow full JPA support (ClassFileTransformers) and CDI implicit/explicit support (bean manager can be passed into the persistence provider).  Without this change, I'm afraid that all deployments with EJB modules, will need to choose between ClassFileTransformers working or use of CDI with entity listeners (performance vs functionality). Does anyone agree or disagree with making a change like the above asap? Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Availability of API jar files</header>
    <date>Thu Jun 13 19:51:03 CEST 2013</date>
    <body>Hi, I was just notified of the following JIRAs created by Matthew Adams: https://java.net/jira/browse/JPA_SPEC-19 (for JPA 2.0) https://java.net/jira/browse/JPA_SPEC-60 (for JPA 2.1) I also looked at the maven repo on java.net and found most of the other JSR API jar files:   https://maven.java.net/index.html#nexus-search;quick~javax Is there any reason why the API jar files are not posted for JPA?  I know the RI is EclipseLink, but it would be great to have all of the Java EE technologies available in a single repo... ---------------------------------------------------------------------------------------------------------------------------------------------------------------- Kevin Sutter, Java EE and Java Persistence API (JPA) architect mail:   sutter@..., Kevin Sutter/Rochester/IBM           http://webspherepersistence.blogspot.com/ phone:  tl-553-3620 (office), 507-253-3620 (office)                       http://openjpa.apache.org/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Availability of API jar files</header>
    <date>Sat Jun 15 05:00:24 CEST 2013</date>
    <body>Interesting, Kevin.  I wasn't even aware of that.  Thanks! All the more reason to get these API jars out there. On Thu, Jun 13, 2013 at 12:51 PM, Kevin Sutter sutter@... Hi, I was just notified of the following JIRAs created by Matthew Adams: https://java.net/jira/browse/JPA_SPEC-19 (for JPA 2.0) https://java.net/jira/browse/JPA_SPEC-60 (for JPA 2.1) I also looked at the maven repo on java.net and found most of the other JSR API jar files:   https://maven.java.net/index.html#nexus-search;quick~javax Is there any reason why the API jar files are not posted for JPA?  I know the RI is EclipseLink, but it would be great to have all of the Java EE technologies available in a single repo... ---------------------------------------------------------------------------------------------------------------------------------------------------------------- Kevin Sutter, Java EE and Java Persistence API (JPA) architect mail:   sutter@... , Kevin Sutter/Rochester/IBM           http://webspherepersistence.blogspot.com/ phone:  tl-553-3620 (office), 507-253-3620 (office)                       http://openjpa.apache.org/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Mon Jun 17 15:56:38 CEST 2013</date>
    <body>A few minor changes to the below proposed interfaces are here (they just need a package change to javax.persistence :-) https://github.com/scottmarlow/jipijapa/commit/db861170076cc9dbc4c07b6fb2facb30c0bc0e8e#L3R30 + https://github.com/scottmarlow/jipijapa/commit/db861170076cc9dbc4c07b6fb2facb30c0bc0e8e#L2R27</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Mon Jun 17 16:18:25 CEST 2013</date>
    <body>Having a two step deployment of the persistence unit is a good idea.  It would be a lot simpler to add a single "preDeploy" or "preCreate" method to the current SPI though. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Mon Jun 17 16:51:00 CEST 2013</date>
    <body>Why is adding a method "simpler"?  "Simpler" in what sense?  From a user/integration perspective I don't think it makes any difference. From a provider perspective, I think the proposed solution is far more conceptually correct and therefore simpler.  And I think these "conceptual" matches tend to work better from a user perspective as well (when an API and the concept it tries to expose/model match up...). I assume you mean adding preDeploy/preCreate to PersistenceProvider. Here is what I mean about "conceptuality": How does the provider track the preDeploy call above as relative to the eventual createContainerEntityManagerFactory call?  Meaning, imagine: Unless we say that a PersistenceProvider instance needs to now conceptually model a single "pu".  But thats a different set of assumptions than what we have today.  A separate contract makes that explicitly clear: There is also the fact that simply adding a method does not allow opting in or opting out of this 2-phaseness.  I guess that depends on how and if this group decides to accept this proposal in general.  And if so, specifically, whether we expect all providers to provide a 2-phase bootstrap mechanism.  Because if its not a requirement (aka, its optional) then having an optional contract is the best way to expose this period. As an additional suggestion, in the Hibernate version of this I actually added methods to "Bootstrap" to handle schema-generation as well.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Mon Jun 17 18:46:06 CEST 2013</date>
    <body>agreed. only the lifecycle hooks could occur as pair of preCreate()/postCreate(). Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware gordon yorke ---06/17/2013 07:29:16 AM---Having a two step deployment of the persistence unit is a good idea.  It  would be a lot simpler to From: To: jsr338-experts@... Date: 06/17/2013 07:29 AM Subject: [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes Having a two step deployment of the persistence unit is a good idea.  It would be a lot simpler to add a single "preDeploy" or "preCreate" method to the current SPI though. --Gordon https://github.com/scottmarlow/jipijapa/commit/db861170076cc9dbc4c07b6fb2facb30c0bc0e8e#L3R30   https://github.com/scottmarlow/jipijapa/commit/db861170076cc9dbc4c07b6fb2facb30c0bc0e8e#L2R27  </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Mon Jun 17 19:07:52 CEST 2013</date>
    <body>Hello All, The PersistenceProvider instance should remain scoped to the provider implementation and continue to manage multiple persistence units.  Any persistence provider can easily track the preDeploy call with the unique persistence unit name. Creating an additional interface to represent an instance of the persistence unit creates possible multiple instances of the same persistence unit which adds unneeded complexity.  Also, the additional interfaces do not provide any functional value and complicate the interface forcing containers to perform "instance of" checks or perform calls and check the result to determine if the new interfaces are supported.   Having the PersistenceProvider manage the persistence units allows the provider to ignore pre-deploy if it is not supported and allows containers to code to a single interface and not require rewriting to now manage an additional artifact should a provider support pre-deploy.  With the interfaces as proposed a container would need to have two code paths, one that interfaces with PersistenceProvider and another for the "bootstrap" interface. Alternatively,  if an artifact is needed an EntityManagerFactory could be returned in a pre-deployed state from createContainerEntityManagerFactory via a new parameter or property and activated through new API.  Although, some of the language in the specification on when certain artifacts are initialized (ie canonical metamodel) would need to change. --Gordon</body>
  </mail>
</mails>

