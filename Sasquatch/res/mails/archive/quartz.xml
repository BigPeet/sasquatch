<?xml version="1.0" encoding="UTF-8"?>
<mails>
  <mail>
    <header>Re: basic drawing: core graphics or app kit?</header>
    <date>Wed Feb 25 04:02:24 CET 2004</date>
    <body>The Cocoa classes fit better in the context of a Cocoa app, and are generally the more reasonable first choice. For instance, you can assign NSImages, NSColors, NSFonts, etc to various Cocoa elements, copy or paste them from NSPasteboard, archive them using NSArchiver, etc. So, in general you'll get better &amp;quot;impedance match&amp;quot; with the Cocoa types. However, the bottom line is pretty much all NS drawing goes through CG, and it's not too difficult to use the CG types in the context of a Cocoa app.  If you encounter any issues in using the NS classes (for instance, the exact functionality you want isn't there in NS but is available in CG, or you are given code which already uses the CG functions, or you have to go through extra steps which might introduce performance issues), then using the CG types is an option. Ali</body>
  </mail>
  <mail>
    <header>basic drawing: core graphics or app kit?</header>
    <date>Wed Feb 25 03:43:03 CET 2004</date>
    <body>Apologies if this is a faq. I've searched the Apple documentation and list archives, but can't seem to find an answer: There seems to be very similar routines for basic drawing (bitmaps, text, bezier curves) in both the Core Graphics and AppKit interfaces. For example, NSImage has a 'drawInRect' method, and Quartz has a CGContextDrawImage function that both appear to do the same thing: draw a scaled bitmap in a destination rectangle. My question is: How does one decide whether to use the lower level CG* routines or the NS* routines for basic drawing? -Mike</body>
  </mail>
  <mail>
    <header>Is CGContextDrawPDFDocument() multi-thread safe?</header>
    <date>Tue Feb 24 15:35:33 CET 2004</date>
    <body>Hi all, My PDF importer works well in a single thread environment, i.e. doing a single import operation. In multi-threading it blows up at the line containing CGContextDrawPDFDocument(). BTW the CGContextRef and CGPDFDocumentRef parameters I pass to CGContextDrawPDFDocument() are local and I do not do any UI stuff during the import operation. Any idea? Thanks JJ ___________________________________________________________ your friends today! Download Messenger Now</body>
  </mail>
  <mail>
    <header>Re: CalcCMask replacement?</header>
    <date>Tue Feb 24 01:51:12 CET 2004</date>
    <body>I'm pretty sure there isn't anything like this in Quartz, however the mask can be done in the alpha channel.  If it were me, I'd probably go ahead and call CalcCMask, then step through the bitmap pixel by pixel setting the alpha channel based on the value of the mask returned from CalcCMask. Nick</body>
  </mail>
  <mail>
    <header>Re: [ANN] Graphviz 1.10 (v4) for Mac OS X</header>
    <date>Tue Feb 24 01:26:57 CET 2004</date>
    <body>Apologies -- if the policy of this list is &amp;quot;no announcements&amp;quot;, I will not announce here. I thought it was relevant since my Graphviz port uses Quartz extensively as a code gen. X. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com</body>
  </mail>
  <mail>
    <header>CalcCMask replacement?</header>
    <date>Tue Feb 24 00:15:15 CET 2004</date>
    <body>In the QD environment when our users import a bitmap TIFF we use CalcCMask to get a mask of the image with the &amp;quot;interior&amp;quot; areas filled in. How would we do this in Quartz? Bryan</body>
  </mail>
  <mail>
    <header>[ANN] Graphviz 1.10 (v4) for Mac OS X</header>
    <date>Mon Feb 23 14:45:30 CET 2004</date>
    <body>Dear All, Thanks for your continued feedback for Graphviz on Mac OS X, it's good to hear from all the people out there playing and working with it. I've implemented the most requested feature -- automatic re-rendering when the dot file is modified in a different program -- which would make it useful to edit dot files in TextEdit for example. Download it, try it, and have fun! Changes ------------ Added re-rendering when file is modified on 10.3 (kqueue thread) [JSc]. Added CLI tools: gc, gvcolor, acyclic, nop, ccomps, sccmap, tred, unflatten and dijkstra. Added agraph library. Fixed open, close and reopen sometimes causing a crash (remove observers during dealloc) [SSe]. Speeded up tool response (prebound on 10.3). Tracked main build of 21 Feb 2004. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com P.S. Thanks especially to Jon Schull and Sanford Selznick, who even provided a crash report.</body>
  </mail>
  <mail>
    <header>Re: excluding a window from expose</header>
    <date>Sat Feb 21 05:25:50 CET 2004</date>
    <body>And for any of you who may not have heard this before, these calls are private to CoreGraphics and subject to change whenever Apple decides to change them. If you want support for this, file bugs requesting the feature at Radar. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: excluding a window from expose</header>
    <date>Sat Feb 21 04:36:23 CET 2004</date>
    <body>Am 21.02.2004 um 00:56 schrieb Jason Harris: The solution was posted in cocoa-dev. You can search it using cocoa.mamasam.com Here ist the relevant mail: ---------- Well, since nobody answered my post of yesterday, I've got my own hands dirty...   ;-) After some poking around, here is a new version of the method that makes windows ignoring Expose __and__ (Applause, please!) gets them back to normal behaviour: - (OSStatus)setSticky:(BOOL)flag if (flag) else Of course, like the other stuff, CGSClearWindowTags() has to be declared somewhere: extern OSStatus CGSClearWindowTags(const CGSConnection cid, const CGSWindow wid, int *tags, int Cheers, Andreas ________________________________________________________________________ __ Aksima Andreas Kummer Helene-Mayer-Ring 10/706 80809 Muenchen Germany Fon: ++49-89-15909290 Fax: ++49-89-15909289 Email: email@hidden ________________________________________________________________________ __</body>
  </mail>
  <mail>
    <header>Re: excluding a window from expose</header>
    <date>Sat Feb 21 00:56:53 CET 2004</date>
    <body>Unfortunately, I don't see a way to search the archives.  quartz-dev isn't listed on the search page and guessing doesn't work. Jason Harris [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: excluding a window from expose</header>
    <date>Sat Feb 21 00:29:20 CET 2004</date>
    <body>The archives are here the page (text &amp;amp; web pages) -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: Help needed with swapping issue around reading Jpeg with  CoreGraphics on 10.2.4 Server</header>
    <date>Fri Feb 20 21:57:57 CET 2004</date>
    <body>I note that you're releasing origImage but not data.  Are you retaining both of these in setImage? Nick</body>
  </mail>
  <mail>
    <header>Re: excluding a window from expose</header>
    <date>Fri Feb 20 21:53:11 CET 2004</date>
    <body>Hi Tyler, It has been discussed on the Cocoa-dev mailing list a few weeks ago. I didn't followed it in depth, so I'm not sure if it has been resolved. Take a look at &amp;lt;&amp;gt;. Eric ___________________________________________________________________ Eric Forget                       Cafederic</body>
  </mail>
  <mail>
    <header>excluding a window from expose</header>
    <date>Fri Feb 20 21:32:29 CET 2004</date>
    <body>Can anyone please explain how to create a window (preferably an NSWindow) that is excluded from being manipulated by exposi?  I wondered initially whether windows above (or below) a certain level might be excluded automatically. For example, the screensaver window and the desktop do not get manipulated. But alas, setting the window level to kCGScreenSaverWindowLevel (or kCGDesktopWindowLevel) had no beneficial effect. By the way, I didn't search the archives about this topic before posting because I couldn't find &amp;quot;quartz-dev&amp;quot; anywhere on the list archive search page (  ). Is it there under some other name, or is there some other way to search the quartz-dev archive, other than manually reading every previous thread? Thanks, Tyler</body>
  </mail>
  <mail>
    <header>Help needed with swapping issue around reading Jpeg with  CoreGraphics on 10.2.4 Server</header>
    <date>Fri Feb 20 20:45:56 CET 2004</date>
    <body>Hi We're having issues in .Mac production environment that started after I changed the code that generate thumbnails. I updated the code to read the JPG files using CoreGraphics instead of libjpeg in order to fix bugs we had we ColorSync not being respected, CMYK images, Grayscale images etc ... Since I've done that we started having memory issues. This is really weird. 6 instances of the process run on a machine, top shows that they stay between 15-30 RSIZE and between 130 - 160 VSIZE, but top shows that the free memory decrease steadily to the point were the machine has to be rebooted. It takes ~ 4 hours, and the box, when the app starts, has aroud 1.8 GB of RAM available. I've run leaks -cycles and it shows me leaks, but few of them. I attach a few of them. Of course these are the only process that run on that box. I would appreciate some qualified insider's look on the following code, and on the attached leaks reports. Moving to Panther isn't an easy option as this process are WebObjects 4.5.1 Objective-C. But bugs might have been fixed post 10.2.4 ? I would also appreciate any offer for a workaroud. Would eventually go to Quicktime layer myself to achieve the same goals could workaround some bugs ? Like in the DeveloperConnection example QTtoCG ? here is the init method that changed where I'm now using CG to get the data out of an image file. I'm doing the same with PNGs - initWithJPGFile:(NSString *)filename origImage = CGImageCreateWithJPEGDataProvider(provider, NULL,1, // setup some general info // create a context c = CGBitmapContextCreate([data mutableBytes],width,height,bitsPerComponent,bytesPerRow,cs,kCGImageAlpha // done.  release everything local // make sure it worked Thanks in advance, Benoit</body>
  </mail>
  <mail>
    <header>Re: Jaguar: Is the result of CGColorSpaceCreateDeviceRGB() a  cached object ?</header>
    <date>Fri Feb 20 01:02:08 CET 2004</date>
    <body>So it doesn't really matter if I call CGColorSpaceRelease(cs); or not ? Benoit</body>
  </mail>
  <mail>
    <header>Re: Jaguar: Is the result of CGColorSpaceCreateDeviceRGB() a  cached object ?</header>
    <date>Fri Feb 20 01:01:45 CET 2004</date>
    <body>Think of it as an optimization, rather than a behavior you can always rely on.  Some very frequently used invariant objects are kept around by the system.  The behavior may change in different releases. Your code has obtained a reference to a CGColorSpace object, and is responsible for releasing it.</body>
  </mail>
  <mail>
    <header>Re: Jaguar: Is the result of CGColorSpaceCreateDeviceRGB() a  cached object ?</header>
    <date>Fri Feb 20 00:49:19 CET 2004</date>
    <body>Yes; ColorSpaceRefs are ref-counted. --(jm)</body>
  </mail>
  <mail>
    <header>Jaguar: Is the result of CGColorSpaceCreateDeviceRGB() a cached  object ?</header>
    <date>Fri Feb 20 00:37:49 CET 2004</date>
    <body>Hi In a server (Jaguar) application with no UI, I keep creating colospace objects with CGColorSpaceCreateDeviceRGB(), but each time I get the same object back, is that expected ? Thanks, Benoit</body>
  </mail>
  <mail>
    <header>Re: Symbol font and quartz</header>
    <date>Thu Feb 19 16:39:18 CET 2004</date>
    <body>On Feb 17, 2004, at 7:37 PM, Andreas Mayer wrote: Here's a post from the carbon-dev list just a couple of days ago... On Feb 17, 2004, at 7:01 PM, Deborah Goldsmith wrote:</body>
  </mail>
  <mail>
    <header>Re: Symbol font and quartz</header>
    <date>Wed Feb 18 02:37:21 CET 2004</date>
    <body>Am 18.02.2004 um 01:17 schrieb stefano iacus: Hm. What's wrong? I can use special characters in TextEdit just fine. Are you sure the characters you want are even defined in Mac Roman encoding? Try to use Unicode instead. bye.  Andreas.</body>
  </mail>
  <mail>
    <header>Symbol font and quartz</header>
    <date>Wed Feb 18 01:17:46 CET 2004</date>
    <body>Hi all, I'm getting mad with displaying Symbol font graphics with quartz under Panther. I need essentially to print Integrals, summations symbols etc. Till Jaguar, my code works fine if I set the font with something like but under Panther there's no way to display such a font. I've seen that no Cocoa application (mine is a carbon one) like TextEdit (and even the font panel!) displays the font correctly. Is this font messed under Panther for some reason? Do you have some hints? stefano p.s. I apologize for cross posting.</body>
  </mail>
  <mail>
    <header>How expensive is CGContextSave/RestoreGState?</header>
    <date>Tue Feb 17 00:40:28 CET 2004</date>
    <body>We might have to call the Save/Restore pair quite a bit and I would like to know if those are expensive calls.   While I'm asking... how expensive is calling CGContextClip()? Thanks, -Mike</body>
  </mail>
  <mail>
    <header>Re: Drawing a Portion of an Image</header>
    <date>Mon Feb 16 19:22:03 CET 2004</date>
    <body>Ahhhhh... I guess I'd do the same thing with CGShading? Kind of a funky way to go about it. Seth Willits ------------------------------------------------------------------------ --- President and Head Developer of Freak Software - REALbasic Guru at ResExcellence - Webmaster for REALbasic Game Central - ------------------------------------------------------------------------ ---</body>
  </mail>
  <mail>
    <header>Re: Drawing a Portion of an Image</header>
    <date>Mon Feb 16 19:11:27 CET 2004</date>
    <body>You'd clip to the area you wanted to see and draw the image at full size.  For example: CGContextSaveGState CGContextClipToRect CGContextDrawImage CGContextRestoreGState Nick</body>
  </mail>
  <mail>
    <header>Drawing a Portion of an Image</header>
    <date>Mon Feb 16 18:52:17 CET 2004</date>
    <body>CGContextDrawImage will resize the image to fit within the rect. What I'd like to do is have it draw the image at the original size, but only draw part of it. How would I do this? Seth Willits ------------------------------------------------------------------------ --- President and Head Developer of Freak Software - REALbasic Guru at ResExcellence - Webmaster for REALbasic Game Central - -- Seth Willits ------------------------------------------------------------------------ ---</body>
  </mail>
  <mail>
    <header>Re: Quartz Drawing Weirdness</header>
    <date>Mon Feb 16 18:22:44 CET 2004</date>
    <body>Oh, I see what Nick meant. Awesome. That was the problem. Thanks for the help guys, Seth Willits ------------------------------------------------------------------------ --- President and Head Developer of Freak Software - REALbasic Guru at ResExcellence - Webmaster for REALbasic Game Central - -- A Bringer of Truth ------------------------------------------------------------------------ ---</body>
  </mail>
  <mail>
    <header>Re: Quartz Drawing Weirdness</header>
    <date>Mon Feb 16 16:45:29 CET 2004</date>
    <body>Your clearing fill is likely happening someplace you don't expect because you are not setting the origin of the rect that you are attempting to clear. So at runtime who knows the location that you will be filling and it will likely vary over time. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Quartz Drawing Weirdness</header>
    <date>Sun Feb 15 20:09:26 CET 2004</date>
    <body>I forgot to say the what is on the left is what it starts out as, and on the right is what it ends up as. That image is a splice of two separate screen shots. Seth Willits ------------------------------------------------------------------------ --- President and Head Developer of Freak Software - REALbasic Guru at ResExcellence - Webmaster for REALbasic Game Central - -- ------------------------------------------------------------------------ ---</body>
  </mail>
  <mail>
    <header>Re: Quartz Drawing Weirdness</header>
    <date>Sun Feb 15 19:21:57 CET 2004</date>
    <body>I'll assume the rectangle size is correct but you really should also set the origin.  FYI, it's easier if you post the code with the question rather than linking to it. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Quartz Drawing Weirdness</header>
    <date>Sun Feb 15 18:40:04 CET 2004</date>
    <body>The applicable code: The effect I'm seeing: I'm calling this function on every iteration of my game loop, however it will only actually draw anything if it needs to (ie, the score changes, you die, etceteras). More often than not, it initially draws normally, (in the actual function that draws everything else too, I often will get a white background instead of black) but the problem is that instead of erasing and drawing everything fresh, the score text is drawn on top of itself several times, and then after 5 or 6 times, it will draw correctly and the process starts all over. The code above is the simplest form in which I can reproduce the problem. I have no idea what is causing this, but it seems to me like Quartz isn't getting a chance to do everything it needs. Am I missing something? Thanks, Seth Willits ------------------------------------------------------------------------ --- President and Head Developer of Freak Software - REALbasic Guru at ResExcellence - Webmaster for REALbasic Game Central - ------------------------------------------------------------------------ ---</body>
  </mail>
  <mail>
    <header>Re: Porting from QuickDraw to Quartz</header>
    <date>Wed Feb 11 22:44:45 CET 2004</date>
    <body>On Feb 11, 2004, at 3:26 PM, Nick Nallick wrote: Yep, it was rhetorical. I'm trying to demonstrate how people coming to Quartz from QuickDraw might think. And with the documentation as it stands today, something this simple can take a while to track down. Bryan</body>
  </mail>
  <mail>
    <header>Re: Porting from QuickDraw to Quartz</header>
    <date>Wed Feb 11 22:26:05 CET 2004</date>
    <body>This may have been a rhetoric question, but I'll throw it out anyway. For bitmap and PDF context's there are specific ways to get the size but in general you can use CGContextGetClipBoundingBox. Nick</body>
  </mail>
  <mail>
    <header>Re: Porting from QuickDraw to Quartz</header>
    <date>Wed Feb 11 22:00:05 CET 2004</date>
    <body>I think this is a great idea. For example, today we needed to draw &amp;quot;page guides&amp;quot; -- the document window's content would be light shaded except for the page area. In QuickDraw we start this by calling GetPortBounds. But there is no CGGetContextBounds, so what do we use? Do we need to have information about the window associated with this context at our lower levels?  (By the way, it might be handy to think about adding properties to contexts ala the window, menu, control properties in Carbon -- it's very handy sometimes to be able to tie arbitrary data to an OS object.) Looking here: Quartz_2D_Ref/qref_main/APIIndex.html offered nothing, but we found CGContextGetClipBoundingBox in the headers which can do what we need in this case. But this is an example where explaining why there is no equivalent of GetPortBounds would help QuickDraw developers understand Quartz a lot better. Bryan</body>
  </mail>
  <mail>
    <header>Re: Documentation Needs</header>
    <date>Wed Feb 11 18:41:04 CET 2004</date>
    <body>This sample shows how to implement simple Quartz 2D replacements for the QuickDraw rounded rect and oval drawing functions. strokeOval, fillOval, strokeRoundedRect, and fillRoundedRect implement Quartz 2D equivalents to the QuickDraw oval drawing and rounded rect APIs, using CGRects instead of Rects. See DrawWindow() for sample usage. Dave</body>
  </mail>
  <mail>
    <header>Re: Documentation Needs</header>
    <date>Wed Feb 11 18:15:39 CET 2004</date>
    <body>Yeah I was aware of that, thanks, but I think if you are asking about overall documentation needs for Quartz there has to be a more cohesive feel to it with more of an emphasis on the QD programmer moving to it. Bryan</body>
  </mail>
  <mail>
    <header>Re: Documentation Needs</header>
    <date>Wed Feb 11 18:13:12 CET 2004</date>
    <body>Am 11.02.2004 um 17:37 schrieb Bryan Pietrzak: ovalteen.htm Manfred</body>
  </mail>
  <mail>
    <header>Re: Documentation Needs</header>
    <date>Wed Feb 11 17:37:44 CET 2004</date>
    <body>I'd go back to Inside Macintosh: QuickDraw and take a look at this style and approach. What's missing is simple, concise, practical examples... 1. For example, how can a QD developer replace the Round Rect calls? (someone really really needs to write a little QD &amp;quot;emulation&amp;quot; library &amp;quot;MoreQuartz&amp;quot; that can help this transition) 2. more examples on shaders, including custom shaders 3. examples of loading an image from the app bundle's resources folder and drawing it somewhere (these are real-life problems that developer will face) 4. PS to PDF converter 5. Marching Ants for 2004 -- how to show selection areas in the OS X way using overlays 6. examples on drawing 1 pt thick lines that look like 1 pt thick lines (and perhaps how to do so regardless of the contexts scale -- perhaps someone just wants to draw some lines around some interface element, but they don't want that magnified along with the 7. More examples demonstrating the differences in coordinate systems and how context scaling impacts that. For the non PS programmer, it's source of confusion I'm sure there's more, but off the top of my head, that's  start. I think the big thing is more practical examples ala the old IM. Bryan</body>
  </mail>
  <mail>
    <header>pure black becoming mixed color</header>
    <date>Wed Feb 11 09:51:32 CET 2004</date>
    <body>Hello! We have a publishing app which (among other things) reads print-ready PDF files and writes them out, combined in one big PDF page. The problem is with the black color in DeviceGray colorspace, which becomes color-managed and is not pure black any more (gets some non-zero CMYK values other than K). If the color is in CMYK color space, everything is ok, because CMYK colors do not change. Is there any way this can be prevented or corected (some kind of filter?) ? The problem is that just reading PDF and writing it back messes up the colors in document... izidor P.S. Also, the additional data such as OPI comments are just stripped away. It would be nice to be able to hook into Quartz and support reading/writing additional PDF data. In this way one could also decide whether to attach icc profile to a color without it or not...</body>
  </mail>
  <mail>
    <header>Minimum Font Smoothing Size</header>
    <date>Wed Feb 11 01:26:31 CET 2004</date>
    <body>I have an app, used for creating insurance forms, that draws a lot of small text into a CG context via ATSUI.  Some of this small text falls below the system's minimum text smoothing size in the Appearance preference panel (even at the minimum 4 point size).  When this happens there is a dramatic difference in appearance between this small text and the imperceptibly larger text next to it.  This difference is maintained even if I enlarge the image by scaling the CTM. I could be wrong, but I'm assuming this is a Quartz issue rather than an ATSUI issue because this is a system-wide preference. I would like to override this system-wide minimum in my app.  How can I do this? Thanks, Nick</body>
  </mail>
  <mail>
    <header>Anti-Aliasing Quality when Drawing PDF</header>
    <date>Tue Feb 10 22:34:04 CET 2004</date>
    <body>I noticed that Preview draws embedded bitmap images anti-aliased, but when I draw PDFs myself bitmap images are aliased. How do I control the anti-aliasing quality when drawing PDF? CGContextSetInterpolationQuality() on the context has no apparent effect. Manfred</body>
  </mail>
  <mail>
    <header>Re: CGContext to CGImage?</header>
    <date>Sat Nov 13 00:33:41 CET 2004</date>
    <body>On Nov 12, 2004, at 4:25 PM, Tomas Franzén wrote: There is no generalized way to convert a CGContext into anything else. What you can do is create a CGBitmapContext that uses the same backing bitmap buffer as a CGImage, then draw into the bitmap context.  It's a little round about, but it gets you there. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>CGContext to CGImage?</header>
    <date>Sat Nov 13 00:25:23 CET 2004</date>
    <body>I want to use an icon as the image of a drag operation. I'm using GetIconRefFromFile to get the icon as an IconRef. From here, it seems like I can throw the icon into a CGContext by using PlotIconRefInContext. However, setting the drag image requires a CGImage... So my question is basically: Is there a way to convert a CGContext into a CGImage  - or - is there a simpler way of doing all this? Tomas Franzén Lighthead Software I'm listening to Machines of Loving Grace - Golgotha Tenement Blues</body>
  </mail>
  <mail>
    <header>vImageConvolve_PlanarF crashes with power of 2 rowbytes</header>
    <date>Sat Nov 13 00:10:37 CET 2004</date>
    <body>I was working with vImageConvolve_PlanarF and the program repeatedly crashed for no apparent reason that I could see.  Looking at the vImage documentation again, I came across the page about buffer size and alignment which has the following comment: should not be a power of 2. If it is, performance may be adversely affected for some functions. (This is a side effect of how some PowerPC machines handle address arithmetic internally.) When I changed my vImage buffer allocation code to match the code given in that section, the only &amp;quot;real&amp;quot; change was that I added the part which ensured that the rowBytes field was not an even power of 2.  Not surprisingly, the crashes stopped. Where can I learn more about how &amp;quot;some PowerPC machines handle address arithmetic internally&amp;quot; to discover why my code was crashing before I added the additional bytes? Should  vImageConvolve_PlanarF not check the rowBytes so that it will not crash when the data is not set up as expected?  I'll report a bug soon. Scott</body>
  </mail>
  <mail>
    <header>Color space selection</header>
    <date>Fri Nov 12 21:38:43 CET 2004</date>
    <body>I remember some discussion at WWDC related to the &amp;quot;best practices&amp;quot; when creating color spaces for CGImages and the like.  Unfortunately, I don't remember which session it was in and have not had any luck finding it. As I recall, from a general perspective, using a device space (i.e. CGColorSpaceCreateDeviceRGB) is &amp;quot;not what I want to do&amp;quot; in most cases. (I seem to recall that Tiger will be adding routines for getting a &amp;quot;better&amp;quot; default RGB space). More to the point, however, if the user is selecting RGB colors from the system color panel, and I've collected those RGB values then used them to create an image in an RGB frame buffer.  When I want to create a CGImage from that frame buffer should I use CGColorSpaceCreateWithName and kCGColorSpaceUserRGB, or would I use CGColorSpaceCreateDeviceRGB?</body>
  </mail>
  <mail>
    <header>Re: Converting PDF graphics to other formats</header>
    <date>Thu Nov 11 20:47:06 CET 2004</date>
    <body>PDF is a scaleable graphics meta-file format (a series of drawing commands) and PNG is a raster file format.  The basic problem you have is that you need to convert the meta-file into raster format and then ask the PNG exporter to export the raster data. One of the easiest ways to do that is to create and offscreen GWorld, of the size you want for your PNG, using QTNewGWorldWithPtr.  On top of that GWorld you can create a CGBitmapContext and draw the PDF into that context using CGContextDrawPDFPage.  Once you have done so then you can point the graphics output component at the GWorld and ask it to export the image from there. I think your problems with the graphics export component will go away once you have rasterized the PDF.  QuickTime is unable to transcode the PDF because it really doesn't know (for example) what resolution you want to draw the PDF at when putting in into the PNG.  You're going to have to take charge of the rasterization of the PDF yourself.</body>
  </mail>
  <mail>
    <header>Re: Converting PDF graphics to other formats</header>
    <date>Thu Nov 11 19:36:16 CET 2004</date>
    <body>... Try drawing into a CGBitmapContext, then using libpng. You should be able to do this with or without the intermediate PDF stage. -- Jens Ayton</body>
  </mail>
  <mail>
    <header>Converting PDF graphics to other formats</header>
    <date>Wed Nov 10 20:32:11 CET 2004</date>
    <body>I've converted my app to draw using Quartz, but I need to be able to &amp;quot;capture&amp;quot; parts of my windows into an in-memory &amp;quot;metafile&amp;quot; format. I used to do this with PICT, but in the brave new world, I am using PDF, which I capture by drawing into a CGPDFContext. (Using PDF for this might be my first mistake, but what's the alternative?) Now that I have a graphic in PDF format, I want to convert it to something else, like PNG. How do I do that? Back in the old days, I would use QuickTime's GraphicsExport facility, but that has numerous problems with PDF. Does it even support PDF? I also have to use Handles everywhere, instead of CGDataProvider/CGDataConsumer. I ended up creating both a GraphicsImporter and a GraphicsExporter. The importer imports PDF  from a handle and the exporter exports to PNG from my importer. Here's some code: // Open a PDF importer // Open a PNG exporter // Copy PDF data into a Handle // Assign PDF handle to importer's input // Assign importer to exporter's input // Assign PNG handle to exporter's output // Let fly the dogs of war Everything proceeds error-free until that last step, where I get -50 (paramErr) returned from GraphicsExportDoExport. Help! Is there some way to do this without QuickTime? Do I need to convert to something else first? (A CGImageRef perhaps?) --Michael -- -- email@hidden JMP, a Business Unit of SAS Institute -- http://www.jmp.com/</body>
  </mail>
  <mail>
    <header>One last scaling question</header>
    <date>Sun Nov 07 21:26:54 CET 2004</date>
    <body>I am almost there.  My only problem now is that when the user resizes the window, what ever was rendered into my view before does not scale properly.  I can get new renderings to be scaled by the updated transform; however, NSView's build in scaling is blowing out the scale to be way too large. Any suggestions on how to have what ever is already rendered to the view scale properly?  I have thought of caching the view, and applying my own CTM to it, and then blipping that back onto the screen...but NSView will still do its thing to I suppose.</body>
  </mail>
  <mail>
    <header>Re: Live Scaling of NSView and animation</header>
    <date>Sun Nov 07 19:41:07 CET 2004</date>
    <body>Thanks.  You put me on the right track. Here is the code that got it to work: bounds].size.height != origHeight)) Then I created a rect from the scaled point and size, and sent that to DrawRect. Works great. B Scott, Well, you are kind of on to my situation but not exactly.  All the notification setup I am fine with, it is getting the new scale of the view (or the delta from its base scale) after it was enlargened that I am after.  I understand the fact that normally a view would scale its self; however, the nature of my rendering is what is unique.  I am updating speicfic 12x6 tiles several times a second.  Each tile will need the scale transformation applied to it inorder to properly render the pixels.  It is getting the scale from the view after it is modified that I am interested in.</body>
  </mail>
  <mail>
    <header>Re: Live Scaling of NSView and animation</header>
    <date>Sat Nov 06 15:54:38 CET 2004</date>
    <body>Scott, Well, you are kind of on to my situation but not exactly.  All the notification setup I am fine with, it is getting the new scale of the view (or the delta from its base scale) after it was enlargened that I am after.  I understand the fact that normally a view would scale its self; however, the nature of my rendering is what is unique.  I am updating speicfic 12x6 tiles several times a second.  Each tile will need the scale transformation applied to it inorder to properly render the pixels.  It is getting the scale from the view after it is modified that I am interested in.</body>
  </mail>
  <mail>
    <header>Re: Live Scaling of NSView and animation</header>
    <date>Sat Nov 06 04:40:35 CET 2004</date>
    <body>Scott, Well, you are kind of on to my situation but not exactly.  All the notification setup I am fine with, it is getting the new scale of the view (or the delta from its base scale) after it was enlargened that I am after.  I understand the fact that normally a view would scale its self; however, the nature of my rendering is what is unique.  I am updating speicfic 12x6 tiles several times a second.  Each tile will need the scale transformation applied to it inorder to properly render the pixels.  It is getting the scale from the view after it is modified that I am interested in. Thanks, Ben All, I am trying to implement a live scale of an NSView that is updating frequently with an animation.  The animation is a series of pixel tiles.  I figure I  not only need to scale the pixel matrix, but also the rect that I am using to update the view.  I have found the method to use to update the rect, and actually applying the CTM to the matrix is not a problem, but what I can not seem to find is a way of getting the new scale of the view when a resize finishes.  I figured I would use the notification WindowDidResize to update the CTM.  Is there a way of getting the new scale of the window's subviews at the time of notification?  I also want to constrain the view to maintain its aspect ration as it is made larger or smaller.</body>
  </mail>
  <mail>
    <header>Re: Live Scaling of NSView and animation</header>
    <date>Sat Nov 06 00:25:37 CET 2004</date>
    <body>All, I am trying to implement a live scale of an NSView that is updating frequently with an animation.  The animation is a series of pixel tiles.  I figure I  not only need to scale the pixel matrix, but also the rect that I am using to update the view.  I have found the method to use to update the rect, and actually applying the CTM to the matrix is not a problem, but what I can not seem to find is a way of getting the new scale of the view when a resize finishes.  I figured I would use the notification WindowDidResize to update the CTM.  Is there a way of getting the new scale of the window's subviews at the time of notification?  I also want to constrain the view to maintain its aspect ration as it is made larger or smaller.</body>
  </mail>
  <mail>
    <header>Live Scaling of NSView and animation</header>
    <date>Sat Nov 06 00:05:10 CET 2004</date>
    <body>I am trying to implement a live scale of an NSView that is updating frequently with an animation.  The animation is a series of pixel tiles.  I figure I  not only need to scale the pixel matrix, but also the rect that I am using to update the view.  I have found the method to use to update the rect, and actually applying the CTM to the matrix is not a problem, but what I can not seem to find is a way of getting the new scale of the view when a resize finishes.  I figured I would use the notification WindowDidResize to update the CTM.  Is there a way of getting the new scale of the window's subviews at the time of notification?  I also want to constrain the view to maintain its aspect ration as it is made larger or smaller.</body>
  </mail>
  <mail>
    <header>Re: Claification about VRAM, AGP, and DMA</header>
    <date>Wed Nov 03 23:38:25 CET 2004</date>
    <body>On Nov 3, 2004, at 2:24 PM, R. Scott Thompson wrote: Correct. Correct. It's also documented with the Apple OpenGL extensions docs: vertex_array_range.txt Correct.  And to correct myself, I should have said &amp;quot;AGP Fast Writes Yes. Mostly correct.  DMA is just a GPU initiated transfer, the bus used (AGP/PCI) doesn't change the nomenclature the DMA term.</body>
  </mail>
  <mail>
    <header>Re: Claification about VRAM, AGP, and DMA</header>
    <date>Wed Nov 03 23:24:13 CET 2004</date>
    <body>So the GPU is able to access the texture information directly out of system ram without having to pull it into VRAM.  This is also called AGP Texturing, or is AGP texturing something altogether different? I gather from some posts you made on the OpenGL lists that GL_STORAGE_SHARED_APPLE can be used for more than just textures.  In particular it looks like it can be used for vertex arrays as well.  Is that the case? It looks like all this stuff is &amp;quot;documented&amp;quot; with sample code alone.  I think I'll send in a bug report. So DMA is the case when the GPU is doing  the heavy lifting. PCI memory exchange mechanism). Can a video card transfer memory back and forth to an AGP graphics card through the &amp;quot;regular&amp;quot; PCI bus? (I barely know what I'm talking about here not being terribly familiar with the way that PCI cards and the rest of the system exchange information) And so DMA is a special case of memory exchange (on the AGP bus?) where the exchange is handled by the memory system on the card without having to involve the main CPU.  The computer is also capable of transferring memory over the AGP bus using the host CPU to mediate the transfer but that's not DMA.  Is that closer to right?</body>
  </mail>
  <mail>
    <header>Re: Claification about VRAM, AGP, and DMA</header>
    <date>Wed Nov 03 22:14:34 CET 2004</date>
    <body>On Nov 3, 2004, at 12:37 PM, R. Scott Thompson wrote: Textures can reside in system memory only and never be copy to VRAM. This can be done by calling glTexParameteri(target, GL_STORAGE_SHARED_APPLE, GL_TRUE) before glTextImage*. DMA (Direct Memory Access) is when the GPU is pushing or pulling data across the AGP bus.  It's called &amp;quot;Direct Memory Access&amp;quot; because when the GPU is doing the operation it is directly accessing system memory. The CPU is not directly involved in the transfer. Another form of transfer is AGP Fast Writes and it slower forms AGP Reads / Writes.  These are when the CPU is pushing (writes) or pulling (reads) the data across the AGP bus. Before system memory can be DMA'd it must be mapped into the AGP GART (Graphics Address Relocation Table).  The GART is simply a memory scatter / gather table for the graphics controller.  All DMA transfers go through the GART, so not mapping a memory range into the table before issue a DMA would be a bad thing.</body>
  </mail>
  <mail>
    <header>Claification about VRAM, AGP, and DMA</header>
    <date>Wed Nov 03 21:37:45 CET 2004</date>
    <body>I have a really, really basic question to ask about the finer points of using video RAM.  Please forgive me as the answer is probably taken for granted by anyone &amp;quot;in the know&amp;quot; that it hardly occurs to them to ask the question. I understand that in order for the GPU to operate on some image data, that image data must reside in a texture.  I also understand that different video cards can handle different geometries for their textures (arbitrarily rectangular, power-of-2 dimensions only, square dimensions only etc...) and the capabilities of any particular OpenGL setup can be queried using OpenGL routines. My question concerns the role of Video RAM, textures, and the capabilities of the video card to work with them.  Fundamentally the question I have is &amp;quot;Can the video co-processor (GPU) work with data (textures) that reside in the system RAM or is it restricted to working ONLY with data that resides in VRAM?&amp;quot;.  To put it another way, do textures have to be &amp;quot;shipped&amp;quot; to VRAM before the GPU can compute with them, or is there another mechanism? - The GPU can work with textures that are stored in VRAM. (pretty much a given... but worth restating) - DMA  refers to a technology that allows the graphics card to quickly transfer data (including textures) from system RAM to VRAM quickly. Where I get confused is with the AGP technology.  Some of the information I've found on the 'net leads me to believe that AGP refers to different speeds of DMA for shuffling data between system ram and VRAM.  Other discussions leave me with the impression that AGP gives the video card a mechanism to directly access the system RAM so a texture doesn't have to be moved to VRAM before the GPU can work with it.  Still more discussions suggest that AGP refers to a separate &amp;quot;special&amp;quot; memory area. Could some kind soul who doesn't mind cluing in the clueless clarify the roles of DMA, and AGP in working with image data and the GPU?</body>
  </mail>
  <mail>
    <header>Is it really possible to use ATSUI and Quartz together in device	independent manner?</header>
    <date>Tue Nov 02 13:42:44 CET 2004</date>
    <body>After switching to Quartz I thought I should not think about what physical device I am drawing onto, but apparently world is not so perfect. When I use ATSUI and Quartz together the ATSUDrawText() does not draw small fonts (ones for which font smoothing is turned off in System Preferences-&amp;gt;Appearance) correctly . It uses fractional metrics for non-antialiased fonts and result is not very good. It is reproducible by launching TextNameTool (from Sample Code) with parameters: echo 'Antialiasing' | ./TextNameTool -x -  -f Geneva -s 10  -o out.jpg Setting line layout options to  kATSLineUseDeviceMetrics | kATSLineNoAntiAliasing fixes the problem for small fonts, but whether font small or not depends on System Preferences settings and whether drawing happens on monitor or on paper. I do not think it is good idea to make code dependendable on kind of device used for drawing. Please correct me if I am wrong, but the right solution for the problem would be to make ATSUI deciding whether to use integral metrics or fractional ones depending on kind of CGContext and its settings. This way no extra code synchronizing ATSUI behavior with System Preferences settings is needed and one should not keep extra flag for CGContext - whether it is for printing or on-screen drawing. Aleksandr Furmanov MetaCommunications Engineering</body>
  </mail>
  <mail>
    <header>Re: How to do PDF encryption/decryption?</header>
    <date>Tue Nov 02 08:28:53 CET 2004</date>
    <body>Hi, I think the reason should be what I have stated below: The Quartz API in Mac OS X 10.3 does not support the last security handler available in Acrobat 6.0 and later. The particular PDF document I used was created by Adobe Acrobat Professional where in the Password Security Settings dialog I chose the value for Compatibility as &amp;quot;Acrobat 6.0 and later&amp;quot;. The other possible values for Compatibility are &amp;quot;Acrobat 5.0 and later&amp;quot; and &amp;quot;Acrobat 3.0 and later&amp;quot;. I also tried the encrypted PDF with either of these two values. The Quartz API in 10.3 can decrypt and display them successfully. Another way to prove this: I have tried to open each of the three encrypted PDF files (with difference Compatibility values) by Preview: -- For the encrypted PDFs with either &amp;quot;Acrobat 5.0 and later&amp;quot; or &amp;quot;Acrobat 3.0 and later&amp;quot;, Preview will ask for the password, and then after provided with the corrected password, display the file, -- but when opening the one of Compatibility &amp;quot;Acrobat 6.0 and later&amp;quot;, Preview even cannot recognize the file is password-protected. Preview doesnt ask for password but just display all the pages as white page. Referring Section 3.5, PDF Reference (4th Edition), revision 4 of the standard security handler in PDF is only introduced in PDF 1.5. Will Quartz in Tiger support these latest security features in PDF? regards, Yu</body>
  </mail>
  <mail>
    <header>Re: New Quartz 2D related documentation</header>
    <date>Mon Nov 01 18:19:41 CET 2004</date>
    <body>Unfortunately it's not under my control. Last I checked it was still planned. Attachment:</body>
  </mail>
  <mail>
    <header>Re: New Quartz 2D related documentation</header>
    <date>Mon Nov 01 18:13:33 CET 2004</date>
    <body>On Jul 7, 2004, at 2:24 PM, Haroon Sheikh wrote: Has anything happened with this?  I'm starting to work with CoreImage and it would be a large help to be able to be able to discuss it.  It's been about four months since WWDC but I haven't heard anything new. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Re: How to do PDF encryption/decryption?</header>
    <date>Mon Nov 01 17:08:15 CET 2004</date>
    <body>Can you file a bug report on this particular PDF document. It appears that this may be a crypt filter and isn't support by 10.3. Can you also comment in the bug report, what the source of the PDF document was and how it was encrypted. Attachment:</body>
  </mail>
  <mail>
    <header>Re: set DPI using CoreGraphics</header>
    <date>Thu Aug 31 19:04:01 CEST 2006</date>
    <body>One thing you definitely want to add to your code is releasing the CGImageDestinationRef once you have finalized it and you should release the properties dictionary you create. Other than that I don't see a problem with your code. Here's code from the &amp;quot;Programming with Quartz&amp;quot; book that creates an image file from a CGImageRef, a destinationURL, the target output format, and the dpi for the image. It definitely works correctly. The only obvious difference I see between your code and this code is that you don't call CFRelease on the CGImageDestinationRef and the fact that this code uses pure CoreFoundation routines to create the dictionary entries for the dpi keys. static void exportCGImageToFileWithDestination(CGImageRef image, CFURLRef url, CFStringRef outputFormat, float dpi) // Create an image destination at the supplied URL that // corresponds to the output image format. The destination will // only contain 1 image. CGImageDestinationRef imageDestination = // Set the keys to be the x and y resolution of the image. // Create a CFNumber for the resolution and use it as the // x and y resolution. values[0] = values[1] = CFNumberCreate(NULL, kCFNumberFloatType, // Create an options dictionary with these keys. options = CFDictionaryCreate(NULL,  (const void **)keys, (const void **)values,  2,  &amp;amp;kCFTypeDictionaryKeyCallBacks, // Release the CFNumber the code created. // Add the image with the options dictionary to the destination. // Release the options dictionary this code created. // When all the images are added to the destination, finalize it. // Release the destination when done with it. Hope this helps, David</body>
  </mail>
  <mail>
    <header>Re: CGPostKeyboardEvent questions</header>
    <date>Tue Aug 29 23:47:17 CEST 2006</date>
    <body>On Aug 29, 2006, at 1:37 PM, patrick machielse wrote: The Command key virtual key code is 55 or 0x37. about 1/3 down the page.  I've attached it for those who get the RTF version of this mail: Mike Paquette email@hidden</body>
  </mail>
  <mail>
    <header>Watermark - hidden in Adobe Reader, visible in Quartz PDF apps</header>
    <date>Thu Aug 31 17:49:25 CEST 2006</date>
    <body>A user sent me a PDF file which is displayed in Adobe Reader without a watermark.  When printing, the watermark appears on each page. However, when viewing in Preview.app, or any other Quartz PDF based application, the watermark is visible in the middle of each page. This is unexpected.  When printing, the watermark appears as expected. Is there a bug in the handling of the Hidden annotation flag when the Print flag is set?  Why isn't the watermark invisible when viewing with Preview.app or my own application? Is there a way around this to emulate the correct behaviour exhibited by Adobe Reader? Regards, Simon</body>
  </mail>
  <mail>
    <header>Re: toggle mouse drag with</header>
    <date>Thu Aug 31 01:35:49 CEST 2006</date>
    <body>On Aug 30, 2006, at 1:18 PM, patrick machielse wrote: Doesn't sound likely, but I don't know for sure.</body>
  </mail>
  <mail>
    <header>Re: toggle mouse drag with</header>
    <date>Thu Aug 31 01:30:40 CEST 2006</date>
    <body>That's a FEATURE! Seriously, it is for Safari. When you've been looking at screens for the last 14 hours and have a splitting headache, the last thing you can take is yet another &amp;quot;clever&amp;quot; web page full of sophomoric animated GIFs, Flash compositions, Java ads, etc. flickering in your face. Since Safari doesn't give you any way to turn off such animation (does it?), clicking on the scroll bar or right-clicking the mouse (bringing up a contextual menu) does the trick in restoring serenity to the screen. Instant suspended animation! I'm serious -- this is a feature, at least until Safari gets a &amp;quot;Disable animations&amp;quot; checkbox. Hmm, I seem to remember OmniWeb having this feature, once upon a time...?</body>
  </mail>
  <mail>
    <header>Re: toggle mouse drag with</header>
    <date>Wed Aug 30 22:18:46 CEST 2006</date>
    <body>Could the event tracking mode also have something to do with the other strange thing I'm observing (CGPostMouseEvent won't work if the mouse pointer is over a menu owned by the application calling CGPostMouseEvent, but will work in menus owned by other programs)? patrick</body>
  </mail>
  <mail>
    <header>Re: toggle mouse drag with</header>
    <date>Wed Aug 30 21:51:22 CEST 2006</date>
    <body>On Aug 30, 2006, at 12:41 PM, patrick machielse wrote: Technically, what's happening here is that Safari updates its windows from a timer, and the timer is installed in a particular mode of the event loop (the default mode). However, when dragging a scrollbar or tracking a menu, the event loop is running in a different mode (the event tracking mode), and Safari's timer isn't installed in that mode, so it doesn't fire. The Safari team is aware of this issue and is tracking it for a possible future fix.</body>
  </mail>
  <mail>
    <header>Re: toggle mouse drag with</header>
    <date>Wed Aug 30 21:41:12 CEST 2006</date>
    <body>It does work in Safari, and in Terminal. I can't really see a pattern in the behaviour between applications. - Mouse down in text area + command-q: app quits on mouse up. - Mouse down on scroll bar + command-q: app doesn't quit on mouse up. - Mouse down in menu bar (menu opened) + command-q: app quits immediately. It seems that event handling behaves differently in these three circumstances. I never really investigated this, (alas), but some things about event handling in OS X have long intrigued / annoyed me as a user. A prime example of strangeness is Safari (or maybe it just is more apparent there). Once the mouse is down in a scroll bar handle, the application just freezes. Rendering is suspended. Same thing goes for dropping down a menu. Rien ne va plus. I think this is more or less what used to happen in System 7 as well, and really disappointing. Open a handful of tabs and Safari soon becomes unresponsive. The lesson seems to be that the side effects of 'mouse down' depend on the targeted application. patrick</body>
  </mail>
  <mail>
    <header>Re: toggle mouse drag with</header>
    <date>Wed Aug 30 12:24:52 CEST 2006</date>
    <body>I will submit a request for this too. Hmm, these may be the 'undocumented special cases and undesirable side effects' the documentation refers on numerous occations. I still don't really grasp why the code I posted earlier makes my application/system become paralytic for an _indefinite_ period of time. I'll experiment with the events filter and see if I can prevent that situation... patrick</body>
  </mail>
  <mail>
    <header>Re: toggle mouse drag with</header>
    <date>Wed Aug 30 01:26:11 CEST 2006</date>
    <body>On Aug 29, 2006, at 3:52 PM, patrick machielse wrote: There is a reference document at: QuartzEventServicesRef/Reference/reference.html#//apple_ref/c/func/ CGSetLocalEventsSuppressionInterval Yes.  Querying the system's state returns the logical OR of all mouse buttons or keyboard keys.  With this API you'll need to track your synthetic mouse button state yourself. I'd look at just why you are trying to control the system at that level, rather than using AppleScript or the Accessibility APIs. Manipulating the low level events entering the system can be problematic, particularly when you are attempting to use that same stream of events to control your manipulation. During a remote control operation such as a mouse drag, most remote control programs do not want a user at a local keyboard to alter a drag by pressing keys.  (Such keypresses can turn a drag and drop move into a copy, link, or other undesirable operation, for example.)   There are suppression states for both normal operations and drag operations that your program can modify.  These values should be set before posting an event.  The values are latched with the synthetic event posted from the calling process and propagate through the system. To turn off all suppression states, so your event generation, including drags, looks like just another device: /* Permit all local events during the interval after we post an event */ CGSetLocalEventsFilterDuringSuppressionState (kCGEventFilterMaskPermitAllEvents,kCGEventSuppressionStateSuppressionIn /* Permit all local events during a remote control mouse drag */ CGSetLocalEventsFilterDuringSuppressionState (kCGEventFilterMaskPermitAllEvents,kCGEventSuppressionStateRemoteMouseDr Mike Paquette email@hidden</body>
  </mail>
  <mail>
    <header>Re: toggle mouse drag with</header>
    <date>Wed Aug 30 00:52:40 CEST 2006</date>
    <body>[] OK, this is useful information. Is there a conceptual document somewhere on this matter? I believe my mistake then is that I'm polling the hardware mouse for the toggleOn parameter, where  I should be remembering the state of my software mouse instead? What alternatives exist for manipulating the mouse pointer? I noticed that there is updated API for Tiger (don't know if that has the same issues), but I need Panther support. I suppose this kind of thing should be possible. So far, just bringing up the force quit panel will break the lock of the virtual mouse. If it comes up... What I don't understand is why the software mouse blocks all regular keys, and sometimes even option- command-esc. patrick</body>
  </mail>
  <mail>
    <header>Re: CGPostKeyboardEvent questions</header>
    <date>Wed Aug 30 00:25:05 CEST 2006</date>
    <body>I'm using AXUIElementPostKeyboardEvent, but my Keycode for the command key is 0x37 (55, not 50).</body>
  </mail>
  <mail>
    <header>Re: toggle mouse drag with</header>
    <date>Tue Aug 29 23:55:00 CEST 2006</date>
    <body>On Aug 29, 2006, at 2:05 PM, patrick machielse wrote: Well, you've pressed the mouse button down, but have not let it up. The hardware mouse can generate a mouse down and up, but your synthetic mouse button is still down.  Since all button-down states are logically ORed together, the applications (including yours!) won't see a mouse-up until you release your synthetic mouse button. Remember that these APIs are meant for remote control of a computer. Trying to use local mouse operations to generate remote control operations acting on the same machine is problematic at best. Killing the app that has generated the synthetic mouse down will generally restore the system to normal operation. Mike Paquette email@hidden</body>
  </mail>
  <mail>
    <header>toggle mouse drag with</header>
    <date>Tue Aug 29 23:05:46 CEST 2006</date>
    <body>The problem is that it doesn't work. After calling this code, I am no longer able to click the mouse (hardware) so in a sense my machine becomes unresponsive. Sometimes I can fix this by bringing up the Force Quit panel, but not always. The only thing left at that point is rebooting... How Classic. Obviously something dangerous is happening here. However, I can't find out from the documentation or  from a web search _what_, and how to properly toggle mouse dragging from software. Any help welcome. In a related issue: I use this CGPostMouseEvent() for moving the mouse programmatically. This works as expected (or better: as advertised) _except_ when the cursor is over an NSMenu owned by my Cocoa application. In that case the mouse pointer does not move. Is this known behaviour, is it logical, and is there a workaround? patrick</body>
  </mail>
  <mail>
    <header>CGPostKeyboardEvent questions</header>
    <date>Tue Aug 29 22:37:52 CEST 2006</date>
    <body>I hope I'm posting this to the correct list. If not, please redirect me... I'm trying to post a 'paste' command (command-v key combination) using CGPostKeyboardEvent with the following code: CGPostKeyboardEvent( (CGCharCode)0, (CGKeyCode)50, true );   // command key down CGPostKeyboardEvent( (CGCharCode)0, (CGKeyCode)50, false );  // command key up CGSetLocalEventsSuppressionInterval(0.25); // default I've used AsyncKeys! to establisth the virtual key codes. However, when this code executes, it posts `v, not 'command-v'. Sure enough the ` (backtick) key has key code 50... How do I differentiate between the ` and the command key? This should be really simple, I just don't see it at the moment :-(. I'm calling this from a Cocoa application, and inside a Carbon hotkey callback. Just so you know. patrick</body>
  </mail>
  <mail>
    <header>Re: NSLoging CIContext fixes my problems?</header>
    <date>Mon Aug 28 18:27:45 CEST 2006</date>
    <body>The logging has no influence on the CIContext, so the problem is still a scheduling issue. It sounds like that you use multiple threads and one context, so there could be an OpenGL threading issue where you don't lock the context correctly. Use the OpenGL profiler and break on thread errors.</body>
  </mail>
  <mail>
    <header>NSLoging CIContext fixes my problems?</header>
    <date>Sat Aug 26 20:46:11 CEST 2006</date>
    <body>We have a video capture app that has been working fine for us on our PPC iMac and even on my MacBookPro.  However, when we switched to a new Intel iMac (and also tested on a new Intel Mac Mini), our video capture returns 640x480 blank frames.  The capture app runs without a UI.  After much gnashing of teeth, I found the line: myCIContext = [[CIContext contextWithCGLContext:myCGLContext So I thought &amp;quot;must be a timing problem&amp;quot; and put some sleeps around just to try to make it break or work differently, and it gives me a blank frame every time unless i NSLog the CIContext.  So then I moved the NSLog of the CIContext out of that method entirely and moved it somewhere else (the constructor of the class that this call is in) and that ALSO fixes it. So my question is whether there is anything in NSLog'ing the CIContext that might cause something to be initialized that otherwise might not be initialized when run without a user interface?  This same code when called from a Java application with a user interface DOES NOT fail even without NSLog'ing the CIContext. Thanks alot! ms</body>
  </mail>
  <mail>
    <header>URL vs. CFData For Image Source</header>
    <date>Wed Aug 23 21:00:34 CEST 2006</date>
    <body>I'm playing with ImageIO and I've found a strange case with a particular Nikon raw image.  Apparently the image file contains a thumbnail image.  When I import the image by creating an image source from a URL I get the normal high resolution image, but when I import the image by first reading it into a CFDataRef and creating my image source from that I only get the thumbnail. Is this a bug or am I doing something stupid?  I've used the same code to import a larger Canon raw image without trouble. Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: CGPathContainsPoint problem?</header>
    <date>Wed Aug 23 15:35:41 CEST 2006</date>
    <body>On Aug 22, 2006, at 11:48 PM, David Catmull wrote: Several people (myself included) have noticed problems with CGPathContainsPoint returning incorrect results in some cases. In my case, I was able to reduce the number of false reports by testing against the bounding box of the curve first (as I recall) (so if the point is not in the bounding box of the curve, don't bother checking to see if it's inside the curve).</body>
  </mail>
  <mail>
    <header>Re: Quartz for Quicktime Display?</header>
    <date>Wed Aug 23 15:32:31 CEST 2006</date>
    <body>On Aug 23, 2006, at 12:58 AM, Colin Doncaster wrote: It's probably the best way to do it.  Core Video provides a mechanism for letting you decompress video frames and get them to the video card very quickly.  OpenGL is the lingua franca of the video card and is the tool that lets you actually use those video frames after they've been shipped across the graphics card bus.</body>
  </mail>
  <mail>
    <header>Re: Newbie question about the Quartz Python binding.</header>
    <date>Wed Aug 23 09:28:08 CEST 2006</date>
    <body>Hello. Ok, I understand that it wasn't very clear what I were trying to do. And it was not my intention to be impolite - sorry if someone got this impression. I have a couple of html pages with a lot of more or less complicated tables (rowspan, colspan, you name it). I would like to convert those tables into PDF. Until now, I am doing this with the PDFlib (PHP binding). Although I feel rather comfortable working with the library, until now PDFlib had very limited support when it comes to tables (with 7.0 it seems to be another story - I haven't had the time yet to test the new version). I was very exited when I saw that there was a Python binding for Quartz, because it seemed to be a very easy way to do this task. import os from CoreGraphics import * from urllib2 import * pageOutline = CGRectMake(0, 0, 612, 792) context = CGPDFContextCreateWithFilename(&amp;quot;/Users/myself/myPDF.pdf&amp;quot;, pageOutline) context.beginPage(pageOutline) test = context.drawHTMLTextInRect(CGDataProviderCreateWithString (html), pageOutline, 12) context.endPage() As long as all CSS is included (copy and paste the CSS into the html source), the page is rendered properly into the output pdf - which means that I am able to set the font, row lines, background color of the table and so on with the style sheet. But when I link the CSS to the html, the CSS file is not recognized/included - neither are linked graphics, such as background pic or any other linked graphics by the way. Quartz is able to render html pages (because &amp;quot;drawHTMLTextInRect&amp;quot; is part of CoreGraphics, which is part of Quartz - as far as I understand). My problem is: How can I get the script to include linked CSS and linked graphics when rendering the html page? When I open the linked CSS document with &amp;quot;css = urlopen(&amp;quot;http:// 127.0.0.1/test.css&amp;quot;).read()&amp;quot;, include it with &amp;quot;test = content.drawHTMLTextInRect(CGDataProviderCreateWithString(css), pageOutline, 12)&amp;quot; I only get plain text from the CSS. Scott Thompson suggested using WebView in order to render the html page and dump the output to a pdf. Are there any other alternatives? I hope that I was able to explain my question more precisely now for you guys. With best regards and thank you for your time, /frank arensmeier 21 aug 2006 kl. 18.36 skrev Jeffrey Oleander:</body>
  </mail>
  <mail>
    <header>Quartz for Quicktime Display?</header>
    <date>Wed Aug 23 07:58:53 CEST 2006</date>
    <body>I'm starting an app that will be displaying quicktime movies in a custom view, I noticed that a lot of the quartz CoreVideo apps that utilize quicktime actually use an NXOpenGLView and opengl for drawing the quicktime video as a texture to a square poly.  Is this the standard, quickest/best way to draw/display the current quicktime frame or was that chosen for other reasons?  Would it be better to use Quartz class for this? Thanks.</body>
  </mail>
  <mail>
    <header>CGPathContainsPoint problem?</header>
    <date>Wed Aug 23 06:48:40 CEST 2006</date>
    <body>I'm working on an open source Cocoa SVG-based drawing app (see http:// xvg.sourceforge.net for details - developers wanted!), and at the moment I'm having a problem with hit testing on my shapes. It works on one computer but not on the other, and the call I'm using is CGPathContainsPoint. The computer where it works is a PowerBook on 10.4.7. The one where it fails is a G4 also on 10.4.7 (I have since installed the Leopard seed on the G4, but the failure predates that so I'm tentatively ruling that out as a cause). I tried a fresh checkout of the code, found that it still fails on the G4, copied the project folder to the PowerBook, and saw it succeed there. Of course I can't say for sure that my problem lies in my use of CGPathContainsPoint, but I'm running out of ideas. If you're willing to have a look at the code, the offending method is -[XVGShape hitTestImpl:inContext:] -- David Catmull email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGContextDrawImage error catching</header>
    <date>Wed Aug 23 03:40:47 CEST 2006</date>
    <body>On 15 Aug 2006, at 20:12, Marc Van Olmen wrote:</body>
  </mail>
  <mail>
    <header>Re: Problem: 16-bit TIFF's and Image IO with	Float	CGBitmapContextCreate</header>
    <date>Mon Mar 12 10:31:51 CET 2007</date>
    <body>I've been researching more on that, and came to the following conclusions which may probably also help you with a workaround. The original CGImageRef retains the full dynamic of the input file, it is only drawing to a bitmap context that will convert that in 8-bit somewhere. Now, if you create a CGImageDestination from the image, and if you take tiff as the output type, it will retain 16-bit per pixels. So what you may do is create a CGImageDestination with data : that way you keep everything in memory, which may be really quick if you minimize the data stream in the whole process. Then you can parse the resulting tiff file in memory, which is not very much complicated, considering that in the end, you only have to make one parser for a whole range of input files. You may not need to make a full tiff parser also, since you have some knowledges of the input file. However, for my particular case, there is a problem with profiles that I'll explain in a different mail.</body>
  </mail>
  <mail>
    <header>Capture drawing commands</header>
    <date>Mon Mar 12 10:17:44 CET 2007</date>
    <body>Hi, I would like to capture the drawing commands sent by all applications to the graphics card.&amp;nbsp; How can I achieve this please?&amp;nbsp; Should I write a Thanks, Jo</body>
  </mail>
  <mail>
    <header>Re: PDF document physical size</header>
    <date>Fri Mar 09 15:44:25 CET 2007</date>
    <body>On Mar 9, 2007, at 7:35 AM, AstroK Software wrote: On Panther and later the best way is to call CGPDFDocumentGetPage() and CGPDFPageGetBoxRect().  There are a number of &amp;quot;boxes&amp;quot; you can get and there are subtle differences between them.  I generally get the &amp;quot;media&amp;quot; box for the bounds. Nick</body>
  </mail>
  <mail>
    <header>Re: PDF document physical size</header>
    <date>Fri Mar 09 15:53:18 CET 2007</date>
    <body>On Mar 9, 2007, at 8:35 AM, AstroK Software wrote: Each page can be on a different size of paper.  What I think you want to do is use CGPDFDocumentGetNumberOfPages and CGPDFDocumentGetPage to retrieve each page of the document.  Then, for each page, you can ask CGPDFPageGetBoxRect for kCGPDFMediaBox which is the bounds of the paper behind that page in points.  To convert points to millimeters: const float kPointsPerMillimeter = kPointsPerInch / kMillimetersPerInch; // 1mm * (1in / 25.4mm) * (72pt / 1in) const float kMillimetersPerPoint = kMillimetersPerInch / Simply multiply the dimension of each side of the page by kMillimetersPerPoint.</body>
  </mail>
  <mail>
    <header>Re: PDF document physical size</header>
    <date>Fri Mar 09 15:51:12 CET 2007</date>
    <body>Thanks, but what is the unit of the result? It looks like it is in pixels... How do I convert it in centimeters? -- AstroK Software Arthur VIGAN email@hidden</body>
  </mail>
  <mail>
    <header>Re: PDF document physical size</header>
    <date>Fri Mar 09 15:47:28 CET 2007</date>
    <body>On Mar 9, 2007, at 8:35 AM, AstroK Software wrote: Individual pages in a PDF document can have different bounds, so it's best to use CGPDFPageGetBoxRect. HTH, Jon -- Jonathan Johnson email@hidden REAL Software, Inc. REAL World 2007 Conference May 9 - 11 in Austin, Texas</body>
  </mail>
  <mail>
    <header>PDF document physical size</header>
    <date>Fri Mar 09 15:35:14 CET 2007</date>
    <body>I have a CGPDFDocumentRef, and I would like to know its physical dimensions in centimeters or millimeters. Is there an easy way to do that? Thanks a lot in advance. -- AstroK Software Arthur VIGAN email@hidden</body>
  </mail>
  <mail>
    <header>Re: Problem: 16-bit TIFF's and Image IO with	Float	CGBitmapContextCreate</header>
    <date>Fri Mar 09 02:39:14 CET 2007</date>
    <body>Just for record sake because maybe people in the future will look into this through archiving. I want to indicate that it seems that Apple is still doing it through an 8-bit pipeline. other more recent threads confirm this. regards, marc . On 16 Feb 2007, at 21:08, Marc Van Olmen wrote:</body>
  </mail>
  <mail>
    <header>Re: Problem with more than 8-bit images with CGBitmapContextCreate</header>
    <date>Thu Mar 08 15:44:28 CET 2007</date>
    <body>Thanks for the answer, your previous one made me really wonder what could be wrong into my code. Finally I've found a very old discussion (2005), and it seems as if everything drawn on the Bitmap context will be drawn in 8 bit, then converted to floats, if you're bitmap context is in floating point format. I wondered how an application like Preview would transform a raw files into a tiff files while preserving the dynamic range, and I think that it might be because the CGImage never gets drawn in a bitmap context, so that internally the path would stay float to an Image Destination. As for previewing, I guess that the CGImage is directly passed to a CIImage, which will retain float values. The 2005 state seems to be the same as today. But what about 10.5 ? Will it be supported ? As anyone more info on this ?</body>
  </mail>
  <mail>
    <header>Problem with more than 8-bit images with CGBitmapContextCreate</header>
    <date>Thu Mar 08 15:20:57 CET 2007</date>
    <body>because I was author of previous e-mail with the kCGImageSourceShouldAllowFloat answer came up. I can tell you that didn't work finally neither, I haven't had the time yet to get back to this issue. But my initially reaction was too soon. so currently it doesn't work neither. I have one advantage that my files are in 16-bit tiff in that case Quicktime should work.</body>
  </mail>
  <mail>
    <header>Problem with more than 8-bit images with CGBitmapContextCreate</header>
    <date>Thu Mar 08 09:46:25 CET 2007</date>
    <body>Basically I need to put an image in a floating point format for processing. So I use the CGImageSource facility, but a subtle posterization effect in the shadows made me make deeper research where it seems that the pixels get into an 8-bit pipeline in some way. Also I saw the recent discussion about this on this list. I use kCGImageSourceShouldAllowFloat, so the problem seems different. In the following snippet of code, I do what I think is the simple to test : the image color space of the bitmap context is set to the one from the image to prevent any mapping of values. It is drawn in one time on a big buffer. The file on input is a RAW file from a 350D (12 bits color). I've also tested with the 16-bit TIFF file coming from the &amp;quot;ImproveYourImage&amp;quot; dmg from apple, to the same problem. The file from apple gives 235, 235, 233. The console does not report any error. I've also tested the following other ways : - feeding CGImageCreate with a data provider url, which works, the bits per component shows 32 in that case, but the final set of values is still exactly the same. - using the Quicktime graphic importer, but it seems to fail to handle raw files CGImageSourceRef image_source_ref = CGImageRef image_ref = CGImageSourceCreateImageAtIndex // it says 16 CGContextRef context_ref = CGBitmapContextCreate ( &amp;amp;buffer [0], width, height, 32, width * 16, CGImageGetColorSpace (image_ref), kCGImageAlphaNoneSkipLast|kCGBitmapFloatComponents|kCGBitmapByteOrder32Host for (int i = 0 ; i &amp;lt; buffer.size () / 4 ; ++i) // will always be less than 256</body>
  </mail>
  <mail>
    <header>Re: QuartzCore Link Problem</header>
    <date>Wed Mar 07 17:40:40 CET 2007</date>
    <body>On Wed, 7 Mar 2007 08:17:08 -0600, Right. Oh yes.  Xcode 2.4.1, OS 10.4.8, building on and targeting Intel. nm says _cglsGetVirtualScreen is exported by the CoreGraphics framework, which I'm definitely pulling (via ApplicationServices). Oh well. -- marco -- It's not the data universe only, it's human conversation. They want to turn it into a one-way flow that they have entirely monetized. I look at the collective human mind as a kind of ecosystem. They want to clear cut it. They want to go into the rainforest of human thought and mow the thing down.</body>
  </mail>
  <mail>
    <header>Re: ATSUI - drawing a line in separate parts</header>
    <date>Wed Mar 07 15:12:57 CET 2007</date>
    <body>On Mar 6, 2007, at 7:00 PM, Alexander Strange wrote: ATSUI has some... er... limitations as far as Core Graphics is concerned.  You run into a similar problem if you want to use Core Graphics to control the color of your ATSUI text (ATSUI Style runs can only contain RGB Colors but Core Graphics can use CMYK Colors for example). The workaround you suggest, drawing your own style runs, is the approach we took as well. It's been several years since I dealt with the problem but as I recall what we did was avoid ATSUIDrawText altogether.  Instead we let ATSUI layout the text then we used the low-level ATSUI calls described in: ATSUI_Concepts/index.html The basic technique was to store the CMYK colors as custom style attributes in the ATSUI text.  Then when we wanted to draw we would run through the text and use ATSUI to get the glyph outlines.  We'd then use Quartz to handle the color information and draw the glyph outlines as paths.</body>
  </mail>
  <mail>
    <header>NSBitmapImageRep orientation tag</header>
    <date>Wed Mar 07 12:46:56 CET 2007</date>
    <body>Is there a way to extract the Exif orientation tag from an NSBitmapImageRep (loaded incrementally from a JPEG file)? I tried what seemed to me like a promising solution: int orientationTag = [[[_incrementalImageRep valueForProperty:NSImageEXIFData] But it didn't work. [_incrementalImageRep valueForProperty:NSImageEXIFData] does return a valid dictionary, with a couple dozen Exif tags in it, but the orientation tag is missing, even if it's present in the image file, and can be read correctly by ImageIO. -- marco -- It's not the data universe only, it's human conversation. They want to turn it into a one-way flow that they have entirely monetized. I look at the collective human mind as a kind of ecosystem. They want to clear cut it. They want to go into the rainforest of human thought and mow the thing down.</body>
  </mail>
  <mail>
    <header>Re: QuartzCore Link Problem</header>
    <date>Wed Mar 07 03:21:58 CET 2007</date>
    <body>On Tue, 6 Mar 2007 18:33:42 -0700, I haven't tried that, but I don't see how changing that would fix a link-time issue. Yes, it's defined as a rotation transform -- I'm pretty sure it's correct because it works fine in other contexts. In case you wonder, what I'm trying to do is re-orient an image loaded incrementally from the web according to its Exif orientation tag. Preview.app honors the orientation tag, but doesn't load images incrementally. Safari does load images incrementally, but ignores the orientation tag. Also, it's not that hard to rotate an NSBitmapImageRep without resorting to CIImage.  It's just that -[CIImage imageByApplyingTransform] seemed so handy, so I thought I'd give it a try. -- marco -- It's not the data universe only, it's human conversation. They want to turn it into a one-way flow that they have entirely monetized. I look at the collective human mind as a kind of ecosystem. They want to clear cut it. They want to go into the rainforest of human thought and mow the thing down.</body>
  </mail>
  <mail>
    <header>Re: ATSUI - drawing a line in separate parts</header>
    <date>Wed Mar 07 03:20:03 CET 2007</date>
    <body>On Mar 6, 2007, at 6:00 PM, Alexander Strange wrote: I don't think this is the right list.  There isn't an ATSUI list that I'm aware of but you might have better luck on the CoreText or Carbon list. I think whatever you do will be a workaround.  The approach you suggest sounds reasonable to me. Nick _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: QuartzCore Link Problem</header>
    <date>Wed Mar 07 02:33:42 CET 2007</date>
    <body>I noticed that your variable is named incrementalImageRep.  Does the problem persist if you use a non-incremental image rep? On Mar 6, 2007, at 5:21 PM, Marco Piovanelli wrote: Attachment:</body>
  </mail>
  <mail>
    <header>ATSUI - drawing a line in separate parts</header>
    <date>Wed Mar 07 02:00:04 CET 2007</date>
    <body>First off, I hope this is the right mailing list for ATSUI — it's not quite clear. I'm doing some things with this API that it wasn't quite designed for and am running into a few design walls, mostly related to outlined text. In this case, I want to change the stroke width in the middle of a line. It can be changed per ATSUDrawText call using CGContextSetLineWidth, but not inside a style run that I see. The solution I thought of was to fake style runs by drawing less than a line of text, changing the setting, and then drawing more. If this is the right approach, then after I've called ATSUDrawText the first time, how would I get the proper pen coordinates for following calls? Do I have to get the advance array for the line and add up drawn glyphs' advances from it? If not, is there a better way to do this?</body>
  </mail>
  <mail>
    <header>QuartzCore Link Problem</header>
    <date>Wed Mar 07 01:21:17 CET 2007</date>
    <body>Hi, As a way to get my feet wet with the QuartzCore framework, I'm trying to use CIImage to apply an affine transform to an NSBitmapImageRep -- something like this: CIImage*			  ciImage = [[CIImage alloc] CIImage*			  transformedCIImage = [ciImage NSImageRep*		transformedImageRep = [NSCIImageRep Unfortunately, my application fails to link with the following error code: _cglsGetVirtualScreen referenced from QuartzCore expected to be defined in ApplicationServices I can't find cglsGetVirtualScreen referenced anywhere, and I have no idea where that symbol is supposed to be defined. I'm already linking a number of frameworks, including: ApplicationServices Cocoa OpenGL GLUT QuartzCore Any help appreciated, -- marco -- It's not the data universe only, it's human conversation. They want to turn it into a one-way flow that they have entirely monetized. I look at the collective human mind as a kind of ecosystem. They want to clear cut it. They want to go into the rainforest of human thought and mow the thing down.</body>
  </mail>
  <mail>
    <header>CIImage imageWithCVImageBuffer producing a garbled image?</header>
    <date>Tue Mar 06 14:17:24 CET 2007</date>
    <body>NSMutableDictionary *pixelBufferAttributes = [NSMutableDictionary [pixelBufferAttributes setObject:[NSNumber numberWithUnsignedLong:k422YpCbCr8PixelFormat] forKey:(NSString*) [pixelBufferAttributes setObject:[NSNumber [pixelBufferAttributes setObject:[NSNumber numberWithUnsignedInt:height] forKey:(NSString*) [pixelBufferAttributes setObject:[NSNumber numberWithUnsignedInt: CVPixelBufferPoolCreate(kCFAllocatorDefault, NULL, (CFDictionaryRef) CVPixelBufferPoolCreatePixelBuffer(kCFAllocatorDefault, CIImage *ciImage = [CIImage imageWithCVImageBuffer: This conversion produce a garbled image most of the time. I've got good results if, and only if, my pixel buffer has a width that is a multiple of 160! (i.e width = 640) Is there something I'm doing wrong? Thanks! -- François Menu email@hidden</body>
  </mail>
  <mail>
    <header>Re: How do I report an API bug?</header>
    <date>Tue Nov 01 08:13:27 CET 2005</date>
    <body>On Oct 31, 2005, at 8:40 PM, Gregory Cooksey wrote: Mac OS X. CGEventCreateMouseEvent may not have official documentation yet, but it's in a public header, so it's part of the OS.</body>
  </mail>
  <mail>
    <header>How do I report an API bug?</header>
    <date>Tue Nov 01 05:40:31 CET 2005</date>
    <body>Well, it looks like there's a bug in the CGEventCreateMouseEvent() function, where it doesn't set the event type field of the event properly.  I'd like to report it officially, but this is the first time I've used bugreport.apple.com, and I'm not sure what to put in the fields on the form. What Product/Component is an undocumented API?  Is it Mac OS X, or DTK (Developer tool kit?) The bug: I call CGEventRef ev = CGEventCreateMouseEvent(mySource, and the event returned does not have the kCGEventLeftMouseDown type. A call to CGEventGetType(ev) returns kCGEventNull.  If I send the event to another application with CGEventPostToPSN() it doesn't respond to the null event. I found a workaround for this: After calling CGEventCreateMouseEvent(), I call CGEventSetType(ev, kCGEventLeftMouseDown) and then it works as expected. But I'd still like to report this so it's more likely to be fixed, and would appreciate advice on how to do that properly.</body>
  </mail>
  <mail>
    <header>Re: Blitter gets chunky on 10.4 with some laptops</header>
    <date>Mon Oct 31 22:29:20 CET 2005</date>
    <body>Maybe the powerbook only supports 1 bit of alpha? Or some other format that is NOT 32 bpp?</body>
  </mail>
  <mail>
    <header>Re: Scrolling bits in a CGContext or CGLayer?</header>
    <date>Mon Oct 31 20:00:39 CET 2005</date>
    <body>On Oct 29, 2005, at 8:39 PM, Scott Thompson wrote: Well. I understand the CGContext being not always bitmap concept. But the document on CGLayer states it's the recommended replacement for offscreen drawing onto a bitmap context. I naturally think of doing the same tricks on bitmaps. Oh well. --</body>
  </mail>
  <mail>
    <header>Re: Blitter gets chunky on 10.4 with some laptops</header>
    <date>Mon Oct 31 18:30:04 CET 2005</date>
    <body>On Oct 31, 2005, at 9:49 AM, John Pattenden wrote: The first thing that strikes me is that QuickDraw has no alpha channel.  QuickDraw ports are XRGB.  There is no reason to expect that the first channel contains meaningful alpha data.  Were you storing the alpha data there yourself manually?</body>
  </mail>
  <mail>
    <header>Blitter gets chunky on 10.4 with some laptops</header>
    <date>Mon Oct 31 16:49:51 CET 2005</date>
    <body>I have code that copies pixels from a QuickDraw Pixel map into an NSBitmapImageRep. On the way it converts from ARGB to RGBA and calculates a new alpha value (0 or 255). The problem is that on my old PowerBook G4 400mhz the alpha is all blocky, as if it is being chunked somehow. The same code works fine on other machines running 10.4 and also runs fine on the same machine running 10.2.8 So this appears to be a dependency combination of hardware and software. I have another way to do this that works using CGImageCreateWithMaskingColors - but that is a lot slower. I would like to test for something - a combination of hardware and OS to determine if my code will work and run the slower code if it will not. Can anyone offer guidance as to what the combination of HW/SW might cause this problem?</body>
  </mail>
  <mail>
    <header>Re: Scrolling bits in a CGContext or CGLayer?</header>
    <date>Sun Oct 30 02:39:25 CEST 2005</date>
    <body>On Oct 28, 2005, at 12:33 PM, Stephen Chu wrote: You are making an assumption about CGContexts and CGLayers that may not be true. Consider, for example, a PDF context.  PDF contexts have no pixels. Asking one to perform a HIViewScrollRect-like operation would not make any sense. CGLayers suffer from a similar problem.  You have no guarantees about how a particular CGLayer might be implemented.  For example, you might imagine a CGLayer that is created on an OpenGL context.  It could be the case that the CGLayer stores it's cached drawing as a display list, not a bitmap.  Once again, scrolling the CGLayer is not an operation that could be done.</body>
  </mail>
  <mail>
    <header>Creating synthetic events with CGEventCreateMouseEvent</header>
    <date>Sat Oct 29 21:33:00 CEST 2005</date>
    <body>I'm writing a fuzz testing [1] tool, and I'm trying to create realistic synthetic user input to send to an application.  It looks like the CGEvent API is going to be the best way for me to create events that applications will think came from the system.  I had no problems with creating keyboard events.  However, I'm running into a problem creating mouse events.  I call CGEventRef ev = CGEventCreateMouseEvent(source, and the target application doesn't seem to receive any event at all. I've installed a monitor at the PSN tap, and it's seeing events of type kCGEventNull instead of kCGEventLeftMouseDown.  I tried adding a click state field to the mouse event in case the problem was due to that being absent, but it didn't help.  Am I missing something else here?</body>
  </mail>
  <mail>
    <header>Re: iMovie Plug-in</header>
    <date>Sat Oct 29 01:33:36 CEST 2005</date>
    <body>taking out doubleBuffer doesn&amp;#8217;t help. I don&amp;#8217;t need double buffering, it&amp;#8217;s just an example. I have checked other attribute combinations, but it doesn&amp;#8217;t work at all in an iMovie plug-in. Eberhard Am 29.10.2005 1:16 Uhr schrieb &amp;quot;Frank Doepke&amp;quot; unter &amp;lt;email@hidden&amp;gt;: Frank On Oct 28, 2005, at 3:19 PM, Eberhard Ammelt wrote: &amp;nbsp;I&amp;#8217;d like to use OpenGL functionality in an iMovie plug-in, but I couldn&amp;#8217;t even manage to initialize an NSOpenGLPixelFormat object using the piece of code below. I&amp;#8217;ve checked various attribute configurations, but the debugger always says &amp;#8220;invalid pixel format attribute&amp;#8221;. The same code works in a stand-alone test application. &amp;nbsp;Anyone any idea what&amp;#8217;s wrong? &amp;nbsp;Thank you, &amp;nbsp;Eberhard Ammelt &amp;nbsp;????NSOpenGLPFADoubleBuffer, &amp;nbsp;????NSOpenGLPFADepthSize, 24, &amp;nbsp;????NSOpenGLPFAAlphaSize, 8, &amp;nbsp;????NSOpenGLPFAColorSize, 32, &amp;nbsp;????NSOpenGLPFANoRecovery, &amp;nbsp;?????0 &amp;nbsp;????? Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list? ? ? (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: iMovie Plug-in</header>
    <date>Sat Oct 29 01:16:30 CEST 2005</date>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>iMovie Plug-in</header>
    <date>Sat Oct 29 00:19:23 CEST 2005</date>
    <body>I&amp;#8217;d like to use OpenGL functionality in an iMovie plug-in, but I couldn&amp;#8217;t even manage to initialize an NSOpenGLPixelFormat object using the piece of code below. I&amp;#8217;ve checked various attribute configurations, but the debugger always says &amp;#8220;invalid pixel format attribute&amp;#8221;. The same code works in a stand-alone test application. Anyone any idea what&amp;#8217;s wrong? Thank you, Eberhard Ammelt &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;NSOpenGLPFADoubleBuffer, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;NSOpenGLPFADepthSize, 24, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;NSOpenGLPFAAlphaSize, 8, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;NSOpenGLPFAColorSize, 32, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;NSOpenGLPFANoRecovery, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;0</body>
  </mail>
  <mail>
    <header>Re: Scrolling bits in a CGContext or CGLayer?</header>
    <date>Fri Oct 28 22:20:59 CEST 2005</date>
    <body>Nothing prevents you from doing that, but I don't think that's officially supported — you may not get the results you expect. You're right; thanks for letting us know about this.</body>
  </mail>
  <mail>
    <header>Re: Scrolling bits in a CGContext or CGLayer?</header>
    <date>Fri Oct 28 20:58:40 CEST 2005</date>
    <body>On Oct 28, 2005, at 1:36 PM, Derek Clegg wrote: Too bad. Can I draw the CGLayer onto itself with an offset then? Well, someone has to change the header then: --</body>
  </mail>
  <mail>
    <header>CoreImage on multiple GPUs</header>
    <date>Fri Oct 28 19:48:49 CEST 2005</date>
    <body>font-family:Arial'&gt;Hi all, font-family:Arial'&gt; font-family:Arial'&gt;I&amp;#8217;m wondering whether Core Image would benefit from multiple GPUs on the new dual-core G5s. Assuming I put 2 or more GPUs, could an app be multithreaded with Core Image filters running in parallel on both GPUs? font-family:Arial'&gt; font-family:Arial'&gt;Ivan font-family:Arial'&gt;</body>
  </mail>
  <mail>
    <header>Re: Scrolling bits in a CGContext or CGLayer?</header>
    <date>Fri Oct 28 19:36:57 CEST 2005</date>
    <body>There's no way to do this in Tiger. You can't pass in NULL — you must always specify a size.</body>
  </mail>
  <mail>
    <header>Scrolling bits in a CGContext or CGLayer?</header>
    <date>Fri Oct 28 19:33:34 CEST 2005</date>
    <body>This is a really quiet group. But it's still the only place for my Quartz questions. So here it goes. Is there a way to scroll bits in a CGContext or CGLayer? Something like HIViewScrollRect? The reason for this question is I am using a CGLayer for my image viewer. CGLayer allows me to draw the image once and reuse it later when user changes the guides and grids that lay on top of it. Now when the image is scrolled, HIViewScrollRect only scroll the on screen bits. The layer is left unchanged with previous image. I am trying to keep as much bits as possible for redrawing the whole image is very costly in my application (huge file). Also, how do I pass NULL as size in CGLayerCreateWithContext? It's not a pointer type. --</body>
  </mail>
  <mail>
    <header>CGLayer in a thread (was Re: Caching of CGImage)</header>
    <date>Thu Oct 27 15:52:01 CEST 2005</date>
    <body>On Oct 24, 2005, at 6:07 PM, Stephen Chu wrote: OK. I ended up creating a multi-line CGImage to hold the original file data then draw it inside a scaled down rectangle. The number of lines is calculated to keep the memory use of the buffer image within a set range (several MBs). This way, the times to create a new CGImage are reduced and thus not as costly. It seems to be a much better solution in a way that I can pass the same CGLayer to my pixel-filling thread. My idea is to draw the buffer image onto a CGLayer instead of a bitmap context. The question is, are there any catches on using CGLayer in a thread? Like can I draw a CGLayer in the main thread while it's being worked on in a separate thread? This will provide a progressive drawing of the image so the user will be entertained while waiting. --</body>
  </mail>
  <mail>
    <header>Re: Can CGImageDestination write 16-bit per component TIFF files?</header>
    <date>Wed Oct 26 19:39:02 CEST 2005</date>
    <body>I've had the same problem, which I also reported here, and Johannes Fortmann too... In case you're interested, our respective messages are archived there: I've personally filed a bug (rdar://problem/4304932) but it apparently hasn't been touched yet. My hope is that this problem will be solved by a software update before Aperture's release (10.4.3?). I can't imagine Aperture being limited to 8 bits per channel (although Aperture's page says absolutely nothing about bit depth, which I find ... surprising). Michel. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Can CGImageDestination write 16-bit per component TIFF files?</header>
    <date>Wed Oct 26 18:03:36 CEST 2005</date>
    <body>I built an image batch processing app using CGImageDestination to output files. Unfortunately documentation for this powerful (Tiger only) function is almost non-existent. Lots of trial and error on my part, but I am stuck with these problems: - How can I output 16-bit per component TIFF files with CGImageDestination? Seems to work fine when using CGImageDestinationAddImageFromSource  (and the source file of course is a high-bit TIFF). This allows me to set some image properties (metadata) but image data itself remains untouched. If I run that same source file through some Core Image Filters and draw it to a CGBitmapContext I end up with 8 bits or 32 bits per channel, depending on how the context was set up. No 16-bit data in sight ... - Is there a way for CGImageDestination to NOT recompress JPEG image data when only metadata properties need to be changed? Thanks for any suggestions! Jojo ______________________________________________________________ Verschicken Sie romantische, coole und witzige Bilder per SMS! Jetzt bei WEB.DE FreeMail:</body>
  </mail>
  <mail>
    <header>Re: Caching of CGImage</header>
    <date>Tue Oct 25 00:07:22 CEST 2005</date>
    <body>On Oct 24, 2005, at 4:16 PM, email@hidden wrote: CGDataProvider doesn't allow me to do background update of the image for I have no control over when it's used. Also, the image size is big as in 100K pixels in width and height. Quartz seems to like all the bits in memory when creating the image. My data provider callback is then ask to fill a buffer several GB in size. Needless to say, it dogs the system to an unusable state. --</body>
  </mail>
  <mail>
    <header>Caching of CGImage</header>
    <date>Mon Oct 24 21:19:18 CEST 2005</date>
    <body>I am moving a QuickDraw-based image file display application to Quartz. I used to read in one scan line of data into a Bitmap then copy it to the scaled destination Bitmap. This way, QD does the scaling and anti-aliasing for me in at least the horizontal direction. Now I am trying to do the same thing in Quartz with CGImage and friends. I find that the CGImage I use as scan line buffer is cached so the same single-line image is copied over and over again to the whole destination context. I don't want to create and destroy the scan line CGImage for each lines I read for it's really, really slow that way. So is there a way to tell Quartz not to cache a CGImage? BTW, the image file is in our own private format, so I don't think the new CGImageSource thingy will work. --</body>
  </mail>
  <mail>
    <header>Re: What does the returned value of CGDataProviderGetBytesAtOffsetCallback do?</header>
    <date>Sat Oct 22 17:04:44 CEST 2005</date>
    <body>On Oct 21, 2005, at 11:52 AM, Stephen Chu wrote: I can only guess that it will call back again for more data, trying to get the data you didn't provide on the first pass. I believe it only stops when you return 0. Of course that is me just reading the tea leaves provided in the documentation... and you have the edge case of being asked for data yet have none available just yet but returning zero is out since that stops things. -Shawn</body>
  </mail>
  <mail>
    <header>What does the returned value of	CGDataProviderGetBytesAtOffsetCallback do?</header>
    <date>Fri Oct 21 20:52:56 CEST 2005</date>
    <body>I am curious. What does the return value of CGDataProviderGetBytesAtOffsetCallback do? If I can't provide as many bytes as requested in the callback, what will happen? --</body>
  </mail>
  <mail>
    <header>Re: Using a CGDataProvider for my own purposes.</header>
    <date>Fri Oct 21 18:13:51 CEST 2005</date>
    <body>Both of these are close, but they are more likely to wind up being a CRC of the &amp;quot;image data as drawn&amp;quot; (or in the PDF case potentially the image data as drawn and then compressed), rather than the image data that is the source of the image. For example, in this particular problem, if I use a JPEG data provider then I want to CRC the JPEG data, not the image that data describes.   It's a very unusual thing to want to do, I'll admit. Option 1 would also CRC the PDF information that surrounds the image... things like the headers, and the PDF catalog.  That might not be a bad thing... but it is a thing :-) I probably will. Since I'm creating my CGImages as part of a larger C++ class, what I've done is capture the data source when the C++ class is created and I'm running the CRC on the data then, rather than waiting until after it has been encapsulated in a CGImage.  This solves my particular need... but I agree with you that being able to encapsulate filters for data providers/consumers as custom data providers/consumers would be a handy feature in some instances.</body>
  </mail>
  <mail>
    <header>Re: Using a CGDataProvider for my own purposes.</header>
    <date>Fri Oct 21 18:04:44 CEST 2005</date>
    <body>On Oct 21, 2005, at 8:06 AM, Shawn Erickson wrote: So after a little more coffee I can think of two ways (out of likely a few more)... 1) Create a data consumer that does your CRC (doesn't even have to store the bytes passed in, etc.). Then uses this data consumer to create PDF context (CGPDFContextCreate) with some reasonably sized media box. Then focus that context, draw your image into it, and flush/close the contex. 2) Create a bitmap image context (CGBitmapContextCreate) of reasonable size (doesn't have to be the fully image likely) and backed by a buffer of larger enough size. Then focus that context, draw your image into it, and flush/close the context. Then do your CRC. Option (1) likely avoids to much additional memory allocation that is implied in option (2) but both incurs a copy and image transformation. It would be nice to be able to more directly chain a data provider to a data consumer and/or access the data consumer directly (of course concurrent access of a data consumer is an issue) when all you care about is the bytes not exact what it represents (bitmap, vector, etc.). It would also be nice to be able to chain data providers together, including for ones you didn't author (implies you need a stable way to access a data providers callbacks). Consider filing an enhancement request. -Shawn</body>
  </mail>
  <mail>
    <header>cggl</header>
    <date>Sat Dec 11 23:00:03 CET 2004</date>
    <body>For regression testing, we are creating a cggl context on top of an agl context, then taking snapshots of the rendered output from quartz using glReadPixels.  It works, but the colors have their colors reversed (b&amp;amp;r channels swapped), relative to when we capture the rendered output from opengl.  I can correct it by looping through the data and swizzling the bytes back, but it would be nice to know why it's happening. Anyone have any ideas?</body>
  </mail>
  <mail>
    <header>Re: Can CGColorSpaceCreateWithName fail?</header>
    <date>Sat Dec 11 04:59:15 CET 2004</date>
    <body>why not always assume all create/copy methods can fail?  it's almost like malloc() or realloc() returning NULL.</body>
  </mail>
  <mail>
    <header>Re: Can CGColorSpaceCreateWithName fail?</header>
    <date>Thu Dec 09 07:13:10 CET 2004</date>
    <body>&amp;gt; &amp;gt; Not always.  Consider a create call that returned a cached object.  It The point I was trying to make was this - it is possible for a create call to be designed such that it never fails.  And if this is the case then it should be documented as such. Of course.  You should not rely on undocumented behavior. My problem is ambiguous documentation.  Some CGCreate functions are documented as being able to fail while others are not.  If they can all fail then they should all be documented as such. Matt.</body>
  </mail>
  <mail>
    <header>Re: Can CGColorSpaceCreateWithName fail?</header>
    <date>Thu Dec 09 04:44:54 CET 2004</date>
    <body>Matthew Drayton: Not necessarily true. The object may be created with a lazy-allocation scheme, in which case the first call can fail. And, as others have said, if the documentation doesn't specify that it never fails, future versions might. -- Jens Ayton</body>
  </mail>
  <mail>
    <header>Re: CGContextMeasurePlainText (in Python interface)</header>
    <date>Wed Dec 08 21:14:34 CET 2004</date>
    <body>On Dec 8, 2004, at 12:56 PM, R. Scott Thompson wrote: I might be misunderstanding what you want to do, but I think you can do that by drawing the text with the text drawing mode set to kCGTextInvisible.</body>
  </mail>
  <mail>
    <header>Re: CGContextMeasurePlainText (in Python interface)</header>
    <date>Wed Dec 08 21:03:43 CET 2004</date>
    <body>One way that works is to set the fill color to completely transparent, draw the string, and compare the way that the text position changes. That won't give you the height of the text (which is not information I need) but it will give you the width :-)</body>
  </mail>
  <mail>
    <header>Re: CGContextMeasurePlainText (in Python interface)</header>
    <date>Wed Dec 08 20:56:07 CET 2004</date>
    <body>On Dec 8, 2004, at 1:46 PM, Joseph Maurer wrote: Is there no way, then, to measure a string as it would be rendered if you used the current font that is &amp;quot;selected&amp;quot; inside a context?</body>
  </mail>
  <mail>
    <header>Re: CGContextMeasurePlainText (in Python interface)</header>
    <date>Wed Dec 08 20:46:09 CET 2004</date>
    <body>It uses [NSFont userFontOfSize:fontSize], which sets the &amp;quot;Aqua Application font&amp;quot;.</body>
  </mail>
  <mail>
    <header>CGContextMeasurePlainText (in Python interface)</header>
    <date>Wed Dec 08 20:33:42 CET 2004</date>
    <body>The Python interface has a routine called CGContextMeasurePlainText. This routine appears to be stand-alone (by that I mean it's not a member of the CGContext class).  The routine takes a fontSize, but what is not clear to me is how does it decide what font it should measure with? The other two variations on this routine, for handling RTF and HTML might have a font designation embedded within them, what font does CGContextMeasurePlainText uses?</body>
  </mail>
  <mail>
    <header>Re: Can CGColorSpaceCreateWithName fail?</header>
    <date>Tue Dec 07 02:20:25 CET 2004</date>
    <body>&amp;gt; It may be the case that on the other side of the routine the system is You can if it is documented behavior.  But I guess I am not going to run into that problem ;) If this is the assumption we are expected to make then fine.  But the documentation should be consistent.  You can't have half the CGCreate functions documented as possibly failing and returning NULL while other half mention nothing.  Pick one and stick to it. Matt.</body>
  </mail>
  <mail>
    <header>Re: Can CGColorSpaceCreateWithName fail?</header>
    <date>Mon Dec 06 16:37:20 CET 2004</date>
    <body>On Dec 6, 2004, at 7:59 AM, R. Scott Thompson wrote: Also, don't forget to use the debug frameworks -- many APIs are parameter checked when using the debug frameworks.</body>
  </mail>
  <mail>
    <header>Re: Creating bevel effect for custom button</header>
    <date>Mon Dec 06 15:03:21 CET 2004</date>
    <body>By drawing four trapezoids, one on either side of the control, and colored so that they look like a bevel? If you want a bit more of a system look and feel then you might consider using HIThemeDrawButton from Carbon (available in 10.3 only).</body>
  </mail>
  <mail>
    <header>Re: Can CGColorSpaceCreateWithName fail?</header>
    <date>Mon Dec 06 14:59:16 CET 2004</date>
    <body>Well... you should always be ready for any create call to fail.</body>
  </mail>
  <mail>
    <header>Creating bevel effect for custom button</header>
    <date>Mon Dec 06 09:42:02 CET 2004</date>
    <body>I need to make a custom button (custom HIView). The button should have a a textured background on top of which text is drawn. How can I create a bevel-like effect for the edges of my button using CoreGraphics? david.</body>
  </mail>
  <mail>
    <header>Re: Is it really possible to use ATSUI and Quartz together in device 	independent manner?</header>
    <date>Mon Dec 06 09:12:19 CET 2004</date>
    <body>&amp;gt;Hi, I'm afraid this wouldn't be a good idea: such text would look ugly (badly spaced) in print. This is the old gotcha: you can't lay out text in one way to look good both on low (screen) resolution and high (print) resolution, period. What Quartz does with antialiasing is raising the effective resolution of the display. This makes it possible to use high-resolution text layout on the display with good results. Otherwise you have the decision to either accept that the text looks bad on one side or use different layout (that is, different line wrapping) for low versus high resolution - not really WYSIWYG. The bottom line is probably that ATSUI can't automate these decisions for you. Kai -- RagTime GmbH                          Tel: [49] (2103) 9657-0 Neustraße 69                          Fax: [49] (2103) 9657-96 D-40721 Hilden Deutschland</body>
  </mail>
  <mail>
    <header>Re: Can CGColorSpaceCreateWithName fail?</header>
    <date>Mon Dec 06 07:08:00 CET 2004</date>
    <body>&amp;gt; Well... you should always be ready for any create call to fail. Not always.  Consider a create call that returned a cached object.  It would never fail. Certainly reading the CoreGraphics documentation gives the impression, to me at least, that CGColorSpaceCreateWithName does not fail. It fails and returns NULL.  But I don't think I should have to go around plugging bogus values into functions to determine how they work.  I think I should be able to read the documentation to figure this out.  Am I being unreasonable? I guess my problem is ambiguous documentation.  I long for the days of Inside Mac documentation.  A lot of people may have complained about IM being too verbose but at least you knew what the hell was going on.  I'll send some feedback to the tech pub guys. --</body>
  </mail>
  <mail>
    <header>Re: High Perfomance Image Viewer</header>
    <date>Sun Dec 05 07:36:46 CET 2004</date>
    <body>On Dec 3, 2004, at 1:48 PM, Travis Heppe wrote: lol. :-) I'm far from being able to make an official recommendation about anything. I appreciate your joke, but in all seriousness I feel compelled to mention that my experience has been that adopting Quartz 2D does in fact offer some nice benefits for cross-platform applications. Unfortunately, you can't re-use Quartz 2D code the same way you can use code from other libraries (QuickTime for example).  However because the imaging model of Quartz 2D is based on PostScript, it finds very strong resonances in other &amp;quot;modern&amp;quot; graphics libraries like GDI+, and Java 2D which have also been designed to work well with PostScript and PDF.  In other words, if your application structures it's drawing so that it works well with Quartz 2D, you stand a good chance of being able to re-use your design with other libraries, if not the actual code itself.</body>
  </mail>
  <mail>
    <header>Re: High Performance Image Viewer</header>
    <date>Sun Dec 05 07:22:31 CET 2004</date>
    <body>My primary experience has been coding a (vector) image viewer for rendering PDF is fast enough for an interactive, slider-based zooming. There are some minor gotchas to clear e.g. inhibiting bitmap caching in most circumstances, but the result is generally worth it. There is some Quartz support for routines to supply arbitrary bitmap data, IIRC. A parallel situation exists in the Graphviz GUI. You can pass the same image to the print driver which will tile it for  you. I haven't done much benchmarking of the tiling algorithms -- I mainly wonder whether Quartz is smart enough to only bother rendering the subset of objects within a tile -- but it works. As you may know already in Quartz they would be the appropriate affine transforms. Yes, plus nice extras like shadowing, gradient fills etc. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com</body>
  </mail>
  <mail>
    <header>Re: High Perfomance Image Viewer</header>
    <date>Fri Dec 03 20:48:49 CET 2004</date>
    <body>Does this mean that the official recommendation to anyone considering writing a cross-platform app is to choose Quartz 2D?  ;-)</body>
  </mail>
  <mail>
    <header>Re: interactions between CGContextSetLineCap	and	CGContextSetLineDash</header>
    <date>Fri Dec 03 20:01:33 CET 2004</date>
    <body>I suppose that since our product (HOOPS) is exacatly such a higher-level library, now would be the right time to mention that we are able to do both of those things, plus a lot more.</body>
  </mail>
  <mail>
    <header>Re: interactions between CGContextSetLineCap	and	CGContextSetLineDash</header>
    <date>Fri Dec 03 19:31:58 CET 2004</date>
    <body>On Dec 3, 2004, at 12:23 PM, Travis Heppe wrote: It might be a useful feature, but it's not a part of the imaging model (I.e. PostScript and PDF don't support it).  Rather than making it part of Quartz, it would probably make more sense to build on a higher-level library that handled that functionality.  Another nice feature for that higher-level library, for example,  is the ability to generate paths with arrowheads.</body>
  </mail>
  <mail>
    <header>Re: interactions between CGContextSetLineCap and	CGContextSetLineDash</header>
    <date>Fri Dec 03 19:23:59 CET 2004</date>
    <body>thanks for the response.  We have our own code to break thick lines down into polygons (needed for less capable graphics systems, like GDI ;-0).  I'll just send it down that path when the endcap is something other than a butt.  Seems like it would be a useful feature in quartz, though.</body>
  </mail>
  <mail>
    <header>Re: [OT] Where to turn for vImage</header>
    <date>Fri Dec 03 18:17:05 CET 2004</date>
    <body>It seems to me Performance would be the best place to look (as there appear to be a lot of people working on Accelerate lurking on that list). Sci-vis might also have a lot of the same people on it, but I don't know as I am not on that list. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>[OT] Where to turn for vImage</header>
    <date>Fri Dec 03 18:11:41 CET 2004</date>
    <body>I'm having some trouble using vImage to do some convolution (it seems to crash a lot for inexplicable reasons).  Which of the mailing lists at lists.apple.com would be a good one to post questions related to vImage?  Would that be here (as a graphics API) or PerfOptimization (for Accelerate framework) or even sci-vis?</body>
  </mail>
  <mail>
    <header>Re: Is it really possible to use ATSUI and Quartz together	in	device independent manner?</header>
    <date>Fri Dec 03 15:07:20 CET 2004</date>
    <body>On Dec 3, 2004, at 7:38 AM, Nick Nallick wrote: It it's only happening for small fonts it could also be a hinting problem.  Drawing with a larger font and then scaling back down can help avoid hinting trouble as well.</body>
  </mail>
  <mail>
    <header>Re: Is it really possible to use ATSUI and Quartz together in	device independent manner?</header>
    <date>Fri Dec 03 14:38:48 CET 2004</date>
    <body>On Dec 3, 2004, at 6:11 AM, Aleksandr Furmanov wrote: I haven't studied your specific problem, but I suspect it has to do with the use (or non-use) of fractional character widths.  Perhaps the non-antialiased characters are using integer-only advances. A workaround for the issue of small fonts not being antialiased is to use a larger point size (e.g., double the size you want) and scale the result back down with the destination context's transform.  You might also be able to achieve the same thing using the text matrix.  This will fool the text engine into anti-aliasing smaller text than it normally would. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Re: Is it really possible to use ATSUI and Quartz together in	device independent manner?</header>
    <date>Fri Dec 03 14:11:54 CET 2004</date>
    <body>Since nobody has answered this question I will try to ask a different one - again the issue is with drawing text with ATSUI, when font size is less than threshold. The problem is that distance between glyphs depends on total number characters being drawn in the line, resulting effect of &amp;quot;dancing&amp;quot; text. To illustrate this I have dumped distances between glyphs, retrieved by ATSUGetGlyphBounds(...) for strings composed from 1 up to 10 characters 's'. The ATSUGetGlyphBounds(...)'s iBoundsCharLength parameter is set to kATSUseCaretOrigins. The font used in the test is Charcoal 12. Threshold value is 12, i. e. text is non-antialiased. 's': [7 ] 'ss': [5 6 ] 'sss': [5 5 6 ] 'ssss': [6 5 5 6 ] 'sssss': [6 5 5 5 6 ] 'ssssss': [6 6 5 5 5 6 ] // Oops: the distance between first and third glyphs has changed from 5 to 6 - dance move! 'sssssss': [6 6 5 5 5 5 6 ] 'ssssssss': [6 6 6 5 5 5 5 6 ] 'sssssssss': [6 6 6 5 5 5 5 5 6 ] The same table with threshold value 10 ( i.e text is antialiased ): 's': [7 ] 'ss': [6 6 ] 'sss': [6 5 7 ] 'ssss': [6 5 6 6 ] 'sssss': [6 5 6 5 7 ] 'ssssss': [6 5 6 5 6 6 ] 'sssssss': [6 5 6 5 6 5 7 ] 'ssssssss': [6 5 6 5 6 5 6 6 ] 'sssssssss': [6 5 6 5 6 5 6 5 7 ] To see how it looks on the screen one can run: echo 's' | ./TextNameTool -x -  -f Charcoal -s 12  -o s01.jpg echo 'ss' | ./TextNameTool -x -  -f Charcoal -s 12  -o s02.jpg ... This problem can be workarounded by setting kATSLineUseDeviceMetrics | kATSLineNoAntiAliasing for line layout options, but determining whether to set this options or not is not easy - see my previous post. FWIW the problem I am trying to solve is to port text editing engine from Windows. Can someone of ATSUI experts help to solve these issues? May be an updated example of TextNameTool with &amp;quot;right&amp;quot; ATSUI line options (or whatever other ATSUI options) would be useful. I would expect visually same text produced by TextNameTool and TextEdit. What they do produce now differs. MetaCommunications Engineering Aleksandr Furmanov</body>
  </mail>
  <mail>
    <header>Re: Can CGColorSpaceCreateWithName fail?</header>
    <date>Fri Dec 03 03:08:21 CET 2004</date>
    <body>On Dec 2, 2004, at 8:01 PM, Matthew Drayton wrote: Well... you should always be ready for any create call to fail. Particularly CGColorSpaceCreateWithName.  What happens if you pass it a name that is nonsense? That's probably the safest bet... sure.</body>
  </mail>
  <mail>
    <header>Can CGColorSpaceCreateWithName fail?</header>
    <date>Fri Dec 03 03:01:55 CET 2004</date>
    <body>Can CGColorSpaceCreateWithName or any of the CGColorSpaceCreate family fail?  Is it safe to assume these functions will always return a valid color space or should I be prepared for them to fail and return NULL?. Likewise, should I assume any CG create function can fail and return NULL?  I would expect this to be the case given these are create functions but the documentation is far from clear.  Some create functions are documented as being able to fail and return NULL while others are not.  Which is it? Matthew. --</body>
  </mail>
  <mail>
    <header>Re: interactions between CGContextSetLineCap and	CGContextSetLineDash</header>
    <date>Fri Dec 03 02:41:03 CET 2004</date>
    <body>On Dec 2, 2004, at 5:00 PM, Travis Heppe wrote: Not when drawing a standard line.  I suppose you could draw a second and third line over the ends of the first to get that effect (provided you weren't using any transparency).  Otherwise I suppose you could do your own dashes by clipping out the gaps in the line. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>interactions between CGContextSetLineCap and CGContextSetLineDash</header>
    <date>Fri Dec 03 01:00:51 CET 2004</date>
    <body>Is it possible to use a butt cap for the interior dashes, but a square or round cap at the very start and end of the dashed lines?</body>
  </mail>
  <mail>
    <header>Size of a shadow</header>
    <date>Thu Dec 02 19:33:30 CET 2004</date>
    <body>I'm writing custom widgets in Cocoa that display text and other types of objects (e.g. focus rings) with drop shadows. The current text system in Cocoa, while it does render shadows, doesn't take the shadow into consideration when computing the string size. For example, NSAttributedString's size method will return the same size for a particular string regardless of what shadow parameters it uses. While I could argue that this is a bug, I can see where it could be trouble for the API to take the shadow into consideration.  For example, multiple lines of text may have shadows such that there would be some overlap. Having said that, I thought of doing my own workaround to ensure that the bounds I use to draw strings, etc. would take a shadow into account.  If I don't, I either get clipping of the shadow, or spacing between various entities are not what I want. Obviously, the shadow's offset can be used to increase the object's ultimate width and height; that one is easy. But the blur radius got a bit tricky.  I wrote a quick Cocoa app and ran it under 10.3.6.  I noticed that the size of a shadow doesn't seem to rely upon the size of the object (e.g. thickness of a line).  Also, it appears as if a blur radius of 8 is the maximum value one can use.  Basically, my app allowed me to dynamically alter a shadow.  I then took screenshots and analyzed them in Photoshop. This leads me to the following equation for the amount of extra width and height an object (e.g. string) needs: Is this correct?  Can I bank on the fact that the shadow system is &amp;quot;limited&amp;quot; to 8 pixels/points?  Is there a better method to query a shadow's rendering size? Finally, if indeed the shadow is limited to 8 pixels of blur, won't this be a problem when wanting to scale up PDFs that contain shadows?  I seem to remember scaling some in Preview.app where the &amp;quot;body&amp;quot; of my image would scale nicely, but any drop shadows would become kinda pixelated.  It now makes sense that if the shadow is limited to a maximum of 8 shades of color (to represent the 8 pixels of blur), that things would get pixelated if those shades had to be used across a large area (e.g. 80 points/pixels). -- Rick Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: High Perfomance Image Viewer</header>
    <date>Thu Dec 02 06:11:50 CET 2004</date>
    <body>In case you are curious.... The name MrSID stands for Multiresolution Seamless Image Database.  It's a proprietary format controlled by LizardTech.  See this site for some more info: and They just recently released a SDK for OSX...  I am hoping I can use it to bring MrSID Support to the Mac (which is lacking).  Lots of government agencies use .sid files for their aerial GIS imagery.  A 3.5 meg .sid file decompresses to about a 60 meg GeoTIFF....  So you can see why I would like to code up a viewer....</body>
  </mail>
  <mail>
    <header>Re: High Perfomance Image Viewer</header>
    <date>Thu Dec 02 03:16:05 CET 2004</date>
    <body>On Dec 1, 2004, at 7:46 PM, Brian Frank wrote: If you are interested in the OpenGL route, you might look at: I've seen some really, REALLY large, high resolution images manipulated in real time through Apple's OpenGL implementation. In any event you are going to have to get the images into memory so you can either upload them to VRAM or map them into textures for OpenGL. You can use the system to import JPEG and TIFF images into image buffers.  In short everything will end up being &amp;quot;raw bitmaps&amp;quot; at some point. You mentioned Core Image later so if you have access to a Tiger seed you might also want to look into Image I/O. From your brief description, don't know that Core Image is going to be very useful.  Core Image is a tool for applying effects to images. Unless you are going to be doing something like applying false color to your maps (seems like a reasonable thing to do in a map viewer), then Core Image may be of limited use. OpenGL can handle transparency and layering of Images as well.  As you note in a comment later on, Quartz 2D and Core Image both will use OpenGL to achieve greater performance (more so in Tiger than in previous releases). In the end, the the tradeoff will likely be one of Performance vs. Complexity.  Writing your code in OpenGL requires your application to pay attention to a lot of details (what kind of graphics card is on the machine, what OpenGL features does that card support, what texture sizes can it handle, does it have a programmable GPU, etc...) This makes the OpenGL code a bit more complicated, but since you're &amp;quot;closer to the metal&amp;quot; you have the potential for higher performance.  Quartz 2D is a much easier API to write to and it insulates you from the details of the underlying hardware, but you may lose some performance.  (Or at the very least you you will have to code Quartz 2D carefully to get performance close to the OpenGL route). As I alluded to before OpenGL is programming closer to the metal.  Just as assembly requires you to juggle more details than C/C++ so do does OpenGL require you to know more of the details of the particular hardware you are running on.  However, just as in the assembly vs C/C++ case if you manage that complexity well, you can get much higher performance. I don't think the learning curve will be nearly as steep to adopt Quartz 2D and your code will be much easier to follow.  Potentially it will be much easier to get your code to run on a broader range of computers as well (since Quartz 2D will abstract you farther from the hardware).  But again, it's a tradeoff of complexity for performance.</body>
  </mail>
  <mail>
    <header>High Perfomance Image Viewer</header>
    <date>Thu Dec 02 02:46:36 CET 2004</date>
    <body>Sorry if this has been asked before....  I did some searching of the archives and didn't find anything, so here goes.... I am looking to code up a Image viewer that I would like to get some serious performance out of.  Real time zooming, panning and rotation of high resolution images, infinite zoom in and out, etc.  I am looking for some initial advice to see which way I should go for the graphic interface Quartz (core image, core graphics,etc.) or OpenGL. Some image formats will be supported by the OS (JPEG, TIFF, etc), and others I will need to write my own decode routines so I can work with raw bitmaps.  I am also looking to add tiling support in the future (multiple images spatially aligned in one view).  In case some of you are wondering what this is for, I am looking to develop a GIS image app to handle GeoTiffs, MrSID (currently no Mac viewer available, but there is a SDK), etc aerial imagery. I am looking for a lightweight high performance viewer, not a full GIS package like GRASS or TNTmips. I just need a little advice on which direction to head.  Which interface will give me the best performance on screen.  I am leaning towards OpenGL, for no other reason than I found some example code that seems to do what I want.... And it seems to have some hooks already developed for the zoom and rotate and panning functions. OpenGL could maype support the building of elevation models (DEM models) and then the skinning of those models with the imagery in 3D, but that would be in the very far future. But the future functions of Quartz and the Core Image seem appealing as well...  Real time transparency and layering of images, etc..  I guess I am a little confused since it seems like the two technologies are somewhat overlapping in some respects (in fact, doesn't Core Image use OpenGL to a degree?).  And for me the learning curve will be equal, so that doesn't sway me in either direction.  Any suggestions??? BTW, I am posting this to the OpenGL list as well to get their opinion.... Thanks in advance, Brian -- Who is the master of foxhounds, and who says the hunt has begun. -Pink Floyd</body>
  </mail>
  <mail>
    <header>PDF preview refreshing problem</header>
    <date>Wed Dec 01 21:09:06 CET 2004</date>
    <body>Hi Everybody I am trying to create a pdf preview replacing pict preview. I have eps file created. I covert it to pdf, and get CGImage with quick time library and converter. I was able to create preview, but I am experiencing refreshing problem. sometimes i see the picture, and sometimes I dont. It is inconsistence. I am thinking may be sometimes it takes longer time to generate pdf and in the mean time view is showed up already. I am using mach-o LView in Code warrior 9. Thanks in advance. SNX Software 90 Park Ave NY, NY 10016 _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: documentation problems</header>
    <date>Wed Dec 01 19:26:37 CET 2004</date>
    <body>that helps.  That is what was missing from the docs.  From reading the original docs, it sounded like you would have a concept of pattern space for stroke patterns similar to ours -- where the coordinate space orients along the line. That appears not to be the case, however.  So if you wanted to draw something like arrowheads at discrete intervals along the line, you would be out of luck. I did not know about CGContextSetLineDash.  A cross-link from the patterns description would have been nice.  CGContextSetLineDash seems to be pretty well hidden inside the docs. CGContextSetLineDash seems like it will do the most important parts of what I am trying to do.</body>
  </mail>
  <mail>
    <header>Re: Coordinate flipping a CGLayer not working</header>
    <date>Mon Apr 30 17:29:44 CEST 2007</date>
    <body>On Apr 29, 2007, at 12:15 PM, Ken Tozier wrote: To flip the context, you need to scale it. but before you do that, you need to move the origin to the right place so the whole thing will look something like:</body>
  </mail>
  <mail>
    <header>Re: Coordinate flipping a CGLayer not working</header>
    <date>Sun Apr 29 21:31:31 CEST 2007</date>
    <body>Adding CGContextScaleCTM did the trick. I've been having a terrible time with coordinate flipping (a mental block or something) and your explanation cleared out some of the fog. On Apr 29, 2007, at 1:47 PM, Nick Nallick wrote:</body>
  </mail>
  <mail>
    <header>Re: Coordinate flipping a CGLayer not working</header>
    <date>Sun Apr 29 19:47:57 CEST 2007</date>
    <body>On Apr 29, 2007, at 11:15 AM, Ken Tozier wrote: A standard (i.e., right-hand) coordinate system has the origin in the coordinate system has the origin in the upper left corner and a positive y value increments down (i.e., it's a left-hand coordinate system).  Therefore to flip a transformation you have to move the origin from one corner to the other (i.e., translate) AND invert the y scale factor.  You're omitting the scale factor.  Your translate call will simply move the context's origin. In general you can invert the y scale factor by calling CGContextScaleCTM(context, 1.0, -1.0).  However you should be aware that this scales about the CTM's current origin, so it's important to consider your translation (to move the origin) and scale (to flip the y axis) in combination.  For example, if you scale first then translate your translation will use the newly inverted y scale so your y translation value must be the negative value of the one you'd use if you translated first then scaled. Nick</body>
  </mail>
  <mail>
    <header>Coordinate flipping a CGLayer not working</header>
    <date>Sun Apr 29 19:15:14 CEST 2007</date>
    <body>I have a CGLayer, whose contents is a PDF, that I draw into a flipped view and can't get the layer to draw unflipped. I read somewhere in the Quartz drawing docs that setting the &amp;quot;y&amp;quot; value in CGContextTranslateCTM would draw flipped (can't find a link now) but, regardless of whether I set &amp;quot;y&amp;quot; to positive or negative, the layer is always upside down. Is there a way to flip a layer without doing a rotate/translate? if ([superView isFlipped]) else Ken</body>
  </mail>
  <mail>
    <header>Inconsistent non-coalescing flags</header>
    <date>Sun Apr 29 15:09:03 CEST 2007</date>
    <body>CGEventGetFlags(eventRef) &amp;amp; kCGEventFlagMaskNonCoalesced) tells me that mouse moved events are being coalesced, as expected. kCGEventFlagMaskNonCoalesced says they are not. I created the event source using kCGEventSourceStateCombinedSessionState. What's the story? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Inconsistent keyboard types</header>
    <date>Sat Apr 28 16:05:55 CEST 2007</date>
    <body>CGEventGetIntegerValueField(eventRef, kCGKeyboardEventKeyboardType) tells me that my keyboard is type 31. CGEventSourceGetKeyboardType(eventSourceRef) says it is type 2. What's the story? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>How to read event flags in an event source?</header>
    <date>Sat Apr 28 14:02:04 CEST 2007</date>
    <body>When I execute this code fragment on a CGEventRef, I get a meaningful result: (CGEventGetFlags(eventRef) &amp;amp; kCGEventFlagMaskNonCoalesced) But when I execute this code fragment on a CGEventSourceRef, my program goes into some sort of infinite loop that I can't debug: kCGEventFlagMaskNonCoalesced) The same issue exists for the other flags, such as kCGEventFlagMaskNumericPad. I created the event source in question using the CGEventCreateSourceFromEvent() function in an event tap callback function, and the event source state ID is 1 (kCGEventSourceStateHIDSystemState). Will I be able to read these flags from the event source if I authenticate or run as root? Or am I completely misunderstanding how this works? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: Disabling Core Image antialiasing?</header>
    <date>Wed Apr 25 11:31:20 CEST 2007</date>
    <body>So assuming your filter is a subclass of CIFilter, it should have an outputImage method. In this you&amp;#39;ll be creating CISampler objects which Something like: This would become: I&amp;#39;ve not tried this, but according to the docs it should work. There&amp;#39;s a couple of other settings you can put on the CISampler. Look at the CISampler Class documentation for them. Paul Hi, I&amp;#39;m working on a Core Image-based application and I&amp;#39;ve run into a little problem. I&amp;#39;m trying to implement simple non-antialiased zoom with Core Image, but the problem is that CIContext ignores both GL_TEXTURE_MIN_FILTER and GL_TEXTURE_MAG_FILTER. According to OpenGL Profiler, CIContext always calls glTexParameter with GL_LINEAR while I need GL_NEAREST. Any workaround for this? I&amp;#39;ve been thinking about messing with frame/ renderbuffers (render CIImage, scale matrix and render again), but performance is really important and perhaps there is a easier way to do it? Even private API or some nasty hacks would be appreciated. Thanks, Aidas</body>
  </mail>
  <mail>
    <header>Which is better?</header>
    <date>Wed Apr 25 04:28:46 CEST 2007</date>
    <body>I am currently in the process of making an application similar to that of Photo Booth, where a user chooses a Core Image filter to apply to live video.  Would it be appropriate, in terms of performance, to a have a Quartz composition rendered in a QCView handle my filters and video controller?  Or would I be better off doing my effects and/or video controller programmatically?  Of course doing it programmatically would give the user more customizability, but would I see a performance increase?</body>
  </mail>
  <mail>
    <header>Re: Disabling Core Image antialiasing?</header>
    <date>Tue Apr 24 17:45:39 CEST 2007</date>
    <body>On 24 Apr 2007, at 16:36, Aidas Dailide wrote: Can you write a Core Image kernel which does the zoom? Something like.... kernel vec4 zoomImage(sampler image, float zoom)</body>
  </mail>
  <mail>
    <header>Disabling Core Image antialiasing?</header>
    <date>Tue Apr 24 17:36:00 CEST 2007</date>
    <body>I'm working on a Core Image-based application and I've run into a little problem. I'm trying to implement simple non-antialiased zoom with Core Image, but the problem is that CIContext ignores both GL_TEXTURE_MIN_FILTER and GL_TEXTURE_MAG_FILTER. According to OpenGL Profiler, CIContext always calls glTexParameter with GL_LINEAR while I need GL_NEAREST. Any workaround for this? I've been thinking about messing with frame/ renderbuffers (render CIImage, scale matrix and render again), but performance is really important and perhaps there is a easier way to do it? Even private API or some nasty hacks would be appreciated. Thanks, Aidas</body>
  </mail>
  <mail>
    <header>Re: Question about nclc/colr image description extension</header>
    <date>Sun Apr 22 22:05:25 CEST 2007</date>
    <body>Can someone give me some information on this? Mark</body>
  </mail>
  <mail>
    <header>Re: Software Rendered kicking in with WhackedTV on PPC</header>
    <date>Sun Apr 22 18:45:16 CEST 2007</date>
    <body>Technical Q&amp;amp;A QA1416: Specifying if the CPU or the GPU should be used for rendering. Specifically: &amp;quot;NOTE: By default, Core Image uses the CPU for rendering on systems with a GeForce 5200 series card because, for most benchmarks, the 5200 can be slower than the CPU on currently -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>IKImageView choppy?</header>
    <date>Fri Jul 30 19:41:20 CEST 2010</date>
    <body>the ImageKit is part of Quartz right? I have an IKImageView, and I'm putting CGImages (That I make out of NSImages) onto it. However, a normal 200DPI 8.5/11 page takes like 3 seconds to come up, appearing in rectangles about 2inches (screen) on a side at a time. This is really annoying. Is there a way around this? Alternatively, is there a way to double buffer the view? To have 2 IKImageViews and draw into one, and then display it? Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Precision issues with Gaussian Blur image unit resulting in	Gamma/darkening</header>
    <date>Thu Jul 29 19:37:19 CEST 2010</date>
    <body>Hi. I'd like to confirm a possible bug I just found with Core Image (or, possibly Quartz Composers Core Image wrapper support) Create a composition with an image input, image out to Apples Gaussian Blur, to a billboard. Note the color of your image. Now, make the radius of the Gaussian Blur very low, something like, 0.0001, note the shift in gamma in your image, it becomes ~10% darker. A value of 0.001 has no effect, so this looks like a precision issue. Now, I know, &amp;quot;dont do that&amp;quot;, unfortunately, when using Quartz Composer (or well, even outside, but I have not tested in a host app), you can get into situations where an LFO, Interpolator, Timeline, or random will give you that value, thus rendering an incorrect image. Simple Patch demonstrating: I'll file a bug, but I thought it worth mentioning on the list. Apologies for the cross post, but I thought both 'teams' (?) might be interested, or have something to say. Thank you. Attachment:</body>
  </mail>
  <mail>
    <header>Re: Porting from QuickDraw to Quartz</header>
    <date>Tue Feb 10 21:52:03 CET 2004</date>
    <body>X. Here are some of the things that I would like to see: 1.  A cross reference table that shows which Quartz functions that we should look at for each QuickDraw command.  I realize that QuickDraw and Quartz are not one to one but it would REALLY help to at least know what API function we should be looking at to replace our existing QuickDraw commands.  It really would only need a one line comment that would say why we should be looking at the specific Quartz function. 2.  A one page overview of the philosophy of QuickDraw and a one page overview of the philosophy of Quartz.  Or maybe just one comparison document that discusses the difference in architecture and how we should approach a port. 3. SetPort, GetPort, GrafPtr, CGrafPtr are used all over in QuickDraw apps (at least mine).  It would be good to have a more detail description of how we move from ports to Contexts. 4. A number of very simple code samples that are written in QuickDraw and also an equivalent Quartz implementation of the same functionality. By simple, I'm talking about not having one huge sample code that has all APIs calls.    Each sample code would explicitly discuss a certain topic: For example: How to replace SetPort, GetPort and GrafPtrs with Contexts. How to replacing the drawing operations like MoveTo, LineTo with the way of drawing in Quartz How to replace InitCursor, SetCursor, HideCursor, ShowCursor, ObscureCursor, with the new way of handling the cursor How to handle interactive graphics using transparent overlays Dealing with fonts ... The sooner we could get even a cross reference the better. Thanks for listening, Mike</body>
  </mail>
  <mail>
    <header>Re: When is PDF v1.4 Created?</header>
    <date>Tue Feb 10 21:42:34 CET 2004</date>
    <body>rdar://3555098 I've sent you the details off-list but the upshot is that the shader function is being passed bogus input values when drawing a PDF (rdar://3148091). Nick</body>
  </mail>
  <mail>
    <header>Re: Porting from QuickDraw to Quartz</header>
    <date>Tue Feb 10 21:14:41 CET 2004</date>
    <body>Mike, Adobe's PDF technical documentation has been moved to For other graphics-related links, visit the ADC Graphics &amp;amp; Imaging Documentation page at &amp;lt;http:/ and scan the lists on the right. Craig</body>
  </mail>
  <mail>
    <header>Re: Porting from QuickDraw to Quartz</header>
    <date>Tue Feb 10 20:50:09 CET 2004</date>
    <body>Gus, Your link worked great. I downloaded versions 1.3, 1.4 and 1.5 of the Reference Guide. Thanks, Mike</body>
  </mail>
  <mail>
    <header>Re: Porting from QuickDraw to Quartz</header>
    <date>Tue Feb 10 20:54:29 CET 2004</date>
    <body>Search the Adobe site for the document PDFReference.pdf. Nick</body>
  </mail>
  <mail>
    <header>Re: Porting from QuickDraw to Quartz</header>
    <date>Tue Feb 10 20:20:25 CET 2004</date>
    <body>I've been out of PostScript for a while but at one time I did write our printer driver in PostScript so hopefully that knowledge will help. Yesterday I read a &amp;quot;Beyond QuickDraw: Quartz&amp;quot; pdf that I found on Apple's site.  They suggested going to to for the Adobe's Portable Document Format Reference Guide.  Unfortunately, the link is no longer valid and I wasn't able to find the document searching on Adobe's site. The &amp;quot;no global state&amp;quot; works for me. I think we will go with the transparent overlay approach since redrawing our entire drawing every move would be impractical. Thanks for your help! Mike</body>
  </mail>
  <mail>
    <header>Re: Porting from QuickDraw to Quartz</header>
    <date>Tue Feb 10 20:13:25 CET 2004</date>
    <body>Excellent!   The Quartz list does not appear in the search list archive so I thought there was no way to get to it. I'd love to get rid of the SetPort/GetPort calls.   Stating the context explicitly should work since most of our routines know exactly where they are trying to draw.  Currently we are doing a GetPort to save previous port, doing a SetPort to draw where we want and then we do a SetPort back to the saved port. Thanks for the help!</body>
  </mail>
  <mail>
    <header>Re: Somewhat unique usage of PDF and Quartz</header>
    <date>Tue Feb 10 19:26:20 CET 2004</date>
    <body>Yeah, I've thought about that, but to be honest, I'm not sure how to even go about creating an ICC profile to do something like that: all white -&amp;gt; 30% yellow nC 0M (100-C/100 *.3 Y) 0K Guess I'll need to do some research on that. Because if that would work, that'd be great. Bryan</body>
  </mail>
  <mail>
    <header>Somewhat unique usage of PDF and Quartz</header>
    <date>Tue Feb 10 16:37:38 CET 2004</date>
    <body>On Jan 26, 2004, at 1:18 PM, Haroon Sheikh wrote: Haroon, after reading this I thought I'd give you some background about what we're doing and a problem we're facing... We have an app that allows the user to create ads for phone books. When working a yellow page ad, we don't actually require that a user build the ad with yellow everywhere. They can just build it normally and then we have a number of PostScript tricks that we use to remap any white areas to the yellow background (30% yellow).  (There is also some special treatment of cyan colors since mixing cyan and yellow yields green we don't just add a straight 30% yellow to cyan elements in the ad, we add a slightly different amount of yellow based on the amount of cyan present.) In today's Mac OS 9 environment, when a user imports an EPS, we get the EPS preview image and then we just walk the pixels and &amp;quot;wash&amp;quot; the image with yellow. And that's our on-screen presentation. As we move to Mac OS X, we are using the PS to PDF converter to really give our users a great experience when working with EPS objects. However, we're facing a similar problem to manfred: without some programmatic access to the filters we can't see how to remap certain colors in the PDF to certain other colors and still keep it vector. We're looking at having to convert it to a bitmap and then manually wash the pixels again. Not the end of the world, just not as ideal as we'd like. So, consider this as support for the idea that we could have programmatic access to the filters. Now the question remains: how flexible are these filters :) Bryan</body>
  </mail>
  <mail>
    <header>Re: Porting from QuickDraw to Quartz</header>
    <date>Tue Feb 10 15:29:11 CET 2004</date>
    <body>I don't know of a document like that.  The Quartz graphics model is similar to PostScript and PDF, which are quite a bit different than QuickDraw.  I'd suggest reading up on the graphics portion of one of those standards. The Quartz concept most analogous to a QuickDraw port is a graphics context (i.e., CGContextRef).  The Quartz API differs from the QuickDraw API in that there is no global state.  In other words, you don't get/set the current context, you pass a context into each graphic call as a parameter.  I suppose for porting you could create your own global state with wrappers for all the CG calls, but it might be easier just to move away from the GetPort/SetPort pattern. There aren't any QuickDraw style transfer modes in Quartz, so you can't do the typical XOR style, draw once to draw and draw again to erase, rubberbanding.  Typically you either redraw the entire graphics stack with each move, or you do your rubberbanding in a transparent overlay window above your standard graphics window. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Re: Porting from QuickDraw to Quartz</header>
    <date>Tue Feb 10 05:04:04 CET 2004</date>
    <body>Since your talking about ports, I'm going to presume your going to be moving to Carbon, so my comments will relate to that. The archives are here the page (text &amp;amp; web pages) Not that I've seen. If your going to move to Quartz, then I recommend also moving to compositing mode. You will be able to get ride of ALL of your SetPort/GetPort calls and GrafPtr references (assuming no more QuickDraw at all). Compositing mode means that all your drawing is done in a ControlDraw callback which gives you your context for any calls you need to make (there is no notion of current context, you must state your context explicitly). -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Porting from QuickDraw to Quartz</header>
    <date>Tue Feb 10 04:04:53 CET 2004</date>
    <body>I'm new to this list and it doesn't appear that there is a way to search the archives for this list.  I apologize in advance as I'm sure this question gets asked a lot. We are considering porting our QuickDraw based program to Quartz.   I'm trying to get a feel of how horrific this process is going to be. We have 35 MoveTo calls, 35 LineTo calls, and a smattering of other QuickDraw calls that I'm not to worried about converting.  We don't do any text or other shapes in QuickDraw as we draw everything ourselves using lines. However, we have 500+ SetPort calls, 250 GetPort calls as well as 300+ GrafPtr references. I've looked at the Quartz Primer and skimmed thru the reference manual. I have yet to find a good &amp;quot;Here's how you port from QuickDraw to Quartz&amp;quot; document. Does anyone know if a porting document exists anywhere? Are the SetPort, GetPort calls easily replaced or does replacing them causes a flood of other changes that must happen? How difficult has the port been for other developers? (I realize this is based on size of program, knowledge of programmer and the like.  I'm just trying to get a general gauge) We do interactive graphics with the &amp;quot;rubberbanding&amp;quot; of objects.  How difficult is that in Quartz? I appreciate any and all feedback. If there are lots of list summaries that discuss all of this I would be very pleased if someone could forward a few of them to my e-mail rather than having to redo all of the previous discussion.    Or if there is a way for me to get copies of them from the Archive I would be happy to read thru those as well. Thanks, Mike</body>
  </mail>
  <mail>
    <header>ColorSync Profile For PDF Image</header>
    <date>Mon Feb 09 21:51:50 CET 2004</date>
    <body>Perhaps this is related to that.  I'm extracting images from the &amp;quot;Resources/XObject&amp;quot; dictionaries in a PDF.  I'd like to convert the dictionary's &amp;quot;ColorSpace&amp;quot; entry into a ColorSync profile for the image. Can anyone provide suggestions or advice on how to do that? For future API directions, it would be nice to have a way to automagically extract color profiles and even whole images from a PDF. Something to get from a &amp;quot;Font&amp;quot; dictionary to an ATS font ID would be nice too.  I'm sure Quartz must have private functions to do those things now. Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: Draw PDF inverted</header>
    <date>Fri Feb 06 17:55:00 CET 2004</date>
    <body>Am 26.01.2004 um 20:18 schrieb Haroon Sheikh: I need to draw portions of the PDF inverted for a &amp;quot;highlight&amp;quot; effect. Whether to do it with a ColorSync effect or by drawing a rect with a special blend mode on top of it wouldn't matter much. Manfred</body>
  </mail>
  <mail>
    <header>Converting IconRef to ImageRef</header>
    <date>Fri Feb 06 16:16:14 CET 2004</date>
    <body>Is anyone out there had a luck of converting IconRef into CGImageRef. I need that for custom modification of the Icon Data while drawing them.(I am using .icns reosource for the Icon resources) How to get ARGB data from the .icns resource. Any Help? Cheers Dan --</body>
  </mail>
  <mail>
    <header>Re: Mouse events &amp;amp; Quartz</header>
    <date>Tue Feb 03 17:02:14 CET 2004</date>
    <body>The toolbox provides automatic bidirectional conversion to/from typeQDPoint and typeHIPoint. It doesn't matter what type of point was originally placed in the event; just ask for typeHIPoint and you'll get back that type instead. -eric</body>
  </mail>
  <mail>
    <header>Re: Mouse events &amp;amp; Quartz</header>
    <date>Tue Feb 03 15:12:13 CET 2004</date>
    <body>The HIToolbox defines HIPoint as a CGPoint and uses it for new API, so it looks like the intention is to move away from Point toward CGPoint. I would transform the point to match the context rather than the reverse as you suggest.  The chances are that you've set up the context in your window to have the origin at either the bottom-left (the CG default) or the top-left (the QD default) corner (perhaps with a scroll offset and scaling) so you can probably do the conversion outside of CG by mirroring the code you use to set up the context.  Another approach would be to adjust your point to use a right-hand coordinate system with the bottom-left corner as the origin, get the context's CTM, and transform your point through it (or maybe it's the inverse of the CTM, I can never get it straight the first time). CGPoint newPt = CGPointApplyAffineTransform(oldPt, or maybe... CGPoint newPt = CGPointApplyAffineTransform(oldPt, Nick</body>
  </mail>
  <mail>
    <header>Mouse events &amp;amp; Quartz</header>
    <date>Mon Feb 02 18:51:46 CET 2004</date>
    <body>Is there any plan to move the mouse events used by Carbon Events from typeQDPoint to typeCGPoint?  Also, how do we handle converting from a Global to Local point with Quartz? I'm looking at our mouse tracking code to move it to Quartz.  Here's the general path it takes. QD Drawing path: - Call TrackMouseLocation() and retrieve a QD Point - Call GlobalToLocal() to convert to local coordinates - Call QD to do our drawing. Would the correct Quartz equivalent be: - Call TrackMouseLocation() and retrieve a QD Point - Convert the mouse location to a float - Save the current graphical state - Transform the origin of the CGContext - Use Quartz to draw - Restore the graphical state Thanks, -Mike</body>
  </mail>
  <mail>
    <header>Re: When is PDF v1.4 Created?</header>
    <date>Tue Feb 03 00:29:54 CET 2004</date>
    <body>After extensive experimentation think I have a handle on this.  You get a v1.4 PDF if you draw a path with a color containing a non-unity alpha component or if you draw a path, text, or a bitmap image into a PDF context with a non-unity alpha value.  If you draw a PDF document into a context with a non-unity alpha value you get a transparent PDF document only if you're drawing into a bitmap context, not if you're drawing into a PDF context.  Whether or not you get a v1.4 PDF depends on the alpha of the other elements drawn, but has no effect on the appearance of a PDF document drawn within the PDF. In other words, in the following code the alpha value (i.e., 0.5) is respected if you're drawing to a bitmap context such as the screen but always ignored if you're drawing into a PDF context. Is this a bug or limitation in CG? In a similar vein, if I draw a shader into a PDF context I get a solid color instead of a gradient.  Is this a known limitation? Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: When is PDF v1.4 Created?</header>
    <date>Mon Feb 02 18:56:12 CET 2004</date>
    <body>Nick, can you provide some sample code that you are using and a sample PDF of the generated output. haroon</body>
  </mail>
  <mail>
    <header>Re: CGImage/CGBitmapContext, guaranteed to be in sync?</header>
    <date>Tue Feb 01 06:01:28 CET 2005</date>
    <body>On Jan 31, 2005, at 8:15 PM, John Kerr wrote: You can't make any assumptions about the way a CGLayerRef is implemented. It may not even involve pixels. If you want pixel level access to some drawing destination that you can also draw into with Quartz, use a CGBitmapContext and a CGImage.</body>
  </mail>
  <mail>
    <header>Re: CGImage/CGBitmapContext, guaranteed to be in sync?</header>
    <date>Tue Feb 01 03:15:10 CET 2005</date>
    <body>When you say re-drawing of new content, is that drawing limited to Quartz functions, or do we get access to the pixels?</body>
  </mail>
  <mail>
    <header>Re: CGImage/CGBitmapContext, guaranteed to be in sync?</header>
    <date>Tue Feb 01 02:59:41 CET 2005</date>
    <body>The recommended approach is to create a CGImageRef whenever the image changes. The CGImageRef is a small data structure so should be relatively inexpensive. In Tiger we have some new CGLayerRefs support that allow re-drawing of new content on an existing CGLayerRef and drawing the layer  ref into the CG Context.</body>
  </mail>
  <mail>
    <header>Re: CGImage/CGBitmapContext, guaranteed to be in sync?</header>
    <date>Tue Feb 01 00:39:50 CET 2005</date>
    <body>What is the recommended approach for drawing an image that is modified, through some non-CG action, after creation? E.g., a bitmap painting program, where pixels are touched directly and the affected portion of the image is then copied to a window? Previously we would have held that data in a GWorld, then used CopyBits to transfer sections of it to the screen. Our current approach is to create a CGImageRef each time we need to draw, ensuring that the current bitmap contents are always used for the CGImageRef. This works, but is this likely to be a performance hit in the longer term? Is creating a CGImageRef always going to be relatively cheap (for a no-copy data provider), or could the act of creation end up pulling Mbs of image data into some internal representation? Ideally there would be some kind of API to tell a CGImageRef &amp;quot;the data for this area in the image has changed&amp;quot;, but it doesn't look like there is one. -dair ___________________________________________________</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics Shading Quality</header>
    <date>Mon Jan 31 00:50:46 CET 2005</date>
    <body>On Sunday, January 30, 2005, at 01:35  PM, Timothy J.Wood wrote: Hi Tim, I was referring to a linear gradient drawn in a long thin rectangle. Not sure how the shadows topic came up. Dr Richard Rothwell Faraday R&amp;amp;D Google: Macintosh contractor Australia email@hidden Educational software developer for Mac/Win/Web</body>
  </mail>
  <mail>
    <header>Re: Quartz-dev Digest, Vol 2, Issue 16</header>
    <date>Sun Jan 30 03:35:52 CET 2005</date>
    <body>On Jan 28, 2005, at 2:55 PM, Richard Rothwell wrote: OmniGraffle 3 doesn't use CoreGraphics shading for the shadows; it's a blur of the alpha channel of the image of the shape being shadowed (set the fill of a shape to clear and note that the text is shadowed, or drag in a PDF with arbitrary transparent regions and note that the shadow conforms to the shape). OmniGraffle *also* doesn't use the CG shadowing support since it wasn't rich enough in 10.2 (only gray shadows, for one thing).  I'm excited about getting rid of this code (probably in OG4); hopefully getting rid of it won't introduce banding!</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics Shading Quality</header>
    <date>Sat Jan 29 06:30:18 CET 2005</date>
    <body>The CGShader calls my callback 4097 times for the 800 pixel long line - which is more than once per pixel !! So it's not directly contributing to the vertical bands I'm seeing. Adding a small amount of random noise (1 or 2 bits) would seem to be an ideal solution to the problem however since the shader is called once for the entire line and then those values are replicated down the screen to fill the area adding noise to the values doesn't change the vertical banding appearance. The bands might move from one run of the application to the next but they are constant all the way down the window for any given run. Unless there's a way for me to co-erce the CGFunction callback to be called for more than one iteration across the row I guess I'm going to have to resort to my own shading loop. Another altenative would be to add one bit of random noise across the entire image after CGShading has drawn it - I'm storing the shaded image to a CGBitmapContext with a CGImage to draw it (so I don't have to shade every time). Still seems like that would be quite time consuming though - this is a 800x600 (or so) image right now, and sometimes it might be quite a bit larger than that.</body>
  </mail>
  <mail>
    <header>Re: Quartz-dev Digest, Vol 2, Issue 16</header>
    <date>Fri Jan 28 23:55:03 CET 2005</date>
    <body>I tried this with OmniGraffle. For 15% and 29%, which is light grey, there is no visible banding and for 85 to 71%, which is dark grey, I imagine there might be just a trace of banding. There is no visible dithering. This is on a PowerBook with LCD display tilted over a range of angles. So _if_  OmniGraffle is using core graphics without dithering it would seem that there is no reason to believe that 256 greys cannot produce the gradient without banding which is what I would expect. This result also suggests that core graphics can do the job. On Saturday, January 29, 2005, at 07:36  AM, email@hidden wrote: Dr Richard Rothwell Faraday R&amp;amp;D Google: Macintosh contractor Australia email@hidden Educational software developer for Mac/Win/Web</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics Shading Quality</header>
    <date>Fri Jan 28 21:14:37 CET 2005</date>
    <body>On Jan 28, 2005, at 12:07 AM, Andrew Kimpton wrote: Don't know if the following will help, but it may be worth a shot... In the QuickDraw days, whenever I needed to fill a rectangle with a linear gradient (only for angles of 0, 90, 180 and 270), I would create an 32-bit offscreen GWorld. The size of the GWorld would always be 1 pixel in one dimension and the &amp;quot;length&amp;quot; of the gradient in the other.  For example, if the rect to fill was 800 x 600 and the gradient went from left-to-right, my GWorld would be 800 x 1. I'd then do the appropriate math and draw into the GWorld.  Finally, I used CopyBits with a mode of ditherCopy to take that smaller GWorld and render it over the larger area.  This gave beautiful dithering regardless of bit-depth of the destination device. I have no idea if CGImage will provide you the same. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics Shading Quality</header>
    <date>Fri Jan 28 17:11:01 CET 2005</date>
    <body>I'm not sure exactly how what you're doing works. Can you explain in more detail? Here's one suggestion, not knowing anything more about what you're doing. Can you convert your percentages to 8 bit integers, the convert back to floating point percentages and propagate the difference between that and the original to the next pixel? This would give you a very simple sort of dithering that might help eliminate the banding. Alternatively, you can add a very small amount of monochromatic noise to the gradient. For 8-bit per channel pixels, a random value of + or - 1 or 2 often gives good results. Darrin -- Darrin Cardani - email@hidden President, Buena Software, Inc. Video, Image and Audio Processing Development</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics Shading Quality</header>
    <date>Fri Jan 28 16:37:49 CET 2005</date>
    <body>On Jan 27, 2005, at 11:07 PM, Andrew Kimpton wrote: Perhaps there's nothing you can do in CG short of going to higher precision pixels (in Tiger).  However if it was me, I'd be interested to know how many times my callback is being called to see how many steps I should expect in the final result.  Then I'd examine the result pixel by pixel to see how my callback values were interpreted in the result.  Once you know what the I/O is to the CG shader you may be able to improve your results.</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics Shading Quality</header>
    <date>Fri Jan 28 16:11:42 CET 2005</date>
    <body>On Jan 28, 2005, at 12:07 AM, Andrew Kimpton wrote: If I had to guess (which I do... sorry) I would say that Photoshop is probably using dithering to reduce banding.  There's not really a good way to replicate that with Quartz 2D without doing something like drawing the shading into an offscreen, dithering it yourself, and returning the results using something like CGContextDrawImage.  If you're going to go through all that trouble... you may as well just skip Quartz 2D altogether and draw your own gradient with the dithering in the offscreen.</body>
  </mail>
  <mail>
    <header>CoreGraphics Shading Quality</header>
    <date>Fri Jan 28 07:07:00 CET 2005</date>
    <body>I'm trying to replicate a shading effect from a photoshop file 'in code' using CGShading etc. The shading is subtle - an axial shading gradation from 15% grey to 29% grey and back again over a span of approx 800 pixels with the 29% grey at the midpoint. My CoreGraphics drawn shading has much more pronounced banding in it than the photoshop example I'm trying to replicate. Is there anything I can do to control the amount of banding ? The headers don't seem to reveal a quality setting - my CGFunction callback just interpolates between the two shades according to the passed in position value. I'm using single precision floats for my calculations since CoreGraphics using single precision too I don't think changing that will help.</body>
  </mail>
  <mail>
    <header>Re: Bitmap context using CreateCGContextForPort</header>
    <date>Wed Jan 26 15:10:50 CET 2005</date>
    <body>On Jan 26, 2005, at 5:17 AM, james roy wrote: Quartz 2D, in general, doesn't support indexed colors.  Your best bet is to draw what you want in a 16 or 32 bit bitmap and then use CopyBits or some other tool that understands indexed colors to move the drawing into your port.</body>
  </mail>
  <mail>
    <header>Bitmap context using CreateCGContextForPort</header>
    <date>Wed Jan 26 12:17:41 CET 2005</date>
    <body>Hi All, Can somebody tell me regarding creating context by passing the bitmap port to CreateCGContextForPort. I have a offscreen graphics world created by NewGWorld with a pixel depth of 8. According to specifications for CreateCGContextForPort offscreen graphics worlds with pixel depths of 1, 2, 4, and 8 are not supported. What should we do in case we need to create a context for the bitmap port with pixel depth of 8. Your suggestions are welcomed. Thanks in advance. Regards, James. __________________________________ Do you Yahoo!? Yahoo! Mail - 250MB free storage. Do more. Manage less.</body>
  </mail>
  <mail>
    <header>Re: vImage - Histograms - beginning</header>
    <date>Tue Aug 22 16:18:48 CEST 2006</date>
    <body>On Aug 21, 2006, at 11:00 AM, George Warner wrote: Just a &amp;quot;me too&amp;quot; agreeing with George. When I've needed help with vImage I've gotten it on the performance list: Scott</body>
  </mail>
  <mail>
    <header>re: vImage - Histograms - beginning</header>
    <date>Mon Aug 21 18:00:46 CEST 2006</date>
    <body>Your issue isn't Quartz related... I'm guessing you probably want the performance mailing list. If nothing else I know that several of the Accelerate engineers hang out on that list. This shouldn't be a pointer (you just pass it by address). It should be a record with four values: the address of your data, its height &amp;amp; width and stride (bytes per row). 3rd parameter should be 256. -- Enjoy, George Warner, Schizophrenic Optimization Scientist Apple Developer Technical Support (DTS)</body>
  </mail>
  <mail>
    <header>Re: Newbie question about the Quartz Python binding.</header>
    <date>Mon Aug 21 12:23:14 CEST 2006</date>
    <body>a) stupid (go, get and read &amp;quot;Quartz for beginners like you&amp;quot;) b) ignorant (RTFM) c) impossible? I am desperately trying to learn more about Quartz. I would appreciate if someone could share a litte amount of your knowledge with me. regards, /frank</body>
  </mail>
  <mail>
    <header>vImage - Histograms - beginning</header>
    <date>Sun Aug 20 17:24:56 CEST 2006</date>
    <body>I hope that is the right list for vImage related stuff, if it isn't maybe someone can point me in the right direction. I am an extreme beginner in this... I have a question about the proper care and handling of vImage_Buffer, In my vImage_Buffer I have a single channel image of type Planar_F histogram calculation (hence using vImage) of this image(localvIMG). All I get are problems. vImage_Buffer *localvIMG; //initializers etc... the buffer does contain data since I am able to display it via CGImageRef status = vImageHistogramCalculation_PlanarF((vImage_Buffer for ( i = 0; i &amp;lt; 256; i++) the status returns okay but... on the printf the hist values are all zero. It all seems pretty straight forward.  Can anyone enlighten me on the use of vImageHistogramCalculation_PlanarF? thanks -- Bruce Johnson email@hidden</body>
  </mail>
  <mail>
    <header>Obtaining the color profile of a CGImage</header>
    <date>Sat Aug 19 01:32:08 CEST 2006</date>
    <body>I have a CGImageRef obtained from QuickTime (i.e., GraphicsImportCreateCGImage).  I want to display the name of the color profile and color model (i.e., RGB, CMYK, etc.), but I can't find a way to get the color profile back from a CGColorSpaceRef.  The closest I've been able to get is to call CFCopyDescription() for the colorspace object.  This sort of gives me some information but it's formatted more for debugging than for user information. Is there some way to get the color profile or its name from a CGImageRef or its CGColorSpaceRef? Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <date>Fri Aug 18 17:53:35 CEST 2006</date>
    <body>On 18 Aug 2006, at 16:18, Shawn Erickson wrote: rdar://4688040</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <date>Fri Aug 18 17:18:14 CEST 2006</date>
    <body>On Aug 18, 2006, at 7:21 AM, Jerry wrote: vImage provides channel permute operations. So you can convert as needed (of course that comes with a cost). It would be helpful if vImage could work with BGRA, etc.... I suggest folks file an enhancement request with Apple (may get it in time for Leopard if folks file now). -Shawn</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <date>Fri Aug 18 16:21:20 CEST 2006</date>
    <body>On 18 Aug 2006, at 15:08, Steve Mills wrote: For most operations this will be true, but for some it won't e.g. the matrix multiply operators where you'd have to create a different matrix. Many of the operations only seem to support ARGB though, so there's not even the option of XXXA. This seems to be a bit of a gap and I was wondering if I'd misread the documentation and there was some hidden flag equivalent to kCGBitmapByteOrder32Host in Quartz. All our images are in host byte order (i.e. ARGB on PPC, BGRA on Intel).</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <date>Fri Aug 18 16:08:28 CEST 2006</date>
    <body>I can't imagine that it would matter which order the rgb fields are in, as long as they're in the correct XXX- or -XXX end of the 32- bits. I'm pretty sure that the algorithms work on each channel individually and one doesn't affect the other. So it doesn't matter if byte 0 is red, blue, or green. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <date>Fri Aug 18 10:10:52 CEST 2006</date>
    <body>On 17 Aug 2006, at 22:41, Shawn Erickson wrote: Glancing at the vImage documentation, it appears that vImage handles ARGB or RGBA data, but I couldn't see any mention of BGRA data as you'll get on Intel Macs if you keep your data in native order. Is this true or can it handle BGRA data?</body>
  </mail>
  <mail>
    <header>Newbie question about the Quartz Python binding.</header>
    <date>Fri Aug 18 09:11:24 CEST 2006</date>
    <body>Actually, I have just begun to understand the basic concept and possibilities of the Quartz engine (I am reading &amp;quot;Quartz 2D for Mac OS X Developers&amp;quot; by Scott Thompson right now). Therefore, I would like to apologize if my questions are obvious for you (sorry for my bad English as well). I am thinking about to integrate some Quartz functionality as a web- service (converting HTML pages to PDF) and the Python binding seems to be the path to go. I have seen that a data provider for generating PDF documents is able to accept common http URLs. Further more, when I include some CSS in the source document, I am also able to give the content some style attributes like background color, position and so on. So, obviously, the HTML page gets rendered when it is provided as a data provider. My problem for now is that I am not able to get linked images or linked CSS documents to be rendered as well. Is it possible to have several different data providers and combine them together? My idea is to parse a HTML document, look for linked files (CSS, images and so on) and add them to the originally provided HTML source. Is this possible? Actually, I have seen a Python script on the Internet that converts HTML pages into PNG images. As far as I can understand, the script uses webkit and some special C binding for Python. It renders the HTML document with webkit and outputs a PNG image. The problem is that this script also uses a newer version of Python which is not supported by the Quartz binding. So, the script will not be able to output a PDF document. If someone could point me to the right direction or could give me some links to relevant documentation for example, I would be very grateful. With best regards, /frank arensmeier</body>
  </mail>
  <mail>
    <header>Re: CGWaitForScreenRefreshRects and CGWaitForScreenUpdateRects</header>
    <date>Fri Aug 18 03:46:18 CEST 2006</date>
    <body>Well I got them to work! My test code was in a foundation only tool without an NSApplication. Once I use NSApplicationLoad() events fire like mad. The non-functioning makes sense given my testing tool had no connection to the window server. To bad that the functions don't return an error when they find themselves in that situation. -Shawn</body>
  </mail>
  <mail>
    <header>CGWaitForScreenRefreshRects and CGWaitForScreenUpdateRects</header>
    <date>Fri Aug 18 02:48:27 CEST 2006</date>
    <body>I am playing around with alternate ways to get information about screen updates from CG services so I decided to see how CGWaitForScreenRefreshRects and CGWaitForScreenUpdateRects work. So far when I call either of those methods they block indefinitely. I assume I need to enable them somehow so that the CG services 1) knows that I want it to watch for screen updates on my behalf (otherwise I don't see how they would work without missing updates) and 2) get them to return in some reasonable amount of time. I know I can use matching callbacks but I want to understand what I may be doing wrong with these APIs... and if I am not doing something wrong file a defect or two about them. Note I have tried with and without my main thread running a runloop with not change in behavior. Thanks, -Shawn System Version: Mac OS X 10.4.7 (8J135) Kernel Version: Darwin 8.7.0 Machine Name: Power Mac G5 Quad Machine Model: PowerMac11,2 CPU Type: PowerPC G5 (1.1) Number Of CPUs: 4 CPU Speed: 2.5 GHz L2 Cache (per CPU): 1 MB Memory: 2 GB Bus Speed: 1.25 GHz Boot ROM Version: 5.2.7f1 Chipset Model: GeForce 6600 Type: Display Bus: PCI Slot: SLOT-1 VRAM (Total): 256 MB Vendor: nVIDIA (0x10de) Device ID: 0x0141 Revision ID: 0x00a4 ROM Revision: 2149 Displays: Cinema HD Display: Display Type: LCD Resolution: 1920 x 1200 Depth: 32-bit Color Core Image: Supported Main Display: Yes Mirror: Off Online: Yes Quartz Extreme: Supported Cinema Display: Display Type: LCD Resolution: 1680 x 1050 Depth: 32-bit Color Core Image: Supported Mirror: Off Online: Yes Quartz Extreme: Supported</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <date>Fri Aug 18 00:55:50 CEST 2006</date>
    <body>On Aug 17, 2006, at 3:22 PM, Steve Mills wrote: There was some discussion about this on the SciTech mailing list about a year ago.  The short answer is to just do (alpha * channel + 127) / 255.  GCC is smart enough to optimize division by numbers close to a power of two into just a modified multiply followed by a shift.  It turns out to be pretty fast.  (You can't fake it in C because you really need just the mulhw (multiply high word) instruction, so you have to write / 255 and hope that your compiler can do the right thing.) Brendan</body>
  </mail>
  <mail>
    <header>Re: Re: image byte order problem</header>
    <date>Thu Aug 17 23:41:05 CEST 2006</date>
    <body>vImage will utilize AltiVec/SSE/etc, memory stream hinting, and even multiple threads (tiling) if multiple cores exist to carry out its various operations. Apple hand tunes it for the hardware they support. -Shawn</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <date>Thu Aug 17 23:37:28 CEST 2006</date>
    <body>Why does it not make sense? I have a CGBitmapContext with kCGImageAlphaFirst data, and I want to create a CGImage that *needs* to be kCGImageAlphaPremultipliedFirst (because the Drag Manager only wants premultiplied data). I said it's too bad there is a version of CGImageCreate that takes the CGBitmapData AND a target alpha type that would cause the function to do the math on the bits and end up with premultiplied data. That makes perfect sense to me. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <date>Thu Aug 17 22:39:35 CEST 2006</date>
    <body>On Aug 17, 2006, at 1:22 PM, Steve Mills wrote: First, you should make sure that both formulas give you correct results — they don't look equivalent to me.  Second, when it comes to speed, opinions are irrelevant.  The only way to find out the truth is to measure the two cases and compare the results.</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <date>Thu Aug 17 22:31:47 CEST 2006</date>
    <body>Re: On Aug 17, 2006, at 11:42 AM, Steve Mills wrote: If your bits were created with non-premultiplied data and the first byte is the alpha data then you should create your CGImage using kCGImageAlphaFirst. Quartz can handle image data with alpha that is either premultiplied or not premultiplied, you just have to tell it which type you are supplying. Re: I don't see that this makes any sense. When you create a CGImage object using CGImageCreate you tell Quartz what type of alpha data you are supplying, premultiplied or non-premultiplied. If you are creating the CGImage yourself then you should specify the correct alpha info type. There should be no reason to change it.</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <date>Thu Aug 17 22:22:02 CEST 2006</date>
    <body>I know nothing about the efficiency of math operations and bit manipulation. Here's a formula I took from a 3rd party graphic lib (32-bit data): Anybody have any opinions on which is faster? I would guess that the bit shift would be faster than the division. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <date>Thu Aug 17 20:42:15 CEST 2006</date>
    <body>The CGBitmapContext is not drawn into with Quartz at all, but with a 3rd party graphics engine (we're cross-platoform), so the data really does contain relevant values in the alpha channel. Yes, it *should* have been created with kCGImageAlphaFirst, but it wasn't and it doesn't matter. I mean the that you give it an alpha info of what the source bits are, and if the alpha info for the CGImage to be created is premultiplied and the source bits are not premultiplied, then CGImageCreate would do the math for you. That wouldn't help with what I suggest, because it doesn't allow you to change the alpha info type of the resulting CGImage. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <date>Thu Aug 17 19:08:41 CEST 2006</date>
    <body>Re: On Aug 17, 2006, at 6:55 AM, Steve Mills wrote: If I understand you correctly, you are creating your data by drawing to a bitmap context that was created with kCGImageAlphaNoneSkipFirst and are creating a CGImage from those bits but using a BitmapInfo value of kCGImageAlphaPremultipliedFirst when you create the image. This makes no sense to me. I would expect you to create a CGImage from those bits using kCGImageAlphaNoneSkipFirst since that is how you generated them. If you do want to capture alpha in the bits that you want to create an image from then I suggest you use kCGImageAlphaPremultipliedFirst when you create your bitmap context, clear the bits using CGContextClearRect prior to drawing to them and then when you create your CGImage from those bits you should supply kCGImageAlphaPremultipliedFirst as the BitmapInfo value passed to CGImageCreate. Re: I'm not sure what &amp;quot;currentAlphaType&amp;quot; means. However you can call CGBitmapContextGetBitmapInfo on a Bitmap Context and get the CGBitmapInfo value that you used to create the context. That would be the correct value to use if you were creating an image from that context. Note that in Mac OS X v10.4 and later, Quartz has the function CGBitmapContextCreateImage which creates a CGImage from the bits in a bitmap context. David</body>
  </mail>
  <mail>
    <header>Re: How to render CIImage into CGBitmapContext or NSBitmapImage</header>
    <date>Thu Aug 17 18:13:26 CEST 2006</date>
    <body>On Aug 17, 2006, at 9:26 AM, Jaret Hargreaves wrote: To render a CIImage you have to draw it into a CIContext.  You can create a CIContext from a CGContext (analogously to how you create a CIImage from a CGImage).  So to render into a CGBitmap you first have to allocate memory for the backing store, create the CGBitmap, create a CIContext from the CGBitmap, and render the CIImage into the CIContext. Nick</body>
  </mail>
  <mail>
    <header>Re: Re: image byte order problem</header>
    <date>Thu Aug 17 17:30:25 CEST 2006</date>
    <body>For your pre-multiply operation consider (it will ensure it is fast on the hardware you are on)... -Shawn</body>
  </mail>
  <mail>
    <header>How to render CIImage into CGBitmapContext or NSBitmapImage</header>
    <date>Thu Aug 17 17:26:10 CEST 2006</date>
    <body>Using Core Image, I've been able to render the result of an image filter I wrote.  For the purposes of post-processing, however, I need to return the resultant image to python.  To do this, I need to generate a CGBitmapContext or NSBitmapImage to return the filtered image. How do I render the product of the CIImage into either a CGBitmapContext or NSBitmapImage?  Any suggestions or pointers to example code (ex. from the ADC sample code base) would be much appreciated. -Jaret</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <date>Thu Aug 17 17:22:21 CEST 2006</date>
    <body>That did the trick. Thanks for the confirmation. Too bad CGImageCreate doesn't take a currentAlphaType so it can be smart enough to do the math itself. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>RE: image byte order problem</header>
    <date>Thu Aug 17 16:27:27 CEST 2006</date>
    <body>Even I have faced the same kind of problem while implementing the Opacity filter. The problem was when the image is semi transparent the image colors appears to be distorted. The problem got solved when I pre-multiplied the components... But I am not sure how not premultiplying them would make only the last channel come out wrong. -----Original Message----- From: quartz-dev-bounces+vinayprabhu=email@hidden [ n Behalf Of Steve Mills Sent: Thursday, August 17, 2006 7:26 PM To: email@hidden quartz-dev Subject: image byte order problem I recently noticed that our drag image looks wrong. A few months ago I made some changes to ensure that we're creating it correctly for PPC and Intel, and it was working correctly on both. When we create our internal CGBitmapContext, the alpha type is kCGImageAlphaNoneSkipFirst. When we create the CGImage to give to the Drag Manager, the alpha type is kCGImageAlphaPremultipliedFirst, because that's what I was told is the proper type to use. I do a straight byte copy, so the colors really are NOT premultiplied. The problem shows up on PPC and Intel when any part of the bitmap has visible transparency (alpha 1-254) and some color. If it's opaque, the color looks correct. The case I noticed involves a transparent blue area (argb = 0x7f ee ee ff), but it shows up as what I guess is 0x7f ee ee 7f. I say &amp;quot;guess&amp;quot; because I brought up a color picker and found a color that looks close to what I see when I drag the transparent blue object. As an experiment, I created the CGImage with kCGImageAlphaFirst instead, and this caused it to show up correctly (again, on both architectures). So it *appears* that the last channel value is being replaced with the alpha channel value, perhaps in CGImageCreate. I don't know if I previously tested the transparent blue case back when I was initially getting this to work on PPC and Intel. Can anyone tell me if anything was changed in 10.4.7 that would cause this? Or am I doing something wrong by not manually premultiplying the colors when creating the CGImage? I don't see how not premultiplying them would make only the last channel come out wrong. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>image byte order problem</header>
    <date>Thu Aug 17 15:55:42 CEST 2006</date>
    <body>I recently noticed that our drag image looks wrong. A few months ago I made some changes to ensure that we're creating it correctly for PPC and Intel, and it was working correctly on both. When we create our internal CGBitmapContext, the alpha type is kCGImageAlphaNoneSkipFirst. When we create the CGImage to give to the Drag Manager, the alpha type is kCGImageAlphaPremultipliedFirst, because that's what I was told is the proper type to use. I do a straight byte copy, so the colors really are NOT premultiplied. The problem shows up on PPC and Intel when any part of the bitmap has visible transparency (alpha 1-254) and some color. If it's opaque, the color looks correct. The case I noticed involves a transparent blue area (argb = 0x7f ee ee ff), but it shows up as what I guess is 0x7f ee ee 7f. I say &amp;quot;guess&amp;quot; because I brought up a color picker and found a color that looks close to what I see when I drag the transparent blue object. As an experiment, I created the CGImage with kCGImageAlphaFirst instead, and this caused it to show up correctly (again, on both architectures). So it *appears* that the last channel value is being replaced with the alpha channel value, perhaps in CGImageCreate. I don't know if I previously tested the transparent blue case back when I was initially getting this to work on PPC and Intel. Can anyone tell me if anything was changed in 10.4.7 that would cause this? Or am I doing something wrong by not manually premultiplying the colors when creating the CGImage? I don't see how not premultiplying them would make only the last channel come out wrong. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>CGContextDrawImage error catching</header>
    <date>Wed Aug 16 02:12:06 CEST 2006</date>
    <body>My applications opens up large PSD files (1.5GB big) Before my code was using a QuickTime Graphics Importer and when the GraphicsImportDraw run out of memory I got an error back and did the necessary steps to open these files different. Now that I converted my code to ImageIO I notice that CGContextDrawImage  doesn't return an error and the image that it is drawn is black and in the console I notice something like this: iProcessor2(7251,0x130b9e00) malloc: *** vm_allocate(size=652402688) failed (error code=3) iProcessor2(7251,0x130b9e00) malloc: *** error: can't allocate region iProcessor2(7251,0x130b9e00) malloc: *** set a breakpoint in szone_error to debug iProcessor2(7251,0x130b9e00) malloc: *** vm_allocate(size=326176768) failed (error code=3) iProcessor2(7251,0x130b9e00) malloc: *** error: can't allocate region iProcessor2(7251,0x130b9e00) malloc: *** set a breakpoint in szone_error to debug Is there a way I can catch these errors? Is there a function that i can use that gives me back something like global errors like in the QuickDraw days. ? I also use cocoa exceptions but they don't seem to help in this case. marc</body>
  </mail>
  <mail>
    <header>Re: Getting pixel values from CIImage processed data</header>
    <date>Tue Aug 15 00:54:37 CEST 2006</date>
    <body>You need to create a CIContext out of a CGBitmapContext. Then you draw in the CIContext and just get the bytes in the array you allocated. Here's basically what I use (might not compile, just copy/pasted and removed the useless details). Of course if you want floating point data you'd need a 'workingPixelData' buffer in float*. CGColorSpaceRef colorSpace = CGColorSpaceCreateWithName if (bitsPerComponent==32) bitmapInfo = kCGImageAlphaPremultipliedLast | CGContextRef workingCGContextRef = CGBitmapContextCreate((unsigned char*)workingPixelData,w,h, bitsPerComponent,bytesPerRow, CIContext* cicontext = [CIContext contextWithCGContext:workingCGContextRef  options:[self //and just draw in the context [cicontext drawImage:[filterBlack valueForKey:@&amp;quot;outputImage&amp;quot;] ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Getting pixel values from CIImage processed data</header>
    <date>Tue Aug 15 00:39:45 CEST 2006</date>
    <body>Thanks to some great feedback, I am closer to solving the problem of returning pixel values from the result of a CIImage. I have little experience in ObjC and would appreciate any suggestions for how to solve the following problem or where example code could be found (I have all the ADC sample code). 1. How do I render the result of a CIImage into a CGBitmapContext? The constructor for this class listed in the ADC docs have it's contents listed, but how the arguments are passed is not clear to me. 2. Once data has been retrieved as an NSData object from the CGBitmapContext, how can it be converted to a string for post-processing? -Jaret</body>
  </mail>
  <mail>
    <header>Saving results of CI filter</header>
    <date>Mon Aug 14 21:46:26 CEST 2006</date>
    <body>I am performing image filtering in a python framework by calling the objC method listed below.  Currently, the python code converts the image slice data into a raw string(&amp;quot;data&amp;quot;), which is handed to the method, and expects a modified string of the same length to be returned. I know that CIImage is a recipe, not actually image data. How do I save the processed image data (represented by &amp;quot;outputImage&amp;quot;, a CIImage) to return back to python?  Essentially, how can I reconstitute a string of processed output after filtering (for use when comparing pre- and post-filtered data in python)? Thanks in advance for any help.  Any suggestions of a simpler or more elegant solution are also welcome. ***ObjC filtering method*** - (id) initWithData:(char*)data xDim:(int)x yDim:(int)y bytesPerPixel:(int)pixelSize k:(float)filterParam iter:(int)iterations // Copy the image into inputData via memcpy // Create a CIImage from inputData for processing NSData *imageData = [NSData dataWithBytes:(char*)inputData length:(x // Initialize the aniso filter [anisoFilter setValue: [NSNumber numberWithFloat: filterParam] // Perform first iteration of filtering // Perform subsequent iterations of filtering as needed for(i=0;i&amp;lt;(iterations-1);i++) // How do we get CIImage &amp;quot;outImage&amp;quot; back into NSData &amp;quot;inputData&amp;quot;? //inputData = // Copy the filtered image directly back to data</body>
  </mail>
  <mail>
    <header>Re: Draw JPEG2000 images into CG PDF contexts</header>
    <date>Thu Aug 10 17:02:16 CEST 2006</date>
    <body>Yes, Image I/O is fine, but I need the image Jpeg2000 encoded for &amp;quot;highly compressed&amp;quot; PDFs. Using bitmap data in a cgImage it will only be zlib compressed as far as I can see. -- Rene Rebe - ExactCODE - Berlin - Europe, Germany +49 (0)30 / 255 897 45</body>
  </mail>
  <mail>
    <header>Re: Draw JPEG2000 images into CG PDF contexts</header>
    <date>Thu Aug 10 16:36:51 CEST 2006</date>
    <body>On Aug 9, 2006, at 6:18 AM, René Rebe wrote: You should be able to use the functionality in Image I/O to read a JPEG 2000 image into a CGImage.  Once you have done so, you can draw it into any CGContext you like, including a CGPDFContext. CGImageSource/index.html CGImageDestination/index.html If you don't want to use Image I/O (because you need to run on pre- Tiger systems) then you can also try using the QuickTime Image Importers and create a CGImage that way.  I don't know for sure that there's a JPEG 2000 importer, but it's a good bet.</body>
  </mail>
  <mail>
    <header>Using PDFKit to move page from a document to another</header>
    <date>Thu Aug 10 16:34:51 CEST 2006</date>
    <body>Hi, i'm trying to use PDFKit to make an application which also allows importing pages from other documents. My problem is that even if i managed doing the import work without crashing, sometimes it just insert a white page, more little than the others and sometimes it insert the page, bigger than the others and all the ones of the original document are made blank. I guess this happens when the documents are not of the same size, but i cannot be completely sure of this either. What could i use to adjust the page to the receiving document before inserting it? I tried the setBoundsForBox: of NSPage passing by setting the NSRect size to the one of a page of the receiving document. I hope i was clear explaining my problem. Anybody knows a workaround? Attachment:</body>
  </mail>
  <mail>
    <header>Draw JPEG2000 images into CG PDF contexts</header>
    <date>Wed Aug 09 15:18:07 CEST 2006</date>
    <body>I wonder if there is a possibility to draw JPEG2000 data into CG PDF contexts or if we have to wait for a JPEG2000DataProvider to be able to do so? -- Rene Rebe - ExactCODE - Berlin - Europe, Germany +49 (0)30 / 255 897 45</body>
  </mail>
  <mail>
    <header>Missing fonts</header>
    <date>Tue Aug 08 15:46:11 CEST 2006</date>
    <body>I am not sure this is the right list to ask this question, so pardon me if this should be asked elsewhere. Where should I look if I want to know if an NSTexView's content (loaded from RTF) requires a font that is currently not available? ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <date>Mon Aug 07 17:46:53 CEST 2006</date>
    <body>On 7 Aug 2006, at 16:13, Nick Nallick wrote: Oh yes. I was thinking of Java2D instead of Quartz.</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <date>Mon Aug 07 17:13:39 CEST 2006</date>
    <body>On Aug 7, 2006, at 6:45 AM, Jerry wrote: The other problem with this is there is no API to extract the stroke path from Quartz.  The best you can do is make the current path into the path equivalent to the stroke of the previous current path (another enhancement request I've filed).  However there's no way to extract the current path from a CG context. Nick</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <date>Mon Aug 07 14:45:36 CEST 2006</date>
    <body>On 7 Aug 2006, at 08:05, Vinay Prabhu wrote: You don't want to use the bounding box in the calculation. What you need to do is walk the path, and at each point work out its direction, then work out a point which is N units &amp;quot;to the left&amp;quot;, i.e. if you have a line segment from x1,y1 to y2, y2, you need to work out dx=x2-x1,dy=y2-y1, normalize it, multiply by N (your line width), and produce a new point x1-dy,y1+dx. Do this at both ends of each segment. You'd have to flatten out Bezier curves to line segments first. This won't be too bad for paths which don't turn too quickly, but for ones which do, you'll have to work out the miter points which is more complicated. How about using the Quartz stroke engine to do the work for you? Take your path, then convert the stroke of that path to another path. You now have one path which has the inner and outer paths joined together. If you could find where the join is, you could separate it into the two paths you want. I suspect that working out the join might be tricky though.</body>
  </mail>
  <mail>
    <header>RE: PenAlignment(GDI+) alternative in Quartz</header>
    <date>Mon Aug 07 09:05:24 CEST 2006</date>
    <body>Hi, I thought of a method to solve this problem, We know the bounding rectangle of the path. Now we keep the center of the rectangle (midX and midY) as reference. While constructing the CGPath, for each point we can draw a imaginary line from center to the point using line dda algorithm. The extrapolate the line with the offset we want. Use the extrapolated points to create a new path which will be parallel to the original path. I have not yet implemented this. Any idea's whether this works or not? Regards Vinay -----Original Message----- From: quartz-dev-bounces+vinayprabhu=email@hidden [ n Behalf Of Nick Nallick Sent: Saturday, August 05, 2006 8:20 PM To: Quartz-Dev (E-mail) Subject: Re: PenAlignment(GDI+) alternative in Quartz As I mentioned before, you can do this for many simple paths but it's not a complete solution.  For example, consider the classic five pointed star used to illustrate the non-zero winding vs. even/odd fill rules (i.e., a five pointed star drawn with five overlapping lines).  Trying to clip to this path won't give you satisfactory results for either the &amp;quot;inside&amp;quot; or &amp;quot;outside&amp;quot; problems. I've illustrated what you get here: stroke_example.jpg I filed an enhancement request for this in March (rdar://problem/ 4472716) which is now closed, so I'd guess the idea wasn't warmly received.  If it's something people want I'd suggest they do likewise. If the original path were only being stroked it wouldn't be a problem, you could just export the path used to create the adjusted stroke (which might be either filled or stroked itself depending on the algorithm).  If the original path was to be both filled and stroked, I would expect this to be handled by breaking the stroke out as a second path drawn over the top of the original filled path. Nick _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: .in This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <date>Sat Aug 05 16:50:08 CEST 2006</date>
    <body>On Aug 5, 2006, at 7:47 AM, Jens Ayton wrote: As I mentioned before, you can do this for many simple paths but it's not a complete solution.  For example, consider the classic five pointed star used to illustrate the non-zero winding vs. even/odd fill rules (i.e., a five pointed star drawn with five overlapping lines).  Trying to clip to this path won't give you satisfactory results for either the &amp;quot;inside&amp;quot; or &amp;quot;outside&amp;quot; problems. stroke_example.jpg I filed an enhancement request for this in March (rdar://problem/ 4472716) which is now closed, so I'd guess the idea wasn't warmly received.  If it's something people want I'd suggest they do likewise. If the original path were only being stroked it wouldn't be a problem, you could just export the path used to create the adjusted stroke (which might be either filled or stroked itself depending on the algorithm).  If the original path was to be both filled and stroked, I would expect this to be handled by breaking the stroke out as a second path drawn over the top of the original filled path. Nick _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <date>Sat Aug 05 15:47:18 CEST 2006</date>
    <body>Vinay Prabhu: If I understand your requirement correctly, I think this can be achieved using clipping. For starters, to draw an outline of thickness N entirely inside a path, first clip to the path and then stroke the path with thickness N + 2. To draw entirely outside the path is trickier, but I believe it can be done, if you’re using the even-odd winding rule, by adding an appropriate surrounding rectangle to the clipping path. Once you’ve drawn the inner and outer outlines, the “center outline” (oxymoron alert) is simply a matter of stroking the path normally. All this breaks down if you need transparency, although on a raster device you could render the colour and alpha channels separately and then composite. -- Jens Ayton</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <date>Sat Aug 05 05:39:43 CEST 2006</date>
    <body>That seems reasonable. I guess you're stuck with using a enough segments to come &amp;quot;sufficiently close&amp;quot; to the parallel path.</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <date>Sat Aug 05 04:28:41 CEST 2006</date>
    <body>There is no Bezier curve (4 control points) that is the exact offset of another Bezier curve. I don't remember the proof (and it being late Friday I'm too lazy to look it up), but it could be looked up. Nope. For this there is an easy proof. You need the above and the fact that a Bezier curve can be split at any point along it into two Bezier curves that together are the exact shape of the original. (de Casteljau's algorithm.) To do so at an arbitrary point requires finding the t value for the point which may be hard but it is possible in principal. Now assume that you could construct the exact offset of a Bezier curve using two Bezier curves. (The two is just for convenience - you can easily extend it to N.) Take the point where your two offset curves join and find the equivalent point on the original curve. (Construct the tangent to the offset at the joint point, take the perpendicular and find its intersection with the original curve.) You would then have *two* Bezier curves, each with an exact offset that is also a Bezier curve. Bob</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <date>Fri Aug 04 23:36:43 CEST 2006</date>
    <body>On Aug 4, 2006, at 4:22 PM, Robert Clair wrote: Sorry... the mathematical geek in me just leaked out. Is it the case that there is no bezier &amp;quot;curve&amp;quot; exactly parallel to another, or is it the case that, given a single segment of a bezier curve, there is no way to create the parallel curve using a single segment. That is to say, given a particular bezier curve segment (with four points in it's control polygon), could I construct a bezier curve of more than one segment which does lie parallel to the original curve at all points? Offhand I would guess the answer is &amp;quot;yes&amp;quot; at least theoretically as you could simply define a bezier curve with an infinite number of segments.  Each of those individual segments is the degenerate case where each of the four control points (in the cubic case) lie on the same point.  I wonder if there is a curve with fewer segments that is also parallel... Hmmm. Of course in graphics you really don't need a curve that is EXACTLY parallel... &amp;quot;close enough&amp;quot; would be fine.  Certainly that is possible using multiple segments. Scott</body>
  </mail>
  <mail>
    <header>Re: Using a CGDataProvider for my own purposes.</header>
    <date>Fri Oct 21 17:37:19 CEST 2005</date>
    <body>On Oct 21, 2005, at 8:06 AM, Shawn Erickson wrote: Of course my suggestion still has you fighting with the fact that a data consumer reference is basically opaque, so looking past that isn't likely the best thing. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Using a CGDataProvider for my own purposes.</header>
    <date>Fri Oct 21 17:06:31 CEST 2005</date>
    <body>On Oct 21, 2005, at 7:23 AM, Scott Thompson wrote: Off the top of my head... why not define your own data providers that wraps a system provided data provider. You basically would pass calls into the real data provider doing your CRC calculations with the bytes it returns. See CGDataProviderCreate and the various callbacks you would implement and provide to that method. Of course use of a data consumer may also make sense... don't know how you are working with the CGImages. -Shawn</body>
  </mail>
  <mail>
    <header>OpenGL, CI, and transparency</header>
    <date>Tue Oct 18 01:06:21 CEST 2005</date>
    <body>I've got a project that, simply put, renders CIImages to a OpenGLView. I'd like to replace some pixels on with transparent ones.  So, a white pixel is fully transparent, etc.  I'd like to do this with as little manual pixel tweaking as possible. Here is what I've done so far. 1) overriden isOpaque to return NO in the view 2) [[self openGLContext] setValues:&amp;amp;notOpaque forParameter: NSOpenGLCPSurfaceOpacity]; // to let clearcolor render clear What I'd like is to have white pixels rendered as fully transparent. I've played with some CI filters.  It seems the color effects filters might be what I want (map white -&amp;gt; clear).  But I saw no documentation on using these.  Before I experiment too much solo, I'd like to make sure I'm going about this the best way. As a more general question, I'd like to know how closely vImage and CI are integrated.  I'd like to do a convolution operation on some frames.  CI doesn't seem to offer an emboss filter right now, but vImage does.  I'm not too familiar with the nuts and bolts of the two frameworks; would doing a convolution using vImage require graphics card system memory copying?  Should I just shoot for writing a custom CIFilter?</body>
  </mail>
  <mail>
    <header>Re: Transforming 16 bits images with CoreImage</header>
    <date>Fri Oct 14 11:34:57 CEST 2005</date>
    <body>Am 13.10.2005 um 14:56 schrieb Michel Schinz: This looks a lot like my recent problem (&amp;quot;CGContext munging hdr images?&amp;quot;). For the record, I couldn't find a problem with my code, and filed a bug report. the more, the merrier. Greetings, Johannes Fortmann</body>
  </mail>
  <mail>
    <header>Transforming 16 bits images with CoreImage</header>
    <date>Thu Oct 13 14:56:06 CEST 2005</date>
    <body>I'm having troubles when trying to transform 16 bits images with CoreImage, as I can't get a 16 bits image as a result. I have written the following test program, which opens an image using ImageIO, passes it through the CIColorControls filter using CoreImage, and saves the result using ImageIO again. I observe the following behaviour: if I give as input a 16 bits image and leaves the filter with its default configuration (which does not change the image), I get back a 16 bits file, as expected. However, as soon as I try to set a filter parameter to a non-default value (i.e. I try to increase the contrast using a value different than 1.0 for the &amp;quot;inputContrast&amp;quot; parameter), I get an 8 bit file. Therefore, it seems to me that CoreImage cannot transform 16 bits images without downgrading them to 8 bits. Am I correct? And if I am, is it actually possible to do something about that and obtain a 16 bits image as a result? Here is my sample code (I'm using a 3*1 pixels image as a test, which explains the various constants appearing here): CGColorSpaceRef cs = CGColorSpaceCreateWithName NSDictionary* opt = [NSDictionary dictionaryWithObject:cs // the following line forces a transformation of the image, which brings it down to 8 bits [flt setValue:[NSNumber numberWithFloat:1.1] CGContextRef bmContext = CGBitmapContextCreate(NULL, CIContext* ctx = [CIContext contextWithCGContext:bmContext options: [NSDictionary dictionaryWithObject:cs CGImageRef cgImg = [ctx createCGImage:[flt CGImageDestinationRef dst = CGImageDestinationCreateWithURL Thanks, Michel.</body>
  </mail>
  <mail>
    <header>Re: Trying out CGEventTapCreate, but it appears to be broken!</header>
    <date>Sun Oct 09 22:19:43 CEST 2005</date>
    <body>Thanks a lot for your help!  This fixed the problem like a charm.  (I'm not nearly as familiar with CoreFoundation apps as I am with Carbon and Cocoa.) On Sun, 09 Oct 2005 13:00:46 -0400, Mike Paquette &amp;lt;email@hidden&amp;gt; wrote:</body>
  </mail>
  <mail>
    <header>Re: Trying out CGEventTapCreate, but it appears to be broken!</header>
    <date>Sun Oct 09 19:00:46 CEST 2005</date>
    <body>On Oct 9, 2005, at 2:49 AM, Brian Kendall wrote: A listen-only event tap listens to events, but does not filter them. Without the kCGEventTapOptionListenOnly option, the event tap acts as a filter, which can effectively stall the flow of events through the system unless it acts on and processes events promptly.  At the very low level that event taps operate at, the system guarantees that events will be delivered sequentially.  (We wouldn't want that modifier key for your mouse click to arrive after the click, right?) The symptoms you are describing are all indicative of the tapping application not yet actually listening for events on the event tap's CFMachPortRef.  In your source code, nothing was done to actually place the tap's CFMachPortRef in the Carbon MainEventLoop. The CGEventTap code doesn't do this automatically, because there may be many different kinds of run loops in applications.   In general, for fitering event tap applications, I recommend running the event tap filter in it's own thread, with it's own CFRunLoop, rather than in the main application thread.  Trying to filter all the system's events in the main application thread effectively throttles the flow of events through the  system to the rate at which your application can process, draw, and fetch the next event. So far, so good.  You have an event tap port.  Now you need to turn it into a runloop source, and in this case, add it to the runloop behind the Carbon main event loop.  Note that I normally don't recommend doing this, as it will throttle the flow of events to the rate at which the main loop can process and draw.  In this case, the main event loop will just be servicing the CGEventTap, so we might be able to get away with this. if ( eventSrc == NULL ) // Get the CFRunLoop primitive for the Carbon Main Event Loop, and add the new event souce CFRunLoopAddSource(GetCFRunLoopFromEventLoop(GetMainEventLoop if ( eventSrc ) In a command line tool, faceless app, or daemon, you can do something similar purely at the CoreFoundation level: CGEventRef printEventCallback(CGEventTapProxy proxy, CGEventType type, CGEventRef event, void *refcon) printf( &amp;quot;Got event of type %d\n&amp;quot;, type ) int main(int argc, char ** argv) eventPort = CGEventTapCreate(kCGSessionEventTap, kCGHeadInsertEventTap, kCGEventTapOptionListenOnly, CGEventMaskBit(kCGEventOtherMouseDown), printEventCallback, if ( eventPort == NULL ) if ( eventSrc == NULL ) if ( runLoop == NULL )</body>
  </mail>
  <mail>
    <header>Re: Trying out CGEventTapCreate, but it appears to be broken!</header>
    <date>Sun Oct 09 18:54:56 CEST 2005</date>
    <body>I would suspect that this API is the core of how GetEventMonitorTarget () works, as such I bet you won't get anything unless Access for Assistive Devices is turned on. But if you are just trying to monitor keyboard &amp;amp; mouse actions while not frontmost, just use GetEventMonitorTarget() with your normal Carbon Events code and your life will be simpler. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Trying out CGEventTapCreate, but it appears to be broken!</header>
    <date>Sun Oct 09 11:49:10 CEST 2005</date>
    <body>I just recently discovered the new CoreGraphics events API that are in Tiger, and in particular I'm interested in event taps.  So far they're undocumented, but there was enough information in /System/Library/Frameworks/ApplicationServices.framework/Frameworks/CoreGraphics.framework/Headers/CGEvent.h for me to get the basic jist of it. So I tried throwing together a quick Carbon app that would make an event tap that outputs a message when it's triggered (for say, pressing the middle mouse button) but it doesn't work.  If I set the event tap to be a listener only, everything on my computer behaves normally but no event is ever received by the tap.  More alarming though is when I set it not to be a listening-only tap.  As soon as I clicked a mouse button &amp;gt; 2, the entire system would lock up.  No keypresses or mouse events would be registered by anything at all, though the cursor would still move around on the screen. This problem persists until a few seconds after the application terminates or the event tap is released.  I've tried making the function the event tap calls do various different minor things like beeping or just doing nothing, but the UI lock still occurs.  Also, changing what events are being monitored seems to have no effect - tapping any kind of event causes this to happen. So... what can I do?  There's no documentation about these functions so I can't check if I'm doing something wrong, and no one else seems to have used them yet.  I would have a hard time believing that they're just broken. CGEventRef eventTapFunction(CGEventTapProxy proxy, CGEventType type, CGEventRef event, void *refcon) int main(int argc, char* argv[]) // window and menus set up here machPortRef =  CGEventTapCreate(kCGSessionEventTap, kCGTailAppendEventTap, 0, CGEventMaskBit(kCGEventOtherMouseDown), (CGEventTapCallBack)eventTapFunction, if (machPortRef == NULL) if (machPortRef) Any help would be greatly appreciated! - Brian</body>
  </mail>
  <mail>
    <header>Populating CGLayers with Adobe Illustrator Layers?</header>
    <date>Sun Oct 09 02:34:43 CEST 2005</date>
    <body>Hello... Is it possible to populate CGLayers with Adobe Illustrator Layers? How?</body>
  </mail>
  <mail>
    <header>ImageIO capabilities</header>
    <date>Wed Oct 05 02:44:19 CEST 2005</date>
    <body>Hello, I searched for information about image file types supported by ImageIO but could not it in documentation. I would like to support as many file formats as I can and I was wondering if ImageIO is a one-stop solution on Tiger, specifically if it falls back to QuickTime importer for images it cannot read by its core libraries (or is it vice versa: QT using ImageIO in Tiger?). Thanks, Tom</body>
  </mail>
  <mail>
    <header>Re: Best way to display/manupulate large images</header>
    <date>Mon Oct 03 21:50:32 CEST 2005</date>
    <body>On Sep 29, 2005, at 2:14 PM, Scott Thompson wrote: Yes. I remember that, too. But I can't find any reference to it on Apple site. Does anyone have a pointer to any documentation about it? --</body>
  </mail>
  <mail>
    <header>CGContextShowTextAtPoint and Alpha Background</header>
    <date>Mon Oct 03 20:47:30 CEST 2005</date>
    <body>Hi All, I don't have large experience on Quartz, so sorry if it is a basic question or a FAQ. I have a strange problem when I call the CGContextShowTextAtPoint API to draw some text on my background. I see the problem only when my background is filled with a color that contais 0 value to alpha (entire transparent). In this case, the drawed text comes with a filled black square behind the caracter. The square has exactly the same size (width and height) of a caracter. Apparentely, It works fine when I change the alpha value of the color to 1 (solid color) and call the API again. In this case I cannot see the black square. Has anyone already seen anything like that ? Following is the source code I'm using to do it. Am I doing anything wrong ? //Getting a Second Display, Capture It and Create a Context to Draw. /* Get a small 32 bit display mode */ mode = CGDisplayBestModeForParameters(display[1], 32, 720, 486, stripeRect  = CGRectMake (0, 0, 720, 486); // stripe //Set the back color with 0 alpha value CGContextSetRGBFillColor (context, 1, 0, 0, 0); &amp;lt;------- See alpha value here CGContextFillRect (context, stripeRect); &amp;lt;---- Clean the back with the fill color //Set the text color to white //Draw the white text on the transparent background. CGContextShowTextAtPoint (context, 15, (5 + pos), (const Thanks for your attention and time. Any help will be really appreciated. William</body>
  </mail>
  <mail>
    <header>Quartz 2D Extreme</header>
    <date>Mon Oct 03 18:45:24 CEST 2005</date>
    <body>The application I develop is now Quartz 2D extreme savvy.&amp;nbsp; There are a few places in the UI that look incorrect due to invalid clipping or a path is fill instead of &lt;SPAN class=593322716-03102005&gt; When will Quartz 2D extreme be accessible by non-developers?&amp;nbsp; Will 10.4.3 likely fix more Q2DX &lt;SPAN class=593322716-03102005&gt; -Jim</body>
  </mail>
  <mail>
    <header>Re: Understanding how to use the NSAffineTransform (Scaling) for an EPS vector image</header>
    <date>Sun Oct 02 22:38:03 CEST 2005</date>
    <body>This isn't a Quartz question but a Cocoa question since all classed you are using are provided by the Cocoa framework. It is better to use the cocoa-dev list[1] for such questions. The following saves (pushes it the stack) the graphics state before you change the graphics states current tranform matrix (CTM). The following line concatenates your scaling transform the CTM. The following restores (pops it off the stack) the graphics state that you save two lines up. This is a graphics state that doesn't have your scaling transform applied. The following asks you super class to draw itself (in this case NSImageView). No scaling transform is in place. I see two issues, one is outlined above... basically your scaling transform is not in place when you attempt to draw your image. I suggest trying the following. [scaleTransform scaleBy:0.5];  // The scaling factor '0.5' is used for testing. [super drawRect:rect];   // This is necessary to actually draw the image. The other issue I see is that you appear to be using NSImageView which is a control simply as a way to display an image. This is likely over kill and possibly problematic for the scaling you are attempting to do. I suggest instead subclassing NSView instead and dropping that custom view into your window/containing view. So something like (ignores any possible use of scroll view, etc.)... @interface epsImageView : NSView ... ... @end ... ... [scaleTransform scaleBy:0.5];  // The scaling factor '0.5' is used for testing. // Assumes myImage is an NSImage instance existing someplace... // (if you want image to blend with background then use NSCompositeSourceOver) [myImage drawAtPoint:NSZeroPoint fromRect:srcRect Note all code examples are written in mail from memory and not tested, bug likely exist.</body>
  </mail>
  <mail>
    <header>Re: javascript called every N frames/seconds</header>
    <date>Sun Oct 02 06:03:55 CEST 2005</date>
    <body>Am 02.10.2005 um 04:20 schrieb Aaron Wallis: I'm not sure what you mean exactly (can't decipher your English grammar, I'm probably too tired ;-). I would expect it would be possible to do the same thing the Quartz Compose application does in your own app. I haven't done anything in that area though, sorry. Mike -- Mike Fischer         Softwareentwicklung, EDV-Beratung Schulung, Vertrieb Note:                 I read this list in digest mode! Send me a private copy for faster responses.</body>
  </mail>
  <mail>
    <header>Re: javascript called every N frames/seconds</header>
    <date>Sun Oct 02 03:28:08 CEST 2005</date>
    <body>Am 02.10.2005 um 02:23 schrieb Aaron Wallis: I see. Didn't even know it could do JavaScript. Cool! Well the window.setTimout() function doesn't work because there is no window object/variable. Also the comments in the main window of Quartz Compser suggest that the JavaScript support is rudimentary at best. So I don't think that would be the way to go. Here is an idea: Use the System Time Tool as a generator, connect it to a Numeric Math component and use a modulo operation (to get a managable frequency). That will give you a fast counter with a fixed range. Then use a Numeric Conditional component with &amp;gt;= to ge a 0 or one result (essentially a square wave generator alternating between 0 and 1) and connect that to a Numeric Counter component as an example. Feed that into &amp;quot;Image with String&amp;quot; and watch the display count up. You'll need to figure out how to put your calculation somewhere into that chain. AFAICT all objects react as soon as an input changes. But I could be wrong. Assuming this is correct then you need some time generator. The above approach using the System Time Tool seems to work fine. Use the JavaScript Tool instead of the counter and do your thing. HTH Mike -- Mike Fischer         Softwareentwicklung, EDV-Beratung Schulung, Vertrieb Note:                 I read this list in digest mode! Send me a private copy for faster responses.</body>
  </mail>
  <mail>
    <header>Re: javascript called every N frames/seconds</header>
    <date>Sun Oct 02 02:23:19 CEST 2005</date>
    <body>I was referencing the Javascript object in quartz composer. I need the equation I have created within the object to be processed every frame, rather than once... Send instant messages to your online friends</body>
  </mail>
  <mail>
    <header>Re: javascript called every N frames/seconds</header>
    <date>Sat Oct 01 21:38:03 CEST 2005</date>
    <body>Am 01.10.2005 um 13:08 schrieb Aaron Wallis &amp;lt;email@hidden&amp;gt;: JavaScript and Quartz have no connection at all. You should ask this question in a general JavaScript forum or on the web- email@hidden mailing list. Check out the JavaScript function setTimeout(). HTH Mike -- Mike Fischer         Softwareentwicklung, EDV-Beratung Schulung, Vertrieb Note:                 I read this list in digest mode! Send me a private copy for faster responses.</body>
  </mail>
  <mail>
    <header>Fwd: What are these errors?  kCGErrorTypeCheck, kCGErrorFailure,	kCGErrorIllegalArgument</header>
    <date>Tue Jun 29 10:39:32 CEST 2010</date>
    <body>Setting breakpoint at&amp;nbsp;CGErrorBreakpoint() doesn't help as the whole stack trace is library stuff and not my code. June 21, 2010 0:31:02  GMT+03:00</body>
  </mail>
  <mail>
    <header>Re: Instruments complains about misaligned images for CAShapeLayers,	when to rasterize?</header>
    <date>Mon Jun 28 03:18:51 CEST 2010</date>
    <body>Unfortunately, I don't know. you might try filing a bug to find out, but are you seeing a performance degradation (vs rasterizing the shape yourself in a regular CALayer)? -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Instruments complains about misaligned images for CAShapeLayers,	when to rasterize?</header>
    <date>Mon Jun 28 00:07:34 CEST 2010</date>
    <body>Unfortunately, I don't know. you might try filing a bug to find out, but are you seeing a performance degradation (vs rasterizing the shape yourself in a regular CALayer)? -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Instruments complains about misaligned images for CAShapeLayers,	when to rasterize?</header>
    <date>Wed Jun 23 07:35:19 CEST 2010</date>
    <body>CALayer (UIScrollView) CALayer (UIScrollView) CALayer (UIScrollView) How do I fix it so that in #3, the shape layer is NOT misaligned? (Or is this a bug?) --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>CGSetDisplayTransferByTable</header>
    <date>Sat Jun 19 01:38:55 CEST 2010</date>
    <body>Does anyone know if the call CGSetDisplayTransferByTable is immediate or does it wait for the VBL. Thanks much,</body>
  </mail>
  <mail>
    <header>Re: Instruments complains about misaligned images for CAShapeLayers,	when to rasterize?</header>
    <date>Fri Jun 18 08:08:39 CEST 2010</date>
    <body>Ah no, the view is actually stable 99% of the time, I only animate when I need to show changes. Checking with Instruments, I notice that shouldRasterize: YES on the deep sublayers doesn't really affect the FPS of the actual animation much. But it does improve the scrolling speed tremendously, thus I'd want to keep it. For your reference, those simple layer arrangements that caused misaligned image under Instruments: CALayer (UIScrollView) CALayer (UIScrollView) CALayer (UIScrollView) --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Re: Instruments complains about misaligned images for CAShapeLayers,	when to rasterize?</header>
    <date>Fri Jun 18 07:21:09 CEST 2010</date>
    <body>This doesn't really sound like a good use case for shouldRasterize, at least not on the view containing the animated contents, especially since it sounds like your view may be blowing the cache size. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Instruments complains about misaligned images for CAShapeLayers,	when to rasterize?</header>
    <date>Fri Jun 18 06:10:19 CEST 2010</date>
    <body>On 18/06/2010, at 12:38 AM, David Duncan wrote: I've tried several different simple arrangements to reproduce the case. Here's the results: CALayer (UIScrollView) CALayer (UIScrollView) CALayer (UIScrollView) Note that all layers are at default position and bounds. Zoom level is allowing the UIScrollView to zoom i.e. the system sets the transform of the next layer down, and I don't mess with it. My actual use case is closest to #3, since I use several plain vanilla CALayers to group together the shape layers. Also keeping shouldRasterize: YES on the custom view layer slows down scrolling dramatically, presumably because of the canvas-sized bitmap that has to be used, and there is a lot of animation happening at the shape layer level. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Re: Instruments complains about misaligned images for CAShapeLayers,	when to rasterize?</header>
    <date>Thu Jun 17 18:38:57 CEST 2010</date>
    <body>Are you seeing this when you have shouldRasterize turned on or always? shouldRasterize can be a performance boost or a performance sink. It depends a lot on both what you are doing and how much rasterization you are doing (there is a limited amount of cache). In particular, shouldRasterize should only be used for layer trees that are relatively static in nature, as it will have a negative performance impact if you modify the appearance of the layer tree (such as by changing layer style properties or animate a sublayer). -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Instruments complains about misaligned images for CAShapeLayers,	when to rasterize?</header>
    <date>Thu Jun 17 18:24:02 CEST 2010</date>
    <body>When I have several CAShapeLayers on screen (iOS project), Instruments often colors them purple for misaligned images. I've managed to avoid these for text layers by carefully aligning the transformed bounds to integer values, but how do I do this for CAShapeLayers? Presumably it's all vectors in there and the layer bounds aren't even used. Another related performance issue: turning on shouldRasterize seems to speed up my CAShapeLayers but only in certain circumstances. Are there some rules of thumb we can use to know when to turn it on? Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Re: Quartz Global Display Coordinate System</header>
    <date>Sat Jun 12 01:28:13 CEST 2010</date>
    <body>This is how you do it, but I suppose I'm not sure what else you were expecting to exist :). The origin of the display system is the upper-left corner of the menu bar. The positive direction is downwards and rightwards, hence the expectation is that a display positioned below the main display would be at some large positive offset. NSScreen would give you a coordinate system like this IIRC, but CG deals in this in the opposite handedness. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Quartz Global Display Coordinate System</header>
    <date>Fri Jun 11 21:13:35 CEST 2010</date>
    <body>Hi, I'm using CGDisplayBounds() to check the origin's of each of my 2 displays which are in extended mode (i.e. NOT mirrored).  I'm writing a method to determine where one display is in relation to another by examining their origins and comparing them to one another.  Just do be sure, no official API exists for doing this, correct? When the displays are side-by-side in the arrangement view in the system preferences, all works as I would expect.  For instance, if the main display (with the menubar) is to the right of the secondary display (without the menubar), the x-coordinate of the secondary display is negative the width of the screen.  However, if the displays are stacked on top of each other and the main display is on the top and secondary display is on the bottom, the main displays origin is (0,0) like expected, however the secondary display below has an origin of (0,1200) instead of (0,-1200) like I would expect. Is this normal behavior?  If so, how come?  Why doesn't the display under the main display have a negative y-coordinate when it's below the main display and a positive y-coordinate when it's positioned above the main display?  I'm having trouble visualizing why I am getting the results I am. Thanks in advance for any insights you may be able to provide! Andrew _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>adding zoom to NSImageView</header>
    <date>Fri Jun 11 00:44:22 CEST 2010</date>
    <body>Hi Everyone, I've displayed a bitmapped image in a NSImageView but need to add control features (zoom, rotation, etc). I would like to include the IKImageView class to support the control features but struggling to integrate it under the NSImageView. Any tips or examples on how to access the IKImageView class from NSImageView? Best, Joe</body>
  </mail>
  <mail>
    <header>Re: Applying a CIFilter - nil value for argument #0 src?</header>
    <date>Thu Jun 03 02:12:48 CEST 2010</date>
    <body>Hi,</body>
  </mail>
  <mail>
    <header>Re: Applying a CIFilter - nil value for argument #0 src?</header>
    <date>Thu Jun 03 01:52:15 CEST 2010</date>
    <body>-- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: Caps lock in event taps doesn't send flags changed upon release</header>
    <date>Thu Feb 01 07:34:18 CET 2007</date>
    <body>On Jan 31, 2007, at 8:14 PM, Ben Gertzfield wrote: Could be.  I haven't designed any USB keyboards. NX_NUMERICPADMASK does not mean what you think it means.  It indicates the keystrokes originate from the numeric keypad area of a keyboard, and does not reflect the state of the Num Lock switch. Arrow keys are considered part of the numeric keypad area. This flag is present to allow applications to differentiate between key characters from the main keyboard matrix and ientical characters from the numeric keypad matrix, such as the number keys and math symbols.</body>
  </mail>
  <mail>
    <header>Re: Caps lock in event taps doesn't send flags changed upon release</header>
    <date>Thu Feb 01 05:14:06 CET 2007</date>
    <body>The Caps Lock, and on many keyboards, Num Lock, is electromechanically implemented as a toggling device.&amp;nbsp;&amp;nbsp;One press toggles it to a &amp;#39;down&amp;#39; state, and a second press toggles it to an</body>
  </mail>
  <mail>
    <header>Re: Caps lock in event taps doesn't send flags changed upon release</header>
    <date>Thu Feb 01 04:32:50 CET 2007</date>
    <body>On Jan 31, 2007, at 7:02 PM, Ben Gertzfield wrote: The Caps Lock, and on many keyboards, Num Lock, is electromechanically implemented as a toggling device.  One press toggles it to a 'down' state, and a second press toggles it to an 'up' state.  On many older keyboards (early IBM M series, for example) the Caps Lock used a mechanical toggle, locking the button in the down state until a second press released the mechanical lock and allowed the button to rise.   On newer keyboards the toggle behavior is implemented in the keyboard microcontroller. Event taps, as well as the modifier flags in Carbon events and NSEvents, are working correctly.  The hardware may not be behaving in the manner you were expecting.</body>
  </mail>
  <mail>
    <header>Re: Caps lock in event taps doesn't send flags changed upon release</header>
    <date>Thu Feb 01 04:03:51 CET 2007</date>
    <body>On 1/31/07, I wanted to email the list to check on this before I submit a bug. My application uses Event Taps to monitor and consume keyboard events at the session level.&amp;nbsp; For modifier keys like Caps Lock and Num Lock, we correctly get events of type kCGEventFlagsChanged.</body>
  </mail>
  <mail>
    <header>Re: Simple question about parameter sizes</header>
    <date>Thu Feb 01 00:24:48 CET 2007</date>
    <body>On Jan 31, 2007, at 3:13 PM, Rick Mann wrote: Normlly, a C enum is the same as an int (whatever size that is for that compiler). It can be different: enumerations-and-bit_002dfields-implementation.html but AFAIK Apple's ABI has enums being the same size as ints. There are language extensions (like gcc's &amp;quot;packed enums&amp;quot;) to make specific enums smaller, for use in structs and the like. (e.g., an enum with only 12 values can fit in a 'char'.) I don't think any of Apple's headers do that. (And probably it wouldn't affect function calls anyway, since function arguments tend to get promoted to int.)</body>
  </mail>
  <mail>
    <header>Simple question about parameter sizes</header>
    <date>Thu Feb 01 00:13:01 CET 2007</date>
    <body>Hi. I'm trying to call CG APIs from another language. So far it's working out fine, but I wanted to verify something. Some APIs use typedef'ed enums to declare parameters. When this stuff is built, is the compiler set to make all enums be 4-byte ints? In general, what's the size of these parameters (that CG is expecting)? kCGLineJoinMiter, kCGLineJoinRound, kCGLineJoinBevel TIA, -- Rick</body>
  </mail>
  <mail>
    <header>Software Rendered kicking in with WhackedTV on PPC</header>
    <date>Sun Apr 22 17:31:45 CEST 2007</date>
    <body>I posted this thread originally on the quicktime list, but Brad Ford suggested that this might be a better place to ask. I'm seeing very poor performance (100% CPU consumption) with the WhackedTV sample code on PPC machines (vs about 10% on an intel MPB). This is with video only preview. One machine is a dual G5@2GHz with a GeForce 5200 video card and 64 MB G4@1GHz Powerbook with a GeForce FX Go5200 Video card and 32MB of VRAM. The video input is an external isight (the MPB performance figure used the external as well). System Profiler reports both machine's displays as supporting Quarz Extreme and Core Image Profiling with Shark shows that the additional CPU on the PPC is being consumed by the calls pasted below, which seems to indicate that the software renderer is kicking in. If this is a genuinely abnormal case affecting a few machines, that's fine, but these machines, while a few years old, are Tiger/Leopard capable, and seem like they should be able to support hardware rendering with no problem. When I play with Quartz Composer on the Dual G5 machine, it seems fine, and cpu consumption is low (&amp;lt; 10%), and other video programs that aren't descended from WhackedTV give acceptable performance on both PPC machines. cheers, m. 67.1%	67.1%	libGLImage.dylib	glgProcessColor(GLDPixelMode const*, GLGOperation const*, unsigned long) 0.0% 	67.1%	libGLImage.dylib	glgProcessPixels 0.0%       67.1%	QuartzCore	fe_bitmap_copy 0.0%	       67.1%	QuartzCore	image_buffer_get_data 0.0%	       67.1%	QuartzCore	texture_retain 0.0%	       67.1%	QuartzCore	fe_texture_new 0.0%	       67.1%	QuartzCore	fe_tree_create_texture 0.0%	       67.1%	QuartzCore	fe_tree_render_apply 0.0%	       67.1%	QuartzCore	fe_tree_render 0.0%	       67.1%	QuartzCore	fe_tree_render_image_ 0.0%	       67.1%	QuartzCore	fe_tree_render_image 0.0%	       67.1%	QuartzCore	fe_image_render_ 0.0%	       67.1%	QuartzCore	fe_image_render 0.0%	       67.1%	QuartzCore	-[CIOpenGLContextImpl renderSoftware:matrix:bounds:] 0.0%	       67.1%	QuartzCore	-[CIOpenGLContextImpl render:] 0.0%	       67.1%	QuartzCore	-[CIContext drawImage:inRect:fromRect:] 0.0%	       67.1%	WhackedTV	-[SampleCIView drawRect:] 19.2%	19.2%	Unknown Library	0x58b9000 [3.5KB] 0.0%	       19.2%	QuartzCore	fe_span_iterate 0.0%	       19.2%	QuartzCore	jit_program_renderer 0.0%	         9.8%	QuartzCore	fe_fragment_thread 0.0%	         9.8%	libSystem.B.dylib	_pthread_body 0.0%	         9.5%	QuartzCore	fe_fragment_render_quad 0.0%	         9.5%	QuartzCore	fe_tree_render_apply_1 0.0%         9.5%	QuartzCore	fe_tree_render_apply 0.0%	         9.5%	QuartzCore	fe_tree_render 0.0%      	  9.5%	QuartzCore	fe_tree_render_image_ 0.0%          9.5%	QuartzCore	fe_tree_render_image 0.0%	          9.5%	QuartzCore	fe_image_render_ 0.0%	          9.5%	QuartzCore	fe_image_render 0.0%	          9.5%	QuartzCore	-[CIOpenGLContextImpl renderSoftware:matrix:bounds:] 0.0%	          9.5%	QuartzCore	-[CIOpenGLContextImpl render:] 0.0%	          9.5%	QuartzCore	-[CIContext drawImage:inRect:fromRect:] 0.0%	          9.5%	WhackedTV	-[SampleCIView drawRect:]</body>
  </mail>
  <mail>
    <header>Aliased arc drawing</header>
    <date>Tue Apr 17 16:08:45 CEST 2007</date>
    <body>Hi, I'm getting some unexpected results when drawing arcs with anti-aliasing turned off. I've uploaded an image of the results here: As the image shows, the fill actually extends one pixel outside the stroked line in some places. I would expect the fill to be completely enclosed by the line, and this is also what I get if I turn anti-aliasing on. Does anybody know a way to get correct painting with anti-aliasing off? - Morten</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate/ImageIO weirdness</header>
    <date>Mon Apr 16 22:20:07 CEST 2007</date>
    <body>Thanks Nick Adding an options dictionary with kCGImagePropertyDPIHeight and kCGImagePropertyDPIWidth fixed the problem. Ken -------------- Original message ----------------------</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate/ImageIO weirdness</header>
    <date>Mon Apr 16 21:16:33 CEST 2007</date>
    <body>On Apr 16, 2007, at 11:24 AM, email@hidden wrote: You have to pass an options dictionary to CGImageDestinationAddImage.  There might be another way to add this dictionary, but that's how I do it.  It can contain keys such as kCGImagePropertyDPIWidth, kCGImagePropertyDPIHeight, and kCGImagePropertyTIFFDictionary.  A TIFF dictionary can contain the key kCGImagePropertyTIFFCompression. Nick</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate/ImageIO weirdness</header>
    <date>Mon Apr 16 20:23:45 CEST 2007</date>
    <body>Am 16.04.2007 um 19:24 schrieb email@hidden: that dumps all tags... Regards, Tom_E</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate/ImageIO weirdness</header>
    <date>Mon Apr 16 19:24:17 CEST 2007</date>
    <body>After more testing, I'm beginning to suspect that there is some info missing from the ImageIO generated tiffs. I imported them into every application I have, that allows image placement, and found that in BBEdit the tiffs were also distorted. I also imported them into a really old version of Quark (v3.3.2) and got the exact same distortion. I was thinking that it might be the tiff lines per inch field but see no way to set that in CGImageDestinationCreateWithURL. Since at least two applications have problems with these images, that seems to indicate something with the images that is slightly off. I couldn't find any lines per inch fields in CGImageDestinationCreateWithURL but does anyone know if there is a way to set this info? Thanks Ken</body>
  </mail>
  <mail>
    <header>Re: CGLayer performance problem</header>
    <date>Mon Apr 16 15:29:03 CEST 2007</date>
    <body>My understanding is that the context that you pass to the CGLayer is not the only context you can draw to... it's simply an example, a sample if you will, of the type of context that you would like to draw into.  The CGLayer will pull information from that context (In the example of a bitmap context that would be color depth, pixel order, color space, etc...) and optimize whatever off-screen representation it keeps for that layer so that it can draw on other contexts with similar arrangements. What that means is that even though you could be drawing to a different individual  CGContext each time, most of the times when you draw to the screen, the characteristics of that context are going to be the same as every other screen context you've seen up to this point (including the one used to create the layer).  The layer's off- screen representation should still be optimized for those conditions. Where you run into a problem is when the destination context has a different environment than the one the layer was created with.  On a two monitor system where one is on thousands of colors, and one is on millions of colors, for example, drawing the same layer in each environment may be sub-optimal.  Or if the user changes the color depth of the monitor and you continue to use layers created with the old setup, you might run into problems.</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate/ImageIO weirdness</header>
    <date>Mon Apr 16 13:49:07 CEST 2007</date>
    <body>On Apr 16, 2007, at 3:29 AM, Thomas Engelmeier wrote: So it does... The odd thing though, is that I intentionally did not apply any of the Quark import options (like size to fit, scale proportional etc...) so these register in Quark's measurements palette as all being scaled at 100% x 100%. I fiddled with high-res vs low res preview, different measurement systems etc to no effect and I'm not aware of any option in Quark which would translate to &amp;quot;import all Ken</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate/ImageIO weirdness</header>
    <date>Mon Apr 16 09:29:01 CEST 2007</date>
    <body>Am 16.04.2007 um 08:56 schrieb Ken Tozier: ... which boils down to (rounded) 4&amp;quot; @ 72 dpi. Everything &amp;quot;works as designed&amp;quot;, you need to fiddle with Quark internals after importing. Regards, Tom_E</body>
  </mail>
  <mail>
    <header>CGBitmapContextCreate/ImageIO weirdness</header>
    <date>Mon Apr 16 08:56:43 CEST 2007</date>
    <body>I'm making these simple on-the-fly placeholder images for ads and  am getting this really weird scaling issue. I think I'm probably not choosing pixel formats correctly, but  after testing all the ones that seemed to make sense, I'm stuck. What's happening is really bizarre, the smaller I define an image, the larger it appears when imported into a Quark picture box. I can open them in Photoshop and the sizes are correct but Photoshop is a dedicated image processing application so maybe it's compensating for odd pixel definitions. For example, here's a matrix of defined sizes vs sizes when the images are imported into Quark Defined size			Import size		Percentage 130 x 72		=&amp;gt;		273 x 160		210% 270 x 504	=&amp;gt;		154 x 288		57% 688 x 715	=&amp;gt;		277 x 288		40% CGContextRef CGUtilCreateBitmapContext(int inPixelsWide, int inPixelsHigh, int inBytesPerPixel, void *inBuffer) context = CGBitmapContextCreate (inBuffer, inPixelsWide, inPixelsHigh, 8,      // bits per component inPixelsWide * inBytesPerPixel, colorSpace, // I really have no clue what to choose here but // I tried several and all give the same result - (void) initPlaceholderImage CGContextRef				context				= CGUtilCreateBitmapContext(width, CGImageDestinationRef		destRef				= CGImageDestinationCreateWithURL // Cocoa draw commands are simpler so wrap CGContext in an NSGraphicsContext NSGraphicsContext			*nsContext			= [NSGraphicsContext graphicsContextWithGraphicsPort: context // create the placeholder image // write img to a tiff file // import image into box If I define an images to be</body>
  </mail>
  <mail>
    <header>Re: CGLayer performance problem</header>
    <date>Mon Apr 16 08:31:09 CEST 2007</date>
    <body>Forgive me for asking, but what is the value of a CGLayer if it has to be re-rendered every time it is used under a new context (for example... every time a window is updated). So far in my experience with CGLayer has been awful.  For those trying to use it like a canvas (ie: your document is 5000 X 7000 but your window is 640 X 480) forget it!  Even after clipping to the destination CGContext the performance is terrible. You might as well write code that simply draws on demand. -- Rick Steele Wildsync Systems L.L.C.</body>
  </mail>
  <mail>
    <header>Bindings, and Core Image</header>
    <date>Sat Apr 14 00:48:10 CEST 2007</date>
    <body>I'm having a bit of a problem, using bindings with a core image filter that I'm working on. The filter is actually constructed as a composite of several other filters, which it uses bindings to keep in sync. The problem I'm having is that changes to one of it's parameters (in this case, inputOrigin) are not reflected by calls to outputImage. inputOrigin is bound to a user interface element, which allows the effect to be positioned on the image. What I would expect to have happen is that when inputOrigin is changed, it would trigger a change notification for outputKeyColor, which would then trigger a change notification for outputImage. What actually happens is that the change notification for outputKeyColor occurs, but the notification for outputImage does not. + (void) initialize [self setKeys: [NSArray arrayWithObjects: @&amp;quot;inputImage&amp;quot;, @&amp;quot;inputOrigin&amp;quot;, // ... nil] [self setKeys: [NSArray arrayWithObjects: @&amp;quot;inputImage&amp;quot;, // ... @&amp;quot;outputKeycolor&amp;quot;, nil] // ... [self setKeys: [NSArray arrayWithObjects: @&amp;quot;inputImage&amp;quot;, @&amp;quot;inputOrigin&amp;quot;, // ... @&amp;quot;outputKeycolor&amp;quot;, nil] The notification for outputImage then occurs, but at the wrong time - before outputKeyColor has been resolved. Currently, outputImage is using setValue: forKey: to connect all of the filter stack's input and output images - this is because core image throws an exception if I bind to a filter's outputImage without first having established all of its inputs. Ron Aldrich Software Architects, Inc.</body>
  </mail>
  <mail>
    <header>Re: CGLayerCreate's context argument</header>
    <date>Thu Apr 12 20:32:31 CEST 2007</date>
    <body>I worked around this problem (once... in a small app) by keeping meta- information beside the layer that kept track of the context in which the layer was created.  For example, when the layer was created to draw to the main display I kept a flag that says &amp;quot;this layer is good for the screen&amp;quot;.  Then, when printing, I would ignore the layer and do the drawing directly. This is similar to your &amp;quot;Give me a blob&amp;quot; idea except that I kept that information myself.</body>
  </mail>
  <mail>
    <header>Re: CGLayerCreate's context argument</header>
    <date>Thu Apr 12 10:00:14 CEST 2007</date>
    <body>As for speeding up redraw, I saw in my test while evaluating the API that the raster case is very different from the non-raster one. In the raster case, I found that using CGImages really speed things up, but with a curious limitation I saw on a macmini ppc : if the CGImage does not match the CGColorSpace of the device, there is a big performance hit right after 24 blocks of 128x128 images are drawn. Amortizing the color space conversion take something like 80ms the first time the CGImage are cached, and then less than 10ms to reuse it, but in the &amp;gt;24 case, the 80ms is never amortized, which makes the redraw from very fluid to very jerky (hope it is the right word) So if I profile manually the images (which require more work), I can draw those images almost instantly (drawing a big image into tiled images). Which makes moving the image on the screen almost instant, even when the original file is something like 45 MB of data. What is weird is that if I open the same image in the Preview app from Apple (which should know a lot about performance issue), the dragging of the image is dog slow, it takes like 1s to redraw each time a move is made. This very special case to say that... The API does pretty good things on the functionnality part. But when it comes to performance, I've found no other solution than creating a new project to test different scheme for doing the same thing, which may/would change the architecture of your app if you're porting it. In fact you've got to know the limitations of it and unfortunately this is not documented. Somehow the knowledge of the limitation of the API in term of performance must be included in your code and &amp;quot;embed&amp;quot; the API. This requires more work, but this makes a huge difference in the end.</body>
  </mail>
  <mail>
    <header>Re: HIViewSetBoundsOrigin (Zoom bug)</header>
    <date>Wed Apr 11 20:38:47 CEST 2007</date>
    <body>On Apr 11, 2007, at 11:34 AM, Hemant Balakrishnan wrote: I don't think we have any existing reports of this, so your next step should be to file a bug with a sample app that shows the problem, and we can investigate. It doesn't sound offhand like something you'll be able to work around, though.</body>
  </mail>
  <mail>
    <header>CGLayerCreate's context argument</header>
    <date>Wed Apr 11 19:28:15 CEST 2007</date>
    <body>I've been converting existing code from using bitmap-based drawing caches to using CGLayers. In general, the conversion goes smoothly and improves performance, which is great. But there's a stumbling block I've run into each time, and that is the 'context' argument to CGLayerCreateWithContext(). There are two general problems I run into repeatedly when using CGLayers, one small and one large. The small one is the necessity of having an initialized CGContext at the instant I want to create the CGLayer. This makes it awkward to pre-cache drawing I know I'm about to do, but it can be worked around. The larger problem is the apparent impossibility of determining whether a given CGLayer is &amp;quot;compatible&amp;quot; with a given CGContext. If I'm going to store complex drawing in a CGLayer so I can re-use it, then I need some way to know whether I should re-use a given layer in a given situation. If my document is being printed to PDF, or rendered to a bitmap, or if the window is moved to a screen with different properties (different GPU hardware, e.g.), or the display's properties such as color depth or resolution are changed, then I need to know to create a new CGLayer rather than re-using an existing one. As a result, right now CGLayers are only really useful for drawing something multiple times within a given view, not for speeding up re- draw, except in some limited situations. What I would really like to see is some API to extract the salient information from a CGContext into an opaque blob, which could be stored and compared to other blobs, and later used to create a CGLayer without direct reference to the CGContext. (Perhaps it could be passed in using the currently-unused 'auxiliaryInfo' argument to CGLayerCreateWithContext().) Then, when drawing, I could get the blob for the CGContext I'm drawing to, compare it to the blobs used to create my various cache CGLayers, and either select the appropriate cache or create a new one. There would probably need to be a CGLayerAreBlobsEquivalent() call, rather than just using CFEquality on the blobs, because some layer attributes --- such as gstate parameters --- do not affect compatibility. In an ideal world, this blob would actually be a CFDictionary, some of whose keys would be private (internal hardware info, for example) but some of whose keys would be public (CTM, native color space, context type) or even modifiable, if that makes sense.</body>
  </mail>
  <mail>
    <header>Threaded Caching CIImage and GraphicContext</header>
    <date>Wed Apr 11 08:17:28 CEST 2007</date>
    <body>we have a problem using NSGraphicContext/CGGraphicContext in threads. One the one (main) thread, we use the context to draw normal stuff. There we also draw semi-transparent CIImages, so we added CGContextBeginTransparencyLayer and CGContextEndTransparencyLayer calls. Also some CGContextClipToRect calls are needed while drawing. This works normally without problems. But than we added another thread to cache the created CIImages. This thread creates a bitmap representation using and uses [[nsContext CIContext] drawImage:] to draw to this bitmap rep. While doing this, everything works fine, but in not determinable circumstances the application crashs with such a stack trace: Thread 0 Crashed: 0   &amp;lt;&amp;lt;00000000&amp;gt;&amp;gt; 	0xfffeff18 objc_msgSend_rtp + 24 1   com.apple.QuartzCore           	0x94298078 -[CICGContextImpl setOwner:] + 40 2   com.apple.QuartzCore           	0x94316c00 -[CIContext invalidate] + 44 3   com.apple.QuartzCore           	0x94316b9c -[CIContext dealloc] + 36 4   com.apple.CoreFoundation       	0x907eace8 CFDictionaryRemoveValue + 492 5   com.apple.AppKit               	0x937a5358 -[NSCGSContext dealloc] + 36 6   com.apple.AppKit               	0x937a5320 - [NSWindowGraphicsContext dealloc] + 76 7   com.apple.AppKit               	0x937a529c -[_NSViewGState dealloc] + 56 8   com.apple.Foundation           	0x92939968 NSPopAutoreleasePool + 536 9   com.apple.AppKit               	0x93702d34 -[NSApplication run] + 544 10  com.apple.AppKit               	0x937f387c NSApplicationMain + 452 Any ideas where a error could be? I tried to retain the context while drawing and release it after drawing, but this has no effect.</body>
  </mail>
  <mail>
    <header>(Solved) Extracting a pixel's color from a  CIImage</header>
    <date>Wed Apr 11 02:02:15 CEST 2007</date>
    <body>On Apr 10, 2007, at 11:54 AM, Jason Harris wrote: colorspace, in order to get the behavior I wanted.  Also, in order to get floating point data which was endian swapped correctly, I had to specify kCGBitmapFloatComponents | kCGImageAlphaPremultipliedLast | kCGBitmapByteOrder32Host for the CGBitmapInfo parameter to CGBitmapContextCreate. I've attached source code to an a CIImage class extension (Based loosely on your sample), in the hope that others will find it useful. Attachment: Ron Aldrich Software Architects, Inc.</body>
  </mail>
  <mail>
    <header>Re: Extracting a pixel's color from a  CIImage</header>
    <date>Tue Apr 10 21:03:41 CEST 2007</date>
    <body>On Apr 10, 2007, at 12:45 PM, Ron Aldrich wrote: It's probably worth noting that a CIImage doesn't contain pixels, it contains instructions to render pixels into a CIContext.  I believe (but I might be wrong) that a CIImage also doesn't have an associated color space.  It uses the colorspace of the CIContext (which usually uses the colorspace of the CGContext used to create it). One thing you could try is to disable the CIContext's colorspace as follows: NSDictionary* contextOptions = [NSDictionary dictionaryWithObjectsAndKeys:[NSNull null], CIContext* context = [CIContext contextWithCGContext:imageContext Nick</body>
  </mail>
  <mail>
    <header>Extracting a pixel's color from a  CIImage</header>
    <date>Tue Apr 10 20:45:14 CEST 2007</date>
    <body>I'm looking for a way to extract a pixel's color from a CIImage, in whatever color space the CIImage happens to be using.  (Basically, I need to do some analysis on an image which is not suitable for a kernel function, and extract a &amp;quot;Color of interest&amp;quot; from the image, which will then be fed into a kernel filter.) Currently, it appears that the values I'm getting are being modified in some way when I render them to an offscreen buffer as follows: NSMakePoint([inputOrigin X], [inputOrigin Y]), NSZeroSize theBitmapContext = CGBitmapContextCreate (theBitmapData,           // data theBoundsRect.size.width,  // width theBoundsRect.size.height, // height 8,                       // bits per component theBytesPerRow,          // bytes per row theColorspace,           // color space NSGraphicsContext* theGraphicsContext = [NSGraphicsContext graphicsContextWithGraphicsPort: theBitmapContext NSGraphicsContext* theSavedContext = [[NSGraphicsContext NSRectFill(NSMakeRect(0, 0, theBoundsRect.size.width, [theImage drawInRect: NSMakeRect(0, 0, theBoundsRect.size.width, theBoundsRect.size.height) fromRect: theBoundsRect operation: NSCompositeSourceOver I would assume that because I'm using CGColorSpaceCreateDeviceRGB() to create the color space, that I'm causing the color conversion at that point. So, first - what color space should I be using for the bitmap context, in order to prevent any conversion of the colors? And third, how would I get the pixels in 32-bit float format, instead of 8 bit fixed? Ron Aldrich Software Architecs, Inc.</body>
  </mail>
  <mail>
    <header>Re: CoreImage Filter Definitions</header>
    <date>Tue Apr 10 18:13:30 CEST 2007</date>
    <body>I does give me a good definition of each mode (the basic ones at least).  So, Thanks! I have a test bench in Quartz Composer that I'm using to experiment with the various filters and I'm seeing very odd things when I give them pixel values out of the range 0.0-&amp;gt;1.0. For example: A Grayscale ramp going from 0.0 to 2.0 is blended with an image which has 1.0 for a pixels, using the Multiply Blend Mode. The result is a ramp from -1.0 to 1.0, when obviously I would expect the same ramp that I put in. In this case the Multiply patch (which I assume maps to CIMultiplyCompositing) does what I expect (and what my reading of the PDF spec says). Unfortunatley that's about the only filter that has a more understandable sibling.</body>
  </mail>
  <mail>
    <header>Re: ImageIO bug when exporting CGImage to data handle?</header>
    <date>Tue Apr 10 16:43:48 CEST 2007</date>
    <body>On Apr 10, 2007, at 6:46 AM, Dinge Raphael wrote: NDA. I can't post any of the APIs. I'm sure there must be a way, just can't find it at the moment... Ken</body>
  </mail>
  <mail>
    <header>Re: ImageIO bug when exporting CGImage to data handle?</header>
    <date>Tue Apr 10 13:22:59 CEST 2007</date>
    <body>On Tue, 10 Apr 2007 05:13:44 -0400, PICT files differ from in-memory PICT handles in that they have a 512-byte header (containing no useful information). Perhaps ImageIO is including this extra junk to the beginning of your CFMutableDataRef.  When you copy the CFMutableDataRef to a Handle, try skipping the first 512 bytes. -- marco -- It's not the data universe only, it's human conversation. They want to turn it into a one-way flow that they have entirely monetized. I look at the collective human mind as a kind of ecosystem. They want to clear cut it. They want to go into the rainforest of human thought and mow the thing down.</body>
  </mail>
  <mail>
    <header>CoreImage Filter Definitions</header>
    <date>Tue Apr 10 13:12:05 CEST 2007</date>
    <body>I'm writing some simple compositing software using CoreImage to do the work. I'm finding myself writing my own versions of some of the shipping CI filters because I can't find a good mathematical definitions of the ones that ship with Tiger. Does anybody know where I might find this information? All I can find is the (rather wooly) descriptions of the intended behaviour in the Core Image Filter Reference Document. The alternative is doing 'black box' analysis on them, but that's slow, any difficult to get all corner cases. C = A / (1.0 - B) : B  &amp;gt; 0.0 &amp;amp;&amp;amp; B &amp;lt;= 1.0 C = A             : Otherwise Obviously only some Filters lend themselves to that kind of description, but I'm only really looking for the CICategoryCompostiteOperation filters, and possibly CICategoryColorAdjustment &amp;amp; CICategoryColorEffect. Of course, source code would be best, but I'm assuming I'm not going to get that. Paul Attachment:</body>
  </mail>
  <mail>
    <header>Re: ImageIO bug when exporting CGImage to data handle?</header>
    <date>Tue Apr 10 12:46:05 CEST 2007</date>
    <body>This is why I think that maybe the data of the PICT gets written differently from ImageIO when writting to memory or writing to disk. Probably could you test this, just looking if the PICT written to disk has resource forks. This makes me remember the old days :) When I was a kid, Quark would take PICT file to put into image blocks, and the neat thing and the use of PICT, is that a PICT could contain quickdraw commands, allowing Quark to interpret not only raster informations, but more general drawing operations to handle texts and lines it could convert to valid PS. (but this was a mess, and I knew only one external program that would generate valid PICTs for Quark to produce a PS valid for those old RIPs) Is the Quark SDK public or are you on NDA ? I would be curious to see how it has changed, and maybe if there is a simple solution to your problem (given that putting raster data in a quark document should be made simple on the dev part I agree...)</body>
  </mail>
  <mail>
    <header>Re: ImageIO bug when exporting CGImage to data handle?</header>
    <date>Tue Apr 10 12:08:37 CEST 2007</date>
    <body>In my case, there are no forks of any kind. The images are built, in- memory, from ad information  (dimensions, color mode, client name, etc...) and passed off to Quark. They never get written to disk (except as embedded data in a Quark document) Unfortunately, after further reading, it looks like Quark may have replaced this simple &amp;quot;set a handle&amp;quot; method with a tortuous process involving literally dozens of functions. The new way is much more powerful, but the learning curve looks like several weeks worth of concerted effort, just to put an image in a box. Ken</body>
  </mail>
  <mail>
    <header>Re: ImageIO bug when exporting CGImage to data handle?</header>
    <date>Tue Apr 10 11:51:18 CEST 2007</date>
    <body>From what I remember, I had a lot of problem with PICTs to use directly into Quartz. As far as I remember, this was a problem with resources. Here I would logically except that the mutable data ref only holds the data fork of the PICT, and maybe for some reason is this different when you specify an URL, where the engine could also add resource forks ? In that case, Quark would only support PICT with resources, and it would fail. Maybe should you try (at least to see) with TIFF files or any other data fork based format supported by both Quark &amp;amp; Quartz.</body>
  </mail>
  <mail>
    <header>ImageIO bug when exporting CGImage to data handle?</header>
    <date>Tue Apr 10 11:13:44 CEST 2007</date>
    <body>I'm generating images on the fly using CGBitmapContexts to create CGImages which are then converted to picts using a CGImageDestinationRef. What I'm finding is that when I pass these pict handles off to Quark, it doesn't like them. It sees that they have data in them and act as if all is working but it then fails silently, not rendering the picts in the specified boxes. If I export the exact same image to a URL, I'm able to open the picts through the import dialog. Has anyone else encountered difficulties with ImageIOs export to CFMutableDataRef functionality? Ken</body>
  </mail>
  <mail>
    <header>Re: inter-process drawing of a CGContext</header>
    <date>Tue Apr 10 02:39:04 CEST 2007</date>
    <body>This is more an outcome of the X11 port of the logic where they don't have to do any of this messing around, and their question why do you have to do that stuff? The hassle is that we have to draw, tell the plugin in the other process what was drawn, wait for it to say it's finished drawing to the screen, handle the case where it does not response, then continue the cycle. On Apr 9, 2007, at 4:46 PM, Christopher Hunt wrote: -- ======================================================================== === Corporate Smalltalk Consulting Ltd. ======================================================================== ===</body>
  </mail>
  <mail>
    <header>Re: inter-process drawing of a CGContext</header>
    <date>Tue Apr 10 01:46:14 CEST 2007</date>
    <body>Cheers, -C</body>
  </mail>
  <mail>
    <header>some issus about CIImage object.</header>
    <date>Mon Apr 09 05:33:05 CEST 2007</date>
    <body>Hi list. I am very interested in the core image and opengl programming.So when I read the document about the core image, I discovered there is a method which create the core image object with opengl texture. - initWithTexture:(unsigned long) An OpenGL texture. Because CIImage objects are immutable, the texture must remain unchanged for the life of the image object. See the discussion for more information.cs for images that don't contain color data (such as elevation maps, normal vector maps, and sampled function tables).It is abosutely useful for my application, for I want to save my opengl sence to the texture and then create the CIImage object. After that, I can&amp;nbsp; apply the CIFilter to the CIImage object to create some visual effort. However , without enough example , I don&amp;#39;t know the exact step to accomplish this task. I really need some guidance. If some master here are willing to help me , we can have a further discussion. Thank you all in advanced. Really appreciate for you help, Cheers.</body>
  </mail>
  <mail>
    <header>Re: inter-process drawing of a CGContext</header>
    <date>Sun Apr 08 07:21:34 CEST 2007</date>
    <body>&amp;quot;You might want to check the archives for this... I think the conclusion I did however tackle changing things but need to allocate 16MB + 20 bytes and other list messages then imply shmget has to be some multiple of 4MB plus then other issues, plus then the example shmgets I used gaved invalid parm, or other types of errors. So I think I'll leave things alone, ship, and gather more input. -- ======================================================================== === Corporate Smalltalk Consulting Ltd. ======================================================================== ===</body>
  </mail>
  <mail>
    <header>Re: inter-process drawing of a CGContext</header>
    <date>Sun Apr 08 04:54:18 CEST 2007</date>
    <body>On Apr 7, 2007, at 7:38 PM, John M McIntosh wrote: You could consider switching to shmget instead of mmap to share memory between processes. I was using mmap for something similar, but couldn't get the performance needed until I switched to using the shared memory manager. It's a little harder to synchronize, but the performance gains are well worth it. HTH, -Jon -- Jonathan Johnson email@hidden REAL Software, Inc. REAL World 2007 Conference May 9 - 11 in Austin, Texas</body>
  </mail>
  <mail>
    <header>Re: inter-process drawing of a CGContext</header>
    <date>Sun Apr 08 04:46:50 CEST 2007</date>
    <body>I'm afraid not. A CGContextRef is only valid in the process of it's creator, so you cannot share it directly with another process. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>inter-process drawing of a CGContext</header>
    <date>Sun Apr 08 02:38:49 CEST 2007</date>
    <body>I have this interesting problem where I need to move the data out of a CGContext in a headless application to a CGContext  belonging  to a browser window. The headless application is forked off by a browser plugin btw, and is fed the netscape api UI events, and responds back with drawing events. Right now I&amp;quot;m using a CGBitmapContextCreate  to share the bitmap data area which mmap() to share the data pointer between the two processes. The shared memory is updated by doing a CGContextDrawImage along setting top,left,bottom,right co-ordinates and the plugin then uses a CGBitmapContextCreate, CGBitmapContextCreateImage, and CGContextDrawImage to draw the bits into the browser's CGContext. Usage of offset into data, and calculation of rowbytes allows me to create subimages out of the larger drawing context. I had used the CGImageCreateWithImageInRect originally but need this to run under 10.3 also. However I'm wondering if there is a way to draw directly from one process into another process's CGContext and avoid all this hassle. -- ======================================================================== === Corporate Smalltalk Consulting Ltd. ======================================================================== ===</body>
  </mail>
  <mail>
    <header>Re: Color space equivalent to	CGColorSpaceCreateWithName(kCGColorSpaceGenericRGB)</header>
    <date>Fri Apr 06 07:52:26 CEST 2007</date>
    <body>On Apr 5, 2007, at 1:25 PM, Mark Munte wrote: That's the color space documented as being equivalent to generic RGB in the Technical QA on the subject:</body>
  </mail>
  <mail>
    <header>Re: Core Image Fun House badly broken?</header>
    <date>Fri Apr 06 02:20:33 CEST 2007</date>
    <body>on 3-04-2007 5:13 email@hidden said the following: I tried with several images, including the four sample images that CIFH prompts for opening when launched. -- Andrea &amp;quot;XFox&amp;quot; Govoni AIM/iChat/ICQ: email@hidden Yahoo! ID: xfox82 Skype Name: draykan PGP KeyID: 0x212E69C1 Fingerprint: FBE1 CA7D 34BE 4A53 9639  5C36 B7A0 605F 212E 69C1</body>
  </mail>
  <mail>
    <header>Re: Core Image Fun House badly broken?</header>
    <date>Fri Apr 06 02:18:43 CEST 2007</date>
    <body>on 3-04-2007 4:39 Ricky Sharp said the following: I tested CIFH on an eMac G4 1,42GHz with an ATI Radeon 9600. I didn't try the builded from source version because I'm referring to the binary shipped with the last Xcode Tools (2.4.1). However I'll try to compile the project to see if it makes any difference. -- Andrea &amp;quot;XFox&amp;quot; Govoni Codice Wii: 0294 5787 0060 3021 Contest of the week: Da quale film è tratta la frase seguente? Il più veloce a rispondere riceverà una salvietta ufficiale di Multiplayer.it! AIM/iChat/ICQ: email@hidden Yahoo! ID: xfox82 Skype Name: draykan PGP KeyID: 0x212E69C1 Fingerprint: FBE1 CA7D 34BE 4A53 9639  5C36 B7A0 605F 212E 69C1</body>
  </mail>
  <mail>
    <header>Re: Color space equivalent to	CGColorSpaceCreateWithName(kCGColorSpaceGenericRGB)</header>
    <date>Thu Apr 05 20:25:16 CEST 2007</date>
    <body>Can someone confirm that kCGColorSpaceGenericRGB is equivalent or not to the ICC profile at /System/Library/ColorSync/Profiles/Generic RGB Profile.icc ? Since CGColorSpace is an opaque type I have found no way to query it's values - or am I missing something obvious?</body>
  </mail>
  <mail>
    <header>ROI and convolution</header>
    <date>Thu Apr 05 19:04:44 CEST 2007</date>
    <body>kernel vec4 convolution3x3(sampler src, float f1, float f2....) ... ... //same for the 8 pixels remaining This works fine, but now I'd like to change the size of the mask from 3 to, say, 9. I could write another kernel, but I don't think that's the right solution. I read on the &amp;quot;Core Image programming guide&amp;quot; that I could supply a ROI function which looks like : - (CGRect) regionOf:(int)samplerIndex destRect:(CGRect)r userInfo:obj float n = [obj floatValue];   //size of the mask But I can't find any example for how to rewrite my kernel according to this. I guess I could do something more or less like : kernel vec4 convol(sampler src, sampler mask). But I don't see how after. G.</body>
  </mail>
  <mail>
    <header>Re: Rectangle, Rotating and Clipping</header>
    <date>Wed Apr 04 07:14:45 CEST 2007</date>
    <body>Thank you Jon J. for your offer and Scott T. for your response. It looks like my call to CGContextClip(myContext) within my draw routine is the culprit. I'm struggling a bit with the clipping rectangles required, but deleting this call gives me what I am looking for. Thank you again and sorry for the noise. -- Thomas C.</body>
  </mail>
  <mail>
    <header>Re: Event types 19 and 20?</header>
    <date>Tue Apr 03 16:08:43 CEST 2007</date>
    <body>On Apr 3, 2007, at 7:22 AM, Bill Cheeseman wrote: Seems I remember something about this from WAY back on the Carbon-Dev list.  I can't effectively search for them at lists.apple.com because 19 is such a popular number :-) As I recall, Eric Schlegel responded and said that there are some undocumented, internal events that get posted to the carbon event queue.  He suggested that the person who posted the question simply pass them on to the other handlers in the Carbon Event stack.  As I recall they were related to the window server... but I don't remember exactly what they were all about.</body>
  </mail>
  <mail>
    <header>Re: Rectangle, Rotating and Clipping</header>
    <date>Tue Apr 03 14:38:32 CEST 2007</date>
    <body>If you're using the built in Graphics calls, I wouldn't expect them to work with custom rotation performed via declares. However, if you're using CoreGraphics calls directly to draw the rectangle, I'm not sure exactly what's going wrong. It appears if in addition to rotating you're also scaling. If you'd like to send me your code off-list, I could take a quick gander. HTH, -Jon -- Jonathan Johnson email@hidden REAL Software, Inc. REAL World 2007 Conference May 9 - 11 in Austin, Texas</body>
  </mail>
  <mail>
    <header>Re: Rectangle, Rotating and Clipping</header>
    <date>Tue Apr 03 14:23:35 CEST 2007</date>
    <body>On Apr 2, 2007, at 10:43 PM, Thomas Cunningham wrote: It looks like you, or potentially Real, are scaling the canvas so that the rectangle always lies within the frame of the canvas.</body>
  </mail>
  <mail>
    <header>Re: Quartz speed</header>
    <date>Mon Jan 24 19:06:46 CET 2005</date>
    <body>On Jan 24, 2005, at 9:46 AM, John Kerr wrote: If you have run numbers yourself, as it appears you have, what more are you looking for? If you want others to run numbers it may be best to provide your &amp;quot;benchmark&amp;quot; code. As said before if you are hitting a particular issue in regards to performance it is best to outline that issue so folks on this list (including Apple folks that frequent) can assist otherwise file specific defects with Apple. The simple fact is that Quartz2D will nearly always be at a disadvantage in terms of performance to QD simply because of the model and fidelity used in Quartz2D (not factoring in Tiger) when running on similar hardware.</body>
  </mail>
  <mail>
    <header>Re: Quartz speed</header>
    <date>Mon Jan 24 18:46:01 CET 2005</date>
    <body>The QDSetDirtyRegion, CGContextAddLine(Path accumulation) tricks are used in my tests. Path accumulation sometimes slows down the drawing speed! Antialiasing, Line End-Caps, Line Join, On/Off screen, variations are all included in my tests. Antialiasing sometimes makes things faster! I am familiar with the test results shown at WWDC.  Interestingly there was not much in there with &amp;gt;1 pixel thick lines.  I have tested with the new stuff in Tiger, and found some caveats. But, back to my original question.  Surely someone has performed similar tests? ________________________________ From: Scott Thompson [] Sent: Monday, January 24, 2005 10:51 AM To: John Kerr Cc: 'Shawn Erickson'; email@hidden Subject: Re: Quartz speed FYI in Quartz 2D I bet you are talking about a 2 point thick line not a 2 pixel thick line since you work in points in user space and those are mapped to pixels in the output device as needed after applying any transformation. It's actually drawing in pixels. Yes... but when you specify the size of the line in user space, you specify it in points. If you scaled the context by 2x, those same two points would draw at 4 pixels The problem is that Quartz is 3x slower at drawing thicker lines. It would be nice to know if anyone has done any kind of timing tests with Quartz, to see if there is some consistancy in these results, or if there is a realistic expectation of anything better. You can rest assured that the process of drawing lines with Quartz has been studied extensively by Apple. There has been quite a lot of work put into comparing the performance of QuickDraw and Quartz 2D in respect to drawing lots of single lines. At WWDC, Apple demonstrated the performance of Quartz 2D vs. QuickDraw with regards to this very problem. They discussed some optimizations that can improve the drawing speed of Quartz 2D when using lots of single lines. They also demonstrated some improvements that are forthcoming that will improve the performance of Quartz 2D DRAMATICALLY (so long as you are not also trying to draw in the window with QuickDraw).. I can remember some of the optimization techniques they described. Turning off Antialiasing is one acceleration technique. Using CGContextAddLines to draw a bunch of lines at once is another one acceleration technique. The last one I remember is to use QDSetDirtyRegion to mark the entire window's port as dirty before drawing a bunch of lines. This is because each line has to add it's pixel representation to the dirty region. If you've pre-dirtied the region, the region union calculations become trivial enough to simply drop out. Scott</body>
  </mail>
  <mail>
    <header>RE: Quartz speed</header>
    <date>Mon Jan 24 18:06:48 CET 2005</date>
    <body>&lt;SPAN class=796431016-24012005&gt;The QDSetDirtyRegion, CGContextAddLine(Path accumulation) tricks are used&amp;nbsp;in my tests. &lt;SPAN class=796431016-24012005&gt;Path accumulation&amp;nbsp;sometimes slows down the drawing speed! &lt;SPAN class=796431016-24012005&gt; &lt;SPAN class=796431016-24012005&gt;Antialiasing, Line End-Caps, Line Join, On/Off screen,&amp;nbsp;variations are all&amp;nbsp;included in my tests. &lt;SPAN class=796431016-24012005&gt;Antialiasing sometimes makes things faster! &lt;SPAN class=796431016-24012005&gt; &lt;SPAN class=796431016-24012005&gt;I am familiar with the test results shown at WWDC.&amp;nbsp; Interestingly there was not much in there with &amp;gt;1 pixel thick lines.&amp;nbsp; I have tested with the new stuff in Tiger, and found some caveats. &lt;FONT face=Arial color=#0000ff size=2&gt;But, back to my original question.&amp;nbsp; Surely someone has performed similar tests? Scott Thompson [mailto:email@hidden] John email@hidden Re: Quartz speed FYI in Quartz 2D I bet you are talking about a 2 point thick line not a2 pixel thick line since you work in points in user space and those aremapped to pixels in the output device as needed after applying anyIt's actually drawing in pixels.Yes... but when you specify the size of the line in user space, you specify it in points. If you scaled the context by 2x, those same two points would draw at 4 pixels The problem is that Quartz is 3x slower at drawing thicker lines. It wouldbe nice to know if anyone has done any kind of timing tests with Quartz, tosee if there is some consistancy in these results, or if there is aYou can rest assured that the process of drawing lines with Quartz has been studied extensively by Apple. There has been quite a lot of work put into comparing the performance of QuickDraw and Quartz 2D in respect to drawing lots of single lines. At WWDC, Apple demonstrated the performance of Quartz 2D vs. QuickDraw with regards to this very problem. They discussed some optimizations that can improve the drawing speed of Quartz 2D when using lots of single lines. They also demonstrated some improvements that are forthcoming that will improve the performance of Quartz 2D DRAMATICALLY (so long as you are not also trying to draw in the window with QuickDraw)..I can remember some of the optimization techniques they described. Turning off Antialiasing is one acceleration technique. to draw a bunch of lines at once is another one acceleration technique. The last one I remember is to use to mark the entire window's port as dirty before drawing a bunch of lines. This is because each line has to add it's pixel representation to the dirty region. If you've pre-dirtied the region, the region union calculations become trivial enough to simply drop out.Scott</body>
  </mail>
  <mail>
    <header>Re: Quartz speed</header>
    <date>Mon Jan 24 16:51:08 CET 2005</date>
    <body>FYI in Quartz 2D I bet you are talking about a 2 point thick line not a 2 pixel thick line since you work in points in user space and those are mapped to pixels in the output device as needed after applying any transformation.</body>
  </mail>
  <mail>
    <header>Re: Quartz speed</header>
    <date>Mon Jan 24 16:41:45 CET 2005</date>
    <body>On Jan 24, 2005, at 9:22 AM, John Kerr wrote: Well, Apple has announced publicly that, in tiger, 2D operations will likely be GPU accelerated. (a) apple probably did timing tests with Quartz (b) drawing via the GPU is probably going to be just a tad bit faster than with software ;) And even without GPU acceleration, every release of OS X, from Public Beta to 10.3 has seen performance improvements in graphics. No reason to think Tiger's software renderers won't be faster once again.</body>
  </mail>
  <mail>
    <header>RE: Quartz speed</header>
    <date>Mon Jan 24 16:22:35 CET 2005</date>
    <body>It's actually drawing in pixels. The problem is that Quartz is 3x slower at drawing thicker lines.  It would be nice to know if anyone has done any kind of timing tests with Quartz, to see if there is some consistancy in these results, or if there is a realistic expectation of anything better. The fact that Quartz is more capable in the long run doesn't really help a customer when they are waiting for their redraw to finish. -----Original Message----- From: Shawn Erickson [] Sent: Friday, January 21, 2005 10:31 PM To: John Kerr Cc: email@hidden Subject: Re: Quartz speed Quartz2D does a lot more work (for one having to map between user space and device space) then the primitives in QuickDraw so I would expect it to be slower for some/all classes of options that have a peer in QD (at least for current Mac OS X versions, wink, wink, nudge, nudge, say no more) but Q2D is more capable and flexible in the long run. If you find an area that is problematic for something you are trying to do folks on this list can likely outline some optimizations that you may want to try if you outline what issues you are having. Also file defects as needed to let Apple know about issues you are hitting. FYI in Quartz 2D I bet you are talking about a 2 point thick line not a 2 pixel thick line since you work in points in user space and those are mapped to pixels in the output device as needed after applying any transformation. -Shawn</body>
  </mail>
  <mail>
    <header>Re: 16-bit per [channel] drawing</header>
    <date>Mon Jan 24 14:57:36 CET 2005</date>
    <body>I think what you're referring to above in terms of 16-bit is &amp;quot;Thousands of colors&amp;quot; (i.e. 16 bits total for all components and/or alpha).  This thread is about 16-bits per channel/component (e.g. 64K levels each of red, green, blue).  The original subject was misleading. -- Rick Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: 16-bit per pixel drawing</header>
    <date>Mon Jan 24 09:38:37 CET 2005</date>
    <body>That's one of the reasons why I'm still sticking with Quickdraw for some things. In one particular app I need to render things to pixel-precision (simulating the form drawing on a Palm device) and the only reliable solution I found was to use a Quickdraw view into which I could appropriately draw in 16 bit and control the pixel-level rendering, especially when drawing rounded rectangles and such. -- Florent Pillet Freelance software developer/consultant - Palm OS &amp;amp; Mac OS X ICQ: 117292463                Skype: callto://florent.pillet</body>
  </mail>
  <mail>
    <header>Re: pattern</header>
    <date>Thu Jan 20 23:08:37 CET 2005</date>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Quartz speed</header>
    <date>Sat Jan 22 04:31:07 CET 2005</date>
    <body>On Jan 19, 2005, at 6:46 AM, John Kerr wrote: Quartz2D does a lot more work (for one having to map between user space and device space) then the primitives in QuickDraw so I would expect it to be slower for some/all classes of options that have a peer in QD (at least for current Mac OS X versions, wink, wink, nudge, nudge, say no more) but Q2D is more capable and flexible in the long run. If you find an area that is problematic for something you are trying to do folks on this list can likely outline some optimizations that you may want to try if you outline what issues you are having. Also file defects as needed to let Apple know about issues you are hitting. FYI in Quartz 2D I bet you are talking about a 2 point thick line not a 2 pixel thick line since you work in points in user space and those are mapped to pixels in the output device as needed after applying any transformation.</body>
  </mail>
  <mail>
    <header>Re: NSImage from encrypted PDF</header>
    <date>Fri Jan 21 22:44:32 CET 2005</date>
    <body>On Jan 21, 2005, at 2:53 PM, Clark Cox wrote: You know, that might just do it :) Thanks for pointing this out!  Guess I was too deep in the APIs to see obvious solutions such as this. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: NSImage from encrypted PDF</header>
    <date>Fri Jan 21 22:02:40 CET 2005</date>
    <body>Would something like this be appropriate?: -- Clark S. Cox III email@hidden</body>
  </mail>
  <mail>
    <header>NSImage from encrypted PDF</header>
    <date>Fri Jan 21 20:10:21 CET 2005</date>
    <body>I'm exploring the possibility of having my apps PDF images all be encrypted.  This is basically to add at least some protection to my artwork. I'm hoping to have my app baselined against 10.3 and so it looks like I need to use APIs such as CGPDFDocumentUnlockWithPassword. What I'm having trouble with is coming up with a &amp;quot;pipeline&amp;quot; that will take the original encrypted PDF and ultimately give me an NSImage. I can figure out how to create a CGPDFDocumentRef and unlock it with the appropriate password.  However, I cannot seem to find how to obtain the (decrypted) PDF data to ultimately build an NSData object which in turn can be used to initialize an NSImage. About the only thing I can get from the document is a CGPDFPageRef (in my case, I'll only ever have single page documents, so this would be easy).  But from there, I still cannot find a function to get any data.  There are some hints that the page's dictionary could contain a PDF data stream, but that's where everything is getting blurry :P Any help greatly appreciated, -- Rick Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: Drawing on top of Sequence Grabber video</header>
    <date>Fri Jan 21 01:13:37 CET 2005</date>
    <body>Scott Thompson: Consider using a grouped overlay window. This way, Quartz Extreme will be used for compositing, which is likely to be faster on most systems. On the other hand, it's likely to be slower on older systems. -- Jens Ayton</body>
  </mail>
  <mail>
    <header>DVI to HDMI video cable on G5</header>
    <date>Thu Jan 20 21:59:30 CET 2005</date>
    <body>I realize this list may not be the most appropriate place to ask this &amp;quot;hardware&amp;quot; question, but nothing else on lists.apple.com seemed more appropriate... Does anyone have any experience or knowledge concerning the use of a DVI-to-HDMI cable for connecting a G5 powermac (NVIDIA GeForce FX 5200 Ultra) to an HD TV that has an HDMI input? Should it work?  I would have assumed that it would work, but recent experience with one such cable and a Sony HD TV seemed to prove otherwise*. I have read a few things stating that cable quality can make a difference between success and failure, and perhaps that is the case here. Any kind of help would be appreciated. * No other display was connected at the time owing to the lack of an ADC-to-VGA adapter.</body>
  </mail>
  <mail>
    <header>Re: Providing shade colors to Calculate CGFunctionCallback</header>
    <date>Thu Jan 20 21:04:08 CET 2005</date>
    <body>On Jan 20, 2005, at 12:45 PM, Toine Heuvelmans wrote: The first parameter of CGFunctionCreate is a pointer to anything you want.  This pointer is passed back to you as the first parameter of your function callback.  Generally you would pass a pointer to a structure containing your colors and whatever else you need here.  Just make sure the data pointed to still exists when your callback is actually called.</body>
  </mail>
  <mail>
    <header>Re: Drawing on top of Sequence Grabber video</header>
    <date>Thu Jan 20 20:47:50 CET 2005</date>
    <body>On Jan 20, 2005, at 1:34 PM, Toine Heuvelmans wrote: The routine QDBeginCGContext and QDEndCGContext allow you to create a CGContext on a QuickDraw port.  Theoretically you will be able to set up a callback on the QuickTime content and can use the calls to create a context, draw your content, and destroy that context.</body>
  </mail>
  <mail>
    <header>Providing shade colors to Calculate CGFunctionCallback</header>
    <date>Thu Jan 20 20:45:04 CET 2005</date>
    <body>I tried implementing the functions from the Quartz documentation, concerning shading. that is, the: MyDrawEventHandler myPaintAxialShading myGetFunction myCalculateShadingValues It worked great, I was drawing rectangles the entire day! I provided a color within the Calculate... function, and calculated a darker and brighter shade, which were the two colors for my shade. But the thing I want to do, is to let the user pick a color, and put this into the Calculate... function (and then the darker and brighter calculus...). I could not find a way to provide any extra info to this function. How do I do this?</body>
  </mail>
  <mail>
    <header>Drawing on top of Sequence Grabber video</header>
    <date>Thu Jan 20 20:34:23 CET 2005</date>
    <body>I'm totally new to Quartz, just drew my first line today, but before I go any further, I just want to know if what I want is possible with Quartz. So... Is it possible to draw on top of QuickTime video output, generated by a Sequence Grabber? (it uses the window as a port, and draws directly on top of it, or at least that's what I believe happens) My application is Carbon based, and uses a HIView for drawing Quartz stuff. Thank you Toine Heuvelmans</body>
  </mail>
  <mail>
    <header>Re: pattern</header>
    <date>Thu Jan 20 15:21:22 CET 2005</date>
    <body>If you are writing a CFM app you need to provide a MachO function pointer for the Callback.  See the CFM_MachO_CFM sample from Apple, and look for MachOFunctionPointerForCFMFunctionPointer.</body>
  </mail>
  <mail>
    <header>pattern</header>
    <date>Wed Jan 19 21:18:39 CET 2005</date>
    <body>I want to draw pattern. I din't succeed. Compilation and link are OK, but the fonction MyDrawColoredPattern is never called. Can someone tell me if what I've write is correct or what is not correct? Thanks Vincent #define H_PSIZE 16 #define V_PSIZE 18 void MyDrawColoredPattern (void */*info*/, CGContextRef myContext) float subunit = 5; // the pattern cell itself is 16 by 18 if(fCGContext) static const	CGPatternCallbacks	 callbacks = if(pattern != NULL) pattern = CGPatternCreate (NULL, CGRectMake (0, 0, H_PSIZE, V_PSIZE), CGAffineTransformMake (1, 0, 0, 1, 0, 0), H_PSIZE, V_PSIZE, kCGPatternTilingConstantSpacing, true,</body>
  </mail>
  <mail>
    <header>Quartz speed</header>
    <date>Wed Jan 19 15:46:40 CET 2005</date>
    <body>Has anyone compared the speed of drawing graphic primitives with Quartz and compared them with QuickDraw?  I am finding Quartz is often slower, especially when line thickness is greater than 1 pixel. Here's my timing results for drawing 10000 primitives. G5, GeForceFX5200, OS10.3.7 1 Pixel thick Line	Rect	RRect	Poly	Ellip	Arc QuickDraw	2.8	3.19	3.36	26.57	4.46	4.15 Quartz	2.51	3.54	18.02	8.63	17.42	14.04 2 Pixel thick Line	Rect	RRect	Poly	Ellip	Arc QuickDraw	2.86	3.25	3.33	27.53	4.06	3.88 Quartz	8.46	9.61	28.02	25.35	30.15	23.7 For QuickDraw, the numbers are for drawing off-screen, since that was the fastest case. For Quartz, the numbers for 1 pixel thick lines are for on-screen, antialiased, accumulated path drawing, since that was the fastest.  The numbers for 2 pixel thick lines are for off-screen, non-antialiased, non-accumulated paths, since they were the fastest. I am curious to know what kind of performance numbers are expected from Quartz.</body>
  </mail>
  <mail>
    <header>Flushing performance (was Re: tiger)</header>
    <date>Wed Jan 19 00:06:50 CET 2005</date>
    <body>Generally we can't discuss the Tiger changes on a public list like this because of the NDA. I probably broke protocol by mentioning the floating point support. While I can't go into the features, one important change that will affect performance (and more likely benchmarks) is flushing. For the past few years we've been telling developers not to flush faster than the display refresh rate as that is just wasteful, and in Tiger we will be enforcing this by making every flush operation vbl syned. This is important to reduce or eliminate visual tearing in the system and continue going forward with glitch free playback in the system. That means that when you have something like the following in your event loop: 1) Do some initital drawing (usually to erase the backgound) 2) Do some other drawing 3) Flush the drawing (either explicitly through CG or through Carbon/Cocoa's flush) Now while the flush operation is asynchronous, when you go back to the top of the event loop to draw things again, the first drawing operation (1) will block until the previous flush has completed. In Tiger, this flush will now wait at least 1/60th of as second for the vbl sync. If you profile your application through shark you will notice that most of the time will be spent on the first drawing operation (not because it is expensive) because it waiting for the previous flush to complete. Benchmarks that test the drawing performance of the system will definitely be affected.  Please note that different graphics cards have had different vbl syncing behavior. With the Tiger change things will be consistent across the graphics hardware. So recommendations (which are still valid for 10.3 and earlier) - Minimize the flush operations in every frame. i.e. don't draw, flush, draw, flush, ... for every update. Instead draw, draw, draw, flush - Consolidate your drawing operations together and issue a single flush. Defer your drawing until necessary. i.e. don't draw as the first thing in your event loop before you go off and do other work in your application. Because the flush is asynchronous, you can avoid blocking by delaying your drawing. - Don't flush faster than 60 Hz. In many cases 30 Hz is more than enough. - Use Quartz Debug to detect cases where you are overflushing or performing unnecessary drawing. These recommendation should not be new but I thought I'm mention them again.</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <date>Tue Jan 18 20:44:14 CET 2005</date>
    <body>On Jan 17, 2005, at 7:58 PM, Eric Schlegel wrote: Thanks Eric, that worked well.  I didn't really understand what SetWindowGroup did since the documentation for that function doesn't say much.  The header file says a little more but I still didn't quite catch what its purpose was.  Now it makes sense.  I'm now just using a simple kPlainWindowClass window for my color palette. I will probably be back using a kOverlayWindowClass window as soon as I start working on drawing to the screen using Quartz instead of QuickDraw.  The overlay window will be important for  our graphic cursor/crosshairs and for our temporary objects.   But that's a multi-month project that's going to have to wait until after the rest of the Carbon work.</body>
  </mail>
  <mail>
    <header>tiger</header>
    <date>Tue Jan 18 19:18:31 CET 2005</date>
    <body>what are the most significant changes in quartz that come with tiger? render to float was mentioned earlier.  Are there others?</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <date>Tue Jan 18 04:58:35 CET 2005</date>
    <body>On Jan 17, 2005, at 6:30 PM, Mike Lazear wrote: If that's the only reason, and you'd prefer to use a different window type, you have an easy solution: just use SetWindowGroup to move whatever window you like into the window group for the overlay window class. You can freely make any window you like appear above floating windows using SetWindowGroup( window, GetWindowGroupOfClass( kOverlayWindowClass ) ).</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <date>Tue Jan 18 03:30:57 CET 2005</date>
    <body>On Jan 15, 2005, at 4:41 PM, Nick Nallick wrote: Nick - you probably are giving much more credit than I deserve.  =) We have a color palette that our users can pick a color from.  This consists of 256 colors.  We don't use the standard Apple color picking because at the moment our database only supports one byte to select a color index as opposed to 6 bytes (2 bytes per color) which are required for RGB.  So in our property window you can either type in a color index or click down on a icon which pops up the color palette. This palette is only visually available until you let go of the button. This color palette is created in an overlay window. So why an overlay window?  Well it happens to be the only window type that will show up in front of my floating properties window.  Movable modals and the rest of the window types show up behind the window where the color icon is located.  The overlay window type shows up in front. So it may be an odd use but it works well and the window only lives for a few seconds and is relatively small.</body>
  </mail>
  <mail>
    <header>Re: 16-bit per pixel drawing</header>
    <date>Mon Jan 17 18:57:32 CET 2005</date>
    <body>Drawing to a 16 bit per channel context isn't available. Tiger will introduce rendering to 32 bit floating point image formats - maybe overkill for your purpose, but it will be the only high precision format to render to we will support for now. Attachment:</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <date>Sun Jan 16 01:41:18 CET 2005</date>
    <body>On Jan 15, 2005, at 1:01 PM, Mike Lazear wrote: I'm sure you know what you're doing, but in general there can't be too many cases where you'd want to use an overlay that wasn't transparent.</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <date>Sat Jan 15 21:01:17 CET 2005</date>
    <body>On Jan 15, 2005, at 6:51 AM, Nick Nallick wrote: Notice that the &amp;quot;must use&amp;quot; is connected with &amp;quot;to preserve the transparency of the window&amp;quot;.  I've had no problem with QuickDraw in an overlay window and in my case I don't want transparency.  That being said the way I'm currently drawing is not ideal but I'm in the middle of converting an application from QuickDraw to Quartz and it's going to take months.  Meanwhile I need to keep the program runable.  I tried pretty much every window type and the overlay window was the only one that would work for my particulars needs. I was not trying to suggest to Ricky that QuickDraw in an overlay window was ideal but rather to explain why I thought he was having the problem that he was seeing.</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <date>Sat Jan 15 15:51:41 CET 2005</date>
    <body>On Jan 14, 2005, at 7:55 PM, Ricky Sharp wrote: I would say that's it's a bad idea to try to use QuickDraw in an overlay window. Note the following from MacWindows.h (emphasis added): Nick</body>
  </mail>
  <mail>
    <header>Re: 16-bit per pixel drawing</header>
    <date>Sat Jan 15 15:11:13 CET 2005</date>
    <body>I was able to create a 16-bit greyscale NSBitmapImageRep, and I could lockFocus on it, but the drawing is never 16-bit because the context is still 8-bit. I went down to the CGContext level but was unable to create a 16-bit per channel context, so I suspect the support is not there. I think the 16-bit per channel CGImageCreate support is probably there to make it possible to view 16-bit per channel images, but I think they</body>
  </mail>
  <mail>
    <header>Re: 16-bit per pixel drawing</header>
    <date>Sat Jan 15 04:24:12 CET 2005</date>
    <body>On Jan 14, 2005, at 8:51 PM, Jamie Cho wrote: I too could not find any Quartz docs that mentioned 16-bit component support.  Forgot where that one table was, but it was something to do with &amp;quot;supported pixel formats&amp;quot; and they only had up to 8 bits per component. mentions that for the initWithDataPlanes:pixelsWide:... API, that the bits per sample (component) parameter can have the values 1, 2, 4, 8, 12 or 16. I would wager a guess that you should be able to pass in these same values for the bitsPerComponent parameter of CGImageCreate. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <date>Sat Jan 15 03:55:42 CET 2005</date>
    <body>Good catch.  Upon closer inspection, I see that CarbonSketch is using the Quartz APIs for drawing into the overlay whereas GrabBag uses QuickDraw. This would also explain why no issues cropped up in the Cocoa examples (since they ultimately use Quartz). Workaround definitely seems to be to use Quartz.  Although, &amp;quot;workaround&amp;quot; is a bad term here since QuickDraw is deprecated and one should migrate to Quartz anyhow. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>16-bit per pixel drawing</header>
    <date>Sat Jan 15 03:51:57 CET 2005</date>
    <body>There is a reference at this page: that claims that Quartz can draw to 16-bit per-channel bitmap contexts. All the documentation that I found indicates that the maximum number of bits per channel is 8. Is there anyway to create a 16-bit per channel context, or is this reference in error? I am particularly interested in creating  a 16-bit grayscale  context.</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <date>Sat Jan 15 01:43:14 CET 2005</date>
    <body>Greetings All, I am currently working with overlay windows under MacOS 10.3.7. The overlay windows, however, are completely invisible when the display is set to 16-bit color (&amp;quot;Thousands of Colors&amp;quot;) resolution. Other bit resolutions (8-bit and 24/32-bit) operate properly. Although this issue appears to be a bug of some sort, (it operates fine in MacOS 10.4 build 8A323), I would like to know why the &amp;quot;CarbonSketch&amp;quot; overlay window sample appears to operate correctly. The following Apple code sample appears to exhibit the aforementioned problem (&amp;quot;Overlay&amp;quot; tab - Large Cursor): http://developer.apple.com/samplecode/GrabBag/listing6.html Does anybody know why this code (as well as my code - overlay window initialization portion available if necessary) does not produce a visible overlay window under MacOS 10.3.7?</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit)	Color	Resolution?</header>
    <date>Sat Jan 15 01:29:18 CET 2005</date>
    <body>On Jan 14, 2005, at 4:17 PM, Ayodeji A. Oshinnaiye wrote: This is definitely a strange one.  I can reproduce this problem also under 10.3.7.  And, I can't see anything in the code that should cause it to fail with 16-bit color.  The CarbonSketch example does work for me also under 10.3.7. I know this won't help you since you are coding in Carbon, but Cocoa overlay windows do not have any problems in 16-bit color.  I ran the FunkyOverlayWindow example and it worked a-ok: ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <date>Fri Jan 14 23:17:53 CET 2005</date>
    <body>Greetings All, I am currently working with overlay windows under MacOS 10.3.7. The overlay windows, however, are completely invisible when the display is set to 16-bit color (&amp;quot;Thousands of Colors&amp;quot;) resolution. Other bit resolutions (8-bit and 24/32-bit) operate properly. Although this issue appears to be a bug of some sort, (it operates fine in MacOS 10.4 build 8A323), I would like to know why the &amp;quot;CarbonSketch&amp;quot; overlay window sample appears to operate correctly. The following Apple code sample appears to exhibit the aforementioned problem (&amp;quot;Overlay&amp;quot; tab - Large Cursor): Does anybody know why this code (as well as my code - overlay window initialization portion available if necessary) does not produce a visible overlay window under MacOS 10.3.7? Thanks in advance, Ayo</body>
  </mail>
  <mail>
    <header>Re: CGImage/CGBitmapContext, guaranteed to be in sync?</header>
    <date>Fri Jan 14 19:37:52 CET 2005</date>
    <body>That's why I asked :) Thanks! -Jon</body>
  </mail>
  <mail>
    <header>Re: CGImage/CGBitmapContext, guaranteed to be in sync?</header>
    <date>Fri Jan 14 19:19:12 CET 2005</date>
    <body>No, please do not rely on this behavior. A CGImageRef, once created is supposed to be immutable, including the bitmap data backing it. That's the model. The fact that you see what you see is just an implementation detail. This will break if CoreGraphics performs any intelligent caching on images, so better not to rely on this behavior. Attachment:</body>
  </mail>
  <mail>
    <header>CGImage/CGBitmapContext, guaranteed to be in sync?</header>
    <date>Fri Jan 14 14:48:58 CET 2005</date>
    <body>I have a question about something I've been noticing, and before deciding to rely on it, I wanted to ask if it's expected to be true :) In some cases I create a CGBitmapContext and a CGImage pointing to the same data. If I draw the CGImage, and then proceed to draw more things to the CGBitmapContext, will the CGImage know to regrab the data before being redrawn? My initial tests show &amp;quot;Yes,&amp;quot; but before relying on anything, I decided it would be better to ask others :) 1) Create the CGBitmapContext and CGImage 2) Fill the CGBitmapContext with blue 3) Draw the CGImage to the screen 4) Draw a red circle on the CGBitmapContext 5) Draw the CGImage to the screen At step 5, am I guaranteed to have a red circle on a blue background show up? Thanks in advance, Jon</body>
  </mail>
  <mail>
    <header>Re: How to avoid stretched screen mode?</header>
    <date>Thu Jan 13 21:19:09 CET 2005</date>
    <body>On Jan 13, 2005, at 11:43 AM, Shawn Erickson wrote: Ok I need to amend what I stated... first off on my system 1280 x 800 x 32 only has a non-stretched mode listed (again walking all display modes), I was getting data points screwed up when I was looking at things because of not ready the numbers correctly. Anyway if I ask for 1280 x 960 I get both a stretched and non stretched version and when I ask for that resolution on the game side of things I get back a non stretched form and (not using my remove hack) the display gets switched to a non-stretched mode. capacity = 11, pairs = ( capacity = 12, pairs = ( I note that the default resolution 1920 x 1200 for my display has width/heigh ratio of 1.6 and that 1280 x 960 has 1.33 while 1280 x 800 has 1.6. Anyway I think the fault is mine now that I look at it... the game is assuming a ratio of 1.33 and hence some images are getting stretched by OpenGL to fill the space when in a screen mode without that ratio. I guess I will work on displaying only ones with the ideal ratio for the game.</body>
  </mail>
  <mail>
    <header>Re: How to avoid stretched screen mode?</header>
    <date>Thu Jan 13 20:43:17 CET 2005</date>
    <body>On Jan 13, 2005, at 11:33 AM, Shawn Erickson wrote: Oh I also meant to state that I am asking for a 1280 x 800 x 32 which on the config side of things has two display modes listed one with the stretched key set to a value of true and the other with no key (listing is built by getting and walking all display modes for the selected monitor). In fact on the config side of things the user is picking the exact same dictionary (contents wise) as the one we attempt to use on the game side of things. capacity = 11, pairs = (</body>
  </mail>
  <mail>
    <header>Re: How to avoid stretched screen mode?</header>
    <date>Thu Jan 13 20:33:29 CET 2005</date>
    <body>On Jan 13, 2005, at 11:03 AM, Mike Paquette wrote: Yes my CFShow is before my attempt to force things (note it is tagged as being immutable). FYI, I tried the key removal as a way to force things only because I couldn't stop stretched mode other ways, it was just dropped in temporarily. In other words the display mode I am firing down to switch mode, even when using the original one returned to me,  is not showing that it will use stretched mode yet it still does. Ok. As a side note my mutable copy of the original works the same as the original (note no key is found to be removed)... I guess the keys and &amp;quot;IOFlags&amp;quot; with the later two being used to map the dictionary to an IOKit mode/configuration. Does the values  for IOFlags give anyone a hit on stretched mode or not? The dictionary implies it isn't stretched because the stretched key-value isn't set. I may be hitting a bug in Quartz services then since I appear to working with a stretched mode however I cannot detect that at my level using the API as documented since the needed key-value isn't being set. Note when walking all supported display configurations I do see stretched and non-stretched configurations so it is setting the key for at least some of them (I wonder Quartz services has things backwards)? Anyway I am running 10.3.7 on a PM G5 2GHz (1st gen) with an Geforce 6800 GT and two 23&amp;quot; cinema displays attached (monitor &amp;quot;0&amp;quot; is my secondary display and monitor &amp;quot;1&amp;quot; is considered the main display as I have things wired and configured via monitor preferences).</body>
  </mail>
  <mail>
    <header>Re: How to avoid stretched screen mode?</header>
    <date>Thu Jan 13 20:03:43 CET 2005</date>
    <body>Also, you cannot create your own CFDictionary and pass it to CGDisplaySwitchToMode().  The CFDictionary passed to CGDisplaySwitchToMode() must be a member of the list returned by CGDisplayAvailableModes().  There's a list lookup that checks for this. The CFDictionary describing a display mode is an encoding of an internal IOKit structure (so it won't break horribly across releases), and we map the object to the IOKit internal data to apply the mode change.  If the CFDictionary originated from an IOKIt display mode description that does not put the display in stretched mode, then you'll get that unstretched mode.</body>
  </mail>
  <mail>
    <header>Re: How to avoid stretched screen mode?</header>
    <date>Thu Jan 13 19:29:45 CET 2005</date>
    <body>On Jan 13, 2005, at 10:13 AM, Mike Paquette wrote: I added a CFShow after I ask for the best mode I get the following which doesn't have mode is stretched key in it yet I still get stretched. capacity = 11, pairs = ( Nope no TV output support is wanted. Yeah I know... I guess you missed the fact that I am making a mutable copy of the dictionary returned. :-) It looks like they may be weighted to much towards that since I cannot stop them from doing it. :-(</body>
  </mail>
  <mail>
    <header>Re: How to avoid stretched screen mode?</header>
    <date>Thu Jan 13 19:13:08 CET 2005</date>
    <body>The  CGDisplayBestMode... functions are helpers intended to make simple requests fairly straightforward.  They can find entries in the list vended by CGDisplayAvailableModes() that meet some basic criteria, but they are not the ultimate in matching systems.  :-) You can call CGDisplayBestModeForParametersAndRefreshRateWithProperty() to obtain a typical display configuration with resolution and refresh rate that meet your needs.  Very often, for each such mode description that meets your specification, but has the kCGDisplayModeIsStretched property, there is another mode with the same values, but without that property.  (I cannot guarantee this, as I am but a software geek, and am not worthy to speak of the works of hardware wizards.) You can call CGDisplayAvailableModes() and inspect the complete list for modes that match the width, height, and refresh rate returned in the mode dictionary from CGDisplayBestModeForParametersAndRefreshRateWithProperty().  In your case, you'll also want to check for the presence of the key kCGDisplayModeIsSafeForHardware, and the absence of the key kCGDisplayModeIsStretched. If you are trying for a TV output, as via the S-Video connector, make sure the kCGDisplayModeIsTelevisionOutput property is present, and the refresh rate is appropriate for the user's TV.  NTSC uses a different rate than PAL or SECAM.  Since there's no monitor feedback (EDID data or similar information) from TV devices, I suggest that you always use a confirm dialog and a timer to revert the display mode on inaction on any modes marked kCGDisplayModeIsTelevisionOutput. Note that the CGDisplayBestMode... functions return a CFDictionaryRef, not a CFMutableDictionaryRef.  There's no guarantee that CFDictionaryRemoveValue() on a CFDictionaryRef won't just fail, or worse, crash your application. In general these functions are weighted to prefer stretch modes, so as to always fill the screen, and reduce complaints about those darn black bands (matte bands).</body>
  </mail>
  <mail>
    <header>How to avoid stretched screen mode?</header>
    <date>Thu Jan 13 18:33:36 CET 2005</date>
    <body>I am working on modernizing a game that uses mode switching to setup full game play. The following is an example of the code that we are trying to use to switch display mode as needed. We would like to avoid the use of stretched mode (stretching of the horizontal pixels to fill the display) and our configuration GUI is code to not offer stretched mode selection at this time when picking resolutions. This game is cross platform so as a result (at least currently) the configuration GUI cannot easily pass the display mode dictionary (I guess we could write it out as a plist) itself to the game application so instead we pass widths, height, bpp, and refresh rate. // Insure we have a reasonable display mode... displayConfig.displayMode = CGDisplayBestModeForParametersAndRefreshRateWithProperty( displayConfig.displayID, mybpp, mywidth, myheight, myrefreshrate, kCGDisplayModeIsSafeForHardware, // Make sure we don't use streched mode (trying to prevent it with following code)... displayConfig.displayMode = (CFDictionaryRef) CFDictionaryCreateMutableCopy( kCFAllocatorDefault, CFDictionaryGetCount(displayConfig.displayMode), CFDictionaryRemoveValue((CFMutableDictionaryRef)displayConfig.displayMod // Switch the monitor into the mode requested... CGDisplaySwitchToMode(displayConfig.displayID, So does a way exist to avoid stretched mode? Do I have to use something other then CGDisplaySwitchToMode? Or am I just missing something obvious?</body>
  </mail>
  <mail>
    <header>CreateStandardAlert / RunStandardAlert</header>
    <date>Fri Jan 07 07:09:25 CET 2005</date>
    <body>I am creating an alert box using CreateStandardAlert / RunStandardAlert from&amp;nbsp;&lt;SPAN class=003455905-07012005&gt;Dialog's filter proc&amp;nbsp;b&lt;SPAN class=003455905-07012005&gt;ut I am unable to see buttons and text on alert box. I see only a blank alert box.&amp;nbsp;Is there&amp;nbsp;anyone who have faced similar problem earlier &lt;FONT face=Verdana color=#000080 size=2&gt; &lt;FONT face=Verdana color=#000080 size=2&gt;Thanks, &lt;SPAN class=493391411-05012005&gt;Manish</body>
  </mail>
  <mail>
    <header>Re: Compositing a mask and an image</header>
    <date>Thu Jan 06 20:01:37 CET 2005</date>
    <body>On 5-Jan-05, at 1:01 PM, Nick Nallick wrote: Thanks to everyone for their input. I'll take a look at vImage. Thanks again, Jon -- Jonathan Johnson REAL Software, Inc. -- REAL World 2005 - The REALbasic User Conference March 23-25, 2005, Austin, Texas</body>
  </mail>
  <mail>
    <header>Re: CGGLContextCreate appears not to release</header>
    <date>Fri Aug 04 23:23:16 CEST 2006</date>
    <body>Thanks for your reply. With regards to returning to the main event loop: yes I am. Cheers, -C</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <date>Fri Aug 04 23:22:52 CEST 2006</date>
    <body>Absolutely. Actually there is no Bezier curve that is exactly parallel (offset) to a given given Bezier curve (except in the degenerate case where the Bezier curve is a straight line. But if you can confine yourself to paths that have no cusp, no self intersections, and &amp;quot;small&amp;quot;  offsets you can construct a reasonable approximation by offsetting the control polygon of the original curve and using the offset polygon as the control polygon for a new curve. Small, here, means small with respect to the smallest radius of curvature of the original curve. ....Bob Clair</body>
  </mail>
  <mail>
    <header>Re: CGGLContextCreate appears not to release</header>
    <date>Fri Aug 04 21:00:12 CEST 2006</date>
    <body>On Aug 3, 2006, at 10:29 PM, Christopher Hunt wrote: Are you returning to the main event loop between calls? should be putting the context into the autorelease pool.  If that autorelease pool is not released itself (between calls to this code) then the context would still have a reference owned by that pool.</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <date>Fri Aug 04 20:57:21 CEST 2006</date>
    <body>Actually there is no way to transform the path to achieve this for an arbitrary bezier shape. For simple shapes, like ovals and rectangles, you can achieve this effect, but even for simple polygons (particularly concave polygons, or those with self-intersections) a simple affine transform won't do what you want it to.  If you want to prove this to yourself, try creating a polygon that looks like a blocky letter &amp;quot;C&amp;quot; and see if you can find a transform that will create a path that lets you stroke only on the interior of the path. Your next attempt would be to try and use clipping to achieve the desired result, but you'lll eventually find that that won't work either. To solve this problem in Quartz 2D you have to construct a path that is &amp;quot;parallel&amp;quot; to the original path which you can stroke.  Creating a bezier path that is parallel to another bezier path is complicated... particularly if the original path has a cusp. Y also have to consider removing self intersections from the new path. and patching up any resulting discontinuities that that generates. As Nick pointed out, the only practical way to implement this functionality would be to add it to the imaging model, but doing so would put Quartz 2D out-of-joint with PostScript and PDF.  As a result, it would probably have to be handled by a library that is outside of Quartz 2D (be it an Apple addition, or a third-party library) and to my knowledge, no such thing exists. :-( I know this same functionality exists in the imaging models of several other graphics libraries (GDI+ and Java 2D come to mind immediately).  It would be interesting to see how those libraries translate the resulting graphics into PostScript and PDF.</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <date>Fri Aug 04 16:13:32 CEST 2006</date>
    <body>On Aug 4, 2006, at 7:07 AM, Vinay Prabhu wrote: You can transform when you draw in various ways, so you can easily achieve what you want above. Of course if the path crosses itself this scheme wont work. It isn't clear if you are working at the Cocoa level or down at the Quartz 2S level (I will assume the later since you are on this list)... -Shawn</body>
  </mail>
  <mail>
    <header>RE: PenAlignment(GDI+) alternative in Quartz</header>
    <date>Fri Aug 04 16:07:00 CEST 2006</date>
    <body>I agree with you, for the basic paths I could achieve all kind of outline. I need to apply outline for the path extracted from mask of an image. Which is a complex arbitrary path. Is it possible to shrink or stretch the path with respect to center? I have tried to scale the path along both x and y axis. But this will retain its origin constant, so the desired effect can't be achieved. Regards Vinay -----Original Message----- From: Nick Nallick [] Sent: Friday, August 04, 2006 7:13 PM To: Vinay Prabhu Cc: Quartz-Dev (E-mail) Subject: Re: PenAlignment(GDI+) alternative in Quartz There isn't an equivalent feature to this in Quartz.  For some basic paths you could probably find a way to do it fairly easily, but I suspect that to make it work with any arbitrary complex path you'd have to implement your own stroking engine. Nick</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <date>Fri Aug 04 15:43:06 CEST 2006</date>
    <body>On Aug 4, 2006, at 7:30 AM, Vinay Prabhu wrote: There isn't an equivalent feature to this in Quartz.  For some basic paths you could probably find a way to do it fairly easily, but I suspect that to make it work with any arbitrary complex path you'd have to implement your own stroking engine. Nick</body>
  </mail>
  <mail>
    <header>PenAlignment(GDI+) alternative in Quartz</header>
    <date>Fri Aug 04 15:30:37 CEST 2006</date>
    <body>Hi, I am having some trouble drawing the outer and inner outline. I have a path which I need to stroke so that, the stroke will draw  with center, outer and inner 3 different color's. The outline center can be obtained by simply calling the stoke method. But for inner outline the drawing should starting from the path and move inwards from the path line. For outer outline it should move outwards. These features can be seen in Photoshop. Using GDI+ this can be achieved by just setting the PenAlignment to center, outer or inner. How to achieve this in Quartz? Is there any alternatives for this feature in Quartz? I thought of a method, keep a copy of the original path. Then shrink or expand it and then add it to the original path. Now if we fill the path with odd even fill, the desired effect might be possible to achieve. Is this method proper? If yes, how to shrink or expand the path? Regards Vinay</body>
  </mail>
  <mail>
    <header>CGGLContextCreate appears not to release</header>
    <date>Fri Aug 04 05:29:23 CEST 2006</date>
    <body>The following CGContextRelease does not appear to release memory associated the created CG GL context: cgGLContext = CGGLContextCreate( [theCurrentContext CGLContextObj], theSize, 0 I have run the above under MallocDebug and noted that my total allocation grew by ~80K each time the above was called. Commenting out the above code eliminates this growth. Cheers, -C</body>
  </mail>
  <mail>
    <header>Re: Quartz API Rendering Load</header>
    <date>Thu Aug 03 08:07:27 CEST 2006</date>
    <body>I'd suggest that you investigate Apple's OpenGL Profiler.app and OpenGL Driver Monitor.app both of which are available in /Developer/ Applications/Graphics Tools/ (Driver Monitor's decoder ring is documented here -- ) Furthermore Apple's OpenGL Programming Guide for Mac OS X (http:// developer.apple.com/documentation/GraphicsImaging/Conceptual/OpenGL- MacProgGuide/index.html) contains useful information related to performance data gathering and analysis -- documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/ opengl_performance/chapter_13_section_3.html#//apple_ref/doc/uid/ TP40001987-CH213-SW14 Max Rupp OpenGL / Core Video Programming email@hidden</body>
  </mail>
  <mail>
    <header>Quartz API Rendering Load</header>
    <date>Thu Aug 03 07:00:52 CEST 2006</date>
    <body>I am developing an application that uses OpenGL, Core Image and Quartz to do some live video processing.  What I am wondering is how I can determine the rendering load (which I am assuming is the GPU load), much like what Quartz Composer shows. My primary reason is I want to be able to work on optimization and it would be useful to know if my bottlenecks are due to hard disk, CPU or GPU.  But I also like being able to monitor so that if I see the rendering load spiking I can start turning off filters to ensure peak performance. Much appreciate it if anybody has any ideas on how to pull this off, Thanks, Daniel</body>
  </mail>
  <mail>
    <header>Re: Convert NSBezierPath to Quartz path</header>
    <date>Wed Aug 02 19:42:01 CEST 2006</date>
    <body>Re: On Aug 2, 2006, at 9:09 AM, Scott Thompson wrote: I think what you meant to say is that you use Cocoa methods to clip to your NSBezierPath and then draw the CGShading.</body>
  </mail>
  <mail>
    <header>Re: Abstract images in Quartz</header>
    <date>Wed Aug 02 19:21:14 CEST 2006</date>
    <body>Yes, but CGImageRef is fixed in pixel count and depth, so my data provider (however sophisticated it is) will not be able to provide image with better resolution when CGContextDrawImage operation is called with bigger rect. This particular aspect - the behavior of the image on draw operation (not how it is stored internally or where did it came from initially) is what actually matters, IMHO. [Compressed pixmaps and regions in QuickDraw are two examples of what I'd call &amp;quot;bitmapped&amp;quot; objects.] What type would you recommend for hypothetical internal API that assigns &amp;quot;abstract single-page image&amp;quot; to object? CGPDFDocumentRef? CGPDFPageRef? Maybe not. [However, windows are implemented as OGL textures - read patterns. Does anyone care? :-) ]. I thought that if I use pattern with step&amp;gt;=size, phase=0 and identity matrix it won't have a chance to tile. And the good thing it has drawing callback not found in other CG objects. IMHO Quartz would only benefit from having support for abstract image type. Something that would allow me to: - CreateWithDrawCallback - Draw itself The rest is optional but nice: - CreateFromFile (TIFF, PNG, PDF, PICT, etc) - CreateFromIcon - CreateFromCGImage - CreateFromQTImporterComponent Mike</body>
  </mail>
  <mail>
    <header>Re: Abstract images in Quartz</header>
    <date>Wed Aug 02 18:21:26 CEST 2006</date>
    <body>On Aug 2, 2006, at 4:45 AM, Mike Kluev wrote: Well, no.  CGImages may be stored in a bitmap, or they may be stored in other ways. For example, a CGImage can draw from a block of compressed PNG data. You are accurate in that a CGImage represents a rectangular array of color samples from a particular data source (as well as some associated metadata).  However, it makes not guarantees as to how that those samples are stored (or generated).  You could, in theory, create a CGDataSource that generates the color samples of an image on- the-fly. It sounds like you are looking for a &amp;quot;retained mode&amp;quot; drawing API and Quartz 2D is not that kind of API. The metafile format for Quartz 2D is PDF.  That sounds closest to what you are looking for. A PDF can be comprised of an arbitrary combination of Images, Text, and Line Art and can be drawn on any CGContext you care to. Depending on your needs, another solution may be to use a CGLayer which is a cached set of Quartz 2D drawings that can be optimally reproduced on a given device.  So long as you are able to guarantee that you will always draw that layer on the given device, it can be a very helpful caching mechanism. I wouldn't.  CGPatterns have a very specific use (tiling an image into a context) and trying to use them for anything else is not likely to be what you want.</body>
  </mail>
  <mail>
    <header>Re: Convert NSBezierPath to Quartz path</header>
    <date>Wed Aug 02 18:09:47 CEST 2006</date>
    <body>On Aug 2, 2006, at 7:46 AM, Vinay Prabhu wrote: I don't know of any way to do this, but it shouldn't be necessary. You should be able to set up the CGShading you want to draw with, obtain the CGContextRef from the current NSGraphicsContext and set up the shading into that context, then just use the [fill] method of the NSBezierPath.</body>
  </mail>
  <mail>
    <header>Re: Debugging CGContext</header>
    <date>Wed Aug 02 17:35:44 CEST 2006</date>
    <body>Cool! I'll play with this. Thanks, Mike</body>
  </mail>
  <mail>
    <header>Re: Debugging CGContext</header>
    <date>Wed Aug 02 17:28:43 CEST 2006</date>
    <body>On Aug 2, 2006, at 6:08 AM, Mike Kluev wrote: Assuming this is just for debugging purposes: otool -Tv /System/Library/Frameworks/ApplicationServices.framework/ Frameworks/CoreGraphics.framework/Versions/Current/CoreGraphics | grep _CGContextGet shows me some stuff I would consider playing with....but not for shippable code of course :) Ditto for otool -Tv /System/Library/Frameworks/ApplicationServices.framework/ Frameworks/CoreGraphics.framework/Versions/Current/CoreGraphics | grep State Cheers, Dave</body>
  </mail>
  <mail>
    <header>Re: Debugging CGContext</header>
    <date>Wed Aug 02 17:21:34 CEST 2006</date>
    <body>But how?! See below. Mike</body>
  </mail>
  <mail>
    <header>Re: Debugging CGContext</header>
    <date>Wed Aug 02 17:18:53 CEST 2006</date>
    <body>On Aug 2, 2006, at 8:09 AM, Mike Kluev wrote: None that I see other then writing a function of your own. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Debugging CGContext</header>
    <date>Wed Aug 02 17:09:25 CEST 2006</date>
    <body>rdar://4664063 Meanwhile, is there a workaround? Mike</body>
  </mail>
  <mail>
    <header>Re: Debugging CGContext</header>
    <date>Wed Aug 02 16:40:22 CEST 2006</date>
    <body>On Aug 2, 2006, at 2:31 AM, Mike Kluev wrote: File an enhancement request to have CGContext instances support CFShow with at least some level of debug information. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Debugging CGContext</header>
    <date>Wed Aug 02 15:08:56 CEST 2006</date>
    <body>Looks like I'll have to implement such debugging facility manually. How to get CG context's blend mode? Or alpha? Or rendering intent? Or any other setting that has CGContextSetXXX but doesn't have the corresponding CGContextGetXXX counterpart? For debugging purposes, is it possible to view the stack of saved context states? Mike</body>
  </mail>
  <mail>
    <header>Convert NSBezierPath to Quartz path</header>
    <date>Wed Aug 02 14:46:40 CEST 2006</date>
    <body>Hi, I am developing a application, where I need to convert NSBezierPath to Quartz path for filling the gradient color's. Any one can help me in this? Regards Vinay</body>
  </mail>
  <mail>
    <header>Re: &amp;lt;no subject&amp;gt;</header>
    <date>Sat Mar 31 23:17:13 CEST 2007</date>
    <body>I suspected as much. Thank you. -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: &amp;lt;no subject&amp;gt;</header>
    <date>Sat Mar 31 22:16:15 CEST 2007</date>
    <body>First of all, there are 2 different things. One typedef -- CGEventSourceStateID and another enum.  Type is unsigned, enum does not have to.</body>
  </mail>
  <mail>
    <header>&amp;lt;no subject&amp;gt;</header>
    <date>Sat Mar 31 22:06:22 CEST 2007</date>
    <body>Please forgive my ignorance of the finer points of C. Is this declaration in CGEventTypes.h legitimate? -- kCGEventSourceStatePrivate = -1, kCGEventSourceStateCombinedSessionState = 0, kCGEventSourceStateHIDSystemState = 1 It is typed as an unsigned int, but one of the values is -1. -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Rectangle, Rotating and Clipping</header>
    <date>Sat Mar 31 05:54:13 CEST 2007</date>
    <body>Hi There, First post - please be kind. :-) I program using REALbasic. I am writing classes that are declaring in to the CoreGraphic header files. For the most part things are moving along nicely. (The declares I write use the same names and prototypes as the header files.) Here is my problem. I draw a rectangle on a resizable window within a canvas. Collapsing my window left-right and up-down all is well, clipping is good within the boundaries of my canvas. Let me label the four edges of my rectangle one through four, clockwise. The horizontal top line is number one, right side vertical line is number two, etc. When I rotate my rectangle clockwise, say 45 degrees, then shrink my window from right to left, as soon as the window reaches the edge 1, edge 2 corner, edge 2 begins to shrink as if it is still unrotated. Can you give me some idea's how I might go about clipping this rectangle properly? Thank you. Thomas C.</body>
  </mail>
  <mail>
    <header>Re: How to embed a font in PDF</header>
    <date>Fri Mar 30 18:18:49 CEST 2007</date>
    <body>Drill down into the XObject.  When we place a source PDF page into an output PDF document, we put it in a form (a type of XObject), for various arcane technical reasons.  In the XObject dictionary, you should find a form containing the contents of your source page with all of the fonts. Derek</body>
  </mail>
  <mail>
    <header>Re: How to embed a font in PDF</header>
    <date>Fri Mar 30 18:15:11 CEST 2007</date>
    <body>SNIP So the font IS embedded in the file, but for some reason these Xerox printers do not see it when the PDF is sent directly to the printer or when printed from Acrobat 6. I sent the same file to a GMG rip which produced a perfect tiff. It also printed fine on a cheapo HP LaserJet 4250. But printing from Acrobat 6 and 7 (reader or full/pro version) ends up with the Xerox approximating the font. Acrobat reader 8 however printed the file perfectly on this Xerox. Is there anything I can switch around in my code to try producing a different PDF structure? I guess I'm tripping on bugs in the printer and these versions of Acrobat, but I'd like to see if they can be worked around. David.</body>
  </mail>
  <mail>
    <header>Re: How to embed a font in PDF</header>
    <date>Fri Mar 30 10:57:17 CEST 2007</date>
    <body>Hmm, being new to programming and not familiar with PDF structure means I am probably making an ass of myself and your help is really appreciated. So... Inspecting the original and one of my new files with Voyeur, the structure of the original is something like this (page 4 in this case): /Pages /Count /Kids 3 /Contents /Type /Parent /Resources /ColorSpace /Font /F1 /F2 /F3 /Xobject /ProcSet /ExtGState /Properties ... And so on for each page. For my new file, the structure is: /Pages /Count /Kids 0 /Contents /Type /Parent /Resources /ProcSet /XObject /MediaBox Printing the original file from Adobe Acrobat 6 on 10.4.8 to a Xerox postscript printer works as expected, but printing the new file causes the printer to substitute Courier for all the text. (And now I've discovered that printing the new file from Preview _does_ work... the font prints as expected.) So that confuses me. I've put an example of a new file here: How will the font data appear in the new file? David.</body>
  </mail>
  <mail>
    <header>Problems parsing pdf with Form XObjects	(CGPDFContentStreamGetResource fails)</header>
    <date>Thu Mar 29 22:37:20 CEST 2007</date>
    <body>I'm trying to parse some pdf files starting with the code from _Programming With Quartz_, but I'm having trouble with Form XObjects. If the XObject stream only has drawing commands, everything is fine. But if the XObject stream contains another Form XObject it doesn't work. On the recursive call to myOperator_Do, it finds the correct name for the secondary form but the line: Here's my modified code (most of the error checking removed for clarity): void myOperator_Do(CGPDFScannerRef s, void *info) CGPDFScannerPopName(s, &amp;amp;name) if(!xobject) CGPDFObjectGetValue(xobject, kCGPDFObjectTypeStream, &amp;amp;stream) CGPDFDictionaryGetName(dict, &amp;quot;Subtype&amp;quot;, &amp;amp;name) if(strcmp(name, &amp;quot;Form&amp;quot;) == 0) CGPDFContentStreamRef localContent = CGPDFScannerRef scanner = else // Images, etc. Form XObjects with Form XObjects are legal pdf and the file renders properly with a variety of programs. Any help much appreciated.</body>
  </mail>
  <mail>
    <header>Re: How to embed a font in PDF</header>
    <date>Thu Mar 29 19:22:51 CEST 2007</date>
    <body>This should happen automatically.  Are you sure that the original fonts are not in the new PDF files?  They will not appear identically, of course, because we create our own font subsets, but the glyph data (the outlines, in particular) should be identical.</body>
  </mail>
  <mail>
    <header>How to embed a font in PDF</header>
    <date>Thu Mar 29 12:49:35 CEST 2007</date>
    <body>Hello, My app reads accepts a PDF file and then writes individual PDF files for each page from the original, using the CGPDFDocument functions. This works well. My problem is that embedded fonts in the original file are not written to the new files. How can I embed fonts in my PDF files? I've looked through the CoreGraphics PDF references, and the newer PDF Kit reference and I don't see a way to write font resources back to a PDF file. There is the Voyeur sample application and the documentation for CGPDFObject, but these handle reading only (as far as I can tell). Pointers, suggestions much appreciated, David.</body>
  </mail>
  <mail>
    <header>'Above' tag does not work in Safari</header>
    <date>Thu Mar 29 02:38:35 CEST 2007</date>
    <body>Hello, I was excited to see the 'ABOVE' tag as part of Quartz embed since nothing can go on top of a quicktime embed. Setting it to True does indeed prevent any html content appearing above it. However setting it to false simply makes it disappear on the web page. Please note I am trying to make the tag in a widget, but need it to work inside of Safari. So this shows up blank in safari : The documentation is here below figure 12 : Any ideas ? Amazing technology this quartz.</body>
  </mail>
  <mail>
    <header>Saving jpg file with embedded thumb</header>
    <date>Tue Mar 27 00:45:36 CEST 2007</date>
    <body>I am processing an image using CIFilters. I can save the image with writeToFile, but the image is saved without a thumbnail. The second part of the code tries to save the image with an embedded thumbnail. The code copiles without error or warnings, but bails in GraphicsImportExportImageFile. I suspect the problem is that I am feeding the method a wrong presentation of the image. I have tried different approaches, but all fail at GraphicsImportExportImageFile. BTW, if GraphicsExportSetThumbnailEnabled is removed, the problem is still there. Thanks, James // crop image with vertical centre crop // more code here // convert CGImage to NSImage outputNSImage = [self CIImageToNSImage: outputCIImage usingRect: // convert into NSdata for saving imageProps = [NSDictionary dictionaryWithObject: [NSNumber numberWithFloat: [self jpgCompression]] forKey: imageData = [imageBitmapRep representationUsingType: NSJPEGFileType // Write to file. // code works to here - if writeToFile is enabled, it will save a JPG file without embedded thumb // I show the above code, to show how the image source file is created. // The code below aims to save the image as a jpg with an embedded thumb // Convert the picture handle into a PICT file (still in a handle) // by adding a 512-byte header to the start. err = OpenADefaultComponent( GraphicsImporterComponentType, kQTFileTypePicture, // create CFURL from path string CFURLRef cfURL = CFURLCreateWithFileSystemPath (NULL, (CFStringRef)pathToOutputFile, kCFURLHFSPathStyle, //Convert the CFURLRef to an FSRef //Convert the FSRef to an FSSpec err = FSGetCatalogInfo(&amp;amp;fsref, kFSCatInfoNone, NULL, NULL, &amp;amp;path, // specify thumbnail presence and size err =  GraphicsExportSetThumbnailEnabled (gi, TRUE,	// enable thumbnail 160.0,	// thumbnail width 120.0 );	// thumbnail height // save jpg with embedded thumb  // always bail here err = GraphicsImportExportImageFile(gi, kQTFileTypeJPEG, 0, &amp;amp;path, bail:</body>
  </mail>
  <mail>
    <header>Re: Weird CGDataConsumer error</header>
    <date>Mon Mar 26 21:50:35 CEST 2007</date>
    <body>That error repeats, but what errors, if any, show just before that one appears the first time?  Maybe they will give us some hints. ____________________________________________________________________________________ Finding fabulous fares is fun. Let Yahoo! FareChase search your favorite travel sites to find flight and hotel bargains.</body>
  </mail>
  <mail>
    <header>Weird CGDataConsumer error</header>
    <date>Mon Mar 26 19:09:59 CEST 2007</date>
    <body>I've got a user with an odd data consumer error. Note that all graphics code is written in Cocoa so this is happening underneath that. Anyway the error is (repeated a bunch of times):</body>
  </mail>
  <mail>
    <header>Re: How do Quartz event kCGEventSource... fields work?</header>
    <date>Sun Mar 25 20:54:43 CEST 2007</date>
    <body>Ah, that's very clear, and helpful as I move forward with this project. Thank you. -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: How do Quartz event kCGEventSource... fields work?</header>
    <date>Sun Mar 25 20:26:27 CEST 2007</date>
    <body>On Mar 25, 2007, at 11:10 AM, Bill Cheeseman wrote: An event originating with the hardware, or technically, with a device driver associated with hardware, will have a kCGEventSourceUnixProcessID of 0, and will have a kCGEventSourceUserID of root (UID 0) and kCGEventSourceGroupID  of wheel (GID 0).  There will normally be no kCGEventSourceUserData value, so that will return 0. An event produced by a user space process will  have kCGEventSourceUnixProcessID set to the process ID (a pid_t), and kCGEventSourceUserID and kCGEventSourceGroupID will reflect the process's user and group ID.  The process creating the event is free to set kCGEventSourceUserData to any value it likes.  This may be used, for example, for a program that acts as an event source to pass identifying or additional information along to a cooperating application or event tap process.</body>
  </mail>
  <mail>
    <header>How do Quartz event kCGEventSource... fields work?</header>
    <date>Sun Mar 25 20:10:12 CEST 2007</date>
    <body>In my test app, I get expected values from the CGEventGetIntegerValueField() function for the kCGEventTargetProcessSerialNumber and kCGEventTargetUnixProcessID event fields. But I always get 0 for the kCGEventSourceUnixProcessID, kCGEventSourceUserData, kCGEventSourceUserID and kCGEventSourceGroupID event fields. How do I receive meaningful information for these fields, and what is its significance (i.e., what is meant by &amp;quot;event source&amp;quot; in this context)? Or are these field selectors simply not implemented? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: problem drawing transparent fill to NSImage</header>
    <date>Fri Mar 23 23:42:20 CET 2007</date>
    <body>I've changed things a bit where I'm now creating an NSImage in place of directly calling NSRectFill(). Oddly, I have ended up having to do two different compositing operations when I draw the image depending on the situation. In one situation I'm drawing the NSImage in an overlay view (transparent window) where I have to use NSCompositeSourceIn in order to see the view underneath properly, and then in another situation where I'm compositing this same NSImage over another NSImage I have to use NSCompositeSourceOver in order to see underneath properly.</body>
  </mail>
  <mail>
    <header>Re: How do you display CMYKA data with Quartz?</header>
    <date>Fri Mar 23 22:31:10 CET 2007</date>
    <body>Scott Thompson: There is no logical connection between alpha blending and additive colour. the term refers to the coefficient in linear blending, which can be applied just as well to subtractive as additive colour -- although in either case you have to convert to a linear colour space for the results to be consistent. That few if any printers support alpha blending is no reason to ignore the possibility of alpha blending in CMYK, unless you have a vested interest in maintaining the status quo. -- Jens Ayton Sed quis custodiet ipsos custodes?</body>
  </mail>
  <mail>
    <header>Re: How do you display CMYKA data with Quartz?</header>
    <date>Fri Mar 23 19:23:15 CET 2007</date>
    <body>On Mar 23, 2007, at 11:33 AM, Nick Nallick wrote: FreeHand has transparency effects and the like which use clipping paths, color manipulation, etc... to achieve transparency effects with CMYK colors.  I'm not trying to say that transparency in a CMYK universe is not useful.  From that perspective, I am being overly pedantic when I say that the Alpha channel doesn't make much sense in CMYK.  But in practical applications, the effect of an alpha channel would have to be accomplished through some other means when the ink hits the paper :-) Scott</body>
  </mail>
  <mail>
    <header>Re: How do you display CMYKA data with Quartz?</header>
    <date>Fri Mar 23 18:00:15 CET 2007</date>
    <body>On Mar 23, 2007, at 10:33 AM, Nick Nallick wrote: Following up my own thought... On Tiger or later, you could achieve the same result as a CMYKA image by combining a CMYK image with a gray image channeled through CGContextClipToMask.  Essentially your alpha channel becomes a clipping mask image. Nick</body>
  </mail>
  <mail>
    <header>Re: How do you display CMYKA data with Quartz?</header>
    <date>Fri Mar 23 17:33:35 CET 2007</date>
    <body>On Mar 23, 2007, at 10:18 AM, Scott Thompson wrote: Clearly it wouldn't be that much use as a final output format but I can imagine using it during a compositing phase.  People do pretty much the same thing now with clipping paths and the like. Nick</body>
  </mail>
  <mail>
    <header>Re: How do you display CMYKA data with Quartz?</header>
    <date>Fri Mar 23 17:18:17 CET 2007</date>
    <body>On Mar 23, 2007, at 10:45 AM, Shawn Erickson wrote: CMYKA does not make much sense really anyway.  CMYK is a printing format... subtractive colors on ink.  Alpha deals with light transmission (additive colors) which really only makes sense on computer screens.   I suppose you could come up with a similar concept to Alpha for subtractive colors, but how are you really going to implement it? In print you might achieve transparency through transparent inks (which would probably be hard to control)  or as it's done today -- through screening.</body>
  </mail>
  <mail>
    <header>CGEventSourceCounterForEventType() issue</header>
    <date>Fri Mar 23 16:59:41 CET 2007</date>
    <body>According to the Quartz Event Services Reference document, this code should return the number of user input events of all types (keyboard, mouse, or tablet) since the window server started: CGEventSourceCounterForEventType(CGEventSourceGetSourceStateID(eventSourceR ef), kCGAnyInputEventType) But it always returns 0. If I specify a specific event, such as kCGKeyDown, I get a reasonable result. Is this a known bug? Or is the documentation wrong? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: QDPictRef to create PICTs?</header>
    <date>Wed Jan 31 22:42:57 CET 2007</date>
    <body>I know of a way which is simple and uses the (often underestimated) pasteBoard capabilities. I start with your -imageDataWithImporter: method: - (NSData *) imageDataWithImporter:(MovieImportComponent) inImporter NSData *imageData = [self TIFFRepresentation];  // use compression !!! // write the imageData to the pasteBoard // the pasteBoard now has 2 types: NSTIFFPboardType AND NSPICTPboardType // due to the docs for NSPictImageRep (see -PICTRepresentation) we create: pictData = [NSMutableData dataWithLength:512];  // 512 zero bytes [pictData append // for a test only (to see if it works): NSLog( @&amp;quot;class is %@\n&amp;quot;, [NSImageRep imageRepClassFor If you want to create a PICT image from an existing image file ( TIFF, jpg, PNG, .. ) you need not create TIFF data. This will do: (written in mail) - (NSData *) PICTDataFromFileAtPath:(NSString *)path [pb set forType:NSTIFFPboardType]; // works also for jpg, png, ... pictData = [NSMutableData dataWithLength:512];  // 512 zero bytes Heinrich -- Heinrich Giesen email@hidden</body>
  </mail>
  <mail>
    <header>Re: programatically exporting quartz composition to quicktime</header>
    <date>Wed Jan 31 15:00:24 CET 2007</date>
    <body>I'd recommend looking through the Archives of the QuickTime list. For example: Although on the QuickTime java list, outlines the what you need to do pretty well.  You'll have to make adjustments for the new mechanisms that let you avoid GWorlds, but by-in-large, I think the outline given in that link should work. IQ_InteractiveMovies/quicktimeandsmil/chapter_10_section_1.html to put together the slide show.  The down-side is that SMIL would require you to actually save the images to disk at some point.  The up-side is that you could write a simple XML description of the slide show in SMIL and let QuickTime do the heavy lifting of getting the media samples into a movie. Attachment:</body>
  </mail>
  <mail>
    <header>programatically exporting quartz composition to quicktime</header>
    <date>Wed Jan 31 09:13:37 CET 2007</date>
    <body>I'm working on a plug-in for Aperture that generates slideshows. I was able to use the Quartz Composer SlideShow example to figure out how to display the images that the user has selected full screen in the form of a slide show.  That works great. The next step is to be able to save what the QCRenderer and NSOpenGLContext are displaying on screen to disk in the form of a quicktime movie.  I've been looking for example code that demonstrates this but I haven't had much luck. I found this ADC article on how to turn a composition in to a quicktime movie QuartzComposer/qc_scr_qtmov/chapter_7_section_3.html but it focuses more on exercising functionality that Quartz Composer provides vs. going over what Quartz or Quicktime framework APIs can be invoked to do the same. that goes over how to render a Quartz composer composition offline and then save the frames to disk as separate compressed TIFF files. Seems to me my code needs to do the same but instead of saving the composition out as separate TIFF files, it needs to save it out as a .mov file instead. Apologies in the advance for the noob question - and I realize this may be more applicable to the quicktime mailing list - just thought I'd try here first.</body>
  </mail>
  <mail>
    <header>Re: QDPictRef to create PICTs?</header>
    <date>Wed Jan 31 01:38:01 CET 2007</date>
    <body>On Jan 30, 2007, at 7:06 PM, Wim Lewis wrote: Hmmm. ImageIO sounds promising, Thanks for the tip. Ken</body>
  </mail>
  <mail>
    <header>Re: QDPictRef to create PICTs?</header>
    <date>Wed Jan 31 01:06:49 CET 2007</date>
    <body>I think you can use the quicktime exporter API to get a PICT directly from a CGImageRef (along these lines: OpenADefaultComponent (GraphicsExporterComponentType, 'PICT', ...), GraphicsExportSetInputCGImage(), GraphicsExportSetOutputHandle(), GraphicsExportDoExport(), CloseComponent()). I'd expect that under 10.4+ you can use the ImageIO library (CGImageDestination*()) to produce a PICT by giving it the &amp;quot;com.apple.pict&amp;quot; UTI, but I haven't tried that. So in that case, you'd need to create a CGBitmapContext, draw into it (possibly wrapping it in an NSGraphicsContext if you want to use the AppKit-style APIs), then use CGBitmapContextCreateImage() to get the CGImageRef for export. Or possibly you can create the CGImage directly from the NSImage's bitmap data. I assume that calling lockFocus on an NSImage creates a CGBitmapContext under the hood.</body>
  </mail>
  <mail>
    <header>Re: QDPictRef to create PICTs?</header>
    <date>Wed Jan 31 00:34:46 CET 2007</date>
    <body>On Jan 30, 2007, at 6:05 PM, David Gelphman wrote: At runtime, the PICTs don't exist, I have to generate them from basic ad info such as width, height, client name, client number etc. Currently, I do the drawing into an NSImage using NSxxx draw commands , convert the image to a tiff and use a graphics converter to convert the tiff to a PICT. Is there a quicker way to convert an NSImage to a PICT than first generating a tiff? Unfortunately, when adding images to picture boxes, Quark 6.5 only accepts PictHandles. If I wanted to use, PDFs, I'd have to write them to a file and tell quark to import them. We're talking upwards of 50 ads or so for any one document and I suspect all that writing/ importing of PDFs would be far worse time-wise than my current method. P.S. I understand the need to modernize and all, but what would have been the harm in adding a method of NSImage that could convert to PICTs for those of us that still need them? Something like: Come to think of it, is there a hack I could use to just wrap a bitmap inside a PICT?</body>
  </mail>
  <mail>
    <header>Re: QDPictRef to create PICTs?</header>
    <date>Wed Jan 31 00:05:17 CET 2007</date>
    <body>One possibility is to use the CGImageSource APIs which can handle PICTs. When passed PICT data this API will create a CGImageRef that represents the bits for that picture. The reason the QDPict API is less performant is that it is converting the PICT data into Quartz line art, text, and image calls. You don't just get bits. The conversion of arbitrary pictures is time intensive and is performed on the fly each time you draw the QDPict. You can do this once and draw the QDPict into a Quartz PDF context to create PDF data that you can render much more quickly upon redraws than possible with a QDPict. Hope this helps, David</body>
  </mail>
  <mail>
    <header>Re: QDPictRef to create PICTs?</header>
    <date>Tue Jan 30 23:19:26 CET 2007</date>
    <body>On Jan 30, 2007, at 2:32 PM, Ken Tozier wrote: The short answer is no.  You can create a QDPictRef from a PicHandle, but you can't go back the other way.  Your best bet for performance is probably going to be to use QuickDraw. Nick</body>
  </mail>
  <mail>
    <header>QDPictRef to create PICTs?</header>
    <date>Tue Jan 30 22:32:59 CET 2007</date>
    <body>I'm writing an XTension for Quark 6.5 and one of the XTension's jobs is generating a placeholder image for ads. In an earlier incarnation of my XTension I used Quickdraw to generate the placeholders on the fly and used those PICTs directly. I found a kludgy way to generate picts in OS X but it's WAY slower than it was on OS 9. In OS 9, I could generate the ads for an entire publication and place them on a Quark document in about 3 seconds. With my kludge, it can take 15 to 20 seconds to just create a dozen or so placeholder images and generating the ads for an entire publication can easily take over a minute. Quark 6.5 only accepts pict handles when placing images directly in boxes so basically I'm wondering if there is some way to generate PICTs faster with QDPictRef? After reading the QDPictRef documentation I'm unclear on whether it's possible to create an image that, when returned, is a PICT. Below is my kludgy conversion method, a category on NSImage that uses a MovieImportComponent  to do the NSImage-&amp;gt; PICT conversion. Basically the workflow needs to be: Generate ad placeholder Convert to a PICT handle - (NSData *) imageDataWithImporter:(MovieImportComponent) inImporter NSData			*imageData		= nil, if (err == noErr) if (GraphicsImportGetAsPicture(inImporter, &amp;amp;picHandle) == noErr) result	= [[NSData alloc] initWithBytes: *picHandle length: else else if (picHandle != NULL) if (imageHandle != NULL)</body>
  </mail>
  <mail>
    <header>CoreImage filter package</header>
    <date>Mon Jan 29 23:11:50 CET 2007</date>
    <body>I have a standalone CoreImage filter, which takes a point (position) as one of its inputs. The filter works except that when I'm using Core Image Fun House, the description of the position parameter is Unfortunately it appears CoreImage is looking for a key that I haven't defined. On other filters, &amp;quot;(null)&amp;quot; is replaced with the name of the filter. I've defined CIAttributeFilterDisplayName in my Description.plist, but CoreImage doesn't appear to be looking for that in this case. Thanks, -andy</body>
  </mail>
  <mail>
    <header>Re: dynamically changing a color table</header>
    <date>Mon Jan 29 17:26:05 CET 2007</date>
    <body>On Jan 29, 2007, at 8:57 AM, Niels Bogaards wrote: A CGImageRef is an immutable object.  Quartz is allowed to optimize image handling based on the assumption this will never change. However it's not very expensive to make another image with the same backing data.  It means some extra overhead, but you can maintain your image data separately from your image and bitmap references and use that data repeatedly when you change color tables. Nick</body>
  </mail>
  <mail>
    <header>dynamically changing a color table</header>
    <date>Mon Jan 29 16:57:24 CET 2007</date>
    <body>I have an application that uses 8-bit bitmaps with a colortable. In QuickDraw, this colortable can be changed interactively, so I don't have to modify the bitmaps values to change their colors. I'm looking for a way to do this with Quartz, but it seems that the colortable can only be set upon creation of the bitmap or image. Am I missing something? As my app is in c++, it would be very inconvenient to have to resort to CoreImage to do this quite simple manipulation. Any ideas? Niels</body>
  </mail>
  <mail>
    <header>Re: Custom cursor icon</header>
    <date>Sat Jan 20 15:28:35 CET 2007</date>
    <body>On Jan 12, 2007, at 5:45 PM, Ricky Sharp wrote: The sample once again lives (just saw it in the what's new section of the developer main page).  Specific link: ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>video input record</header>
    <date>Fri Jan 19 15:53:30 CET 2007</date>
    <body>I'm having trouble searching the site so I hope i'm not going over a subject that's all ready been discussed, however my question is: Is it possible to set up a live video feed (i.e. the video input and billboard patch) that will also record or save direct to the harddrive when opened. I'm hoping to use the video input quicktime movie within macromedia, sorry, adobe director. Barret</body>
  </mail>
  <mail>
    <header>CoreImage gaussian blur</header>
    <date>Thu Jan 18 04:25:17 CET 2007</date>
    <body>I have different results on PPC and Intel with the CoreImage gaussian blur filter. The appended code works perfectly well on a PowerMac G5 with X800XT (10.4.8). However, on a MacPro with a 7300GT (10.4.8), on some images, the output image is &amp;quot;cropped&amp;quot;. I have a simple test case with a 1644x2544 image, and on the MacPro the output image is 1644x2050 only (actually I believe the missing 500 or so lines are alpha-masked). Is this a known problem? Is there a bug in the code below? I can email a testcase xcode project to anyone willing to help me. url   = [NSURL fileURLWithPath: [[NSBundle mainBundle] // Create a new NSBitmapImageRep. fondsImageRep = [[NSBitmapImageRep alloc] initWithBitmapDataPlanes:NULL pixelsWide:newSize.width pixelsHigh:newSize.height bitsPerSample:8 samplesPerPixel:4 hasAlpha:YES isPlanar:NO colorSpaceName:NSDeviceRGBColorSpace // Create an NSGraphicsContext that draws into the NSBitmapImageRep. NSGraphicsContext *nsContext = [NSGraphicsContext [[nsContext CIContext] drawImage:fondsImage atPoint:imageRect.origin</body>
  </mail>
  <mail>
    <header>Disable interpolation on a CIFilter</header>
    <date>Tue Jan 16 19:14:41 CET 2007</date>
    <body>I'm using the CIAffineTile and CICrop filters to tile a small pattern to a source image size, and then use CIMultiplyCompositing with the resulting image and a source image.  The problem is that the resulting tiled and cropped image has artifacts from the bilinear interpolation.  It seems that CISampler has a way to turn off the interpolation by using kCISamplerFilterNearest, but I cannot figure out how to use that with an existing CIFilter.  The filters create their own sampler, right?  However, it seems that it is possible, as Quartz Composer allows you to turn off interpolation on standard CIFilters in the &amp;quot;Settings&amp;quot; pane. Can anyone provide pointers to disabling bilinear interpolation on a CIFilter?</body>
  </mail>
  <mail>
    <header>Re: Runloop source for CGDisplayRegisterReconfigurationCallback?</header>
    <date>Tue Jan 16 05:48:51 CET 2007</date>
    <body>Hi Spencer, I'm pretty sure that's what's happening with this API, too. Unfortunately, I can't run a CFRunLoop in this thread, as it already has its own runloop.  (And as anyone who's ever tried to run two runloops in a single thread can tell you, it's always a mistake to try to run two runloops in a thread.) Anyway, I'll probably just spawn a separate thread whose sole purpose in life is to call CGDisplayRegisterReconfigurationCallback() and run a CFRunLoop waiting for the response. It's too bad that there are so many APIs that assume the thread that calls into them is running a CFRunLoop.  I wish every API that involved callbacks (Cocoa excepted, naturally) would provide a CFRunLoopSourceRef or CFMachPortRef. Yeah.  That's my main problem with this and other similar asynchronous APIs that Apple provides; they really mean &amp;quot;the thread that calls this API, assuming that thread also uses a CFRunLoop&amp;quot;. Ben</body>
  </mail>
  <mail>
    <header>Re: Runloop source for CGDisplayRegisterReconfigurationCallback?</header>
    <date>Tue Jan 16 03:00:37 CET 2007</date>
    <body>Hi Ben, This is mostly a shot in the dark, but Ive noticed that in one of Apple's other C APIs (IOBluetooth specifically), if the API didn't supply a CFRunLoopSourceRef or CFMachPortRef to work with, the callback would be called if I let the run loop run in the kCFRunLoopDefaultMode mode in the same thread.  So, in other words, the source associated with the callback was added to the run loop of the current thread in kCFRunLoopDefaultMode.  So it might be worth testing if this is the case for CGDisplayRegisterReconfigurationCallback().  If this is in fact the case, it would be sufficient to call CFRunLoopRun() in whichever thread registered the callback to have your callback called when appropriate. The header declaring that function states: &amp;quot;Callbacks are invoked when the app is listening for events, on the event processing thread, or from within the display reconfiguration function when in the program that is driving the reconfiguration.&amp;quot; (CGDisplayConfiguration.h) Unfortunately &amp;quot;event processing thread&amp;quot; isn't a well-defined notion in Mac OS X development, so far as I am aware. In general I agree that it would be nice to just get a CFRunLoopSourceRef or CFMachPortRef.  s hope this helps, spencer</body>
  </mail>
  <mail>
    <header>Re: Runloop source for CGDisplayRegisterReconfigurationCallback?</header>
    <date>Tue Jan 16 01:54:45 CET 2007</date>
    <body>Hmm.  Well, in a Cocoa application with a standard AppKit/Core Foundation runloop on the thread from which I call CGDisplayRegisterReconfigurationCallback(), my callback works when reconfiguring a display. In a standalone process with no AppKit/Core Foundation runloop running on the thread from which I call CGDisplayRegisterReconfigurationCallback(), I don't get any callbacks when reconfiguring a display. The standalone process definitely has a connection to the window server, because I can call other Core Graphics calls like CGWarpMouseCursorPosition(). Ben</body>
  </mail>
  <mail>
    <header>entry level developer needed</header>
    <date>Tue Jan 16 01:34:32 CET 2007</date>
    <body>I'm looking for an entry level developer to write some Core Image and OpenGL imaging plug-ins in Objective-C. If you're reading this list and have experience in Objective-C, Core Image, and OpenGL and want to do some programming, please contact me. You should have some Objective-C and Cocoa development experience, understand 3D matrix transformations, know a bit of Open GL (i.e. do you know what glMatrixMode(GL_MODELVIEW) means?), how to build Core Image objects, and how to use Xcode. This is a great opportunity to work on a fun project for an entry level programmer. If interested, please send me some examples of projects or details of projects that you've done that would be relevant to the job. If you've taken a graphics programming course at a university, you probably have enough experience and knowledge!</body>
  </mail>
  <mail>
    <header>Re: Runloop source for CGDisplayRegisterReconfigurationCallback?</header>
    <date>Tue Jan 16 01:27:21 CET 2007</date>
    <body>That's right.  Sorry for not being clear.  I have one thread whose sole purpose in life is to wait on a set of Mach ports retrieved from CFRunLoopSourceRefs and CFMachPortRefs and dispatch the callbacks from those ports to our pre-existing runloop mechanism. It's possible that this particular Core Grapics API actually spawns a thread which dispatches the callback.  I'm reading the disassembly in CoreGraphics now to see what CGDisplayRegisterReconfigurationCallback() does, and it doesn't deal with CFRunLoops at all, so I'm going to do some more experimentation.  As far as I know, there's no concept of a &amp;quot;main&amp;quot; thread in Core Foundation or Quartz outside of the documentation.  There's definitely no API to add a Mach port or callback to a particular thread's runloop, only the current thread's runloop. Our application definitely doesn't have a &amp;quot;main&amp;quot; thread runloop that is compatible with Core Foundation, but our approach works great with other Core Foundation and I/O Kit APIs that give us Mach ports or CFRunLoopSourceRefs. Ben</body>
  </mail>
  <mail>
    <header>Re: Runloop source for CGDisplayRegisterReconfigurationCallback?</header>
    <date>Tue Jan 16 01:21:44 CET 2007</date>
    <body>Every thread can have a run-loop and it will be created automatically as needed... I guess you mean that the thread doesn't actually run the run-loop. I have found that these callbacks (or related &amp;quot;wait&amp;quot; methods) depend on the main thread running a run-loop and of course they require a connection with the window server. -Shawn</body>
  </mail>
  <mail>
    <header>Runloop source for CGDisplayRegisterReconfigurationCallback?</header>
    <date>Tue Jan 16 00:52:12 CET 2007</date>
    <body>I'm working on a cross-platform application that has its own runloop, to which I dispatch events via a CFRunLoopSourceRef bridge I've developed. In this application, I'd like to be notified whenever the local displays are reconfigured. I found CGDisplayRegisterReconfigurationCallback(), which unlike many other Core Foundation and I/O Kit functions, does not provide a CFRunLoopSourceRef which I can add to my own runloop. Since I have my own runloop, I can't just call CGDisplayRegisterReconfigurationCallback(),  as I assume it will fail if the thread that calls this function has no runloop. How can I get a CFRunLoopSourceRef or, even better, a Mach port on which I can receive display reconfiguration notification? Ben</body>
  </mail>
  <mail>
    <header>Re: Quartz and daemons</header>
    <date>Mon Jan 15 23:11:43 CET 2007</date>
    <body>If you need to call only into CoreGraphics then you should be fine, but if you need AppKit or Carbon windowing functionality, there is strong possibility they will try and attach to the window server and you will run into problems. So it depends on what graphics functionality you need from Quartz and what components of quartz you require.</body>
  </mail>
  <mail>
    <header>Re: Quartz and daemons</header>
    <date>Sat Jan 13 23:56:59 CET 2007</date>
    <body>On Jan 13, 2007, at 4:48 PM, Mike wrote: Aside from using a non-Quartz framework (such as those used with web- scripting languages), you can get around the daemon limitations by not using a daemon. By running a process as the currently logged in user, you will be able to connect to the window server, which will satisfy Quartz's needs. HTH, Jon -- Jonathan Johnson email@hidden REAL Software, Inc.</body>
  </mail>
  <mail>
    <header>Quartz and daemons</header>
    <date>Sat Jan 13 23:48:00 CET 2007</date>
    <body>So if one wants to write a faceless background app with no UI that uses Quartz, how does one go about doing that? The Daemons &amp;amp; Agents technote says that Quartz is not a daemon-friendly framework. I need to write an app that has no UI and runs silently and stealthily in the background without the user's knowledge. This app has to be able to perform some graphics functionality using Quartz. Is there an alternate way to do this without using a daemon to do so that will fullfill the above requirements? Mike</body>
  </mail>
  <mail>
    <header>Re: Custom cursor icon</header>
    <date>Sat Jan 13 01:06:27 CET 2007</date>
    <body>That's the correct name.  It was actually mistakenly posted the first time. I have to make some changes to the sample that were requested by DTS. I'm not sure who to give feedback on this, but I'd suggest writing up bugs for the others if there's no other feedback path.</body>
  </mail>
  <mail>
    <header>Re: Custom cursor icon</header>
    <date>Sat Jan 13 00:45:01 CET 2007</date>
    <body>On Jan 11, 2007, at 6:36 PM, Chilton Webb wrote: I think it's the CarbonCocoa_PictureCursor sample as referenced on: But alas, that sample appears to not exist anymore.  It was just posted in July, 2006, so it's not that old.  Some of the other sample links on that page are also broken. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: Custom cursor icon</header>
    <date>Fri Jan 12 01:36:02 CET 2007</date>
    <body>I've been looking for this exact thing. Which example is it? -Chilton</body>
  </mail>
  <mail>
    <header>Re: Custom cursor icon</header>
    <date>Fri Jan 12 01:18:19 CET 2007</date>
    <body>On Jan 11, 2007, at 3:35 PM, Luigi Castelli wrote: Quartz does not support custom cursors, but AppKit does, and we recommend the use of NSCursor. There's sample code on developer.apple.com showing how to use NSCursor from a Carbon app.</body>
  </mail>
  <mail>
    <header>Custom cursor icon</header>
    <date>Fri Jan 12 00:35:56 CET 2007</date>
    <body>Hi there, I am developing on MacOS 10.4 in Carbon. I am porting all my Quickdraw code to Quartz and I am stuck on a small problem. In Quickdraw I used to include a CURS resource in my project, draw pixel by pixel the cursor icon with a resource editor (i.e. resEdit) and I was able to switch to that custom cursor pretty easily. Is there anything like it available in Quartz ? I basically need to change the standard arrow cursor to some custom types. From the Quartz documentation on line I see that there are indeed some predefined types I can use with the SetThemeCursor() call. It works well but I really do need a custom icon in this case. Is there anyway to do it in Quartz ? Any solution or tip highly appreciated. Thank you. - Luigi ------------------------------------------------------------ THIS E-MAIL MESSAGE IS FOR THE SOLE USE OF THE INTENDED RECIPIENT AND MAY CONTAIN CONFIDENTIAL AND/OR PRIVILEGED INFORMATION.  ANY UNAUTHORIZED REVIEW, USE, DISCLOSURE OR DISTRIBUTION IS PROHIBITED.  IF YOU ARE NOT THE INTENDED RECIPIENT, CONTACT THE SENDER BY E-MAIL AT email@hidden AND DESTROY ALL COPIES OF THE ORIGINAL MESSAGE. WITHOUT PREJUDICE UCC 1-207. ------------------------------------------------------------ ____________________________________________________________________________________ Do you Yahoo!? Everyone is raving about the all-new Yahoo! Mail beta.</body>
  </mail>
  <mail>
    <header>Re: Control Transparency of CIImage in OpenGL Context</header>
    <date>Sun Jan 07 18:10:32 CET 2007</date>
    <body>Would be very interesting to see benchmarks comparing the two techniques. Anyone have the expertise required to test it?</body>
  </mail>
  <mail>
    <header>Re: Control Transparency of CIImage in OpenGL Context</header>
    <date>Sun Jan 07 17:05:58 CET 2007</date>
    <body>kernel vec4 makeTransparent(sampler inputImage, float inputAlpha) But I guess that's not really simpler than using the color matrix filter. Cheers, Martin Norrkross Software</body>
  </mail>
  <mail>
    <header>Re: Adding A Virtual Display</header>
    <date>Thu Jan 04 23:42:28 CET 2007</date>
    <body>I&amp;#39;m not sure if this is exactly a Quartz related problem, but Quartz seems to be the closest thing to it. I&amp;#39;m trying to figure out how to add a &amp;quot;virtual&amp;quot; display to a I mean that I want my software to be able to make the system think it has an external monitor and then process the video image destined for that monitor to do something useful.&amp;nbsp;&amp;nbsp;For example, I could encode the image and transmit it to a remote location for display there.</body>
  </mail>
  <mail>
    <header>Control Transparency of CIImage in OpenGL Context</header>
    <date>Wed Jan 03 22:17:58 CET 2007</date>
    <body>I've been struggling to find a simple way to control the overall transparency of a CIImage when rendering it to a OpenGL Context. I am using the CIContext drawImage function to draw the CIImage. I've tried messing around with OpenGL stuff (like using setting GL_TEXTURE_ENV_MODE to GL_BLEND and setting the GL_TEXTURE_ENV_COLOR) but to no avail. I suppose I can use a Color Matrix filter to adjust the overall alpha, but I was hoping for a simpler/faster way to do this. Obviously the &amp;quot;Billboard&amp;quot; module of Quartz Composer can handle this -- adding in rotations, colorization, and transparency. Is that because it is converting the CIImage to an Open GL texture? If so, what is the fastest way to handle that conversion. (There's been various suggestions posted here and on the net, but no definitive solution.) Thanks in advance, Mark -- ======================================================================= Mark Coniglio             |  email@hidden TroikaTronix              | Isadora®                  | |  (interactive media processing environment) =======================================================================</body>
  </mail>
  <mail>
    <header>Adding A Virtual Display</header>
    <date>Wed Jan 03 21:41:40 CET 2007</date>
    <body>I'm not sure if this is exactly a Quartz related problem, but Quartz seems to be the closest thing to it. I'm trying to figure out how to add a &amp;quot;virtual&amp;quot; display to a I mean that I want my software to be able to make the system think it has an external monitor and then process the video image destined for that monitor to do something useful.  For example, I could encode the image and transmit it to a remote location for display there. To me this implies I want to write a custom display driver, but I can't find any information on this, other than a little bit for writing a window server for IOKit in Darwin (but not Mac OS X).  I think that what I want to do is hook into the system under the Mac OS X window server to let it think I'm an external monitor.  Is there any documentation available publicly on writing display drivers? Apart from this approach I suppose I could write some variant of a frame grabber that would take a screen snapshot every few milliseconds and use that.  The problem with this is that I'd really prefer to have the option to either extend the desktop or mirror it. A screen grabber would seem to be only able to mirror an existing display. Can anybody suggest a good approach for this and/or point me at any documentation? Thanks, Nick Nallick</body>
  </mail>
  <mail>
    <header>Re: Custom image formats</header>
    <date>Wed Jan 03 14:34:55 CET 2007</date>
    <body>On 03.01.2007, at 09:51, Rene Rebe wrote: If the only use case is TIFF (and not GIF, PSD and PDF which also support multiple subimages), then not necessary. your app will likely already multiple times write and read the images - uncompressed due to VM swapping and finally once compressed. Stitching together an TIFF container with multiple subimages is fairly trivial.  I'm not sure too many implementations will be happy with &amp;gt; 2 GB TIFF images (32 bit tag offsets, potentially interpreted as &amp;quot;signed&amp;quot;...), though. Regards, Tom_E</body>
  </mail>
  <mail>
    <header>Re: Custom image formats</header>
    <date>Wed Jan 03 09:51:47 CET 2007</date>
    <body>Hi, Which has two major limitations: - bad performance (writing data twice) - might get out of disk space creating the final file while buffering many high res pages in the GB range Yours, -- René Rebe - ExactCODE GmbH - Europe, Germany, Berlin +49 (0)30 / 255 897 45</body>
  </mail>
  <mail>
    <header>Re: Custom image formats</header>
    <date>Wed Jan 03 09:20:33 CET 2007</date>
    <body>Well, a simple work around I can think of for this is to simply collect all the images before you ask ImageIO to write the file. Have you filed a bug about it? Can you send me the radar number? -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Custom image formats</header>
    <date>Wed Jan 03 08:23:25 CET 2007</date>
    <body>Hi, When we are at it, let me also once again express that I find it extremely limitating that the number of images in a image file needs to be specified at creation time. In nearly any use-case of mine I do not know that number in advance (e.g. the image come from a ADF copy machine, lab microscopes and the like) and I just want to fill a TIFF no matter if the user has placed 6 or 66 pages into the machine. I also tried to specify 0, -1 and 999999 as count in: CGImageDestinationCreateWithURL but neither combination worked out. -1 and 0 where NSLog'ed as invalid and with 999999 the class complained at: CGImageDestinationFinalize about missing images. I certainly would have expected such a limitation in an hobbyist open source project, but we are talking about what is suppost to be the next gen ImageIO on Mac OS X here ... That said we are back to our own in-house image io library due this (and other) limitations. Yours, -- René Rebe - ExactCODE GmbH - Europe, Germany, Berlin +49 (0)30 / 255 897 45</body>
  </mail>
  <mail>
    <header>Re: Custom image formats</header>
    <date>Wed Jan 03 00:02:22 CET 2007</date>
    <body>David is correct; this facility isn't available in ImageIO.  But your feedback is well understood and appreciated. In addition to your post here I'd recommend also filing a feature request so we can have the details on record. (&amp;amp; please send me the radar ID offline) Are you working around this by converting your format to one supported by ImageIO or going a different route altogether (Quicktime, etc)? Allan ___________ Allan Schaffer 2D &amp;amp; 3D Graphics Evangelist email@hidden</body>
  </mail>
  <mail>
    <header>Re: Variable Parameters in a Path</header>
    <date>Mon Jan 01 16:34:16 CET 2007</date>
    <body>On Dec 30, 2006, at 4:19 PM, Gordon Apple wrote: To vary the width over a stroked path you'd need to write your own stroke engine.  Quartz allows a subset of the graphics defined in PDF so the only way to vary color parameters within a single object is via the shader mechanism.  However I don't think that will work for what you're describing, so you'd have to create multiple objects. Nick</body>
  </mail>
  <mail>
    <header>Variable Parameters in a Path</header>
    <date>Sun Dec 31 00:19:44 CET 2006</date>
    <body>&amp;gt; Send Quartz-dev mailing list submissions to Has anyone found a way to vary the context state by element in a Bezier path?  What I am specifically looking for is a way to record (I can do that) and then draw (what I'm asking) paths of varying width, color, darkness, alpha, whatever, in response to pen stroke on a pressure sensitive tablet. It would be nice to have one path (possibly a subclass of NSBezierPath if done in Cocoa) that would allow capturing and drawing everything as a single path between mouse down and mouse up. A somewhat more difficult issue would be capturing time (or time differential) per point for replay rendering. Is drawing every segment separately the only way to do this to? -- Gordon Apple Ed4U Little Rock, AR email@hidden</body>
  </mail>
  <mail>
    <header>transforming/releasing/appending CGPDF...</header>
    <date>Fri Dec 01 01:27:01 CET 2006</date>
    <body>I am trying to emulate w/ Quartz what we used to do in CFM w/ QD &amp;amp; rsrc PICTs. Reading the docs, I understand the recommendation about gathering resources inside PDFs. So I went ahead and used Apple's examples and built a program that allowed us to convert thousands of rsrc graphic elements to multipage PDFs. So far so good. I can read/write to/fro bitmap/window/pdf contexts freely. This function's rotation arg is supposedly in int degrees, but it only seems to accept X = 90*N, N=0,1,2.. Otherwise, it crashes. Are we not allowed to give it an arbitrary value? If not, what are the alternatives? When accessing a PDF file, we end it with this. Before that, we seek a particular page (CGPDFPageRef) in that file. Exactly at what point are we supposed to free the CGPDFPageRef? Freeing it causes crash for some reason. 3) how do I _append_, _delete_ or _insert_ a CGPDFPageRef to/fro an existing PDF file? TIA, -Shin</body>
  </mail>
  <mail>
    <header>Extending Quartz</header>
    <date>Fri Dec 01 00:32:29 CET 2006</date>
    <body>I realise that I'm at the edge of what Quartz can do, and as the author of Lineform (), a drawing and illustration app, have different requirements to most people. But I still want to push Quartz further. Specifically I know Apple's rendering engine can at least cope with various features of PDFs  that I can't access. Soft masks (not the bitmap kind), although their rendering seems slow. PDF gradient types 4-6 Shadings with better control, specifically, not the current PDF type 0 function with 8 samples, but a type 3 stitched function of type 2 linear functions I can generate PDF data myself, but I don't want to replace my entire quartz backend with a bespoke system. It'd be great to see these features in Quartz, but I'm not holding my breath. What would be nice is to have some way to pass my own PDF data through Quartz untouched. Quartz parses then regenerates any included PDF data (usually rasterising anything it doesn't like) at the moment. I realise this is fairly unlikely, but if anyone knows of any way I can augment Quartz, or has any other suggestions. Let me know. regards -- will</body>
  </mail>
  <mail>
    <header>Re: Converting Postscript File to PDF</header>
    <date>Wed Nov 29 03:17:48 CET 2006</date>
    <body>You're right, thanks. That solved my problem. But brought back an old problem. I forgot why I chose NOT to use CGPDFPageGetDrawingTransform until now. CGPDFPageGetDrawingTransform has a bug (others have called it a feature) where it will scale the page DOWN, but it will not scale it up. So if I want to draw the page larger than 100%, it does not enlarge the page. I can't think of why it would be made to behave this way. So I ended up doing the transformations on my own. I didn't include a rotation transformation since I didn't plan on rotating the page. I was unaware that there is a /rotate value in a PDF page (as Dave Gelphman mentioned in another message) -- email@hidden</body>
  </mail>
  <mail>
    <header>Event types 19 and 20?</header>
    <date>Tue Apr 03 14:22:25 CEST 2007</date>
    <body>When I set an event tap to monitor all events, I see events with types 19 and 20 from time to time (I'm monitoring the Finder).  I can't find these defined anywhere. Can somebody point me to a header file? The closest I can come is IOLLEvent.h, which says immediately after NX_KITDEFINED (15), &amp;quot;There are additional DPS client defined events past this point.&amp;quot; I assume &amp;quot;DPS&amp;quot; is Display PostScript. This file dates back to 1988 (!!!) with a 1991 note referring to a dpsclient/event.h file, which I can't find. Is this note obsolete? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: Rectangle, Rotating and Clipping</header>
    <date>Tue Apr 03 06:59:43 CEST 2007</date>
    <body>Am 03.04.2007 um 05:43 Uhr schrieb Thomas Cunningham: I think you should rephrase your question. Maybe it's just me, but it isn't at all clear 1. what you are trying to do 2. what you expect to happen vs. what really happens. And you should also post the code in question. Otherwise I don't think anyone will be able to help. You may want to read: Andreas</body>
  </mail>
  <mail>
    <header>Re: Rectangle, Rotating and Clipping</header>
    <date>Tue Apr 03 05:43:43 CEST 2007</date>
    <body>So anyone have any suggestions on where I might look for errors in my code so solve this clipping problem? Thank you. -- Thomas C.</body>
  </mail>
  <mail>
    <header>Re: Core Image Fun House badly broken?</header>
    <date>Tue Apr 03 05:13:19 CEST 2007</date>
    <body>On Apr 2, 2007, at 9:39 PM, Ricky Sharp wrote: The error you're seeing most likely indicates that, for some reason, the application couldn't create a CISampler from the image you provided.  Perhaps a different image might work? Brendan Younger</body>
  </mail>
  <mail>
    <header>Re: Core Image Fun House badly broken?</header>
    <date>Tue Apr 03 04:39:29 CEST 2007</date>
    <body>On Apr 2, 2007, at 9:23 PM, Andrea XFox Govoni wrote: Works fine for me.  Dual 2GHz G5, 10.4.9.  Clean build of FunHouse with Xcode 2.4.1.  Graphics card is ATI 9800 Pro. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Core Image Fun House badly broken?</header>
    <date>Tue Apr 03 04:23:46 CEST 2007</date>
    <body>Is Core Image Fun House broken? I have a PowerPC Core Image enabled Mac with Mac OS X 10.4.9 and Xcode Tools 2.4.1 installed and when I try to apply an Image Unit nothing happens and I get errors in console.log like this: 2007-04-01 04:08:50.667 Core Image Fun House[458] CIColorInvert: negative: nil value for argument #0 (src) I got discordant feedbacks about the issue (someone says it works, someone says it doesn't) and I'd like some more confirms prior to submit a bug report. -- Andrea &amp;quot;XFox&amp;quot; Govoni AIM/iChat/ICQ: email@hidden Yahoo! ID: xfox82 Skype Name: draykan PGP KeyID: 0x212E69C1 Fingerprint: FBE1 CA7D 34BE 4A53 9639  5C36 B7A0 605F 212E 69C1</body>
  </mail>
  <mail>
    <header>Re: Meaning of kCGTabletEventTangentialPressure?</header>
    <date>Mon Apr 02 21:54:17 CEST 2007</date>
    <body>That's very helpful information. Thanks for the reference. -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: Meaning of kCGTabletEventTangentialPressure?</header>
    <date>Mon Apr 02 21:18:01 CEST 2007</date>
    <body>On Apr 2, 2007, at 09:23 , Bill Cheeseman wrote: Tangential pressure is usually the value that is controlled by the scroll wheel on an airbrush. NxtGenImpGuideX.pdf Cheers, Dave</body>
  </mail>
  <mail>
    <header>Re: Meaning of kCGTabletEventTangentialPressure?</header>
    <date>Mon Apr 02 18:23:32 CEST 2007</date>
    <body>In the Core Graphics API, there are two separate pressure fields for tablet pointers, 'pressure' and 'tangential pressure'. I believe you're talking about the first. I'm asking about the second. Bill</body>
  </mail>
  <mail>
    <header>Re: Meaning of kCGTabletEventTangentialPressure?</header>
    <date>Mon Apr 02 16:18:52 CEST 2007</date>
    <body>On Apr 1, 2007, at 6:51 AM, Bill Cheeseman wrote: Some tablets have the ability to detect how much pressure the artist is putting on the stylus.  This field gives you some idea of how much pressure that is.  The meaning of that field is largely up to interpretation by the application.  Bitmap programs like photoshop might use it to set the aperture of an airbrush tool.  Applications like FreeHand and Illustrator might use it to set the line width of a hand-drawn line-art element. I assume that the value runs from 0 to 1 because the artist can change the pressure sensitivity of the stylus to suit their drawing needs.  That means 0 to 1 is &amp;quot;relative to whatever the artist feels is maximum and minimum pressure&amp;quot;. Scott</body>
  </mail>
  <mail>
    <header>Color space equivalent to	CGColorSpaceCreateWithName(kCGColorSpaceGenericRGB)</header>
    <date>Mon Apr 02 03:29:49 CEST 2007</date>
    <body>Hi all, I'm trying to create a color space that matches CGColorSpaceCreateWithName(kCGColorSpaceGenericRGB). I tried this with CGColorSpaceCreateCalibratedRGB using values from / System/Library/ColorSync/Profiles/Generic RGB Profile.icc but the color spaces do not match (I can see this when drawing). Does kCGColorSpaceGenericRGB represent another color space, how can I find out it's values, or am I maybe setting the values wrong: cs18 = CGColorSpaceCreateCalibratedRGB( Thanks, Mark</body>
  </mail>
  <mail>
    <header>Meaning of kCGTabletEventTangentialPressure?</header>
    <date>Sun Apr 01 13:51:36 CEST 2007</date>
    <body>The Quartz Event Services Reference says that the kCGTabletEventTangentialPressure field has a value of 0 for no pressure and 1 for maximum pressure. The Cocoa NSEvent header says the Cocoa equivalent has a range of -1 to 1. I'm not sure what -1 means in this context, but presumably something to do with linear direction. What is the range and meaning of the Quartz event field? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Meaning of kCGTabletEventTiltX and Y?</header>
    <date>Sun Apr 01 13:48:24 CEST 2007</date>
    <body>The Quartz Event Services Reference says that kCGTabletEventTiltX and Y have a value of 0 for no tilt and 1 for maximum tilt. The corresponding Cocoa NSEvent header says the range is -1 (which I take to be left or top tilt) to 1 (right or bottom tilt). Do the Quartz event values have the same range and meaning as the Cocoa values? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Meaning of kCGTabletEventRotation?</header>
    <date>Sun Apr 01 13:44:52 CEST 2007</date>
    <body>The Quartz Event Services Reference and the CGEventTypes.h header say only that the kCGTabletEventRotation field is a double &amp;quot;encoding the tablet pen What is the range of valid values, and what do they mean? Or is this hardware-dependent? (I don't have a suitable tablet to test.) according to the NSEvent.h header. Is Cocoa just doing extra calculation, or is this what Quartz events do, too? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: Setting deltaX</header>
    <date>Sun Apr 01 13:11:48 CEST 2007</date>
    <body>Trying to think this through, I'm guessing that if I specify the event as a mouse moved or mouse dragged event, then setting the one value will result in the other being calculated automatically, with the item set last being the one that governs. Is this correct? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Setting deltaX</header>
    <date>Sun Apr 01 12:53:02 CEST 2007</date>
    <body>When creating events, what are my responsibilities with regard to fields like kCGMouseEventDeltaX? -- 1. If I call the CGEventSetLocation() function to set the location of a mouse event, does the system automatically calculate the field values returned by the CGEventGetIntegerValueField for kCGMouseEventDeltaX and the like? 2. If I call the CGEventSetIntegerValueField() function to set the kCGMouseEventDeltaX field value, does the system automatically calculate the value returned by the CGEventGetLocation() function? Or is the idea that it is entirely the developer's responsibility to ensure that manufactured events are internally consistent, and consistent with previous events? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: CoreImage leak when drawing offscreen</header>
    <date>Thu Sep 28 22:52:08 CEST 2006</date>
    <body>I created a CIImage using CGImageSource, applied CIFilters, and then rendered back out again using ImageIO.  The work-around I found was to switch from using CGDataProviderCreateWithData to CGDataProviderCreateWithCFData when creating the provider to use for the CGImageSourceRef. I was, of course, providing the correct sort of &amp;quot;free&amp;quot; callback in the original, leaky implementation. This was being used in a batch image processor and would eventually result in a kernel panic, I assume due to memory being used up and not being able to be paged out. On Sep 28, 2006, at 10:50 AM, Will Thimbleby wrote: Attachment:</body>
  </mail>
  <mail>
    <header>Re: CoreImage leak when drawing offscreen</header>
    <date>Thu Sep 28 19:50:47 CEST 2006</date>
    <body>Yes, it still leaks even if the graphics context is the current context. The only solution I've found is to use the screen's context, I guess this is because it is the only method that does not create a temporary OpenGL context. There might be some room for working with CGL contexts you create yourself overlapping in memory with the CG context, but I haven't looked at it. -- Will</body>
  </mail>
  <mail>
    <header>Re: Inverse clipping</header>
    <date>Wed Sep 27 17:21:17 CEST 2006</date>
    <body>On 27 Sep 2006, at 15:07, Hemant Balakrishnan wrote: There is one in Tiger: CGContextReplacePathWithStrokedPath. For earlier systems you'll have to roll your own.</body>
  </mail>
  <mail>
    <header>Inverse clipping</header>
    <date>Wed Sep 27 16:07:21 CEST 2006</date>
    <body>I need to draw something like a picture frame. Which has the frame part and a transparent picture part. As inputs I have two points a &amp;amp; b and two widths w &amp;amp; w' (where w &amp;gt; w') . Since a picture is worth a thousand words let me use the following image to explain this further. 6aa7re2&amp;amp;.dnm=f328re2.jpg&amp;amp;.src=ph To draw my frame I first construct a ::path (line) of width w between the two points a &amp;amp; b (refer to (i) in the image) and ::stroke it black. This gives me a black rectangle as shown in (ii). Now I compute the points c &amp;amp; d (this is trivial with the info we have as input) and construct another ::path (line) of width w' between the points c &amp;amp; d. After this I want to &amp;quot;inverse clip&amp;quot; this second ::path from my ::context to get the frame, as shown in (iii). I'm in a position where I cannot use rectangles to draw this. Can this be done. If there is a different approach I could take, please do advice. FInally I want to be able to draw a (open) path through a series of lines and obtain something as shown below. ad13re2&amp;amp;.dnm=ec5ere2.jpg&amp;amp;.src=ph If there were a function to ::fill and ::stroke a ::path (not the polygon formed by closing it) it would make things much easier.</body>
  </mail>
  <mail>
    <header>Re: CoreImage leak when drawing offscreen</header>
    <date>Wed Sep 27 15:40:37 CEST 2006</date>
    <body>Very interesting... this is similar to a leak I stumbled across earlier this month: Again, it was CGLContextObject that was leaking.  My solution was to use a different graphics context with my Core Video texture cache. It almost seems that Core Image is leaking if the current OpenGL context is not the one drawImage: wants to use.  It seems to create a temporary OpenGL context, and then doesn't clean up after it, or something. Could you see if your code still leaks when you make the graphics context current, before using it: NSGraphicsContext *nsgc = [NSGraphicsContext</body>
  </mail>
  <mail>
    <header>Re: CoreImage leak when drawing offscreen</header>
    <date>Wed Sep 27 15:15:38 CEST 2006</date>
    <body>For posterity here is a work-around. CoreImage appears to leak when using a CIContext that is not the screen. Thus code like this leaks memory: NSGraphicsContext *nsgc = [NSGraphicsContext However if you use the screen context (not the bitmap context) to render to a CGImage first, then there are no memory leaks: NSGraphicsContext *nsgc = [NSGraphicsContext Yes this is slower but it works, and it doesn't leak memory like a sieve. thanks - Will</body>
  </mail>
  <mail>
    <header>Scaling PDF data to a CIIMage</header>
    <date>Wed Sep 27 14:06:28 CEST 2006</date>
    <body>is there any simple way to scale a PDF output to a CIIMage? I get PDF dafa out of a NSTextView, pass it through a NSImage in order to get iin in my CIIMage at the end. Now the question is how do I scale the data returned by dataWithPDFInsideRect:bounds ? I want an uniform scale of the result, not modify the content of the textview. And of course I don't want to use an NSAffineTransform, I want high quality text. Here's what I do: any better idea to get a CIIMage* from the output of textView? And what is the correct way to scale the PDF output? ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>maximum radius</header>
    <date>Wed Sep 27 00:10:26 CEST 2006</date>
    <body>font-family:Arial'&gt;CGContextAddArcToPoint takes a radius parameter.&amp;nbsp; I am see that some large radius values cause the path to jump off to far away places instead of going straight. &amp;nbsp;I was wondering where the limit was on this radius. font-family:Arial'&gt; font-family:Arial'&gt;2.3e+7 seems to be normal font-family:Arial'&gt;2.3e+8 starts to form a small bulge protrusion font-family:Arial'&gt;2.3+11 shoots the path segment off to never land. font-family:Arial'&gt; font-family:Arial'&gt;We get these radius values when the curve is nearly straight.</body>
  </mail>
  <mail>
    <header>Re: CoreImage leak when drawing offscreen</header>
    <date>Tue Sep 26 19:22:56 CEST 2006</date>
    <body>Unfortunately no particular objects are leaking. I have a test app that just draws a CIImage to a CGBitmapContext then to the screen. Just by causing it to redraw in live-resize I can get it to eat several MB per second. I've eliminated most other possibilities, to get a leak all I need to do is draw a CIImage to some offscreen bitmap context. It seems to be CGLCreateContext that is leaking. Using OmniObjectMeter I get a lot of anonymous byte leaks like this: 0x900083b6 in _calloc 0x15a9aca6 in _gldGetString 0x15a304b0 in _gldGetString 0x15a8bc34 in _gldGetString 0x15a11674 in _gldGetQueryInfo 0x159ec497 in _gldCreateContext 0x025b4b4e in _gliCreateContext 0x932f66ed in _cglInitializeContext 0x932f57d1 in _CGLCreateContext 0x9402c685 in _allocate_context 0x9402c598 in _fe_cgl_create_context 0x94002a0b in _create_context 0x93ffd0d6 in _fe_accel_new 0x93ffd03e in _fe_context_accel 0x93ffcf20 in _fe_context_gl_new 0x94031a3b in -[CICGContextImpl _feContext] 0x9401ccbe in _provider_ensure_data 0x940315dd in _provider_get_byte_pointer 0x90336156 in _CGAccessSessionGetBytePointer 0x903350a6 in _img_data_lock 0x903330bd in _CGSImageDataLockWithReference 0x943f3534 in _ripc_AcquireImage 0x943f12cb in _ripc_DrawImage 0x90331773 in _CGContextDrawImage 0x93fe6d39 in -[CICGContextImpl render:] 0x94014bf7 in -[CIContext drawImage:inRect:fromRect:] or like this 0x900083b6 in _calloc 0x15a9aca6 in _gldGetString 0x15a304b0 in _gldGetString 0x15a8bc34 in _gldGetString 0x15a11674 in _gldGetQueryInfo 0x159ec497 in _gldCreateContext 0x02585b4e in _gliCreateContext 0x932f66ed in _cglInitializeContext 0x932f57d1 in _CGLCreateContext 0x9402c685 in _allocate_context 0x9402c598 in _fe_cgl_create_context 0x94002a0b in _create_context 0x94002935 in _fe_accel_context_ 0x9400281d in _fe_accel_context 0x94071d16 in _fe_gl_current_context 0x94004479 in _fe_gl_bitmap_collect 0x940156f6 in _fe_gl_purge 0x94015547 in _fe_context_purge 0x94015909 in _context_finalize 0x93ff1ab3 in _fe_release 0x93fea673 in -[CICGContextImpl dealloc] 0x94015464 in -[CIContext invalidate] 0x940153fa in -[CIContext dealloc] thanks -- Will</body>
  </mail>
  <mail>
    <header>Re: CoreImage leak when drawing offscreen</header>
    <date>Tue Sep 26 16:23:04 CEST 2006</date>
    <body>On Sep 26, 2006, at 6:48 AM, Will Thimbleby wrote: How have you determined that something is leaking... (i.e. what method are you using)? Scott</body>
  </mail>
  <mail>
    <header>CoreImage leak when drawing offscreen</header>
    <date>Tue Sep 26 13:48:09 CEST 2006</date>
    <body>I need to render a CIImage into a bitmap, and everything I've tried leaks memory. Does anyone know of any way to render a CIImage offscreen without leaking memory? Anything would do. These are the methods I have tried so far (all of them leaking on my MacBook Pro) Using CIContext's createCGImage method: CGContextDrawImage([[NSGraphicsContext currentContext] Using CIContext's drawImage method to draw dirextly into a CGBitmapContext NSGraphicsContext *nsgc = [NSGraphicsContext Any ideas would be much appreciated. thanks -- Will</body>
  </mail>
  <mail>
    <header>Re: Getting data out of an image.</header>
    <date>Mon Sep 25 17:31:44 CEST 2006</date>
    <body>For float pixel processing you need to read back the pixels from the GPU using glReadPixels or texture readback. Hopefully we have some sample for that soon.</body>
  </mail>
  <mail>
    <header>Re: Getting data out of an image.</header>
    <date>Mon Sep 25 17:15:43 CEST 2006</date>
    <body>Thanks for the info.  I'll try my code with 16 bit data and see how it goes. I posted this problem to Apple's dev support group and they claim that you can get float data if you base your CIContext on an OpenGL context.  I haven't been able to try that yet so i don't know. I personally think it's a bug in the code.  The API allows you to specify float pixel formats but the final rendering isn't in floats. fishcamp engineering 105 W. Clark Ave. Orcutt, CA  93455 TEL: 805-937-6365 FAX: 805-937-6252</body>
  </mail>
  <mail>
    <header>Re: Getting data out of an image.</header>
    <date>Mon Sep 25 06:41:37 CEST 2006</date>
    <body>There is, AFAIK, no public way of obtaining something else than 8 bit output out of Core Image right now. However, the non-public method &amp;quot;createCGImage:fromRect:format:&amp;quot; in CIContext enables you to obtain at least a 16-bits image by passing kCIFormatRGBA16 as the format (I tried). I suppose you could get your floats out by passing kCIFormatRGBAf, but I didn't try. BTW the reason why you get your input image back when you set the brightness to 0 is that Core Image ignores filters that are configured in such a way that they have no effect on the incoming image (search for CIAttributeIdentity in the documentation if you're interested).</body>
  </mail>
  <mail>
    <header>Re: Getting data out of an image.</header>
    <date>Sun Sep 24 04:20:01 CEST 2006</date>
    <body>Le 24 sept. 06 à 01:39, bob piatek a écrit : ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Is this a RADAR was Re: rect from ATSUGetGlyphBounds no aligned	with text, am I missing something?</header>
    <date>Sat Sep 23 00:19:35 CEST 2006</date>
    <body>Hello, So in response to this, I did some testing here is what I've come up with. I used the sample code from Quartz2DBasics, and replaced &amp;quot;drawStrokedAndFilledRects()&amp;quot; with this: //test // style first // get a font and set the font ID GetThemeFont(kThemeSystemFont, smSystemScript, fontName, NULL, status = ATSUFindFontFromName( &amp;amp;fontName[1], StrLength(fontName), kFontFullName, kFontMacintoshPlatform, kFontNoScriptCode, // set size // set color // layout // set the context //set the text #define kTestStringLengthinUnicode 24 ATSUSetTextPointerLocation(layout, theText, kATSUFromTextBeginning, // copy to layout 2 to show both unangled and angled issues // set angle // set the run style ATSUSetRunStyle(angledlayout, style, kATSUFromTextBeginning, ATSUGetGlyphBounds(layout, FloatToFixed(40), FloatToFixed(100), kATSUFromTextBeginning, kATSUToTextEnd, kATSUseDeviceOrigins, 1, CGContextMoveToPoint(context,FixedToFloat CGContextAddLineToPoint(context,FixedToFloat CGContextAddLineToPoint(context,FixedToFloat CGContextAddLineToPoint(context,FixedToFloat CGContextAddLineToPoint(context,FixedToFloat ATSUGetGlyphBounds(angledlayout, FloatToFixed(200), FloatToFixed (100), kATSUFromTextBeginning, kATSUToTextEnd, kATSUseDeviceOrigins, CGContextMoveToPoint(context,FixedToFloat CGContextAddLineToPoint(context,FixedToFloat CGContextAddLineToPoint(context,FixedToFloat CGContextAddLineToPoint(context,FixedToFloat CGContextAddLineToPoint(context,FixedToFloat ATSUDrawText(layout, kATSUFromTextBeginning, kATSUToTextEnd, ATSUDrawText(angledlayout, kATSUFromTextBeginning, kATSUToTextEnd, In doing so I get what I describe. The bounding box is rotated in the opposite direction as the text, and in a non-rotated state the top of the bounding rect is centered on the text. Cheers Rob</body>
  </mail>
  <mail>
    <header>Re: Getting data out of an image.</header>
    <date>Sat Sep 23 00:12:16 CEST 2006</date>
    <body>You can check out the CIAnnotation sample from the Apple Developer website. It shows how to write out a CIImage through ImageIO</body>
  </mail>
  <mail>
    <header>Getting data out of an image.</header>
    <date>Fri Sep 22 22:34:07 CEST 2006</date>
    <body>I'm writing a network server that takes images and runs them through a stack of core image filters for an image processing system we are developing. I'm a beginner at this so excuse the simple question. It's easy to read in the data from an network, place it into an NSData object and then get call CIImage +initWithData to build the image which I can then feed it through my image pipeline. What I want to do is take the output of the process, which is a CIImage, back into a tiff or jpeg or whatever. It seems to be fairly complicated. does anyone have a simple example or explanation of the process? I'm trying to make heads or tails of ImageIO at this point.</body>
  </mail>
  <mail>
    <header>Boxwork: has several Mac Software engineer positions in NYC</header>
    <date>Fri Sep 22 21:00:23 CEST 2006</date>
    <body>We are currently interviewing for the following job openings. Please send resume’s to: email@hidden We are looking for a Software Engineer with OpenGL experience that can implement creative ideas. Join a team of Software engineers that design commercial software in the field of digital photography, designing the human interface of the software with OpenGL on Mac OSX. Job Qualifications include: * OpenGL * Human Interface design * C/C++ * Object-Oriented design * Interest in image processing * Good communication skills (reporting/documentation) Good but not required: * Mac OSX API’s * Cocoa Client Server Software Engineer (full time position) This job opening is more specific to develop the next generation of our database client for our Image Assessment Management software solution. Job Qualifications include: * Cocoa/Objective-C * Multi-threading in Cocoa * MySQL * Java (to understand the older version of the software) * Good communication skills (reporting/documentation) Software Engineer (full time position) This job opening is more specific to assist in the different software’s that are being developed at Boxwork. Job Qualifications include: * Cocoa/Objective-C * C/C++ * Object-Oriented Design * Good communication skills (reporting/documentation) * Able to develop specific parts of the software by following strict guidelines Mathematician in the field of Image Processing (part time position)</body>
  </mail>
  <mail>
    <header>Re: rect from ATSUGetGlyphBounds no aligned with text,	am I missing something?</header>
    <date>Fri Sep 22 20:54:34 CEST 2006</date>
    <body>On Sep 21, 2006, at 5:53 PM, Robert Kukuchka wrote: It sounds like the text matrix that you use to measure your text is flipped (on the y axis) relative to your drawing context. Try shifting the origin to the opposite side of your drawing context and then reflecting the y axis.</body>
  </mail>
  <mail>
    <header>CIKernel Particle Dynamics</header>
    <date>Fri Sep 22 03:08:04 CEST 2006</date>
    <body>I'm using CoreImage to do a particle dynamics simulation on the GPU rather than the CPU.  This gives a very large performance gain for the computation portion of the simulation. The images I'm using are not bitmap data - each pixel is a float[4] that contains state information for the particle at the given pixel location, most specifically, the velocity of the particle and the mass of the particle. Once during each timestep, I need to move each particle (located at a certain pixel location in my 2D &amp;quot;bitmap&amp;quot; of particles).  The new position of the particle is oldPosition + velocity * timestep.  Mass at the new location is summed - multiple particles might contribute to a given destCoord during a given timestep. The problem is that if I treat the CIKernel &amp;quot;destCoord&amp;quot; as &amp;quot;the particle I'm going to compute trajectory for&amp;quot;, then I potentially need to be able to read/write to any arbitrary destination coordinate in the outputImage. I don't see any good way of doing this in a CIKernel, since CIKernels work on the assumption that nothing is mutable and the return value is the only place where anything can be changed.  And, the operation I require basically boils down to a seed fill, which the CIKernel docs expressly say is not something that CoreImage supports.  So I suspect I'm out of luck. But the CIImageAccumulator docs talk about fluid dynamics, which is very similar to particle dynamics, so I wonder whether I'm missing something.  I think fluid dynamics uses the same assumption that the particle being processed could wind up anywhere after a timestep. - Rather than treat the destCoord as the particle to which I'm going to apply trajectory, treat it as the location a trajectory might arrive at.  This implies that for each destCoord, I need to perform particle dynamics on every possible srcCoord.  This is ugly, obviously - it's N^2 rather than N.  But more importantly, the limitations on &amp;quot;for&amp;quot; loops in a CIKernel might make it impossible.  I haven't tried it, but it might work.  Yuck. - Copy from VRAM to system memory and do the transformation there. Then go back to VRAM and do everything else on the GPU.  Doing this decimates my performance.  Kills it completely. Could I write a custom fragment shader that would do what I need, without pulling the data out of VRAM?  Is there something about CIImageAccumulator I'm missing (keeping in mind that I need random access to the destination image)? I've implemented the &amp;quot;move to system memory&amp;quot; option using a cached pbuffer and glReadPixels.  Is there any more efficient way of doing it?  Every little bit of performance counts if I have to go this route. Is there some completely different approach I should be using that still gives me the performance benefit of doing most of my computation on the GPU? Attachment:</body>
  </mail>
  <mail>
    <header>Re: CGGLCreateContext: one time only deal?</header>
    <date>Thu Sep 21 22:20:06 CEST 2006</date>
    <body>Are you calling CGLSetCurrentContext? -- ______________________________________________________________________ Michael Sweet, Easy Software Products           mike at easysw dot com Internet Printing and Document Software</body>
  </mail>
  <mail>
    <header>CGGLCreateContext: one time only deal?</header>
    <date>Thu Sep 21 22:10:24 CEST 2006</date>
    <body>In my application, I have an OpenGL context that I need to render a CGImage into.  I looked into ways to do this, and it seemed to me that the simplest way to do this was to use CGGLCreateContext and render to the resulting CGContext.  At first this seemed to work quite well.  Unfortunately, whenever I try to do this more than once in an application, it just doesn't work.  That is, the resulting CGContext just doesn't render CGImages -- they don't show up, even though CGClearRect and CGFillRect work fine. In the test case that I have failing, what I do is create an offscreen OpenGL context, do random setup on it, save GL state, set all matrices to the identity, create the CGGLContext, draw the CGImage, release the CGGLContext using CGContextRelease, restore the matrices to what I normally want them to be, restore GL state, and finally read the pixels using glReadPixels.  I then destroy the GL context. Does anyone know why this isn't working?  Always the second and later attempts just don't draw the image even though I know with certainty (having saved them) that they are valid. Damian Frank</body>
  </mail>
  <mail>
    <header>Core Image Developer required</header>
    <date>Wed Sep 20 18:18:26 CEST 2006</date>
    <body>Hi, I run a small UK based web development company and am currently seeking to sub-contract(not in-house) the services of a talented core image programmer to develop a custom sharpen filter for an OS X based cocoa application. We have a been developing an application that takes video frames and outputs static images(jpeg, png etc). We're using the basic core image filters at present to apply effects. However, it turns out that we are unhappy at the quality of the core image sharpen filters and would therefore like to develop our own custom sharpen filter. I require the services of a developer on a sub contract basis who can create a custom sharpen filter to drop into our application. The developer must know GLSLang and write a kernel function according to CoreImaging/index.html#//apple_ref/doc/uid/TP30001185. Our goal is to have a sharpen filter comparable in quality to that of the new 'Smart Sharpen' filter that is present in Adobe Photoshop CS2. More details can be provided upon response. You will be required to communicate(via email) with my other developer on this project in order to ensure the new filter meets the requirements... please feel free to email if you have any queries about the nature of this enquiry. mark</body>
  </mail>
  <mail>
    <header>CGPostKeyboardEvent problem</header>
    <date>Mon Sep 18 23:53:59 CEST 2006</date>
    <body>I'm trying to send keystrokes to the front application using CGPostKeyboardEvent. I'm using the function at bottom of this post. It takes as parameters a modifier mask and a virtual key code. For instance, to send 'command-N' one would use: This works fine, _except_ when the shift modifier is combined with other modifiers. In that situation the shift modifier event seems to be ignored. To build on the example above (assuming text edit is front application): pressKeyCombination(0, 0x2d);			//  -&amp;gt; prints 'n' pressKeyCombination(NSShiftKeyMask, 0x2d);	//  -&amp;gt; prints 'N', shift works pressKeyCombination(NSCommandKeyMask, 0x2d);	//  -&amp;gt; creates new document but: also creates new document Also, shift-command-p should bring up page setup: print dialog... // //  code used: // void pressKeyCombination(int modifiers, CGKeyCode key) CGSetLocalEventsSuppressionInterval(0.0);	//	don't wait after posting CGEnableEventStateCombining(false);		//	ignore real keyboard state //	depress modifiers //	press/release key //	release modifiers CGSetLocalEventsSuppressionInterval(0.25);	//	default</body>
  </mail>
  <mail>
    <header>Re: CGImageDestinationCreate* and image count</header>
    <date>Sat Sep 16 18:48:36 CEST 2006</date>
    <body>In absence of any response I gave it a try to find out neither is supported: Sep 16 18:36:36 rene-rebes-computer /Users/rene/.../Contents/ MacOS/...: CGImageDestinationCreate capacity parameter is zero\n Also passing some fictive value, such as let's say 666 does not work either: Sep 16 18:37:26 rene-rebes-computer /Users/rene/.../build/Debug/.../ Contents/MacOS/...: CGImageDestinationFinalize image destination does not have enough images\n So there is no way to write multi page TIFF files with an unknown page count at creation time in OS X? -- René Rebe - ExactCODE - Berlin (Europe / Germany) +49 (0)30 / 255 897 45</body>
  </mail>
  <mail>
    <header>Synthesizing Scroll Wheel Events with Quartz Event Services</header>
    <date>Sat Sep 16 11:52:50 CEST 2006</date>
    <body>I need to synthesize low level events for automating GUI-level app testing, and I had success with key up/down and mouse up/down events. Anyways, not so with scroll wheel events. Below is my code. F.ex., when passing the center point &amp;quot;textViewCenterPoint&amp;quot; of some NSTextView, i.e. calling postScrollWheelEvent(textViewCenterPoint, 1, 1, 0, 0), it correctly moves the cursor to the desired location, but no scrolling happens, it behaves just like kCGEventMouseMoved. void postScrollWheelEvent(CGPoint point, int64_t xDelta, int64_t yDelta, int64_t zDelta, CGEventFlags modifiers) CGEventSetIntegerValueField(event, kCGScrollWheelEventDeltaAxis1, CGEventSetIntegerValueField(event, kCGScrollWheelEventDeltaAxis2, CGEventSetIntegerValueField(event, kCGScrollWheelEventDeltaAxis3, CGEventSetIntegerValueField(event, Thanks a lot and best regards, -- Martin</body>
  </mail>
  <mail>
    <header>Re: ImageIO Exif GPS</header>
    <date>Sat Sep 16 05:53:06 CEST 2006</date>
    <body>I am going to take the lack of response as an answer to my question and consider writing GPS Exif data using ImageIO not possible and find another solution. I can totally understand why ImageIO doesn't deal with this currently as it seems rather difficult to do correctly and the number of applications requiring this functionality is most likely pretty low. On Sep 10, 2006, at 11:59 PM, Byron Wright wrote:</body>
  </mail>
  <mail>
    <header>Re: When is PDF v1.4 Created?</header>
    <date>Sat Jan 31 21:54:10 CET 2004</date>
    <body>I'm getting the version by calling CGPDFDocumentGetVersion(), does that provide the true version?  In any event it's somewhat beside the point, I get transparency when I draw to the screen but not when I draw to a PDF context.  This means I don't have WYSIWYG through the print path. I'm trying to figure out if I'm doing something wrong, or if this is a bug and/or limitation in CG. FWIW, I'm having the same problem with shaders.  They're imaged to the screen, but not to a PDF context. Nick</body>
  </mail>
  <mail>
    <header>Re: When is PDF v1.4 Created?</header>
    <date>Sat Jan 31 02:14:20 CET 2004</date>
    <body>Are you looking at the %PDF-1.3 at the start of the file to determine the version?  If so, that's not sufficient to determine the version. You need to look for a /Version key in the /Root entry of the document to determine the true version.  See section 3.4.1+ of the PDF 1.4 spec. Derek</body>
  </mail>
  <mail>
    <header>Re: When is PDF v1.4 Created?</header>
    <date>Sat Jan 31 00:26:27 CET 2004</date>
    <body>Derek, I guess I don't understand how to set transparency then.  How do I set the transparency of a PDF to make this happen? For example, if I create a create a PDF context, set the context alpha to 0.5 and draw a couple paths I get a v1.3 PDF with two paths correctly shown at 50%.  So far so good, however if I create the same PDF context with 50% alpha and instead draw an embedded v1.3 PDF (from a CGPDFDocumentRef) into it I also get a v1.3 PDF and the transparency is ignored, although doing the same thing into a screen context I do see the transparency in the embedded PDF.  There doesn't seem to be any way to get a v1.4 PDF. Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: Basic forms with NSBezierPath</header>
    <date>Fri Jan 30 23:41:47 CET 2004</date>
    <body>None that I'm aware of besides the basic forms defined in NSBezierPath.h.  If there are other common shapes that you think would be widely useful feel free to file an enhancement request at The &amp;quot;ovalteen&amp;quot; Graphics 2D code sample contains functions for drawing rounded rects.  It's a Carbon app that calls CG functions directly, but the general idea is the same: ovalteen.htm &amp;quot;This sample shows how to implement simple Quartz 2D replacements for the QuickDraw rounded rect and oval drawing functions. strokeOval, fillOval, strokeRoundedRect, and fillRoundedRect implement Quartz 2D equivalents to the QuickDraw oval drawing and rounded rect APIs, using CGRects instead of Rects. Troy Stephens Cocoa frameworks, Apple</body>
  </mail>
  <mail>
    <header>Re: When is PDF v1.4 Created?</header>
    <date>Fri Jan 30 23:33:01 CET 2004</date>
    <body>Typically, the version is bumped up to 1.4 only for 1.4-only features (such as transparency). Derek</body>
  </mail>
  <mail>
    <header>RE: Printing With PDF's...</header>
    <date>Fri Jan 30 20:27:43 CET 2004</date>
    <body>What a difference moving a line of code makes. Thanks very much. Andy -----Original Message----- From: David Gelphman [] Sent: Friday, January 30, 2004 12:30 PM To: 'email@hidden' Subject: Re: Printing With PDF's... If you want to use PMSessionGetGraphicsContext to get a CGContextRef, you need to call PMSessionSetDocumentFormatGeneration BEFORE you call PMSessionBeginDocument or PMSessionBeginDocumentNoDialog. David</body>
  </mail>
  <mail>
    <header>Re: Printing With PDF's...</header>
    <date>Fri Jan 30 19:30:26 CET 2004</date>
    <body>If you want to use PMSessionGetGraphicsContext to get a CGContextRef, you need to call PMSessionSetDocumentFormatGeneration BEFORE you call PMSessionBeginDocument or PMSessionBeginDocumentNoDialog. David</body>
  </mail>
  <mail>
    <header>RE: Printing With PDF's...</header>
    <date>Fri Jan 30 19:03:39 CET 2004</date>
    <body>I pulled the WWDC DVD and looked at the same slide to generate my code. Andy -----Original Message----- From: Joseph Maurer [] Sent: Friday, January 30, 2004 12:00 PM To: Brace, Andy Cc: 'email@hidden' Subject: Re: Printing With PDF's... You didn't tell how you are passing kPMGraphicsContextCoreGraphics to PMSessionSetDocumentFormatGeneration. But because I ran into a similar problem just yesterday (in my work on future Quartz sample code), I should show you what works for me (thanks to David Gelphman who pointed me at a slide from his 2001 WWDC presentation, which contained the code): ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Now, tell the printing system that we promise never to use any Quickdraw calls: CFArrayRef  graphicsContextsArray = CFArrayCreate(NULL, (const PMSessionSetDocumentFormatGeneration(printSession, ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ --(jm)</body>
  </mail>
  <mail>
    <header>Re: Printing With PDF's...</header>
    <date>Fri Jan 30 19:00:12 CET 2004</date>
    <body>You didn't tell how you are passing kPMGraphicsContextCoreGraphics to PMSessionSetDocumentFormatGeneration. But because I ran into a similar problem just yesterday (in my work on future Quartz sample code), I should show you what works for me (thanks to David Gelphman who pointed me at a slide from his 2001 WWDC presentation, which contained the code): ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Now, tell the printing system that we promise never to use any Quickdraw calls: CFArrayRef  graphicsContextsArray = CFArrayCreate(NULL, (const PMSessionSetDocumentFormatGeneration(printSession, ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ --(jm)</body>
  </mail>
  <mail>
    <header>Re: Printing With PDF's...</header>
    <date>Fri Jan 30 18:48:35 CET 2004</date>
    <body>Doesn't the printing system already take care of this for you? I believe it will rasterize postscript automatically for printers that don't support it. If not automatically the tools to do this are already in the printing system IIRC. -Shawn</body>
  </mail>
  <mail>
    <header>RE: Printing With PDF's...</header>
    <date>Fri Jan 30 18:46:37 CET 2004</date>
    <body>Didn't work. Also should state that the PDF I generated can be saved out to disk and Acrobat opens it just fine. Andy -----Original Message----- From: Haroon Sheikh [] Sent: Friday, January 30, 2004 11:43 AM To: Brace, Andy Cc: 'email@hidden' Subject: Re: Printing With PDF's... Give that a try and see if the problem goes away.</body>
  </mail>
  <mail>
    <header>Re: Printing With PDF's...</header>
    <date>Fri Jan 30 18:43:24 CET 2004</date>
    <body>Give that a try and see if the problem goes away.</body>
  </mail>
  <mail>
    <header>RE: Printing With PDF's...</header>
    <date>Fri Jan 30 18:41:47 CET 2004</date>
    <body>No I am not. Andy -----Original Message----- From: Haroon Sheikh [] Sent: Friday, January 30, 2004 11:36 AM To: Brace, Andy Cc: 'email@hidden' Subject: Re: Printing With PDF's... Out of curiosity, are you performing a CGContextBeginPage before calling CGContextDrawPDFDocument? haroon</body>
  </mail>
  <mail>
    <header>Re: Printing With PDF's...</header>
    <date>Fri Jan 30 18:36:14 CET 2004</date>
    <body>Out of curiosity, are you performing a CGContextBeginPage before calling CGContextDrawPDFDocument? haroon</body>
  </mail>
  <mail>
    <header>Printing With PDF's...</header>
    <date>Fri Jan 30 18:17:42 CET 2004</date>
    <body>I am developing an application that uses Postscript to output to postscript printers and presses.  On occasion however, I need to output to a non-postscript printer.  The solution I am trying to come up with involves taking the postscript I generate and convert it to a CGPDFDocumentRef using the CoreGraphic API's.  I then want to take the CGPDFDocumentRef and output this to a CGContextRef that I retrieved from a call to PMSessionGetGraphicsContext which I had first initialized the PMPrintSession with a call to PMSessionSetDocumentFormatGeneration passing kPMDocumentFormatPDF and kPMGraphicsContextCoreGraphics.    When I call CGContextDrawPDFDocument passing the context and PDFDocument I crash with an &amp;quot;unknown PowerPC exception&amp;quot;. What exactly am I doing wrong, and is this a valid way of doing what I need to do. Thanks Andy Brace Dex Media</body>
  </mail>
  <mail>
    <header>When is PDF v1.4 Created?</header>
    <date>Fri Jan 30 17:17:17 CET 2004</date>
    <body>I recall hearing at WWDC that Quartz would write PDF as version 1.3 for greater compatibility unless the graphics to be written required PDF version 1.4.  Can anybody tell me if this is correct in Panther and if so, which graphic features cause Quartz to write version 1.4? Thanks, Nick Nallick</body>
  </mail>
  <mail>
    <header>vImage Questions?</header>
    <date>Mon Jan 26 20:58:46 CET 2004</date>
    <body>Is this a good place to ask questions about vImage?  If not, is there a better place? I know it's not part of Quartz, but it's a graphics technology and this is probably the closest thing to a general graphics mailing list. Thanks, Nick Nallick</body>
  </mail>
  <mail>
    <header>Re: Insideness testing</header>
    <date>Sun Jan 25 00:55:44 CET 2004</date>
    <body>Sorry, I didn't read your method carefully the first time -- you are making a bitmap with exactly one pixel, and seeing whether it got painted by the path. Actually that's quite clever -- it solves the issue where I might need a bit of slop around the shape, simply set the stroke larger... when I get around to testing, I'll see if it actually is faster. If Quartz images row by row anyway, the algorithm is essentially the same. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com</body>
  </mail>
  <mail>
    <header>Re: Insideness testing</header>
    <date>Sat Jan 24 21:17:57 CET 2004</date>
    <body>If there is a Quartz function it's not exported in the public API. Assuming you have decent clipping logic, I don't see any reason why imaging a single pixel in a path would be appreciably slower than determining if an arbitrary point will be painted by a path or not.  In both cases you have to construct a table to trace a ray through the point.  Imaging one pixel from that table isn't going to make much difference. Nick</body>
  </mail>
  <mail>
    <header>Basic forms with NSBezierPath</header>
    <date>Sat Jan 24 13:41:26 CET 2004</date>
    <body>Hi there, is there any convenience functions/macros to initialize NSBezierPath instances with basic forms, for example a rectangle with rounded corners, and the like ? Not that I can't do it by myself but I could make me save some time... Thanks in advance Pierre DOUCY</body>
  </mail>
  <mail>
    <header>PDF generation and spot colors</header>
    <date>Sat Jan 24 09:53:26 CET 2004</date>
    <body>Hi! I ask this question already in cocoa-dev without any replies. Maybe someone here can give an answer: As PDF format supports spot colors within a PDF document, I am wondering if COCOA is able to handle spot colors, that are saved as real spot colors to a PDF and shown as CYMK or RGB color on screen. NSColor does not support this directly, i think. Any documentation seen or ideas or experiences on this issue? MiMo -- ======================================================= And refashioning the fashioned lest it stiffen into iron is work of endless vital activity. [Goethe] All about CALAMUS DTP suite: =======================================================</body>
  </mail>
  <mail>
    <header>Re: Insideness testing</header>
    <date>Sat Jan 24 08:15:44 CET 2004</date>
    <body>containsPath: is implemented in NSBezierPath. Ali</body>
  </mail>
  <mail>
    <header>Re: Insideness testing</header>
    <date>Sat Jan 24 03:00:45 CET 2004</date>
    <body>The Cocoa NSBezierPath has a method containsPoint: that does this, and I presume it must be implemented by the underlying CoreGraphics function -- but which one? Mathematically, I can shoot a vertical or horizontal ray from the point and count how many intersections with the path to determine insideness, but I'd rather not do that if there already exists such a Quartz function. I suppose using a bitmap context to test is doable -- filling the shape -- but I doubt it's going to be fast... Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com</body>
  </mail>
  <mail>
    <header>Re: Insideness testing</header>
    <date>Fri Jan 23 15:23:14 CET 2004</date>
    <body>You can &amp;quot;hit test&amp;quot; a specific pixel but I don't know of any way to do an abstract &amp;quot;insidedness&amp;quot; test.  To hit test a pixel you have to draw the path and test the pixel of interest.  What I do for this is to create a 1x1 gray bitmap context so the bitmap data is only one byte, translate the context to test the point I'm interested in, draw the path, and test for a non-zero value in my bitmap byte. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Insideness testing</header>
    <date>Fri Jan 23 11:01:39 CET 2004</date>
    <body>There seem quite a lot of functions in the Quartz dylib that aren't documented. In particular, how does one do insideness or hit testing? If I have a path, how do I determine whether a particular point is &amp;quot;inside&amp;quot; it? (Cocoa has such a function, but I'm interesting in a CoreGraphics only solution.) Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com</body>
  </mail>
  <mail>
    <header>Draw PDF inverted</header>
    <date>Fri Jan 23 03:13:49 CET 2004</date>
    <body>I need to draw PDF with inverted colors. How can I do this? Thanks Manfred</body>
  </mail>
  <mail>
    <header>How does Quartz vector graphics internally works ?</header>
    <date>Thu Jan 22 11:44:06 CET 2004</date>
    <body>Hello, I'm currently having a look at several moderns 2D Graphic Engines, and have been playing a little with Quartz 2D. From my tests (drawing a couple of polygonal shapes, and timing the rendering), I have deduced a couple of things about how Quartz vector drawing internally works, and I would like to get some confirmations of my deductions... - Quartz 2D rasterization is based on a Scanline algorithm. - It's using a Gordon-like (also called &amp;quot;Critical Points&amp;quot; or &amp;quot;Local Minimums&amp;quot; on the litterature) approach for its Active Edge Table. Can anybody with access to Quartz sources confirms (or deny) this ? Regards, Raphael email@hidden</body>
  </mail>
  <mail>
    <header>CGContextShowGlyphs -- not for drawing strings?</header>
    <date>Sun Feb 29 19:39:47 CET 2004</date>
    <body>I wanted to draw some text with CGContextShowGlyphs, but if e.g. I use combining glyphs then these are not overlayed onto the character to which they bind. Am I to assume that this function is really only for drawing the actual glyph, and layout should be done by a higher level system (like ATSUI), which will draw glyphs one at a time (if necessary)? One of the free ADC TV videos showed a slide with international text (arabic, hebrew, etc.) and the speaker said that &amp;quot;this was all drawn with Quartz&amp;quot; -- but either I have overlooked something, or the speaker literally meant drawn by Quartz, not positioned by Quartz. Regards Allan --</body>
  </mail>
  <mail>
    <header>PDF Marked Contexts</header>
    <date>Sat Feb 28 20:59:29 CET 2004</date>
    <body>I'm looking for a way to insert marked-context points and/or sequences into a PDF graphics stream while recording it with a CG PDF context. Is there any way to do this currently, or should I file an enhancement request? Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: How expensive is CGContextSave/RestoreGState?</header>
    <date>Fri Feb 27 23:11:10 CET 2004</date>
    <body>To be honest, even/odd fill is so rarely used I don't think anyone's ever made this measurement. I'd probably stick with winding rule simply because it gets the lion's share of the attention. Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: How expensive is CGContextSave/RestoreGState?</header>
    <date>Fri Feb 27 20:02:20 CET 2004</date>
    <body>Here's one I wonder about from time to time.  Is there much performance difference between using a winding rule vs. even/odd fill?  For example, if I'm just filling something simple like a rectangle or circle should I prefer one over the other?  It seems like they could be using essentially the same algorithm, or completely different algorithms. Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: How expensive is CGContextSave/RestoreGState?</header>
    <date>Fri Feb 27 18:45:35 CET 2004</date>
    <body>Just to add to this: save/restore is very cheap, so you shouldn't worry about calling them.  On the other hand, what you do _after_ saving the gstate is what you should focus on:  for example, clipping, depending on the path, may be cheap or expensive. As Haroon said, the best thing is to profile your app.  That way you'll know what the impact is in your particular case. Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: Minimum Font Smoothing Size</header>
    <date>Fri Feb 27 06:38:59 CET 2004</date>
    <body>If there's no kosher way to do it (and if there's not, that would be a good subject for an ATSUI enhancement request), then maybe you can draw text at a large size (large enough that it's greater than the antialias minimum) and then apply a downscaling affine transform. It's worth a try anyway. Dave</body>
  </mail>
  <mail>
    <header>Re: Jaguar: Is the result of CGColorSpaceCreateDeviceRGB() a   cached object ?</header>
    <date>Thu Feb 26 20:26:33 CET 2004</date>
    <body>If you call CGColorSpaceCreateDeviceRGB(), you should always call CGColorSpaceRelease when you are done with it. haroon [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: Is CGContextDrawPDFDocument() multi-thread safe?</header>
    <date>Thu Feb 26 20:21:45 CET 2004</date>
    <body>Do you have a backtrace for your crash? [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: Hardware requirements</header>
    <date>Wed Jan 05 21:43:23 CET 2005</date>
    <body>On Jan 5, 2005, at 2:29 PM, John Kerr wrote: I don't believe Apple has said anything publicly about that. Well, they've said that programmable shaders are required so that would certainly up the ante on the video card needs. I don't know that that information has been made available. But Apple has said that CI requires programable shaders, so it's a good bet that CI and Q2DE share the same requirements. fwiw, the only data for CI requirements can be found here if you haven't already seen it: Bryan</body>
  </mail>
  <mail>
    <header>Re: Hardware requirements</header>
    <date>Wed Jan 05 21:35:52 CET 2005</date>
    <body>On Jan 5, 2005, at 12:29 PM, John Kerr wrote: This is all under NDA still so if it isn't on Apple website then folks cannot say (unless an Apple employee chimes in). Additionally you are also sharing information about a product (your findings that you outlined) that you likely got under NDA (if not that is another issue) and hence could be considered in violation of the NDA.</body>
  </mail>
  <mail>
    <header>Hardware requirements</header>
    <date>Wed Jan 05 21:29:01 CET 2005</date>
    <body>Does anyone know the hardware requirements of Quartz2D Extreme? It seems it is greater than Quartz Extreme.  I find that a GeForce4 MX with 64MB won't use Quartz2D Extreme, but a GeForceFX5200 with 64MB does work. Are the requirements the same as for CoreImage?</body>
  </mail>
  <mail>
    <header>Re: Compositing a mask and an image</header>
    <date>Wed Jan 05 20:01:50 CET 2005</date>
    <body>On Jan 5, 2005, at 8:57 AM, Jonathan Johnson wrote: The CoreGraphics API doesn't currently export much in the way of &amp;quot;bitblit&amp;quot; operations.  You'll probably have to use pieces from Carbon and/or Cocoa or manipulate the raster data yourself.  One thing to remember is that CG expects alpha data to be premultiplied, whereas you're probably dealing with non-premultiplied data.  This is probably where you'd find vImage to be useful (as others have implied).</body>
  </mail>
  <mail>
    <header>Re: Compositing a mask and an image</header>
    <date>Wed Jan 05 19:07:30 CET 2005</date>
    <body>On Jan 5, 2005, at 7:57 AM, Jonathan Johnson wrote: You may want to look at using functionality provided by vImage since it would like be the fastest way to do this type of processing. Of course if a Quartz2D method exists I would use it instead since it likely would leverage vImage or similar code itself.</body>
  </mail>
  <mail>
    <header>Re: Compositing a mask and an image</header>
    <date>Wed Jan 05 18:52:42 CET 2005</date>
    <body>One method may be to use CGImageCreate.  Most of the parameters such as width, height, bitsPerPixel would be the same values as used for your &amp;quot;source&amp;quot; image.  You'd then also specify that the newly created image will have an alpha channel. I would then imagine that the data provider you pass to CGImageCreate would ultimately know how to access data from two input streams (source image and alpha image). Another option to explore is an image with pre-multiplied alpha.  That would definitely speed up rendering. You may want to also consider filing an enhancement request for a API that would effectively join a source and alpha into a single CGImage. I would think this would be limited to bitmap-based images. The QuickDraw-based version of one of my apps relied heavily on CopyMask.  I always had separate source and alpha images.  When I heard about the deprecation of QuickDraw, I first considered staying with Carbon and just using Quartz APIs.  But I didn't want to end up combining all my images at runtime especially since there was no convenient API. But now I've moved to Cocoa and all my images are either PDF with transparency effects or TIFF images with an alpha channel.  Thus, I never have to combine my images. If it's the case where you're in complete control of your images, you may want to consider combining them ahead of time. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Compositing a mask and an image</header>
    <date>Wed Jan 05 16:57:58 CET 2005</date>
    <body>If I have two CGImageRefs -- one representing the image and another representing the mask -- what's the best way to combine them into one image with an alpha channel? Thanks, Jon -- Jonathan Johnson REAL Software, Inc. -- REAL World 2005 - The REALbasic User Conference March 23-25, 2005, Austin, Texas</body>
  </mail>
  <mail>
    <header>Re: documentation problems</header>
    <date>Wed Dec 01 00:13:21 CET 2004</date>
    <body>On Nov 30, 2004, at 2:51 PM, Travis Heppe wrote:</body>
  </mail>
  <mail>
    <header>Re: documentation problems</header>
    <date>Wed Dec 01 00:05:51 CET 2004</date>
    <body>On Nov 30, 2004, at 4:51 PM, Travis Heppe wrote: Pattern space is the coordinate system into which you draw your pattern.  To put it another way, the pattern lives within its own coordinate system that is separate from all other spaces.  You establish the relationship between Pattern Space and the current &amp;quot;base space&amp;quot; when you call CGPatternCreate.  The affine transform you pass to that routine is used to map from pattern space to the &amp;quot;base space&amp;quot;. Usually the &amp;quot;base space&amp;quot; is the same thing as &amp;quot;user space&amp;quot; but does not necessarily have to be.  If you are drawing a PDF document, for example, and that PDF document contains a pattern, the transformation of the pattern will map to the &amp;quot;base space&amp;quot; of the PDF document you are drawing.  That way if you draw the PDF rotated, scaled, or otherwise, the CGPattern will be drawn relative to the coordinate system of the PDF itself, and not the User Space into which the PDF is being drawn. I guess I don't understand the effect you are trying to achieve.  Are you trying to replicate something similar to CGContextSetLineDash, but within a filled shape?  What do you mean by &amp;quot;diagonal dashes&amp;quot;? It sounds like you are trying to do the equivalent of a stroke dash a lá CGContextSetLineDash...  How is what you are trying to do different from that?</body>
  </mail>
  <mail>
    <header>documentation problems</header>
    <date>Tue Nov 30 23:51:12 CET 2004</date>
    <body>In the &amp;quot;Painting stencil patterns&amp;quot; section of there are a few errors: The biggest problem I have with the document, however, is that it has no (as far as I have found) definition of the term &amp;quot;pattern space&amp;quot;.  To the best that I have been able to determine by trial and error, &amp;quot;pattern space&amp;quot; is simply the standard context with an appropriate positioning matrix, but no scale or orientation. I am trying (so far without luck) to create a stencil stroke pattern for a rotation.  In other words, diagonal dashes should not look longer or shorter than horizontal ones.  I was hoping that stroke patterns would get me there, but am I perhaps barking up the wrong tree? Quartz' pattern API seems oriented towards 2d patterns, and unsuitable for the kind of 1d patterns that orient along the line vector that I am trying to create.</body>
  </mail>
  <mail>
    <header>Re: poor rendering quality when printing pdf's that use	transparency	layers</header>
    <date>Sat Nov 27 09:04:53 CET 2004</date>
    <body>The rendering resolution was indeed 72 dpi in some usage cases of transparency layers. This is a known issue and will very likely be fixed in an upcoming 10.3 software update. A few other transparency layer related bugs will also be fixed in an upcoming software update. Can't say when it will be made public though. Attachment:</body>
  </mail>
  <mail>
    <header>Re: poor rendering quality when printing pdf's that use	transparency layers</header>
    <date>Fri Nov 26 06:51:11 CET 2004</date>
    <body>On Nov 25, 2004, at 9:37 AM, R. Scott Thompson wrote: the question i'm raising is &amp;quot;when should rasterization occur?&amp;quot;. transparency and shadows are a slightly different issue from transparency layers, they can be used without transparency layers.  my issue is that i'd like the rasterization of transparency layers to be delayed until render time. a 300 dpi image will be scaled down to 72 dpi.  that is what i meant. this isn't a shadowing issue but a transparency layer issue.  i don't have a problem with shadowing or transparency in pdf, it looks good with apple's pdf renderer and when printed it looks good with apple's rip.  my problem is with transparency layers, these look bad when printed or even when simply zoomed in in preview.app the print preview is going to generate pdf, for the most part the resolution of the images is going to be app dependent and should either match the resolution of the printer or the native resolution of the images themselves.  whichever makes sense.  however, if transparency layers are used everything is going to be rendered into a bitmap at 72dpi apparently and look bad. let me try to clarify what i was saying.  from my testing, it looks like when quartz creates a transparency layer it simply renders all drawing into a 72 dpi bitmap ref, and then composites that 72 dpi image onto the page using the transparency and shadowing parameters.  what i'm suggesting is drawing into a separate pdf xobject stream and embedding that stream.  thus, any embedded images (or text or vector graphics i imagine) would be preserved at full resolution.  this allows the quartz renderer to rasterize and composite that xobject at the target resolution that matches the output device.  if pdf is going to be rendered on-screen it has to be rasterized at some point.  it looks like transparency layers are rasterizing at document creation time.  i think this could be delayed to document rendering time. apple hacked shadowing into pdf and it looks good with their renderer even when printing.  i can live with that.  transparency layers are introducing an additional level of restrictions that look bad even when simply zooming in on the display, as well as when printing. it would vary according to the rip and i'm willing to accept that. that's the current situation. i just want to reiterate that shadows and transparency aren't the problem.  i can deal with the current situation there (which looks good when using quartz's rip and quartz pdf).  my issue is with transparency layers.</body>
  </mail>
  <mail>
    <header>Re: poor rendering quality when printing pdf's that use	transparency	layers</header>
    <date>Thu Nov 25 15:37:42 CET 2004</date>
    <body>Unfortunately rasterization unavoidable.  The soft drop shadows generated by Quartz 2D are images.   Even worse, depending on the output device, the printer may not support transparency.  The only way to reliably reproduce the effect is through some kind of rasterization. I don't know what you mean by &amp;quot;scaled down&amp;quot; but the images are most likely rasterized at 72 dpi because the shadow effect is primarily intended for on-screen use (which, by convention, is understood to be 72 pixels/inch on current systems... this may change). As an experiment, it might be interesting to see if a print preview of a saved to PDF, whose destination was a high-resolution ink-jet printer, uses the native resolution of the printer or just 72 DPI. Presumably you mean a transparency group XObject? But what would you put in the XObject stream?  Consider that Quartz is already recording your drawing commands into a PDF and it is not able to satisfy your request.  Trying to package drawing commands that are just as effective into an XObject is not going to buy you anything. I think the problem with your suggestion is that, XObject or not, there are no PDF operators for generating a soft drop shadow. You'd still be stuck having to put some kind of rasterization of the shadow into the PDF. You would also run into the problem that the same PDF would behave VERY differently on different printers.  Printing it to a low-resolution Ink-Jet printer might give you reasonable behavior.  If you took that same document to a 2400 DPI image setter it would suddenly generate a print stream that is something on the order 60 or 70 times larger and require much more memory to boot. I don't really think the shadow APIs are intended for print work. Transparency, in general, is a dicy prospect with regards to printing. Another ambitious user, for example, noted that shadings with transparency could cause problems when printing earlier. If the drop shadows are important to your content, then there are at least two approaches to consider. You could work with Quartz to rasterize the shadows at a higher resolution yourself. Just use the same technique the system is using... create an offscreen Context with enough pixels to represent your drawing and it's shadow at (say) 300 dpi resolution.  Set the scaling on that context to scale from 72 DPI to 300 DPI and draw your object into the offscreen, then use CGDrawImage to place it in the final document.  You're pulling the same trick... but at a higher resolution which may make the results more palatable. You could even make the resolution user-selectable.  That way if the user wanted to bloat their PDF with the shadow effect rendered at 2400 dpi resolution then they could do that.  That's the approach taken in FreeHand 11 for printing raster effects (with the additional caveat that they are all generated in an RGB color space and not properly color managed which pretty much makes them useless for anything on a real printer... but I digress). Another technique would be to generate the drop shadows as a &amp;quot;blend&amp;quot; of successively smaller objects with increasing alpha channels.  The first Object would be close to transparent.  The next, slightly smaller, would be more opaque.  So on and so forth until you get to something roughly the size of your object.  For simple shapes like a rectangle this would work pretty well.  If you have more complex shapes (curves and such) then it's going to be much harder to get it right.</body>
  </mail>
  <mail>
    <header>poor rendering quality when printing pdf's that use transparency	layers</header>
    <date>Thu Nov 25 14:47:48 CET 2004</date>
    <body>i've been using quartz 2D to generate pdf's with shadows and transparency effects, and it appears that when transparency layers are used in a pdf context, when that data is rasterized for printing (or rendering to screen) the transparency layers appeared to be rendered at a very low resolution (72 dpi i guess) and then scaled up.  this looks terrible when either printing or simply zooming in on an image in preview. i'd guess that the current implementation simply renders the pdf calls to a bitmap and then embeds that image.  one specific result of this is that high-res images are effectively scaled down to 72dpi at whatevever size the image is drawn into. i'd think that a better implementation would simply record the pdf commands into a pdf context, and then embed that pdf data as a x-object, and then when the pdf is rasterized for printing this would preserve full image fidelity. is this an accurate assessment of a known problem or am i simply using the api incorrectly?  are there any workarounds?</body>
  </mail>
  <mail>
    <header>Re: Reading pixels though translucent window</header>
    <date>Thu Nov 25 03:00:58 CET 2004</date>
    <body>If you're going to read the pixels straight from the screen, you may want to talk to DTS about a sample called glGrab that uses OpenGL to do just that. The sample used to be available on the ADC website but it comes and goes. DTS should either give you the sample for free or help you get your code working in exchange for a tech support incident. On Nov 24, 2004, at 5:43 PM, Jamie Hodkinson wrote:</body>
  </mail>
  <mail>
    <header>Re: Reading pixels though translucent window</header>
    <date>Thu Nov 25 00:43:10 CET 2004</date>
    <body>If there is no way to read the contents of windows underneath a window (and I can't assume anything about the windows underneath), is it possible to read the composite pixels from my window on top (assuming it is translucent?) - or will this have the same problem? Should I be reading pixels directly from the screen buffer?</body>
  </mail>
  <mail>
    <header>Re: Reading pixels though translucent window</header>
    <date>Thu Nov 25 00:09:21 CET 2004</date>
    <body>If you know, for some reason, that there's only a single window underneath your overlay (like for example if you created it deliberately to overlay that single window), you might be able to read its backing store directly. If you know that the underlying window is always your own window, you double-buffer it yourself. Dave Howell ProApps Engineering</body>
  </mail>
  <mail>
    <header>Re: Reading pixels though translucent window</header>
    <date>Wed Nov 24 23:59:35 CET 2004</date>
    <body>There is no mechanism to do this. The Quartz Compositor is going to composite all windows with the Source Over Porter-Duff composite operation. There really is no way to read the contents of all windows underneath a window.</body>
  </mail>
  <mail>
    <header>Reading pixels though translucent window</header>
    <date>Wed Nov 24 23:29:34 CET 2004</date>
    <body>Is there a simple way to read a pixel colour from the screen, though a translucent window? I'd like to create a 'filter' window, which is mainly translucent, except for certain parts, depending on the graphics underneath the window at that point, eg. if the window detects that the character 'a' is underneath it, it will highlight it in some way. I've looked in all the obvious places, but it is perhaps an unusual thing to be doing. Any help gratefully received.</body>
  </mail>
  <mail>
    <header>Re: gouraud triangles</header>
    <date>Wed Nov 24 21:23:17 CET 2004</date>
    <body>On Nov 24, 2004, at 1:52 PM, Travis Heppe wrote: The interpolation mechanism is context dependent.  You can provide a hint to the context as to what quality of interpolation you would like (CGContextSetInterpolationQuality) but I don't think you can dictate which mechanism the context uses.</body>
  </mail>
  <mail>
    <header>Re: gouraud triangles</header>
    <date>Wed Nov 24 20:52:54 CET 2004</date>
    <body>Thanks for the help. 3D hardware was indeed made to handle things like gouraud shading.  We **do** use opengl for display.  What I am trying to implement here is our hardcopy solution. On your suggestion, I'll try out the sampled image approach.  The docs don't say anything about how images are resampled when a scaling is applied.  Can I assume that it is bilinear?</body>
  </mail>
  <mail>
    <header>Re: gouraud triangles</header>
    <date>Wed Nov 24 20:32:54 CET 2004</date>
    <body>There is no way of creating general Gourad shaded triangles in Quartz 2D at the moment.  Quartz 2D currently only &amp;quot;understands&amp;quot; two types of shadings, linear and radial. While you can use linear gradients and transparency to reproduce some combinations of colors in a gourad shading, there is no way to reproduce that effect in the general case for all colors. Most notably, if you have three linearly independent colors (like red, blue, and green) then there is no way to reproduce a barycentric blend of those colors using linear gradients. like the ones in Quartz 2D. Your best bet is probably to use the sampled image approach.  If you want to generate the image as fast as possible, you could easily use OpenGL as the 3D hardware was MADE to handle this kind of shading.</body>
  </mail>
  <mail>
    <header>gouraud triangles</header>
    <date>Wed Nov 24 19:25:30 CET 2004</date>
    <body>I know I asked this question before, but I am still looking for an answer, and it is really important that I find something reasonable in this area. ----------------- What is the most appropriate API to implement gouraud triangles in Quartz2d (i.e. 3 point linear path, 3 independent colors, bilinear interpolation of color values).   I was looking at Axial and Radial shadings, but both seem to only have linear, not bilinear, interpolation. In postscript (language level 3), I believe it's something like &amp;quot;ShadingType 4&amp;quot;, but I haven't found the Quartz2d equivalent. ----------------- I was considering investigating: a) putting the vertex colors into a sampled image b) putting two linear shadings on top of each other, with the second being semi-transparent Some pointers from someone familiar with quartz could possibly save me from going down the wrong path.</body>
  </mail>
  <mail>
    <header>Re: OT?: Fixed Width fonts</header>
    <date>Mon Nov 22 21:37:59 CET 2004</date>
    <body>I should have mentioned.  If you want to monkey with the tables, you can use the Mac OS X font tools (duh) from fonts.apple.com. ftxdumperfuser - t hhea MPW ftxdumperfuser - t hmtx MPW</body>
  </mail>
  <mail>
    <header>Re: OT?: Fixed Width fonts</header>
    <date>Mon Nov 22 20:50:47 CET 2004</date>
    <body>On Nov 22, 2004, at 1:39 PM, R. Scott Thompson wrote: Thanks Scott, this gives me some places to look.  Seeing as MPW is an Apple font, maybe I'll see if Greg Branche can track someone down inside Apple to look at this ;) I tried, I really did! But I'm thoroughly addicted to using the MPW font in source code. :(</body>
  </mail>
  <mail>
    <header>Re: OT?: Fixed Width fonts</header>
    <date>Mon Nov 22 20:39:21 CET 2004</date>
    <body>I think what you're fighting against is whether or not the font will use fractional metrics. I have a vague, and potentially flawed, recollection that fixed width fonts do not use fractional spacing (because the fixed widths are usually integral?) Presumably the MPW font is some kind of FOND based font?  Bits 15 of the FOND's family flags denotes whether or not the font is a fixed width font.  You might de-rez the FOND and see what the value of that flag might be.  If it's not set to 1 then you could use Rez/DeRez to hack the font to include that bit and see if that makes a difference. (another alternative would be DumpFOND and FuseFOND at developer.apple.com/fonts/Tools/index.html). That's not to say for certain that the system will care about the flags field of the FOND (in general it doesn't care about FONDs any more) but it might be worth a shot. Beyond that you may be stuck having to look at the metrics tables within the font itself.  The 'hhea' table indicates how many metrics entries there are in the font and the 'hmtx' table contains each advance width.  If you edit the 'hhea' table and set it up to say there's only one metrics value, then change the 'hmtx' to provide a single left side bearing and advance width... then the font should be come monospaced.</body>
  </mail>
  <mail>
    <header>OT?: Fixed Width fonts</header>
    <date>Mon Nov 22 20:00:23 CET 2004</date>
    <body>I have a font that I used for development (the venerable MPW font). I've used it for ages in MPW, BBEdit and CodeWarrior. I'd like to use it in Xcode as well, but it gets anti-aliased in Xcode and looks terrible. And the problem goes away, but that's a bit of a hack, and doesn't allow my comments in Verdana 9 to be anti-aliased ;) It seems like the &amp;quot;Right Thing&amp;quot; would be for the OS to recognize the MPW font as a Fixed Width font (e.g. it does NOT appear in the Fixed Width collection in the Font panel) where (I assume) the AppleSmoothFixedFontsSizeThreshold preference would apply a different threshold. (I assume that's why Monaco looks ok in Xcode.) So, any advice on where to go for more information about making OS X realize that the MPW font is a fixed width font?</body>
  </mail>
  <mail>
    <header>Re: Text tracks</header>
    <date>Sun Nov 21 22:11:53 CET 2004</date>
    <body>On Nov 21, 2004, at 2:54 PM, CFlatow wrote: Try asking on one of the QuickTime lists.</body>
  </mail>
  <mail>
    <header>Text tracks</header>
    <date>Sun Nov 21 21:54:47 CET 2004</date>
    <body>I have just been using text tracks to annotate audio QT movies. I was using 42 point text which worked fine but when I changed the text size designation to 72 it still came out 42 pt. Does anyone know if there's a size limit on text tracks in QT movies? Thanks, Carl</body>
  </mail>
  <mail>
    <header>CGGLContext</header>
    <date>Fri Nov 19 23:06:43 CET 2004</date>
    <body>When quartz is operating on top of an OpenGL context, what OpenGL state affects the quartz drawing? glRasterPos? GL_DEPTH_TEST? GL_MODELVIEW matrix? glFrontFace? Conversely, do any quartz calls affect OpenGL state?</body>
  </mail>
  <mail>
    <header>Re: Converting PDF graphics to other formats</header>
    <date>Fri Nov 19 08:29:30 CET 2004</date>
    <body>I've converted my app to draw using Quartz, but I need to be able to &amp;quot;capture&amp;quot; parts of my windows into an in-memory &amp;quot;metafile&amp;quot; format. I used to do this with PICT, but in the brave new world, I am using PDF, which I capture by drawing into a CGPDFContext. (Using PDF for this might be my first mistake, but what's the alternative?) Now that I have a graphic in PDF format, I want to convert it to something else, like PNG. How do I do that? Back in the old days, I would use QuickTime's GraphicsExport facility, but that has numerous problems with PDF. Does it even support PDF?</body>
  </mail>
  <mail>
    <header>gouraud triangles</header>
    <date>Thu Nov 18 21:27:22 CET 2004</date>
    <body>What is the most appropriate API to implement gouraud triangles in Quartz2d (i.e. 3 point linear path, 3 independent colors, bilinear interpolation of color values).   I was looking at Axial and Radial shadings, but both seem to only have linear, not bilinear, interpolation. In postscript (language level 3), I believe it's something like &amp;quot;ShadingType 4&amp;quot;, but I haven't found the Quartz2d equivalent.</body>
  </mail>
  <mail>
    <header>Re: Output to PNG</header>
    <date>Wed Nov 17 20:08:46 CET 2004</date>
    <body>On Nov 17, 2004, at 12:27 PM, Tomas Franzén wrote: You're going to have to use an external agent to do the PNG writing.  A good tool in this case is QuickTime which has exporter components that can write to a large number of file formats, including PNG.  The basic QuickTime Export takes approximately 5 lines of code as I recall.  You can get fancier about it if you want to set the parameters of the export (say the compression settings of a JPEG) and QuickTime will even go so far as to put up an image compression dialog for you, allow the user to set the settings, and then give you back an exporter set up to carry out the user's wishes.  That takes quite a bit more than 5 lines of code, however. Scott</body>
  </mail>
  <mail>
    <header>Output to PNG</header>
    <date>Wed Nov 17 19:27:27 CET 2004</date>
    <body>How do I output a CGContext/CGImage to a PNG (or some other format) file? I tried using CGBitmapContextGetData, but the data seems just to be the raw bytes. Thanks. Tomas Franzén Lighthead Software I'm listening to Moby - Harbour</body>
  </mail>
  <mail>
    <header>Re: QuickDraw Behavior in VRAM</header>
    <date>Tue Nov 16 22:40:28 CET 2004</date>
    <body>Oh... well my question was not so much directly related to NewGWorld as it was me trying to learn a bit more about the way memory is shuffled back and forth between main RAM and the video card.  It seemed to me that NewGWorld would (presumably) be creating a texture of some sort... somewhere... and I was trying to figure out how it could change that texture since the QuickDraw commands probably don't have access to VRAM directly. (Can the CPU somehow map a range of VRAM like the video card can map a section of system RAM?) Perhaps a better question might be... If I have a texture and want to change a part of it... but that texture resides in VRAM, (i.e. not system RAM shared through the magic of AGP) then how doe the computer manage getting the changes back and forth? I would almost be willing to be, however, that I can answer my own question by learning  a bit more about OpenGL ;-)  I'll go read some more documentation.</body>
  </mail>
  <mail>
    <header>Re: QuickDraw Behavior in VRAM</header>
    <date>Tue Nov 16 22:26:17 CET 2004</date>
    <body>On Nov 16, 2004, at 1:16 PM, R. Scott Thompson wrote: (...) When create a GWorld in VRAM using the &amp;quot;useDistantHdwrMem&amp;quot; flag to NewGWorld, and I subsequently want to draw in that GWorld, how are my drawing commands transferred to VRAM? (...) As the comment in QDOffscreen.h says, the useDistantHdwrMem is just one of the many old flags which NewGWorld royally ignores on Mac OS X.</body>
  </mail>
  <mail>
    <header>QuickDraw Behavior in VRAM</header>
    <date>Tue Nov 16 22:16:03 CET 2004</date>
    <body>I know this is a Quartz list, but I think my question has a better chance here than anywhere else. When create a GWorld in VRAM using the &amp;quot;useDistantHdwrMem&amp;quot; flag to NewGWorld, and I subsequently want to draw in that GWorld, how are my drawing commands transferred to VRAM? Does LockPixels on such a GWorld allocate a temporary local GWorld where QuickDraw draws followed by the results being flushed to VRAM on the corresponding call to UnlockPixels... or is there something else going on?</body>
  </mail>
  <mail>
    <header>Re: Compressing Images in a PDF</header>
    <date>Tue Nov 16 14:50:28 CET 2004</date>
    <body>On Nov 15, 2004, at 7:06 PM, Derek Clegg wrote: This is simply not true.  Flate compression can only compress regular images, CG-generated images.  It is next to useless for actual photographs, and the vast majority of images in user files are photographs. I noticed this problem in the first releases of OS X and thought that this would be fixed ASAP, because it is just so obvious.  When it wasn't fixed by Jaguar, I took an afternoon to hack together PdfCompress. Marcel Weiher                Metaobject Software Technologies email@hidden        www.metaobject.com Metaprogramming for the Graphic Arts.   HOM, IDEAs, MetaAd etc. 1d480c25f397c4786386135f8e8938e4</body>
  </mail>
  <mail>
    <header>Re: Compressing Images in a PDF</header>
    <date>Tue Nov 16 01:22:02 CET 2004</date>
    <body>On Nov 15, 2004, at 12:01 PM, Nick Nallick wrote: Yes, that's correct.</body>
  </mail>
  <mail>
    <header>Re: [OT] Regressing to EPS</header>
    <date>Mon Nov 15 22:31:37 CET 2004</date>
    <body>On Nov 15, 2004, at 3:24 PM, R. Scott Thompson wrote: Nevermind.  I learned that ghostscript can do this for me if necessary. Sorry for the noise.</body>
  </mail>
  <mail>
    <header>[OT] Regressing to EPS</header>
    <date>Mon Nov 15 22:24:03 CET 2004</date>
    <body>I'm trying to submit some artwork to a printer and they expect the files to be delivered as EPS files.  Now it would seem to me that PDF is, for all practical purposes, a better format for submitting artwork than EPS and PDF/X should be another step along the way as well. Nevertheless, I'm curious to know if anybody has an idea for strategies to regress a PDF illustration back to an EPS file? My first thought was to write a script that ran the files through CUPS and collected the PS output then tagged that output with the required EPS comments and previews and such.  That doesn't sound like an overly egregious process.  But it would be quite a bit less egregious if someone knows of an Open Source tool (or similar) that could do it all for me :-)</body>
  </mail>
  <mail>
    <header>Re: Compressing Images in a PDF</header>
    <date>Mon Nov 15 21:01:11 CET 2004</date>
    <body>On Nov 15, 2004, at 12:06 PM, Derek Clegg wrote: Okay, thanks for the info.  I see what's happening.  I'm doing something similar with regard to Flate compression myself but in the case of JPEG I'm failing to pass that through to the PDF as JPEG so it's defaulting to Flate (which is considerably larger than the original JPEG).  It sounds like I should use a JPEG data provider for that case when going to PDF. In a related issue:  Does the JPEG data provider handle JPEG-2000 or just vanilla JPEG? Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: Compressing Images in a PDF</header>
    <date>Mon Nov 15 20:06:35 CET 2004</date>
    <body>Unless you've encountered a bug, this isn't correct.  It's true that we don't automatically JPEG-compress image data (because it's lossy), but we Flate-compress all image data, which should reduce image size considerably.  &amp;quot;Flate compression&amp;quot; is better known as the compression provided by zlib, and it's the same thing that's used in PNG image compression.</body>
  </mail>
  <mail>
    <header>Re: Compressing Images in a PDF</header>
    <date>Mon Nov 15 18:48:46 CET 2004</date>
    <body>On Nov 13, 2004, at 8:41 AM, Nick Nallick wrote: Yes, JPEG will be saved as JPEG and PNG with flate compression. Nothing special is happening. The data is decompressed on each draw operation. Attachment:</body>
  </mail>
  <mail>
    <header>Re: Color space selection</header>
    <date>Mon Nov 15 00:36:45 CET 2004</date>
    <body>On Nov 14, 2004, at 2:50 PM, Matt Deatherage wrote: I'll check that out as well. Actually the session I was referring specifically referring to was the one on &amp;quot;High Definition Imaging with Image I/O&amp;quot; where they discuss the changes in Tiger for ColorSync.  I eventually found it but the ColorSync content was not obvious from the session title :-)</body>
  </mail>
  <mail>
    <header>Re: Color space selection</header>
    <date>Sun Nov 14 21:50:23 CET 2004</date>
    <body>I believe the session you're looking for was &amp;quot;203 - New Developments in Quartz 2D,&amp;quot; from Tuesday afternoon (2004.06.29, 2 PM), in the &amp;quot;New APIs&amp;quot; section. -- -- Calvin</body>
  </mail>
  <mail>
    <header>Compressing Images in a PDF</header>
    <date>Sat Nov 13 17:41:54 CET 2004</date>
    <body>I notice that when I draw a CGImageRef into a CGPDFContext the image data isn't compressed in any way.  This can result in a fairly large PDF file so it would be nice to able to create a PDF using compressed image data. I'm currently creating the CGImage with CGImageCreate.  If I were to create the image with a JPEG or PNG data provider would this cause the image data to be compressed in the final PDF? If I use a compressed data provider to create an image it would be good to know a little bit about what's happening under the covers before making design decisions.  What kind of data is being retained within the image object?  For example, is the image data decompressed every time the image is drawn, is an uncompressed copy of the image data being cached, or does the image hang onto a copy of both the compressed and uncompressed data? Thanks, Nick Nallick</body>
  </mail>
  <mail>
    <header>Re: CGContext to CGImage?</header>
    <date>Sat Nov 13 01:46:41 CET 2004</date>
    <body>On Nov 12, 2004, at 6:05 PM, Tomas Franzén wrote: To create a CGBitmapContext you use CGBitmapContextCreate.  Before you can do that, however you have to allocate memory for the buffer, and get ahold of a CGColorSpaceRef. CGColorSpaceRef rgbColorSpace = If you have the height and width of your desired image you can allocate memory like: unsigned rowBytes = 4 * width CGContextRef offscreenContext = CGBitmapContextCreate(imageBuffer, width, height, 8, rowBytes, rgbColorSpace, Assuming that that allocation goes OK then you can use your plot icon in CGContext using the routine you mentioned earlier. After you do that you can free up the bitmap context (unless you want to draw something else). Your image data is still stored in the imageBuffer.  Now you want to create a CGImage from the image buffer.  The first thing you'll need is a data provider. CGDataProviderRef dataProvider = CGDataProviderCreateWithData(NULL, CGImageRef cgImage = CGImageCreate(width, height, 8, 4, rowBytes, rgbColorSpace, kCGImageAlphaPremultipliedFirst, dataProvider, NULL, Now you have your CGImage and you can draw it with impunity.  With this code you need to keep the imageBuffer around for as long as the CGImage exists.  You could be a bit more clever about the way you create your data provider and use the last parameter to CGDataProviderCreateWithData to provide a callback routine to deallocate the buffer memory for you when the CGImage is destroyed. I'll leave that as an exercise for the reader. P.S.  The code in this e-mail was typed directly into e-mail so you may have to play with it to get it to compile right.  I'd add error checking as well.</body>
  </mail>
  <mail>
    <header>Re: CGContext to CGImage?</header>
    <date>Sat Nov 13 01:05:43 CET 2004</date>
    <body>Thanks for your prompt reply. :-) So what I should do is to make the CGBitmapContext and CGImage to use the same chunk of memory to store the image data? Any idea of how I would proceed implementing this? Have you, or someone else, got some sample code to demonstrate this, or maybe some tips on what functions to use and how? That'd be great. Thanks. Tomas Franzén Lighthead Software</body>
  </mail>
  <mail>
    <header>Re: vImageConvolve_PlanarF crashes with power of 2 rowbytes</header>
    <date>Sat Nov 13 00:52:59 CET 2004</date>
    <body>On Nov 12, 2004, at 4:10 PM, R. Scott Thompson wrote: This is probably unrelated, but I have an outstanding bug on file (although it seems to dropped off of Radar so maybe it's fixed) where vImageConvolve_ARGB8888 seems to be reading outside of it's source buffer.  I've found through trial and error that it will sometimes crash with an memory access violation unless I make the source buffer twice the size it should be.  I can only surmise that it's trying to read someplace it shouldn't be and sometimes this puts it into an unallocated address space causing a crash.</body>
  </mail>
  <mail>
    <header>Re: How do you display CMYKA data with Quartz?</header>
    <date>Fri Mar 23 16:45:09 CET 2007</date>
    <body>You have CMYK with alpha? If so I don't believe Quartz2D supports that natively. It supports CMYK without alpha IIRC. -Shawn</body>
  </mail>
  <mail>
    <header>How do you display CMYKA data with Quartz?</header>
    <date>Fri Mar 23 16:11:34 CET 2007</date>
    <body>I have some data stored in memory in a CMYKA format.  How can I display this to the screen with Quartz?  Do I have to convert it to RGBA?  Is there another way? Thanks, Tony _________________________________________________________________ Get a FREE Web site, company branded e-mail and more from Microsoft Office Live!</body>
  </mail>
  <mail>
    <header>Re: problem drawing transparent fill to NSImage</header>
    <date>Thu Mar 22 17:54:40 CET 2007</date>
    <body>On Mar 21, 2007, at 11:54 PM, john wrote:</body>
  </mail>
  <mail>
    <header>Re: What to do with kCGEventTapDisabledBy...?</header>
    <date>Thu Mar 22 16:21:02 CET 2007</date>
    <body>On Mar 22, 2007, at 6:54 AM, Bill Cheeseman wrote: Your callback should return the event that was passed in, or return NULL.  Do not release the event.  Returning a released event will produce the error you are reporting (as will overwriting the event's internal data, a less frequent problem). That will work just fine.   The system tracks the resources created around the callback and will release them as appropriate. The kCGEventTapDisabledByTimeout and kCGEventTapDisabledByUserInput events just carry the special event type.  The event timestamp is not set, as the event didn't actually pass through the usual event system process. When your callback returns from handling a kCGEventTapDisabledByTimeout event notification, by the way, the event tap should be re-enabled.  The notification is sent to let you know that the system determined that your tap was holding up event traffic and that events have bypassed your tap while it was unresponsive.   The notification is usually just seen while debugging an event tap.</body>
  </mail>
  <mail>
    <header>What to do with kCGEventTapDisabledBy...?</header>
    <date>Thu Mar 22 14:54:44 CET 2007</date>
    <body>What should an event tap callback function return when the triggering event type is kCGEventTapDisabledByTimeout or kCGEventTapDisabledByUserInput? I find that if I return an event of this type from the callback function, I get a system error when the system tries to post the event. If I return NULL from the callback when the event type is one of these values, everything seems to work correctly. Is this the right thing to do? (By the way, I notice that the event passed to the callback function with one of these event types appears to yield valid values for flags, location on the screen, and so on -- except that the timeout is always 0. Why doesn't one of these event types have a valid timeout value?) Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: problem drawing transparent fill to NSImage</header>
    <date>Thu Mar 22 13:25:19 CET 2007</date>
    <body>NSRectFill uses NSCompositeCopy which will blow away the pixels in the destination. Try using NSRectFillUsingOperation with NSCompositeSourceOver. Later, Guy</body>
  </mail>
  <mail>
    <header>problem drawing transparent fill to NSImage</header>
    <date>Thu Mar 22 07:54:08 CET 2007</date>
    <body>I'm drawing into NSImage by drawing another NSImage and then using NSRectFill() ot overlay a semi-transparent colour. The image is created fine if I don't draw the semi-transparent fill, but if I do then the fill covers the image and the image is not visible. - John</body>
  </mail>
  <mail>
    <header>CGEventKeyboardGetUnicodeString() count</header>
    <date>Wed Mar 21 22:26:48 CET 2007</date>
    <body>The reference document for CGEventKeyboardGetUnicodeString() says this, under &amp;quot;Discussion&amp;quot;: &amp;quot;When you call this function and specify a NULL string or a maximum string length of 0, the function still returns the actual count of Unicode But it always returns 0 for me. Is this code wrong? -- Or is it a bug in the function, or the documentation? I get the correct result (both the correct count and the string) when I do this: -- UniCharCount preCount = 10; // arbitrary buffer size returnString = [NSString stringWithCharacters:buffer Related question: is there a rule of thumb for the size of buffer I should use with this function -- to avoid having to call it twice, once to get the size and again to get the string? Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: Setting the color space for an entire PDF context/document</header>
    <date>Wed Mar 21 16:53:42 CET 2007</date>
    <body>I agree to your, more precise terminology. To be more precise about the task to be accomplished: The source PDF is in RGB colorspace with a single ICC printer profile, the same for all objects, and the destination PDF is supposed to be in RGB colorspace with a single ICC color profile, the generic RGB profile, for all objects. The Quartz-Filter is very simple to construct with the ColorSync Utility: crate a new filter, add one color management component, with the following parameters: assign profile, generic RGB profile, for all RGB-Data, to all objects. domain = PDF Workflows. If you apply the filter with ColorSync Utility to PDF with graphics, e.g. a cyan rectangle, text and a bitmap image, you see on the monitor that the bitmap graphics changes color, which is correct when you replace the profile, while the color of text and graphics stays the same. I verified with Voyeur and Acrobat, that the source PDF is RGB and there is, as far as I can tell, the same ICC color profile for all elements. When the I look at the destination PDF, or the output from the Quartz filter, I see different color profiles for different objects with Voyeur. objects. That's why I filed a bug. (radar:5074473). Thanks Thomas</body>
  </mail>
  <mail>
    <header>Re: Setting the color space for an entire PDF context/document</header>
    <date>Wed Mar 21 15:59:23 CET 2007</date>
    <body>On Mar 19, 2007, at 5:56 AM, Thomas Hartwich wrote: Before I start I should state that I may use slightly different terminology for color concepts.  I consider a color &amp;quot;space&amp;quot; to be defined by the type of components used to specify a color (e.g., RGB, CMYK, Lab, etc.), whereas a color &amp;quot;profile&amp;quot; is a transfer function to adjust your input components to output components within a color space (e.g., a ColorSync profile).  CG and PDF combine both of these concepts into the term colorspace. CGPDFContextCreate creates a CG context where drawing operations are recorded as native PDF operations.  This is considerably different than a more familiar CG bitmap context where drawing operations are converted to pixels.  When you draw vector graphics objects into any CG context the colorspace and profile have already been established for that context.  In this regard CG follows the PDF model which means the CG calls can be translated directly into PDF operations with virtually no semantic changes.  For example consider the following CG code fragment.  It loads an RGB profile, sets the context fill color to red from the loaded profile, and fills a rectangle with the resulting color.  If myContext in the code below is a PDF context the result will be a PDF file containing the color profile used for drawing and operations to set the fill color and draw a rectangle using a different syntax but the same semantics as what you see here. CMProfileRef colorProfile = LoadMyRGBColorProfile();	// e.g., sRGB, SMPTE-C, etc. CGColorSpaceRef colorSpace = CGColorSpaceCreateWithPlatformColorSpace profile used a different color space (e.g., gray or CMYK) we'd probably need a different component count Therefore, in answer to your question, CGPDFContextCreate doesn't contain a color space parameter because it can use any number of color spaces (and potentially multiple profiles within each color space).  The color used for any given drawing operation is the result of the current color space, color profile, and color components in effect at the time the drawing is performed.  Bitmap images are somewhat different because they can contain embedded color profiles. They may also be able to reference external profiles in a similar manner to the code above, but I'm a little fuzzy on the details. I don't have any experience with the filters.  Perhaps somebody else can provide feedback on that.  I would expect you'd be able to substitute one color profile for another, but probably not be able to change color spaces since they have different component counts. Nick</body>
  </mail>
  <mail>
    <header>drawHTMLTextInRect needs to have a transparent bkg...</header>
    <date>Wed Mar 21 04:13:41 CET 2007</date>
    <body>i'm using the python CoreGraphics binding to set some text onto an existing PDF document. I was originally using showTextAtPoint() to set this text, but this function has no built-in ability to wrap the text to a given width, so i wrote a little method that generates an html snippet and then uses drawHTMLTextInRect() to place the text onto the PDF. This all works very well. However, i cannot find any way to have the output of drawHTMLTextInRect() be on a transparent background (so i can see the PDF content through the whitespace of the text). I've tried explicitly setting the background in the HTML, but while i can set the background to any color, i cannot make it transparent. This is the latest snippet in question: Typewriter; font-size:24px; color:rgb(0,0,127)'&amp;gt;this is a bit of a michael geary</body>
  </mail>
  <mail>
    <header>Re: CGLayer performance problem</header>
    <date>Tue Mar 20 08:36:11 CET 2007</date>
    <body>On Mar 20, 2007, at 12:08 AM, David Duncan wrote: This is especially true if the layer is created using a nil context. Deferring CGLayer creation until I have a valid view context fixes my issue, and everything now works beautifully. Too much coding, too little sleeping.  :)  New frameworks do that to me....</body>
  </mail>
  <mail>
    <header>Re: CGLayer performance problem</header>
    <date>Tue Mar 20 08:08:22 CET 2007</date>
    <body>CGLayers are created optimized to the context that they are created from - using them with another context may not be as optimal. In general with view systems, the context that you get on any given draw event may not be the same type as you are given on another call, and if that happens the CGLayer will not necessarily be optimal. The actual implementation of a CGLayer depends on the context that it is associated with. There are many implementations of layers, as there are many types of CGContext. If you are unconcerned with potentially scaling (that is, you are willing to handle view size changes yourself) then you can instead create a CGBitmapContext to draw the background that is the size of the view, then create a CGImage to cache and draw that image. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>CGLayer performance problem</header>
    <date>Tue Mar 20 05:52:31 CET 2007</date>
    <body>I'm new-ish to Quartz, so perhaps I am misunderstanding CGLayers. Anyway: I have an NSView subclass that's animated by way of an NSTimer. Because the background image is time-consuming to draw, I decided to use a CGLayer.  It is my understanding that any drawing done into the layer context is cached (and thus, speedy to use later on in some other context).  However, I'm seeing little to no performance gain over drawing the background directly into the current CGContext. during NSView awakeFromNib: - create layer from current context - get the new layer context - set some context parameters - draw ~1000000 lines during the animation callback triggered by my NSTimer: - CGContextDrawLayerAtPoint(currentContext, CGPointZero, NULL) The animation is dog slow.  The thing is, if I reduce the number of lines drawn when creating the layer, my animation loop performance improves.  This suggests to me that CGLayer is more of a display-list type of construct.  Is this correct?  Should I be using some other Quartz context to achieve a very fast background render? Some profiling in Shark shows the my code spends most of it's time (97%) in CoreGraphics draw_line. Again, I'm new to Quartz so I suspect the problem is with my understanding.  But the Apple's documentation and _Programming with Quartz_ seem to suggest otherwise -- that the time-consuming part of drawing to the CGLayer happens once, and reuse is snappy.</body>
  </mail>
  <mail>
    <header>Re: CGGetEventTapList expected behavior?</header>
    <date>Mon Mar 19 17:57:08 CET 2007</date>
    <body>Well, I've been targeting the Finder. That explains everything, right? :) Thanks very much for your responses. I think I've got most of what I need to know about this pretty well in mind, now -- or at least some assurance that I'm on the right track. I'll try targeting some other applications to see whether they are less profligate with their window server connections. Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: CGGetEventTapList expected behavior?</header>
    <date>Mon Mar 19 16:35:35 CET 2007</date>
    <body>On Mar 19, 2007, at 4:48 AM, Bill Cheeseman wrote: CGEventTapCreateForPSN().  That's important... CGEventTapCreateForPSN() allows you to monitor and interact with all the CGEvents going to a particular process.  To do that, it needs to tap all communications channels established between that process and the window server. The window server tracks it's clients by the connections that they have established with the server.  Each connection represents a path events may take from the server to the application, and also represents ownership rights for resources allocated in the window server (windows, mostly) established using that connection. A process running on Mac OS X  may establish multiple connections.   This is done to support certain usage patterns, such as a single task or Unix process which runs multiple 'applications' within it's address space.  One could, for example, write a server process which runs multiple Java applets, each on it's own thread, with it's own windows and event streams using a connection to the window system for each thread. CGEventTapCreateForPSN() must place a tap on all connections between the process being tapped and the window server, and will route data from all these connections to the CFMachPortRef for your tap.   The tap callback function is passed a CGEventTapProxy, which is used when the callback generates new events to route them back to the appropriate communications channel. It sounds like the process that you are placing the event tap on may have established a large number of communications channels, or connections, to the window server.   That may indicate a problem with that program, as most applications only require one connection.</body>
  </mail>
  <mail>
    <header>Re: CGGetEventTapList expected behavior?</header>
    <date>Mon Mar 19 12:58:03 CET 2007</date>
    <body>My test app is a Cocoa app. It wraps a single event tap in a Cocoa class I call PFEventTap. The test app puts up a single window with 5 buttons: Install Event Tap, Remove Event Tap, Enable Event Tap, Disable Event Tap, and Log All Event Taps. It did it this way to ensure that each of my function calls is isolated in time and each function is called only once. Each of the buttons works correctly; that is, left or right clicking on a Finder icon works or doesn't work, as expected, after pushing one of the first four buttons. The Log All Event Taps button allows me to run CGGetEventTapList() at will, at any time, outputting the result as a Cocoa NSArray and logging it to console. Here's how I count and list the CGGetEventTapList() array: // Returns the count of all Event Taps installed on the system. Not all Event Taps included in the count are necessarily managed using PFEventTaps. // Covers the CGGetEventTapList function to return the count of installed Event Taps. // Returns an array of all Event Taps installed on the system. Each element of the array is a dictionary. Not all Event Taps in the array are necessarily managed using PFEventTaps. // Covers the CGGetEventTapList function to return the list of installed Event Taps. CGEventTapInformation *list = (CGEventTapInformation *)calloc(preCount, CGTableCount postCount; // should always equal preCount NSMutableArray *returnArray = [[[NSMutableArray alloc] NSDictionary *info = [NSDictionary dictionaryWithObjects:[NSArray arrayWithObjects:[NSNumber numberWithUnsignedInt:list[i].eventTapID], [NSNumber numberWithUnsignedInt:list[i].tapPoint], [NSNumber numberWithUnsignedInt:list[i].options], [NSNumber numberWithUnsignedLongLong:list[i].eventsOfInterest], [NSNumber numberWithInt:list[i].tappingProcess], [NSNumber numberWithInt:list[i].processBeingTapped], [NSNumber numberWithBool:list[i].enabled], [NSNumber numberWithFloat:list[i].minUsecLatency], [NSNumber numberWithFloat:list[i].avgUsecLatency], [NSNumber numberWithFloat:list[i].maxUsecLatency], nil] forKeys:[NSArray arrayWithObjects:@&amp;quot;eventTapID&amp;quot;, @&amp;quot;tapPoint&amp;quot;, @&amp;quot;options&amp;quot;, @&amp;quot;eventsOfInterest&amp;quot;, @&amp;quot;tappingProcess&amp;quot;, @&amp;quot;processBeingTapped&amp;quot;, @&amp;quot;enabled&amp;quot;, return nil; // error Here's how I install an event tap, after allocating one of my PFEventTap objects: - (id)initWithPid:(int)pid eventMask:(CGEventMask)mask appendAtTail:(BOOL)tailPlacement listenOnly:(BOOL)listenOnlyOption notificationDelegate:(id)delegate callbackSelector:(SEL)callback // Initializes a newly created PFEventTap object using the target application's process ID. The target application must be running. // Covers the CGEventTapCreateForPSN function. Key up and down events are available only if the &amp;quot;Enable access for assistive devices&amp;quot; setting in the Universal Access pane of System Preferences is turned on. // This method saves a pointer to the receiver and the incoming contextInfo (if any) in the CGEventTapCreateForPSN function's refcon parameter. When the system's event tap mechanism calls the callback function, it retrieves them from its refcon parameter and uses them, along with the information passed by the system in the callback function's other parameters, to create an NSInvocation object. It then uses the invocation object to invoke the client's Objective-C callback selector. The callback function obtains the receiver's notificationDelegate and callbackSelector from the receiver as it was saved in the CGEventTapCreateForPSN function's refcon parameter upon initialization here. This mechanism allows the client's callback selector to be written as an Objective-C method instead of a C function. // Use the -[PFEventTapManager eventMaskByAddingType:toMask:] utility method to build a mask by adding types. // Set up parameters for the CGEventTapCreateForPSN function call. CGEventTapPlacement placement = (tailPlacement) ? CGEventTapOptions option = (listenOnlyOption) ? kCGEventTapOptionListenOnly : 0x00000000; // the kCGEventTapOptionDefault constant is missing from CGEvent.h in Mac OS X 10.4 Tiger // Set up refcon parameter for the CGEventTapCreateForPSN function call. NSMutableDictionary *tempRefcon = [[NSMutableDictionary alloc] if (contextInfo) [tempRefcon setObject:contextInfo // Create and install the event tap. machPortRef = CGEventTapCreateForPSN(&amp;amp;psn, placement, option, mask, // Create the run loop source. eventSourceRef = CFMachPortCreateRunLoopSource(NULL, // Add it to the run loop. CFRunLoopAddSource(CFRunLoopGetCurrent(), eventSourceRef, // Set instance variables. notificationDelegate = [delegate retain]; // private callbackSelector = callback; // private registeredEventTapLocation = 3; // there is no constant for the case where the Event Type is created for a specific target application, but the system appears to assign 3 (which is the next available constant slot in the CGEventTapLocation enum). NSLog(@&amp;quot;\n\tclient path = %@&amp;quot;, [[NSBundle mainBundle] NSLog(@&amp;quot;\n\tclient pid = %@&amp;quot;, [NSNumber clientPid = [[NSNumber numberWithInt:[PFEventTap // The pendingEvent iVar is reinitialized in the callback function every time this Event Tap is triggered. //#ifdef PF_DEBUG NSLog(@&amp;quot;PreFab PFEventTap -initWithPid:eventType:appendAtTail:listenOnly:notificationDelegate:callback Selector:contextInfo: method failed to add run loop source for pid %d.&amp;quot;, //#endif //#ifdef PF_DEBUG NSLog(@&amp;quot;PreFab PFEventTap -initWithPid:eventType:appendAtTail:listenOnly:notificationDelegate:callback //#endif Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Setting the color space for an entire PDF context/document</header>
    <date>Mon Mar 19 12:56:02 CET 2007</date>
    <body>Hi, I would like to open an existing PDF document which has the same RGB color space for all objects in the document and write the contents out to an other PDF document into one other color space. I do not want to change the pixels or objects in the original document, just a change of the color space tag(s) for all of them. I read some documentation around creating PDF documents and found the function CGPDFContextCreate(), where I can set the mediabox and auxiliaryInfo. I also read the example in &amp;quot;Programming with Quartz&amp;quot;,p450. But no parameter to specify the color space for the destination document, unlike CGBitmapContext. I tried using a quartz filter, in the ColorSync Utility and using the python bindings, and assign the generic RGB profile to &amp;quot;allObjects&amp;quot; but that failed to change text and graphics objects. Is it a bug or a feature? I do not want to render to a CGBitmapContext, because I want to keep font and graphics information. I am out of ideas at that point and would appreciate any new hints and comments. Thanks in advance Thomas</body>
  </mail>
  <mail>
    <header>drawing a CIImage instead of rendering an openGL texture</header>
    <date>Mon Mar 19 12:54:47 CET 2007</date>
    <body>I've got some code that is based on the QTQuartzPlayer example, where a decompression session is initialized with ICMDecompressionSessionCreateForVisualContext, and then new frames are retrieved from the openGL context. I would like to optionally filter the output, before rendering it. Converting the frame to a CIImage, and filtering it is fine. If I I save off the filtered image, the output looks good. [mCIContext drawImage:highlightedImage atPoint:CGPointMake((int)(([self frame].size.width - imageRect.size.width) * 0.5), (int)(([self frame].size.height - imageRect.size.height) * 0.5)) // use integer coordinates to avoid interpolation Where mCIContext was initialised with the same openGLContext as the view. This doesn't seem to draw my filter output - instead my video just becomes really jerky. Can anyone give me any hints on how to render my CIImage correctly in place of the original frame? cheers, Martin</body>
  </mail>
  <mail>
    <header>Re: CGGetEventTapList expected behavior?</header>
    <date>Mon Mar 19 12:48:15 CET 2007</date>
    <body>Thanks, that was very helpful. I'm using CGEventTapCreateForPSN(), not CGEventTapCreate(). The debugger confirms that I am only calling it once. Nevertheless, whether immediately or long after creating the event tap, CGGetEventTapList() consistently shows me a large number of event taps, all identical except for the ID. Today, it's showing me 45 of them, including the one the system installs. Quitting a couple of other apps then running my test app again doesn't change this number, but from work session to work session it does seem to change somewhat, for no reason I can deduce. When I perform the indicated event (left or right click on a Finder icon), my callback function is called only once. When I call CGEventTapEnable() to disable the machPortRef, then log the CGGetEventTapList() function result again, I get the same number of event taps (45, currently). All of them now show that they are disabled. When I run your code to invalidate and release the machPortRef, then log CGGetEventTapList() again, all the event taps except the one installed by the system are gone. I have to conclude that it is a bug, probably in CGEventTapCreateForPSN(). Could there be any other explanation? Of course, these things usually turn out to be my own bug, after all, so I'll post some of my code in the next message, just in case somebody has the time to skim through it. Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: CGGetEventTapList expected behavior?</header>
    <date>Sun Mar 18 23:37:42 CET 2007</date>
    <body>There is a dependency between the CFRunLoopSource and the CFMachPortRef that produces an 'extra' retaincount on the CFRunLoopSource.   Before releasing the runloop source and the CFMachPortRef, call CFMachPortInvalidate() on the port.   This will switch off the event tap, and invalidate both the runloop source and the port.  It also drops the retain count on the source and the port by one, so the CFRelease will release the objects and not leak. // Stop! // Invalidate the CFMachPort, switching off the event tap // Remove the runloop source for the tap from the runloop CFRunLoopRemoveSource(CFRunLoopGetCurrent(), // Release the removed runloop source //Finally, release the event tap port. If your event taps are passive listeners, then no, other than consuming precious memory and kernel resources, there is no effect. If each tap is an active filter, then it will 'clog' the system's event flow until some internal safety mechanisms determine that there is a malfunction and disable it.  (It's re-enabled should it start responding again.) The 'stop' code above should shut down an event tap. From your previous mail: You may want to check to see if your code is being invoked multiple times.  35 identical event taps except for the ID would indicate that there are in fact 35 event taps. Each call made to CGEventTapCreate() will create one event tap, as reported by CGGetEventTapList().    Try using a debugger to break on calls to CGEventTapCreate(), to see if you are getting multiple calls in your app.</body>
  </mail>
  <mail>
    <header>Re: CGGetEventTapList expected behavior?</header>
    <date>Sun Mar 18 21:08:44 CET 2007</date>
    <body>I think I may have already answered some of my own questions, but I would appreciate confirmation: If I call CFMachPortInvalidate and CFRunLoopSourceInvalidate before releasing them, the CGGetEventTapList function no longer reports that they exist. I'm a novice at mach ports and run loop sources. Is this the correct technique? See no. 2, above. I'm still curious to know the answer, but no. 2, above, is easy enough to do, just in case. Subparts (2) and (3) apparently should be replaced by no. 2, above. Bill Cheeseman</body>
  </mail>
  <mail>
    <header>CGGetEventTapList expected behavior?</header>
    <date>Sun Mar 18 20:02:09 CET 2007</date>
    <body>Using a simple test application, I push a button to install an event tap using CGEventTapCreateForPSN to block the left and right mousedown events in the Finder. It works: clicking an icon on the desktop in the Finder does nothing. I push another button in my test application to disable the event tap, and clicks in the Finder work again. So far, so good. The trouble begins when I list the installed event taps. Before my test app quits, it calls CGGetEventTapList twice, once to log the count of installed event taps and again to log the list of installed event taps. These calls to CGGetEventTapList show that my test app installed 35 new event taps (all of them identical except for the ID), not the one event tap I expected. (There is also one event tap that the system apparently installed at boot time.) I've tried everything I can think of to remove these event taps, but they continue to be listed by CGGetEventTapList as long as my test app is still running. In fact, if I click the button to install the event tap again, I get 35 more event taps listed by CGGetEventTapList; push the button again, and I'm up to 115 installed event taps, and so on. When I run my test application again, two calls to CGGetEventTapList at the beginning show only the single event tap apparently installed by the system. I guess terminating the process automatically removes event taps, at least if they were installed by CGEventTapCreateForPSN. All this leads to several questions: 1. Is it expected that CGGetEventTapList would report 35 new event taps when I installed only one? Why? 2. Is it expected that CGGetEventTapList would still report the new event taps after I release the machPortRef and the eventSourceRef? Why? 3. Is there no way to remove event taps short of terminating the process? 4. Does it matter (i.e., do these listed event taps take up memory or slow the system or the process down)? 5. What is the recommended strategy? (I'm guessing: (1) never create the same event tap twice; (2) rely on terminating the process to remove all event taps for that process; (3) don't release the machPortRef and the eventsourceRef in the belief that it will remove the event taps, because it won't; (4) to turn an event tap off temporarily, just disable it, and re-enable it again later if it's needed again. But I'm only guessing.) Bill Cheeseman</body>
  </mail>
  <mail>
    <header>PDF -&amp;gt; BitmapContext: Howto avoid antialiasing artifacts without	loosing text antialiasing</header>
    <date>Fri Mar 16 15:07:00 CET 2007</date>
    <body>I am running into the problem that I get antialiasing artifacts when rendering a PDF page to a CGBitmapContext. I can disable antialiasing for the context, but then I get ugly and unreadable text. The artifacts I get are thin lines of boxes, which probably represent pdf objects and are clearly a result of antialiasing. I have checked the pdf documents with acrobat and the artifacts are also visible unless I turn of antialiasing for line art in the acrobat prefs. So my question is, is there a way to disable antialiasing for a context but keep the text antialiasing? Just the same way I can do with acrobat? Dominik</body>
  </mail>
  <mail>
    <header>Re: Completely work in the video card</header>
    <date>Fri Mar 16 05:14:23 CET 2007</date>
    <body>On Mar 14, 2007, at 5:40 AM, Mark Munte wrote: Beware, though, CIImage has no way to 'retain' the texture for you.  So, you will have to manually track the texture name and CIImage and not delete the texture until the last rendering operation using the CIImage is done. Additionally, the behavior is undefined if you modify the texture after giving it to the CIImage, so don't do that :)</body>
  </mail>
  <mail>
    <header>Re: Completely work in the video card</header>
    <date>Wed Mar 14 18:54:59 CET 2007</date>
    <body>Well, that just depends on how you set up OpenGL. If you do it all correctly, the CIFilters will be applied by the GPU. If the GPU runs out of resources, Open L / Core Image might fall back to an software renderer, I'm not sure how one would detect that, besides dramatic drop of framerate/increae of cpu :) I believe it is possible to disable the fallback using OpenGL pixel attributes (look for NSOpenGLPFAAccelerated and NSOpenGLPFANoRecovery), but I'm not sure. There are some good samples from Apple on OpenGL and texture uploads. Once you have OpenGL setup, applying CIFilters is really easy.</body>
  </mail>
  <mail>
    <header>Re: Completely work in the video card</header>
    <date>Wed Mar 14 17:49:44 CET 2007</date>
    <body>On 3/14/07, Yes: create the CIImage from an OpenGL texture. That means, take your buffer from main memory and upload it as a texture to an OpenGL context. Create your CIContext bound to the same OpenGL context, then create your CIImage from the texture and simply draw it to the GLContext&amp;#39;s</body>
  </mail>
  <mail>
    <header>Re: Completely work in the video card</header>
    <date>Wed Mar 14 13:40:31 CET 2007</date>
    <body>Yes: create the CIImage from an OpenGL texture. That means, take your buffer from main memory and upload it as a texture to an OpenGL context. Create your CIContext bound to the same OpenGL context, then create your CIImage from the texture and simply draw it to the GLContext's NSOpenGLView. For best performance, it is important to have the buffer in an optimal format, so you avoid any swizzling from the OpenGL driver when the data is uploaded. There are lots of old posts on this on the OpenGL list, take a look there.  For even better performance upload the buffer to the texture using OpenGL Pixel Buffer Objects (PBO).</body>
  </mail>
  <mail>
    <header>Re: ImageIO and read-only Exif tags</header>
    <date>Wed Mar 14 13:03:17 CET 2007</date>
    <body>On Mon, 5 Feb 2007 18:13:36 +0100, I'm very pleased to notice that this particular problem with GPS tags not being saved to JPEG files has been resolved in Mac OS X 10.4.9.  Way to go, Apple. -- marco -- It's not the data universe only, it's human conversation. They want to turn it into a one-way flow that they have entirely monetized. I look at the collective human mind as a kind of ecosystem. They want to clear cut it. They want to go into the rainforest of human thought and mow the thing down.</body>
  </mail>
  <mail>
    <header>Completely work in the video card</header>
    <date>Wed Mar 14 20:31:30 CET 2007</date>
    <body>Is there any way to create a CIImage object directly from the video card buffer? More precisely, I mean, if someone want to use the core image filter to some image which is currently in the video buffer to create some ripple/copy machine/swap efforts, etc, one of the way is first to read the image in the video buffer to the memory, then use some API to create the CIImage object, pass the object as the parameter to the CIFilter object, Then show it. 1.there are some data in the memory presenting a picture 2.copy the data to the main memory (in contract to the video card memory) 3.to use the data above to create the CIImage object. 4.create the CIFilter using the CIImage 5.get the result image processed by the CIFilter 6.display the result image You can see if the resolution of the screen is very big, (for example of me, mine is 2048 x1152), you will suffer the performance degrade for you have to move the huge data from the video to the memory then back. Anyone knows how to create the CIImage directly from the video card buffer? Thanks for reply.</body>
  </mail>
  <mail>
    <header>Re: Quartz-dev Digest, Vol 4, Issue 46</header>
    <date>Wed Mar 14 06:15:52 CET 2007</date>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Problem: 16-bit TIFF's and Image IO with	Float	CGBitmapContextCreate</header>
    <date>Wed Mar 14 04:24:13 CET 2007</date>
    <body>I finally solved my issue by reading TIFF's with QuickTime. similar to this example:</body>
  </mail>
  <mail>
    <header>Re: CGEventTapInformation</header>
    <date>Tue Mar 13 22:27:29 CET 2007</date>
    <body>Well, yes, I can see how it would be useful to Apple to have an internal tracking number. But you expose it publicly in the CGEventTapInformation, which suggests that you think there may be some external utility in it, too. But it isn't made available to me when I create an event tap, so it isn't easy for a developer to use in any way that seems obvious to me. I guess you could consider this a feature suggestion, but I'm hardly far enough into event taps to appreciate all the nuances. Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: Removing event taps</header>
    <date>Tue Mar 13 22:27:07 CET 2007</date>
    <body>I'm now releasing the machPortRef explicitly when my app quits normally. The accumulation of old event taps seems to be a result of hitting the Terminate button in Xcode, instead of quitting my test app normally. When I reboot, the excess event taps go away, of course. Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: CGEventTapInformation</header>
    <date>Tue Mar 13 21:53:15 CET 2007</date>
    <body>On Mar 13, 2007, at 8:57 AM, Bill Cheeseman wrote: That is an internal tracking value which will persist as long as your event tap persists. When you create an event tap, you can provide a (void *) 'refcon' referring to any identifying data you want to associate with a particular tap, such as a data structure to establish a filtering context: CG_EXTERN CFMachPortRef CGEventTapCreate(CGEventTapLocation tap, CGEventTapPlacement place, CGEventTapOptions options, CGEventMask eventsOfInterest, CGEventTapCallBack callback, The callback prototype shows that this 'refcon' is passed to the callback function: typedef CGEventRef (*CGEventTapCallBack)(CGEventTapProxy proxy, Use this 'refcon' to hold any data related to your event tap, such as identifying information and state.</body>
  </mail>
  <mail>
    <header>Re: Removing event taps</header>
    <date>Tue Mar 13 21:43:34 CET 2007</date>
    <body>On Mar 13, 2007, at 9:02 AM, Bill Cheeseman wrote: Your tapping process may not have exited, but instead may be lingering around in an odd state.  When a process that has placed an event tap does actually exit, the window server gets a port death notification for the Mach port associated with the event tap, and will remove any remaining references to the tap.</body>
  </mail>
  <mail>
    <header>CGEventTapInformation</header>
    <date>Tue Mar 13 16:57:49 CET 2007</date>
    <body>I have a specific question about Event Taps (see Quartz Event Services): The CGEventTapInformation data type, a struct, includes a uint32_t field called &amp;quot;eventTapID&amp;quot;. What is this supposed to be used for? The reason I ask is that there doesn't seem to be any way to get an Event Tap's ID when I create it, but only when I get a list of all Event Taps already installed. Thus, there doesn't seem to be any way to identify which Event Tap in the list of installed Event Taps is mine (except by examining the tappingProcess pid, and then I'm not sure exactly how I would distinguish among several Event Taps that I installed). -- PLEASE NOTE MY NEW E-MAIL ADDRESS: Bill Cheeseman - email@hidden</body>
  </mail>
  <mail>
    <header>Removing event taps</header>
    <date>Tue Mar 13 17:02:14 CET 2007</date>
    <body>After experimenting for a couple of days, I see that there are over 100 event taps installed on my computer, most of them bearing my test application's pid. How do I remove them? I'm guessing that I should have removed them before quitting each session with my test app, by releasing the machPortRef that I keep around after creating and installing an event tap. But how do I deal with them if I neglected to do that or if, say, my test app crashed? -- PLEASE NOTE MY NEW E-MAIL ADDRESS: Bill Cheeseman - email@hidden</body>
  </mail>
  <mail>
    <header>Event Taps and CGEventRef in Cocoa</header>
    <date>Tue Mar 13 16:52:44 CET 2007</date>
    <body>I'm playing around with Event Taps and CGEventRef, for use in a Cocoa application. Initially, I thought I would like to convert back and forth between CGEventRef and NSEvent for convenience in manipulating flags and the like. They don't seem to be toll-free bridged, so I assume I would have to create lookalike events the hard way. Is this a bad idea? Are there any special issues I should be aware of? (After a couple of days, I'm starting to think that CGEventRef has enough special features that I should forget about NSEvent in this context. But I'm still curious about thoughts anyone might have.) -- PLEASE NOTE MY NEW E-MAIL ADDRESS: Bill Cheeseman - email@hidden</body>
  </mail>
  <mail>
    <header>Re: Capture drawing commands</header>
    <date>Tue Mar 13 01:19:05 CET 2007</date>
    <body>It might also be nice to be able to catch it for SQA regression testing, just as in ye olden days we would capture the serial instructions to graphics terminals. ____________________________________________________________________________________ TV dinner still cooling? Check out &amp;quot;Tonight's Picks&amp;quot; on Yahoo! TV.</body>
  </mail>
  <mail>
    <header>Transferring a CIImage to another machine for display</header>
    <date>Mon Mar 12 20:58:25 CET 2007</date>
    <body>I have a CIImage that was obtained from an sequence grabber. I would like to encapsulate this in an NSData object, and send this over the network to another computer, for display. However, when I unarchive the NSData on the other side, and either try and render the CIImage into an NSCoreImageView, or to convert it into an NSImage, I get a crash. I know that the original CIImage is fine, because I can render it locally with no problem. If I convert the CIImage into a NSImage before sending, and then archive and despatch the NSImage, that works fine (but sucks CPU, and seems to leak memory). If not, is there any efficient way to convert from the CIImage to a portable representation. I'm just going to be displaying it in an NSImageView or NSOpenGLView at the other end. cheers, m.</body>
  </mail>
  <mail>
    <header>Re: Capture drawing commands</header>
    <date>Mon Mar 12 18:58:15 CET 2007</date>
    <body>On Mar 12, 2007, at 9:33 AM, Josianne Grima wrote: If it were possible to capture all drawing commands, then in theory it would be faster, but it's not possible to capture all drawing commands (for the reasons I gave earlier), so your only way of doing this is to just capture the changes as they appear onscreen.</body>
  </mail>
  <mail>
    <header>Re: Capture drawing commands</header>
    <date>Mon Mar 12 17:33:12 CET 2007</date>
    <body>Thanks for the clarification. Then as I mentioned before I also would like to try to manipulate these commands so as to display them with some customized effects -- just for the fun of it :P jo On 3/12/07, There are a variety of ways that an app can get output to the screen, and there&amp;#39;s no architecture on Mac OS X that would let you capture all of them. For example: - an app could use the QuickDraw API, which draws directly to offscreen window buffers managed by the window server - an app could use the Quartz 2D API, which (depending on the implementation) might draw directly to the window buffer, or might offload drawing operations to the GPU - an app could draw directly to its window buffers itself - an app could use OpenGL directly and not modify its window buffers at all David mentioned OpenGL because in the first three cases, the window server eventually uses OpenGL to push the window buffers into the video card, so it&amp;#39;s a little bit more a central bottleneck. Detecting and intercepting all of these cases would be difficult, if not impossible. So to give any useful recommendation, we really need to know in more detail what you want to accomplish. What&amp;#39;s the actual product that you&amp;#39;re trying to build here? What is the end-user value of capturing drawing commands? -eric</body>
  </mail>
  <mail>
    <header>Re: Capture drawing commands</header>
    <date>Mon Mar 12 16:53:56 CET 2007</date>
    <body>On Mar 12, 2007, at 8:46 AM, Josianne Grima wrote: There are a variety of ways that an app can get output to the screen, and there's no architecture on Mac OS X that would let you capture all of them. For example: - an app could use the QuickDraw API, which draws directly to offscreen window buffers managed by the window server - an app could use the Quartz 2D API, which (depending on the implementation) might draw directly to the window buffer, or might offload drawing operations to the GPU - an app could draw directly to its window buffers itself - an app could use OpenGL directly and not modify its window buffers at all David mentioned OpenGL because in the first three cases, the window server eventually uses OpenGL to push the window buffers into the video card, so it's a little bit more a central bottleneck. Detecting and intercepting all of these cases would be difficult, if not impossible. So to give any useful recommendation, we really need to know in more detail what you want to accomplish. What's the actual product that you're trying to build here? What is the end-user value of capturing drawing commands?</body>
  </mail>
  <mail>
    <header>Re: Capture drawing commands</header>
    <date>Mon Mar 12 16:46:10 CET 2007</date>
    <body>Hi David, I need to capture the actual drawing commands so as to then be able to manipulate them. Why did you mention only OpenGL commands?&amp;nbsp;&amp;nbsp; Can&amp;#39;t applications also be written using other APIs such as Quartz 2D? Jo</body>
  </mail>
  <mail>
    <header>CGImageSource profile confusion</header>
    <date>Mon Mar 12 11:03:03 CET 2007</date>
    <body>Basically I'm trying to get the datas coming from a camera raw files. To retain maximum dynamics, it appears that I have to create a tiff file from the camera raw file programmatically, then parse the tiff file manually. But... When I get the dictionary properties of the Image source, I get the following : - the exif colorspace tells the data colorspace is sRGB - the profile name tells the profile name is Adobe RGB 98 When I create the CGImageRef from the source, the colorspace ends up in adobe rgb. So what is strange here is that the color space is wrong, that is the data contained in the raw files is almost linear to the XYZ space, and not gamma corrected. However, the resulting image is &amp;quot;right&amp;quot;, so it seems that there is an implicit profiling from some space I don't have control to, to the adobe rgb 98 color space. But here what I would like to do is to have control on the input color space, and control of the output color space. Is there a way to do this at all with CG ? Raphael</body>
  </mail>
  <mail>
    <header>Re: Creating a CGImage with a blurred edge</header>
    <date>Wed Nov 29 02:49:13 CET 2006</date>
    <body>CGRect rect = CGRectMake(0, 0, [self extent].size.width, [self [self getShapeContext];		// gets a cached context for this object [self drawShape];			// draws this object into the context image = createImageFromContext(_context, rect.size.width, rect.size.height, TRUE, TRUE);  // convert to CGImageRef initialImage = [CIImage imageWithCGImage:image];									  // convert to CIImage CIContext *c = [CIContext contextWithCGContext:_context options: nil];					  // create CIContext for filter to operate in CIFilter *f = [CIFilter filterWithName:@&amp;quot;CIGaussianBlur&amp;quot;];								  // create filter [f setValue:[NSNumber numberWithFloat:_edgeWidth] I may then composite the blurred result with the initialImage to have only the edge blurred and not the entire shape, but I'll play around with it a bit first to see how the various shapes work when blurred. Thanks again for your reply.  I knew there had to be a simpler method to achieve what I wanted.</body>
  </mail>
  <mail>
    <header>Re: Converting Postscript File to PDF</header>
    <date>Wed Nov 29 01:58:21 CET 2006</date>
    <body>This sounds like you are not respecting the /Rotate key for each page. The Rotate key specifies the degrees of clockwise rotation you should apply prior to displaying the page. If you were to use PDFKit to display your PDF documents I would expect you to get the same results as Preview. If you are using CoreGraphics directly then you need to obtain the value for the Rotation for each page and transform the context appropriately prior to drawing. The &amp;quot;Programming with Quartz Book&amp;quot; by Bunny Laden and me has discussion and code examples for handling this, going back to 10.0. Hope this helps, David On Nov 28, 2006, at 4:45 PM, Gerry Beggs wrote:</body>
  </mail>
  <mail>
    <header>Re: Converting Postscript File to PDF</header>
    <date>Wed Nov 29 01:54:23 CET 2006</date>
    <body>On Nov 28, 2006, at 5:45 PM, Gerry Beggs wrote: How does the page look when converted by Preview?  If Preview is getting it right it probably means you need to draw the resulting PDF using the transform returned from CGPDFPageGetDrawingTransform(). Nick</body>
  </mail>
  <mail>
    <header>Converting Postscript File to PDF</header>
    <date>Wed Nov 29 01:45:25 CET 2006</date>
    <body>I'm trying to support reading Postscript files in my program. I use Quartz's ability to convert Postscript to PDF so that I can display the Postscript file. I'm having a problem converting some Postscript files to PDF. The converted PDF file gets rotated 90 degrees in my program. However, if I open the file in Preview, it displays correctly. Postscript files generated by by the system through the Print dialog window (Save PDF as Postscript) convert and display correctly in my program. I haven't tried this with other sources, but the postscript files I'm having problems with are generated by Adobe Pagemaker 6.5.1. Any ideas what I might be doing wrong here? Why would Preview display it correctly, but my program doesn't? I don't do any rotation transformations on the graphics context. -- email@hidden</body>
  </mail>
  <mail>
    <header>Re: Creating a CGImage with a blurred edge</header>
    <date>Tue Nov 28 15:35:23 CET 2006</date>
    <body>On Nov 27, 2006, at 8:14 PM, Chip Coons wrote: One way to do this in Tiger would be to create an image containing a copy of the shape in white on a black background and perform a gaussian blur on it.  Then use the blurred image as a clipping mask via CGContextClipToMask().  You could also do something similar in CoreImage. Nick</body>
  </mail>
  <mail>
    <header>Re: Exporting 16-bit-per-component files from CoreImage</header>
    <date>Tue Nov 28 09:17:55 CET 2006</date>
    <body>As I reported there: I managed once to get 16 bits output from CoreImage, but I had to resort to a private method.</body>
  </mail>
  <mail>
    <header>Re: Exporting 16-bit-per-component files from CoreImage</header>
    <date>Tue Nov 28 07:10:12 CET 2006</date>
    <body>I think that you are out of luck.  I say this because I'm out of luck. The tack that I took was to give CoreImage floating point data (made fp from 16-bbp) from the start.  As you know, a floating point image can be any color depth.  So I didn't give CoreImage a chance to even guess.  But guess it did.  It truncated the range of values in the histogram from black to white.  When I converted back to 16bpp from the floating point, there were many gaps in the histogram. My methodology was as follows: -&amp;gt;  convert 16 image data to floating point CGDataProviderCreateWithData CGImageCreate -&amp;gt; make a ciimge CIImage	*cImg = [CIImage imageWithCGImage: cgImg options: ciOptionsDict] -&amp;gt; do some coreimage stuff -&amp;gt; allocate some memory for rendering the ciimage (context) CGColorSpaceRef colorSpaceRef = CGColorSpaceCreateWithName(kCGColorSpaceGenericGray); //single channel image CGContextRef outputCGImageContext = CGBitmapContextCreate(outputData, width, height, bitsPerComponent, bytesPerRow, colorSpaceRef, kCGBitmapByteOrder32Host | kCGImageAlphaNone | NSDictionary *outputContextOptions = [NSDictionary dictionaryWithObjectsAndKeys: [NSNumber numberWithBool: NO], kCIContextUseSoftwareRenderer, colorSpaceRef, CIContext *ciContext = [CIContext contextWithCGContext: [ciContext drawImage: outImg atPoint: CGPointZero I get float data, but it has been truncated as I mentioned.  So therefore it is no good to me.  I post this because maybe you can see my error or just give you a few more resources to experiment with. -- ---- Bruce Johnson email@hidden</body>
  </mail>
  <mail>
    <header>Creating a CGImage with a blurred edge</header>
    <date>Tue Nov 28 04:14:38 CET 2006</date>
    <body>I'm trying to blur the edge of an arbitrary shape drawn using Quartz.  Ideally, the edge would have a gaussian fall-off in color/ alpha, with the width of the blur being defined by the user. I've got a rudimentary solution, scaling the CGContext down as I progressively increase the alpha component of the drawing, but I can't help but think there is a better method of solving this. Any thoughts or suggestions would be appreciated.</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate and its data</header>
    <date>Mon Nov 27 20:39:42 CET 2006</date>
    <body>On Nov 27, 2006, at 12:19 PM, Steve Green wrote: IMO, the bitmap context is inconsistent with the rest of CG in this regard.  For example, there's no way to provide a data deallocator the way you can with a data provider.  The way I work around this is to create a &amp;quot;bitmap release&amp;quot; wrapper function that checks the retain count of the bitmap context before releasing it.  If the retain count is one I get the bitmap's data pointer and dispose of it.  The limitation to this is that you have to assume a particular allocation scheme is always used to create and dispose the buffer (e.g., malloc and free). Nick</body>
  </mail>
  <mail>
    <header>CGBitmapContextCreate and its data</header>
    <date>Mon Nov 27 20:19:37 CET 2006</date>
    <body>I've been using CGBitmapContextCreate for several days now and I would just like to confirm what I think I know about the data pointer.  It's quite clear from the documentation that the data pointer is not owned by the context.  That design decision works well in most cases however it's causing a problem in my specific instance. Generally speaking I should be able to create an object, pass it to some other code and release the object.  The other code might have retained the object, and I shouldn't have to care.  If it did retain the object, I would be effectively transferring ownership of the object to that other code.  As it pertains to bitmap contexts, I am being forced to care.  Who free()s the data, and when?  Even worse, the other code in question only has knowledge of CGContextRef.  It's never heard of bitmap contexts. Since I control both halves of the equation in my scenario, I can easily code around the problem but it's ugly to do so.  What (if any) are the possible workarounds for this problem? I've considered messing with the allocator context on the bitmap context to also free the bitmap data but I don't see any supported way to do that.  I've also investigated &amp;quot;deriving&amp;quot; from CGBitmapContext but I don't see how to do that either.  Any thoughts? ~S</body>
  </mail>
  <mail>
    <header>Re: Most efficient way to move images between processes</header>
    <date>Thu Nov 23 01:35:25 CET 2006</date>
    <body>And in fact it was that easy.  Not only did it work, but it worked the first time ;)</body>
  </mail>
  <mail>
    <header>Re: Most efficient way to move images between processes</header>
    <date>Wed Nov 22 17:30:20 CET 2006</date>
    <body>Thanks for confirming my suspicions.  As for mutex waiting, that's exactly what I am doing via a posix semaphore.  I am open to any better ideas in that arena too, but it's off topic and probably not appropriate for this list.  Please feel free to contact me directly. ~S</body>
  </mail>
  <mail>
    <header>Re: Most efficient way to move images between processes</header>
    <date>Wed Nov 22 16:52:55 CET 2006</date>
    <body>On Nov 22, 2006, at 7:02 AM, Steve Green wrote: You might try looking at the Cocoa distributed objects architecture. In particular the classes NSConnection and NSDistantObject.</body>
  </mail>
  <mail>
    <header>Re: Most efficient way to move images between processes</header>
    <date>Wed Nov 22 16:27:47 CET 2006</date>
    <body>Hi, You definetly do not want to pass PNG, compression (and decompression) does waste way too much time. If you avoid compression, TIFF could be ok - but something more low-level definetly wastes less time. For high data rates a shared memory architecture is definetly a pro but remeber to use some form of notification without latency (but no busy waiting on some mutex). Yours, -- René Rebe - ExactCODE - Berlin (Europe / Germany) +49 (0)30 / 255 897 45</body>
  </mail>
  <mail>
    <header>Re: Most efficient way to move images between processes</header>
    <date>Wed Nov 22 16:18:49 CET 2006</date>
    <body>On 22 Nov 2006, at 15:02, Steve Green wrote: I know nothing about shared memory, but you can certainly create a CGBitmapContext to draw into a block of memory you've allocated and a CGImageRef which reads its data from a block of memory, so it should be easy :-)</body>
  </mail>
  <mail>
    <header>Most efficient way to move images between processes</header>
    <date>Wed Nov 22 16:02:21 CET 2006</date>
    <body>I am writing an application that manipulates images in a background application.  I have a client app that can attach and optionally observe the images in near real time.  My question is this.. what is the most efficient way to architect this? My initial idea is something like this.  Get each image as TIFF data and write the data into shared memory.  The client app could easily reconstruct a TIFF image from TIFF data and display it.  Maybe PNG is better?  While that will probably work fine, I can't help but think that there's a much better way.  In my ideal world, I would be able to construct a graphics context such that the background app draws directly into shared memory.  Then on the client side, somehow create an image from that shared memory and display it with. Any pointers, suggestions, or even words of encouragement would be greatly appreciated. Thank you..</body>
  </mail>
  <mail>
    <header>Re: Quartz-dev Digest, Vol 3, Issue 249</header>
    <date>Sun Nov 19 08:55:57 CET 2006</date>
    <body>How can I do that without CGS? Message: 2 Date: Fri, 17 Nov 2006 23:45:27 -0800 Subject: Re: Quartz-dev Digest, Vol 3, Issue 248 Cc: email@hidden 10.4 because But why do you need the cursor's image? Most of the time you just need to set the cursor to whatever you want it to be and don't really care about the image that is actually used. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan -- Cotton Chen</body>
  </mail>
  <mail>
    <header>Re: Re: Quartz-dev Digest, Vol 3, Issue 248</header>
    <date>Sat Nov 18 08:48:52 CET 2006</date>
    <body>Efficient&amp;nbsp;screen&amp;nbsp;casting&amp;nbsp;is&amp;nbsp;one&amp;nbsp;reason&amp;nbsp;you&amp;nbsp;may&amp;nbsp;want&amp;nbsp;to&amp;nbsp;know&amp;nbsp;the&amp;nbsp;current&amp;nbsp;cursor&amp;nbsp;image.-Shawn David Duncan &amp;nbsp;&amp;nbsp;&amp;nbsp; I want to get the cursor's image, and have read lot's of discussion in ADC's mailing list. &amp;nbsp;&amp;nbsp;&amp;nbsp; Is there any API could accomplish the the purpose desired? I mean to get the mouse cursor's image. &amp;nbsp;&amp;nbsp;&amp;nbsp; Than you so much.</body>
  </mail>
  <mail>
    <header>Re: Quartz-dev Digest, Vol 3, Issue 248</header>
    <date>Sat Nov 18 04:02:07 CET 2006</date>
    <body>&amp;nbsp;&amp;nbsp;&amp;nbsp; Than you so much. email@hidden Send Quartz-dev mailing list submissions to &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; email@hidden To subscribe or unsubscribe via the World Wide Web, visit or, via email, send a message with subject or body 'help' to You can reach the person managing the list at &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; email@hidden When replying, please edit your Subject line so it is more specific Today's Topics: &amp;nbsp;&amp;nbsp;1. About CGSGetGlobalCursorData (Cotton Chen) &amp;nbsp;&amp;nbsp;2. Re: About CGSGetGlobalCursorData (David Duncan) ---------------------------------------------------------------------- Message: 1 Date: Fri, 17 Nov 2006 18:19:38 +0800 Subject: About CGSGetGlobalCursorData To: Message-ID: Content-Type: text/plain; charset=UTF-8; format=flowed Hello, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;I recently used CGSGetGlobalCursorData in my Carbon application. But it show an error in link phase. &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Shell I include any framework or library ? And does Tiger still support this API? &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Thanks a lot. Cotton ------------------------------ Message: 2 Date: Fri, 17 Nov 2006 09:11:58 -0800 Subject: Re: About CGSGetGlobalCursorData To: Cotton Chen &amp;lt;email@hidden Cc: Content-Type: text/plain; charset=US-ASCII; delsp=yes; format=flowed The CGS prefix means that is an Apple Private SPI. They are not supported, and can change at anytime. What are you trying to accomplish that you believe you need to use an SPI? -- David Duncan Apple DTS Quartz and Printing email@hidden ------------------------------</body>
  </mail>
  <mail>
    <header>Re: About CGSGetGlobalCursorData</header>
    <date>Fri Nov 17 18:11:58 CET 2006</date>
    <body>On Nov 17, 2006, at 2:19 AM, Cotton Chen wrote: The CGS prefix means that is an Apple Private SPI. They are not supported, and can change at anytime. What are you trying to accomplish that you believe you need to use an SPI? -- David Duncan Apple DTS Quartz and Printing email@hidden</body>
  </mail>
  <mail>
    <header>About CGSGetGlobalCursorData</header>
    <date>Fri Nov 17 11:19:38 CET 2006</date>
    <body>Hello, I recently used CGSGetGlobalCursorData in my Carbon application. But it show an error in link phase. Shell I include any framework or library ? And does Tiger still support this API? Thanks a lot. Cotton</body>
  </mail>
  <mail>
    <header>CIImage and 2D LUTS</header>
    <date>Thu Nov 16 00:36:50 CET 2006</date>
    <body>Of the stock filter/image units that I see, there is only the &amp;quot;colorcube&amp;quot; filter, which to my understanding performs more like a 3D lut. At the end of my ciimage processing chain I need to apply a 2-d look up table to the image.  This LUT is calculated programatically so it cannot be &amp;quot;emulated&amp;quot; with other ciimage filters. It would be something similar to vImage's vImageInterpolatedLookupTable_PlanarF functionality. thanks in advance. -- ---- Bruce Johnson email@hidden</body>
  </mail>
  <mail>
    <header>Re: Handling multiple CGLayers</header>
    <date>Thu Nov 16 00:22:43 CET 2006</date>
    <body>On Nov 15, 2006, at 4:11 PM, Hemant Balakrishan wrote: Depending on what context you use when you create the layer, the computer may store a lot of information.  In Tiger, for example, I believe that each layer maintains a full pixel map for the content of the layer.  I suspect you're chewing up a lot of memory with lots of offscreen pixel maps for each layer. Quartz 2D is really fast.  Did you try just redrawing everything (within the bounds of the object being moved) and checking the performance of that first? If you still need a performance boost, what you might be able to do is create two special layers when the user gets ready to move an object.  Into one layer you draw everything underneath the object that is moving, and on the other you draw everything that is above it.  Then when the user moves the object, you draw the &amp;quot;underneath layer&amp;quot;, the &amp;quot;item being moved&amp;quot; and finally the &amp;quot;overlay layer&amp;quot;. Those special layers need only exist for the duration of the move operation.</body>
  </mail>
  <mail>
    <header>Handling multiple CGLayers</header>
    <date>Wed Nov 15 23:11:27 CET 2006</date>
    <body>My application requires me to draw several objects on a canvas and then the user get to move a groups of selected objects or manipulate the dimensions of a single object. I'm trying to create a list of Layers (one per object) and reuse the layer if the corresponding object is not moved or modified (this saves some time). For the objects that are moved/modified I try to redraw the layer, store it and then use it. For some reason storing these layers between redraws seems to be space intensive and eventually slows down the application. The other issue is with resolution, when I zoom in the stored layers do not scale up to the current resolution.</body>
  </mail>
  <mail>
    <header>Re: Display a NSView into a CGContextRef</header>
    <date>Wed Nov 15 04:07:06 CET 2006</date>
    <body>If you use the other advices, setting the current context, don't try to save the &amp;quot;old current context&amp;quot;, just do: Note that your NSGraphicsContext flipped parameter has to be equal to your view isFlipped value. -[NSView displayRectIgnoringOpacity:inContext:]; which is much cleaner and avoid doing all the &amp;quot;current context&amp;quot; voodoo. -- Julien</body>
  </mail>
  <mail>
    <header>Re: Display a NSView into a CGContextRef</header>
    <date>Mon Nov 13 22:04:50 CET 2006</date>
    <body>I haven't tried this, but you could probably do something like this (untested) code: void DrawNSViewWithContext(CGContextRef cgContext, BOOL flipped) NSGraphicsContext *gc = [NSGraphicsContext Jason On Nov 13, 2006, at 5:54 AM, laurent grangier wrote:</body>
  </mail>
  <mail>
    <header>Re: Display a NSView into a CGContextRef</header>
    <date>Mon Nov 13 17:19:07 CET 2006</date>
    <body>On Nov 13, 2006, at 4:54 AM, laurent grangier wrote: Is the hosting application Cocoa based? Is the call into your plugin from the main thread? -[NSGraphicsContext graphicsContextWithGraphicsPort:flipped:] // -[NSView lockFocusIfCanDrawInContext:] // note not sure this does anything at the moment... With that said I am note sure how you would position things in that context or what Cocoa framework initialization would be needed for this to work (need to get AppKit attached to the window server).</body>
  </mail>
  <mail>
    <header>Forcing CIContext not to interpolate images?</header>
    <date>Mon Nov 13 16:24:57 CET 2006</date>
    <body>When drawing, it seems that CIContext always interpolates images. Is there any workaround for this? Is this a bug anyway? It seems that CIContext has no interpolation setting at all. Using CIPixellate filter might help, but what if accurate pixel conversion is needed (CIPixellate is innnacurate?) ? Regards, Aidas</body>
  </mail>
  <mail>
    <header>Display a NSView into a CGContextRef</header>
    <date>Mon Nov 13 13:54:49 CET 2006</date>
    <body>Hello, I'm developing a plugin and I can get a CGContextRef (CoreGraphics) from the main application. My question is how can I display a Cocoa NSView on the CGContextRef ? Is it possible ? To be sure you understand my question, here is an other explanation: the main application gives me a CGContextRef pointer (it is the only graphical element I receive from it). I want to display Cocoa NSView on it. Thanks for your answers, Laurent Grangier</body>
  </mail>
  <mail>
    <header>Re: pixel data from CGImageRef</header>
    <date>Sat Nov 11 17:54:35 CET 2006</date>
    <body>The only recommended way to do this is to create a bitmap context with a memory buffer that you specify and draw the CGImageRef to it using CGContextDrawImage. Now you have a copy of the data in the buffer you specified. No other official way to do this.</body>
  </mail>
  <mail>
    <header>Re: Question regarding a custom image unit</header>
    <date>Thu Nov 09 21:25:44 CET 2006</date>
    <body>I don't see you setting the class? Also is the ivar that you pass to the kernel of NSNumber type? Check out the CIDemoImageUnit in /Developer/Examples/Quartz/Core Image for guidance.</body>
  </mail>
  <mail>
    <header>Re: Question regarding a custom image unit</header>
    <date>Wed Nov 08 14:24:09 CET 2006</date>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Question regarding a custom image unit</header>
    <date>Wed Nov 08 01:38:12 CET 2006</date>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: View flipping effect</header>
    <date>Mon Nov 06 09:54:27 CET 2006</date>
    <body>On 5 Nov 2006, at 19:14, Thomas Hartwich wrote: I have a Core Image kernel which will do the flipping effect if you have the images for the front and back of the thing to be flipped. You just pass in the time value. I'm not sure if you're trying to flip the whole window or just part of its content. If it's the whole window, that's more tricky because you need to get a screen shot of the desktop under the window and include that in your transition. Email me if you want the kernel and a Quartz Composer project which demonstrates it.</body>
  </mail>
  <mail>
    <header>View flipping effect</header>
    <date>Sun Nov 05 20:14:01 CET 2006</date>
    <body>Hi, I have been looking through quite a few Quartz examples, but failed to find a solution for the following task: I have a regular window with a content view in Cocoa and I want to present a flipping effect like a dashboard widget, when the user hits a control, e.g. a button in the view. The front view presents some buttons and the backside some others. So far I have not found an example or a core image effect doing this kind of flipping. I also looked in to NSAnimation without success. Do you have any pointers to some conceptual information or even sample code how to achieve this? Thanks in advance Thomas Hartwich</body>
  </mail>
  <mail>
    <header>QCView with composition and output keys not available</header>
    <date>Thu Nov 02 02:41:27 CET 2006</date>
    <body>I'm loading a Quartz Composition in q QCView in Cocoa and when I start the rendering the first time, I can get the list of Output Keys, but not the value of a specific key. Here is the code: [circleGraphic loadCompositionFromFile:pointerPath]; NSArray NSLog(@&amp;quot;All output keys %@&amp;quot;,[circleGraphic outputKeys]); NSLog (@&amp;quot;Output key value (should be a string): '%@'&amp;quot;,[circleGraphic 2006-10-25 20:18:02.841 TestApp?[1217] Output key value (should be a string): '' 2006-10-25 20:18:02.841 TestApp?[1217] Output key value (should be a string): 'Excellent Value' Where &amp;quot;Excellent Value&amp;quot; is the proper value stored in the Composition's output key. It seems like as long as the composition is not rendering the values appear empty, and just starting the rendering before may not leave enough time... not too sure. What would be the right way to do this? It looks like there are notifications, should I use those? I tried to, but the notification doesn't seem to get posted... - Renaud</body>
  </mail>
  <mail>
    <header>Re: CGSResolveShmemReference Problems</header>
    <date>Tue Dec 21 14:59:14 CET 2004</date>
    <body>Nuts :( It just seemed very weird that for about one whole day I was seeing crashers in Mail.app, Safari, and other relatively stable applications, and now the problems are gone; I *hate* heisenbugs (as do you, I imagine). In the interests of semi-reproducibility I'm going to try and use the SVHS output of the powerbook again since it's the only thing I can think of that precipitated the error.  Otherwise, thanks for information and fingers crossed that it was a fluke.</body>
  </mail>
  <mail>
    <header>Re: CGSResolveShmemReference Problems</header>
    <date>Tue Dec 21 06:23:36 CET 2004</date>
    <body>Generally, this particular error simply indicates that there is a memory smasher in the app.  The CG code just happens to sanity check some internal data structures for consistency before using them.</body>
  </mail>
  <mail>
    <header>CGSResolveShmemReference Problems</header>
    <date>Mon Dec 20 16:36:11 CET 2004</date>
    <body>This might not be the best place to ask, but since my Google searches haven't turned up anything more useful... I have recently installed 10.3.7 on my PB15FW800 and am now getting a large number of errors ranging from visual tearing through to application quits.  According to the console the problems are of the form: From what little I can tell from searching the web, the &amp;quot;core&amp;quot; of the error looks to be in the WindowServer API, which is why I've chosen to post here. Any assistance, or an opportunity to help debug the error so it can be fixed in 10.3.8 or 10.4, would be greatly appreciated.</body>
  </mail>
  <mail>
    <header>Re: cggl</header>
    <date>Wed Dec 15 23:19:36 CET 2004</date>
    <body>Actually, it looks like we were getting correct colors from glReadPixels all along.  The channel swapping was happening further downstream as we were trying to write the image.  So it looks like I'll be eating crow for a while -- I'll take mine cordon bleu.</body>
  </mail>
  <mail>
    <header>Re: cggl</header>
    <date>Mon Dec 13 07:47:28 CET 2004</date>
    <body>anyways, it doesn't matter much.  We can swap bytes back, and this code is not particularly performance-critical.</body>
  </mail>
  <mail>
    <header>Re: cggl</header>
    <date>Sun Dec 12 18:08:20 CET 2004</date>
    <body>On Dec 11, 2004, at 7:52 PM, Travis Heppe wrote: Why not? feature of OpenGL on Mac OS X.</body>
  </mail>
  <mail>
    <header>Re: cggl</header>
    <date>Sun Dec 12 02:52:35 CET 2004</date>
    <body>I don't think that would work for us. is the exact line we're using.  I could accept that it would be a little slower than reading back a native format, but what state could quartz be bashing to cause that not to work?   RGB is the format that the data needs to get to, so someone (either osx or us) has to swap bytes around.</body>
  </mail>
  <mail>
    <header>Re: cggl</header>
    <date>Sat Dec 11 23:32:11 CET 2004</date>
    <body>glReadPixels takes a parameter to grab the data in a number of formats. The GL_UNSIGNED_INT_8_8_8_8_REV option will read back a 32 bit source in the 'native' format, ARGB or XRGB. glReadPixels(xOrigin, yOrigin, width, height, GL_BGRA, GL_UNSIGNED_INT_8_8_8_8_REV,</body>
  </mail>
  <mail>
    <header>Re: CIImage from RGB data / CoreImage OGL texture basic questions</header>
    <date>Fri Sep 15 21:08:27 CEST 2006</date>
    <body>You could possibly use Core Video to do this.  Create a CVPixelBuffer with your existing data, using the correct pixel format.  Then create a CVOpenGLTexture from the CVPixelBuffer.  You can then create a CIImage from this CVOpenGLTexture directly (+[CIImage imageWithCVImageBuffer]).  You'll need to create a CVOpenGLTextureCache just to create one texture, but it may be workable.</body>
  </mail>
  <mail>
    <header>Re: Image rescaling (20-30fps)</header>
    <date>Fri Sep 15 20:47:48 CEST 2006</date>
    <body>Actually, Im using glCopyTexSubImage2D and glGetTexImage for the capturing.  However; my OpenGL knowledge is probably less than my Quartz knowledge, so I've no idea how to scale using OpenGL.  I've already got a Q on the list about it tho, in case it's a better avenue to explore.</body>
  </mail>
  <mail>
    <header>Re: Image rescaling (20-30fps)</header>
    <date>Fri Sep 15 20:16:04 CEST 2006</date>
    <body>&amp;gt; Yes, I tried turning of all interpolation (using the None setting) I see. I'm assuming you're capturing with glReadPixels(), so you can't simply use OpenGL to resize using a texture. If you haven't already, you might want to have a look at vImage: Best regards, Robert van Uitert</body>
  </mail>
  <mail>
    <header>Re: Image rescaling (20-30fps)</header>
    <date>Fri Sep 15 19:49:01 CEST 2006</date>
    <body>Yes, I tried turning of all interpolation (using the None setting) and also anti-aliasing (just in case). It's better - but still quite slow.   e.g: it's faster for me to capture full screen and then simply resize the video in quicktime (at runtime, not as an export).   I was hoping that by resizing a full screen capture down to 1/2 (or somethin\g like that)  I'd be able to get a faster capture rate (the actual OpenGL capture is taking only about 12% CPU).</body>
  </mail>
  <mail>
    <header>RE: Image rescaling (20-30fps)</header>
    <date>Fri Sep 15 19:27:39 CEST 2006</date>
    <body>Did you try different interpolation settings? For example: Best regards, Robert van Uitert</body>
  </mail>
  <mail>
    <header>Image rescaling (20-30fps)</header>
    <date>Fri Sep 15 19:04:50 CEST 2006</date>
    <body>I'd like to be able to scale images that I'm capturing from the screen (using OpenGL), so that when I put them into a QT movie, they don't have to be the original size.        (I doing this on an Intel MBP, btw) I've done what is probably a very naive implementation, using CGImage/ Bitmap, which is this: // Make a new bitmap to hold the data newBitmap = CGBitmapContextCreate(newData, scaledRect.size.width, scaledRect.size.height, 8, byteWidth, [frame colourSpace], // Create an image from the existing frame bitmap data, so that we can draw this image to the new bitmap Some notes: 1) The frames being captured use pre-allocated buffers (so they are not allocated for each frame, which would hurt). 2) newData is also a piece of pre-allocated memory. 3) From what I can tell from the docs the CGBitmapContextCreateImage should be fast - as it potentially doesn't do much.  Shark seems to confirm this, as all the time is spent inside CGContextDrawImage - and then within that rgba32_image_rgba32 Thanks, Neil Clayton</body>
  </mail>
  <mail>
    <header>Re: Thread safety of drawing a PDF page in an offscreen context</header>
    <date>Fri Sep 15 17:24:12 CEST 2006</date>
    <body>Generally speaking, this is safe.  The key thing is that only a single thread may access the contents of a CGPDFDocument at any given time. It's okay, however, for multiple threads to access the document as long as you put in locks to enforce this constraint.</body>
  </mail>
  <mail>
    <header>CIImage from RGB data / CoreImage OGL texture basic questions</header>
    <date>Fri Sep 15 16:04:16 CEST 2006</date>
    <body>I am having some trouble finding out how to create a CIImage from RGB data (my app is receiving RGB data buffers as unsigned char *). If I understand it correctly, CIImage's +imageWithBitmap +imageWith only work with data in ARGB format (32 bytes per pixel), so to use any of these I would need to convert my buffer to ARGB first. Doesn't CoreImage ultimately create a OGL texture from the pixel data provided? Can I upload my RGB data directly to an OpenGL texture and then use CIImage +imageWithTexture:size:flipped:colorSpace - and thus bypass the convertion to ARGB? Unfortunately I could not find any information on how to create OGL textures compatible with Core Image. The CIImage documentation refers to three CIImage pixel formats (kCIFormatARGB8, kCIFormatRGBA16 , kCIFormatRGBAf) - do these relate to the OGL texture &amp;quot;internal format&amp;quot;? What are the parameters that need to be passed to glTexImage2D for any of the CIFormats above? Thank you &amp;amp; best regards, Mark</body>
  </mail>
  <mail>
    <header>Thread safety of drawing a PDF page in an offscreen context</header>
    <date>Fri Sep 15 03:27:44 CEST 2006</date>
    <body>Is it safe to call CGContextDrawPDFPage() from a separate thread, if the CGPDFPage and offscreen CGContext are also created and later released in that same thread and are never used by any other thread? Thanks, Robert van Uitert</body>
  </mail>
  <mail>
    <header>Re: Correct way to find out if CIFilter supports infinite input?</header>
    <date>Thu Sep 14 17:14:08 CEST 2006</date>
    <body>On Sep 14, 2006, at 8:18 AM, Santiago (Jacques) Lema wrote: I think the only way to know is to look at the extent of the image you get back.  One way to deal with this is to crop the image to the intersection of its extent and some arbitrarily large rectangle. Nick</body>
  </mail>
  <mail>
    <header>Correct way to find out if CIFilter supports infinite input?</header>
    <date>Thu Sep 14 16:18:45 CEST 2006</date>
    <body>Is there a way to know if a CIFilter accepts to render with a given input (eg. an infinite one) ? Typically if your input are checkers (infinite) and you give them to a CICircularWrap it won't work, which is normal. But how do I know before that it won't ? Is  there any attribute for this in the filter? Currently I just do the following: try to set the inputImage and see if output is nil.  Otherwise crop it. Is there any better way' if ([filter valueForKey:@&amp;quot;outputImage&amp;quot;]==nil) ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Re: CGImageRef from CIImage</header>
    <date>Wed Sep 13 18:37:29 CEST 2006</date>
    <body>So assuming the CI filter chain doesn't change geometry (i.e. I can reuse the width/height of the input image in this example), would the code below be the proper/fastest way to go from input CGImageRef to an output CGImageRef? Which leads me to another question -- if I CICrop as one of my filters, geometry changes.  What is the &amp;quot;proper&amp;quot; way to get the resultant x/y/width/height on the way out for rendering the final image?  When I did this in my code, it seemed that my code had to &amp;quot;be smart&amp;quot; about keeping track of its geometry changes and there wasn't an obvious (to me) way to just say &amp;quot;give me the final CGImageRef and figure out the bounds yourself&amp;quot;. ... do stuff in CoreImage ... // assuming no image size changes, that we can reuse the width/height from the original CGColorSpaceRef colorSpaceRef = CGColorSpaceCreateWithName CGContextRef outputCGImageContext = CGBitmapContextCreate(outputData, width, height, bitsPerComponent, bytesPerRow, colorSpaceRef, NSDictionary *outputContextOptions = [NSDictionary dictionaryWithObjectsAndKeys: [NSNumber numberWithBool: NO], CIContext *ciContext = [CIContext contextWithCGContext:outputCGImageContext options: CGImageRef outputCGImage = [ciContext createCGImage:outputCIImage</body>
  </mail>
  <mail>
    <header>Re: CGImageRef from CIImage</header>
    <date>Wed Sep 13 18:32:08 CEST 2006</date>
    <body>On Sep 13, 2006, at 9:49 AM, Frank Doepke wrote: Generally the answer is twofold.  Sometimes you want to do something other than display these images on the screen so you have to export them, and with the current implementation its usually substantially faster to do the CI rendering once and cache the results than to draw it over and over. Nick</body>
  </mail>
  <mail>
    <header>Re: CGImageRef from CIImage</header>
    <date>Wed Sep 13 18:19:12 CEST 2006</date>
    <body>Got the fish on the hook now (but then again I don't like seafood). Anyhow: I am quite aware of the desire to get to the pixels of a CIImage. To do that you have to create a CGBitmapContext, then create a CIContext from it and render into that. Or for those inclined to venture into OpenGL can read the pixels back from the graphic card. Here is the point where I should say: Yes you can do it but I would like this to be easier and I have a bug in my court regarding this. When you are done fiddling around with the pixels, you can either create a CGImage from that again and then a CIImage as you stated correctly or use imageWithData. And now comes the the 'but' part: When you just want to use CI on images that get combined with some CG rendering, there is no need to wrap those image into a CGImage as you can create the CIContext based on the CGContext and just draw the CIImage into it. Having a CIContext also allows you to create a CGImage or a CGLayer using the following APIs: /* Render the region 'r' of image 'im' into a temporary buffer using * the context, then create and return a new CoreGraphics image with * the results. The caller is responsible for releasing the returned * image. */ /* Create a CoreGraphics layer object suitable for creating content for * subsequently rendering into this CI context. The 'd' parameter is * passed into CGLayerCreate () as the auxiliaryInfo dictionary. */ - (CGLayerRef)createCGLayerWithSize:(CGSize)size info: These are helpful for operations, where you definitely need a CGImageRef, like writing them into a file - as demonstrated in the CIAnnotation example. Or when you want to use the result in CG operations as a mask or something like that. Thats where feedback is always welcomed so we can make it less confusing This contradicts the rest of the email :)</body>
  </mail>
  <mail>
    <header>Re: CGImageRef from CIImage</header>
    <date>Wed Sep 13 18:14:43 CEST 2006</date>
    <body>- (void)displayImage:(Image*)tempImage CGContextTranslateCTM(ctx,bounds.size.width/2.0 CIContext	*myCIContext = [CIContext contextWithCGContext:ctx CIImage		*myCIImage = [CIImage imageWithCGImage:currentImage [hueAdjust setValue:[NSNumber numberWithFloat: 1.1] [myCIContext drawImage:result inRect:CGRectMake(-w/2.0,-h/2.0,w,h) // Image without CIFilters... My problem is that it is very slow when I try to move the image by dragging it, so I thought that I could apply the transformation to the CIImage, and store the result in a CGImageRef to draw it when I need. But my approach is probably wrong. This is the first time I use CoreImage, so I may have missed something... -- AstroK Software Arthur VIGAN email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGImageRef from CIImage</header>
    <date>Wed Sep 13 18:11:52 CEST 2006</date>
    <body>On Sep 13, 2006, at 8:49 AM, Frank Doepke wrote: Because that seems to be the only way to get access to the raw pixel data.  For instance when  you wish to do further processing or be able to store the resulting image to a file. Bob</body>
  </mail>
  <mail>
    <header>Re: CGImageRef from CIImage</header>
    <date>Wed Sep 13 17:57:43 CEST 2006</date>
    <body>On Sep 13, 2006, at 8:49 AM, Frank Doepke wrote: Okay, I'll bite :-) Let's say I have a CIImage, and I want to do something with that I can't express as some CIFilter operation (like flood fill on the pixels that will result when that CIImage is evaluated). In other words, if I need to get pixels from something that up until now was happy to be expressed algorithmically with a CIFilter/CIImage chain, the shortest path from a CIImage to pixels is a CGImageRef, right?  Then once I've done work in pixels I want to loft it back up into a CIImage so I can continue on in the filter chain, which I can do with [[CIImage alloc] initWithCGImage:], but I'm not sure how expensive that is... --&amp;gt; Michael B. Johnson, PhD --&amp;gt;  (new &amp;amp; improved!) --&amp;gt;  (alum) --&amp;gt; MPG Lead --&amp;gt; Pixar Animation Studios</body>
  </mail>
  <mail>
    <header>Re: CGImageRef from CIImage</header>
    <date>Wed Sep 13 17:49:42 CEST 2006</date>
    <body>Generally the question is, why do you need a CGImage, as you can create a CIContext from your CGContext and draw the CIImage into that.</body>
  </mail>
  <mail>
    <header>Re: CGImageRef from CIImage</header>
    <date>Wed Sep 13 17:39:15 CEST 2006</date>
    <body>On Sep 13, 2006, at 9:15 AM, AstroK Software wrote: I don't have any sample code handy, but I can give you the high-level view... You have to create a CGBitmapContext and use that to create a CIContext, render your CIImage into the CIContext, and finally create a CGImage from the backing store of the CGBitmapContext (or use CGBitmapContextCreateImage). Nick</body>
  </mail>
  <mail>
    <header>CGImageRef from CIImage</header>
    <date>Wed Sep 13 17:15:50 CEST 2006</date>
    <body>in my application I work mostly with Quartz 2D for drawing and images, but I would like to use the CoreImage filters. My first question is: is it possible to use CoreImage directly from Quartz 2D? The researches I've made seem to tell me that it's not. So here is second question: how can I get an CGImageRef from a CIImage? It's probably not trivial, because CIImages can have infinite extent, but the CIImages I would manipulate would be of finite extent. -- AstroK Software Arthur VIGAN email@hidden</body>
  </mail>
  <mail>
    <header>ImageIO Exif GPS</header>
    <date>Mon Sep 11 08:59:36 CEST 2006</date>
    <body>doing a little research on the web and my own prototyping is seems as though you cannot write GPS Exif data using ImageIO. Is this true?</body>
  </mail>
  <mail>
    <header>Re: applying a filter recursively?</header>
    <date>Sun Sep 10 03:02:27 CEST 2006</date>
    <body>Instead of drawing the image, you would directly pass it to the blurFilter and then draw the final result.</body>
  </mail>
  <mail>
    <header>Re: CGCaptureDisplay() / CGDisplayHideCursor() on auxiliary	displays messing with main display</header>
    <date>Sat Sep 09 19:45:13 CEST 2006</date>
    <body>On Sep 9, 2006, at 4:39 AM, nick briggs wrote: That's a deliberate feature of display capture.  Most hot keys, including Command-Tab are disabled.  Many full screen applications, including game ports, have a large set of predefined and documented key combinations that otherwise conflict with system-defined hot keys.  In addition, switching applications while displays are captured can lead to an unusable system. If your app wants to capture displays, while supporting Command-Tab, the app should be coded to catch Command-Tab, release all displays, and then resign foreground, effectively switching to another app. When your app becomes foreground again, recapture the displays. That API is a little misleading.  There is a single global cursor and hide/show count, and calling CGDisplayHideCursor() hides the cursor on all displays. Mike Paquette email@hidden</body>
  </mail>
  <mail>
    <header>applying a filter recursively?</header>
    <date>Sat Sep 09 19:22:00 CEST 2006</date>
    <body>i'm pretty new to core image, so bear with me. i want to apply a blur filter to a CIImage, then apply the filter again to the resulting image and so on. i noticed that doing something like is an especially bad idea because the intermediate steps are rendered again and again, so in the nth loop the blur filter is computed n times. whoops! what would be the correct way to achieve what i want? should i use an CIImageAccumulator and copy the results into it after each filter operation? i would like to get a fluent animation in real time, so i would like to execute this at least 10 times a second, so i would prefer to re-use the computed pixel data instead of copying it somewhere. is that possible? regards, sebastian mecklenburg</body>
  </mail>
  <mail>
    <header>CGImageDestinationCreate* and image count</header>
    <date>Sat Sep 09 14:24:13 CEST 2006</date>
    <body>I have a question regarding the count specified to the CGImageDesintationCreate* methods: count The number of images (not including thumbnail images) that the image file will contain. I get the images from a device and thus do not know beforehand how many image I'll get. Maybe there is something supported like passing 0 or -1 to indicate the count is not known yet? The images I deal with are usually pretty large and thus forbid storing them into memory of disk to count them before the go into the final file. -- René Rebe - ExactCODE - Berlin (Europe / Germany) +49 (0)30 / 255 897 45</body>
  </mail>
  <mail>
    <header>CGCaptureDisplay() / CGDisplayHideCursor() on auxiliary displays	messing with main display</header>
    <date>Sat Sep 09 13:39:13 CEST 2006</date>
    <body>We're calling CGCaptureDisplay() / CGDisplayHideCursor() on 3 auxiliary displays attached to our system and experiencing problems with the main display: 1) The Command-tab feature breaks, that is to say while an auxiliary display is captured you can no longer command-tab between applications. As soon as the auxiliary display is released with CGDisplayRelease() then Command-tab works again. 2) Calling  CGDisplayHideCursor() on any auxiliary display hides the cursor on all displays. CGDisplayErr err = CGGetActiveDisplayList(kMaxDisplays, _displays, ..... for(i=1; i&amp;lt;_numDisplays; i++)  /* i=1 skipping main display */ /* never happens */ if(_displaysCaptured ) else</body>
  </mail>
  <mail>
    <header>Re: Composite PDF pages transparently?</header>
    <date>Thu Sep 07 15:28:37 CEST 2006</date>
    <body>On Sep 7, 2006, at 7:09 AM, Matthew Chartier wrote: CGContextDrawPDFPage doesn't draw a background so in effect, everything you draw with it is transparent.  If you're seeing a background it's probably because the PDF image itself contains a solid (e.g., white) rectangle behind its other content.  There's no easy way to remove this, apart from deconstructing the PDF and taking it out. Nick</body>
  </mail>
  <mail>
    <header>CoreVideo and YUV420</header>
    <date>Tue Sep 05 16:14:09 CEST 2006</date>
    <body>CoreVideo provides great support for YUV422 color space data but it doesn't seem to provide any support for YUV420 (the dominant MPEG-1/2/4 distribution format). Is that correct ? Or have I missed something ? Certainly I can up-convert YUV420 to YUV422 before display but I was wondering if there is any way I can offload that work to the graphics card ? A custom CoreImage filter perhaps ?</body>
  </mail>
  <mail>
    <header>CGPath thread safe?</header>
    <date>Tue Sep 05 07:33:43 CEST 2006</date>
    <body>Can CGPath be used in a different thread if all the neccessary locks and whatnot are used? The header and docs don't say anything. TIA Mark</body>
  </mail>
  <mail>
    <header>Clipping with NSOpenGLView</header>
    <date>Mon Sep 04 17:56:51 CEST 2006</date>
    <body>Hi all, In my vector drawing app, i want to use clipping to exclude some regions when drawing 2 Bezier paths. I use apple's method as described here: (Setting the Clipping Region). Each one of my paths subtracts the other path from his clipping region (i want to draw the difference) : Here is my code /**************************************************************** [NSGraphicsContext saveGraphicsState];  [clipPath	appendBezierPath:[[self graphicForClipping] // Add the path to the clip shape. // Draw the image (( Add my graphic storked CGPathRef to the context )) ****************************************************************/ While it all works great when the view I draw into is a standard NSView, it fails when I draw in a CGLayer, create a CIImage from it and composite it in an NSOpenGLView (clipping doesn't work in my layer). My CIImage based code uses some standard drawing code from FunHouse (draw in a layer, create a CIImage and composite it with the background CIImage). I've tried CGContextClip and CGContextEOClip, it doesn't works like I want. Is clipping at all supported when drawing using such techniques. What am I doing wrong ? Any help would be much appreciated. Christophe p2.vert.ukl.yahoo.com uncompressed/chunked Mon Sep  4 12:13:57 GMT 2006 ___________________________________________________________________________ Découvrez un nouveau moyen de poser toutes vos questions quelque soit le sujet ! Yahoo! Questions/Réponses pour partager vos connaissances, vos opinions et vos expériences.</body>
  </mail>
  <mail>
    <header>Quartz Event Services CGEventCreate Hanging?</header>
    <date>Sat Sep 02 18:59:50 CEST 2006</date>
    <body>I think I've misunderstood something basic.  I'm trying to create a HID event for injection into the window system.  Any idea why CGEventCreate hangs in the following simple example? Thanks in advance, Mark Rahner // Create a reference to event source associated with HID system. eventSource = CGEventSourceCreate // Create a reference to an event.  Want to pass eventSource as // an argument but that causes app to hang.  Running as root // doesn't help so I don't think it's a permission related problem.</body>
  </mail>
  <mail>
    <header>Re: glGrab / CGLCreateContext leak?</header>
    <date>Sat Sep 02 15:39:46 CEST 2006</date>
    <body>I have now followed your approach, which indeed leads to less allocations. While I still do not fully understand the allocation behavior i was seeing and whether it was a real issue or not, this does seem more efficient and gives piece of mind :-). This may not be the right list, but can anyone here by accident explain to me what Real Memory in Activity monitor really means. Is it actually used memory, or is it the total allocated space (including unused blocks that were released but not reused because of memory fragmentation)? At some point i see my real memory jump from 13 MB to 98 MB without me doing anything special in the app, not does MallocDebug report any major allocations of this size... david.</body>
  </mail>
  <mail>
    <header>Re: glGrab / CGLCreateContext leak?</header>
    <date>Sat Sep 02 03:17:35 CEST 2006</date>
    <body>On Sep 1, 2006, at 2:37 PM, David Niemeijer wrote: I am not seeing leaks in code that I have setup for this type of task (similar to the glGrab sample code). Of course I only create contexts once at the beginning for each screen I will be capturing from and simply make the context I need the current context when I go to capture. I have let test applications run for days without issue. -Shawn</body>
  </mail>
  <mail>
    <header>Re: glGrab / CGLCreateContext leak?</header>
    <date>Sat Sep 02 00:01:27 CEST 2006</date>
    <body>I have something similar where i grab the screen.  Here is the content from my radar bug -- maybe it will be helpful: 05-Apr-2006 04:28 PM Mike Schrag: Every call to setFullscreen on NSOpenGLContext leaks a small amount of memory.  You can see this by running Apple's NSOpenGL Fullscreen example and repeatedly go fullscreen and back.  The leaked memory leaks past the release and dealloc of the context.  This happened on 10.4.5 as well, so it's not specifically a 10.4.6 problem. 07-Apr-2006 08:55 PM Mike Schrag: 1) I should have attached the system profile when I logged -- sorry about that.  Attaching now. 2) Initially it was just watching the real size in activity monitor/ top grow.  I was calling setFullscreen and clearing/disposing many times a second and it just continued to grow.  I later ran the app through Shark, but I've read that Shark can give bogus information when you attempt to track down OpenGL issues because of symbol problems.  What appeared to be the leak was coming from (I need to look it up again, but I have to roll back to an older version of code to get it) a GetString method that was chained WAY deep in a stack trace in the ATI GL driver.  Basically, I'm not sure the best way to track the problem accurately -- I'd be happy to try whatever would be helpful to you.  What I DO know is that when I removed the setFullscreen call, memory stayed constant.  When I put it back in, memory increased without bound (albeit slowly) 3) I just tried this again with the NSOpenGL Fullscreen sample code on the Apple dev site.  It's a little hard to nail down in this app, because it's actually always increasing memory normally, but if you quickly flip back and forth between fullscreen and not (position your mouse over the button and click ESC click ESC click ESC like a crazy person :) ), I ran memory up by about a meg in 10-15 seconds.  My particular case where I ran into this issue was flipping back and forth to fullscreen mode 10-15 times a second, which was kind of ugly.</body>
  </mail>
  <mail>
    <header>glGrab / CGLCreateContext leak?</header>
    <date>Fri Sep 01 23:37:01 CEST 2006</date>
    <body>I have a situation that is strange to me. I use the glGrab source that was discussed here in the past. It calls CGLCreateContext then does most of its work and finally calls CGLDestroyContext. So all seems ok, but when I look in Activity Monitor my Real Memory seems to grow and grow each time the grab takes place. So I ran with MallocDebug and found no leaks, but if I look at Allocations from mark i see that most of the growth is coming from CGLCreateContext. Now I am not an expert on these tools and unix memory management. but is the growth of real memory (starting at 20 MB and in some time grown to 150 MB) a reason for concern, or is this just inefficient allocation and will not cause infinite growth? Is there something wrong? an issue with the glGrab code? david.</body>
  </mail>
  <mail>
    <header>Re: Core Image leaking when using a Core Video OpenGL texture</header>
    <date>Fri Sep 01 21:31:22 CEST 2006</date>
    <body>Okay, I think I solved my problem, though I'm not exactly sure why. Lemme postulate out loud here.  I think I was passing the wrong CGLContextObj to CVOpenGLTextureCacheCreate(). I have two contexts, let's call them SCREEN and PBUFFER.  I use SCREEN to draw the CIImage to the actual screen in the CV callback. And I use PBUFFER in the main thread to render the scene.  The problem was I used PBUFFER when I created the texture cache.  I believe the texture isn't actually created until -[CIImage drawRect:] is called.  Of course SCREEN is the current context by the the CIImage is rendered, which means the texture cache was using the wrong context.  Thus I needed to use SCREEN instead of PBUFFER, when I created the texture cache.</body>
  </mail>
  <mail>
    <header>Re: Core Image leaking when using a Core Video OpenGL texture</header>
    <date>Fri Sep 01 20:32:15 CEST 2006</date>
    <body>Okay, it could be the buffer.  I am using a CVOpenGLBufferPool, as well.  As long as we're on the topic, how does one return a buffer created using CVOpenGLBufferPoolCreateOpenGLBuffer() back to the pool?  Does CVOpenGLBufferRelease() put it back in the pool, or does it actually release it? Also, what is the life cycle of an CVOpenGLBuffer?  I create it, draw to it, then create a texture from it.  Finally I draw the texture onto the main screen.  At what point can I release the buffer?  Do I have to wait until *after* I draw the texture?</body>
  </mail>
  <mail>
    <header>Re: Core Image leaking when using a Core Video OpenGL texture</header>
    <date>Fri Sep 01 20:18:38 CEST 2006</date>
    <body>I have been running into this same problem (see the thread &amp;quot;Quicktime/ CoreImage memory leak&amp;quot;). As best I can tell, it is caused by associating the CV/quicktime buffer with one openGL context and then trying to draw it into another openGL context.  And it is exactly as you have said, if you don't actually draw the image, it is fine, but if you draw the image it leaks (though in my case MallocDebug does not show it as an actual leak, just allocated memory that has not been freed). I have tried various things, even creating a raw memory buffer that I draw the image into but that still leaks too. I still have not found a solution, but my next attempt will be to replace the image creation code.  Get rid of the CV stuff and simply have Quicktime render into an offscreen buffer and then create a CIImage directly from that offscreen buffer (see the quicktime ASCII movie player) and see if that solves the memory leak.  I am sure it will be slower, but at this point a program that is slower but works is better for me than one that crashes after about an hour because it hits the 2GB memory limit.</body>
  </mail>
  <mail>
    <header>Re: Core Image leaking when using a Core Video OpenGL texture</header>
    <date>Fri Sep 01 20:18:42 CEST 2006</date>
    <body>On Sep 1, 2006, at 11:14 AM, Dave Dribin wrote: That is a housekeeping call you can call anytime. Most likely you are retaining the buffer or the image. If you can see that you really release everything and it still leaks, file a radar bug with a little sample app that demonstrates the problem.</body>
  </mail>
  <mail>
    <header>Re: Core Image leaking when using a Core Video OpenGL texture</header>
    <date>Fri Sep 01 20:14:33 CEST 2006</date>
    <body>Yeah, I've got an autorelease pool.  This is my CV callback: I don't think it's the CIImage itself that is leaking.  From the stack trace at the end, it looks like it has something to do with a texture being allocated during drawing: CVOpenGLBufferTextureBacking::initWithOpenGLBackingContext() CVOpenGLBufferBacking::createTextureBackingForContext() CVImageBacking::provideCachedImageTextureForContext() I *am* using a CVOpenGLTextureCache to create frameTexture.  Maybe I'm not using the cache correctly?  One part that confused me was when to call CVOpenGLTextureCacheFlush().  The documentation just says &amp;quot;periodically&amp;quot;, so I do it in every CV display link callback. Is that too often?  Should I do it in the main thread, instead of the display link thread?</body>
  </mail>
  <mail>
    <header>Re: Core Image leaking when using a Core Video OpenGL texture</header>
    <date>Fri Sep 01 20:00:34 CEST 2006</date>
    <body>Are you releasing the CIImage? In your case make sure you wrap an Autorelease pool around your render callback otherwise you accumulate images.</body>
  </mail>
  <mail>
    <header>Core Image leaking when using a Core Video OpenGL texture</header>
    <date>Fri Sep 01 19:35:16 CEST 2006</date>
    <body>According to MallocDebug, I'm getting a memory leak when trying to draw a CIImage (one stack dump pasted at the end of the mail).  If I comment out the call to drawImage:, there is no leak, so it's definitely drawImage: that triggers it.  I'm assuming this is a leak in my own code in how I use Core Image/Core Video, but I'm stumped. I'm doing the fairly standard procedure in the CV display link: CVOpenGLTextureRef frameTexture = //.... CIImage * inputImage = [CIImage imageWithCVImageBuffer: [ciContext drawImage: inputImage atPoint: CGPointMake(0, 0) Now, it is my own code that is generating the frameTexture object. I'm trying to use a CV buffer pool and texture cache, and I could very well be doing it incorrectly.  I can provide more detail, if necessary. STACK Address /041fc010/size/00000b30 0x9b14f468  / MDmalloc / libMallocDebug.A.dylib 0x01789590  / 0x1789590 / (null) 0x04395e3c  / 0x4395e3c / (null) 0x043a7aa8  / 0x43a7aa8 / (null) 0x043a7a0c  / 0x43a7a0c / (null) 0x9429ceec  / CVOpenGLBufferTextureBacking::initWithOpenGLBackingContext (CVOpenGLBufferBacking*, CVOpenGLContext*, int*) / QuartzCore 0x9429cd6c  / CVOpenGLBufferBacking::createTextureBackingForContext (__CFAllocator const*, CVOpenGLContext*, __CFDictionary const*, int*) / QuartzCore 0x942ca4bc  / CVImageBacking::provideCachedImageTextureForContext (__CFAllocator const*, _CGLContextObject*, __CFDictionary const*, int*, __CFDictionary const*) / QuartzCore 0x942cd0dc  / CVTextureBacking::provideImageTexture (_CGLContextObject*, long, long, _CGLPixelFormatObject*, unsigned long, CGRect*, __CFDictionary const*) / QuartzCore 0x942e91e4  / provide_texture / QuartzCore 0x942c2b64  / fe_gl_texture_load / QuartzCore 0x942e906c  / fe_context_texture_load / QuartzCore 0x942e8da8  / image_buffer_texture_ref / QuartzCore 0x942e8ccc  / fe_image_texture_ref / QuartzCore 0x9434ea88  / texture_retain / QuartzCore 0x942e89bc  / fe_texture_new / QuartzCore 0x942e8670  / fe_tree_create_texture / QuartzCore 0x94304fa0  / fe_tree_render_unary / QuartzCore 0x942e7994  / fe_tree_render / QuartzCore 0x94354310  / fe_tree_render_image_ / QuartzCore 0x942e72f0  / fe_tree_render_image / QuartzCore 0x942e719c  / fe_image_render_ / QuartzCore 0x942e70d8  / fe_image_render / QuartzCore 0x942e7024  / -[CIOpenGLContextImpl renderAccel:matrix:bounds:] / QuartzCore 0x942e6220  / -[CIOpenGLContextImpl render:] / QuartzCore 0x942fbeec  / -[CIContext drawImage:inRect:fromRect:] / QuartzCore 0x942fe5f4  / -[CIContext drawImage:atPoint:fromRect:] / QuartzCore 0x000048cc  / -[MyController(Private) drawFrame] / myapp 0x000036b4  / myCVDisplayLinkOutputCallback / myapp 0x94279db0  / CVDisplayLink::performIO(CVTimeStamp*) / QuartzCore 0x9427d0bc  / CVDisplayLink::runIOThread() / QuartzCore 0x9002bc28  / _pthread_body / libSystem.B.dylib ENDSTACK</body>
  </mail>
  <mail>
    <header>Core Image/OGL problem: last image appears on desktop</header>
    <date>Fri Sep 01 18:32:48 CEST 2006</date>
    <body>I'm having a weird problem with Core Image drawing to a custom NSOpenGLView: after I draw the last image to the view, I'm opening a NSSavePanel. As soon as the panel shows up, the last ciimage gets drawn on the top- left position of my desktop - above all other widows. As soon as I move around some window or open exposé´, the image is cleared and my desktop appears normally again. I have no idea what is going wrong and already tried a lot of things like calling clearGLContext, clearDrawable and other stuff before opening the panel. But nothing seems to resolve the problem. Please help. The NSOpenGLView is mostly implemented as the SampleCIView from the WhakedTV sample. The NSSavePanel is opened with ...runModalForDirectory... Best regards, Mark _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: toggle mouse drag with</header>
    <date>Fri Sep 01 17:40:28 CEST 2006</date>
    <body>On 30 August 2006, at 18:30, Roland Torres wrote: Come on mate, you're turning into Microsoft :)</body>
  </mail>
  <mail>
    <header>Re: toggle mouse drag with</header>
    <date>Fri Sep 01 17:37:17 CEST 2006</date>
    <body>On 29 August 2006, at 16:55, Mike Paquette wrote: I've had this problem with an app I'm using on my old 400mHz G4 box... and quitting everything didn't help. It's not an app I wrote, it's one of Skype, X-Chat Aqua (I'm betting on X-Chat causing it), Adium, TextEdit, Terminal, and (gasp) Finder. Logging out fixes it... for a minute.</body>
  </mail>
  <mail>
    <header>Re: IKImageView choppy?</header>
    <date>Sat Jul 31 03:14:34 CEST 2010</date>
    <body>Question:  Have you profiled the code using Instruments to see what segment of code is being used during the 3 seconds?</body>
  </mail>
</mails>

