<?xml version="1.0" encoding="UTF-8"?>
<mails>
  <mail>
    <header>Re: basic drawing: core graphics or app kit?</header>
    <body>The Cocoa classes fit better in the context of a Cocoa app, and are generally the more reasonable first choice. For instance, you can assign NSImages, NSColors, NSFonts, etc to various Cocoa elements, copy or paste them from NSPasteboard, archive them using NSArchiver, etc. So, in general you'll get better &amp;quot;impedance match&amp;quot; with the Cocoa types. However, the bottom line is pretty much all NS drawing goes through CG, and it's not too difficult to use the CG types in the context of a Cocoa app.  If you encounter any issues in using the NS classes (for instance, the exact functionality you want isn't there in NS but is available in CG, or you are given code which already uses the CG functions, or you have to go through extra steps which might introduce performance issues), then using the CG types is an option. Ali</body>
  </mail>
  <mail>
    <header>basic drawing: core graphics or app kit?</header>
    <body>Apologies if this is a faq. I've searched the Apple documentation and list archives, but can't seem to find an answer: There seems to be very similar routines for basic drawing (bitmaps, text, bezier curves) in both the Core Graphics and AppKit interfaces. For example, NSImage has a 'drawInRect' method, and Quartz has a CGContextDrawImage function that both appear to do the same thing: draw a scaled bitmap in a destination rectangle. My question is: How does one decide whether to use the lower level CG* routines or the NS* routines for basic drawing? -Mike</body>
  </mail>
  <mail>
    <header>Is CGContextDrawPDFDocument() multi-thread safe?</header>
    <body>Hi all, My PDF importer works well in a single thread environment, i.e. doing a single import operation. In multi-threading it blows up at the line containing CGContextDrawPDFDocument(). BTW the CGContextRef and CGPDFDocumentRef parameters I pass to CGContextDrawPDFDocument() are local and I do not do any UI stuff during the import operation. Any idea? Thanks JJ ___________________________________________________________ your friends today! Download Messenger Now</body>
  </mail>
  <mail>
    <header>Re: CalcCMask replacement?</header>
    <body>I'm pretty sure there isn't anything like this in Quartz, however the mask can be done in the alpha channel.  If it were me, I'd probably go ahead and call CalcCMask, then step through the bitmap pixel by pixel setting the alpha channel based on the value of the mask returned from CalcCMask. Nick</body>
  </mail>
  <mail>
    <header>Re: [ANN] Graphviz 1.10 (v4) for Mac OS X</header>
    <body>Apologies -- if the policy of this list is &amp;quot;no announcements&amp;quot;, I will not announce here. I thought it was relevant since my Graphviz port uses Quartz extensively as a code gen. X. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com</body>
  </mail>
  <mail>
    <header>CalcCMask replacement?</header>
    <body>In the QD environment when our users import a bitmap TIFF we use CalcCMask to get a mask of the image with the &amp;quot;interior&amp;quot; areas filled in. How would we do this in Quartz? Bryan</body>
  </mail>
  <mail>
    <header>[ANN] Graphviz 1.10 (v4) for Mac OS X</header>
    <body>Dear All, Thanks for your continued feedback for Graphviz on Mac OS X, it's good to hear from all the people out there playing and working with it. I've implemented the most requested feature -- automatic re-rendering when the dot file is modified in a different program -- which would make it useful to edit dot files in TextEdit for example. Download it, try it, and have fun! Changes ------------ Added re-rendering when file is modified on 10.3 (kqueue thread) [JSc]. Added CLI tools: gc, gvcolor, acyclic, nop, ccomps, sccmap, tred, unflatten and dijkstra. Added agraph library. Fixed open, close and reopen sometimes causing a crash (remove observers during dealloc) [SSe]. Speeded up tool response (prebound on 10.3). Tracked main build of 21 Feb 2004. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com P.S. Thanks especially to Jon Schull and Sanford Selznick, who even provided a crash report.</body>
  </mail>
  <mail>
    <header>Re: excluding a window from expose</header>
    <body>And for any of you who may not have heard this before, these calls are private to CoreGraphics and subject to change whenever Apple decides to change them. If you want support for this, file bugs requesting the feature at Radar. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: excluding a window from expose</header>
    <body>Am 21.02.2004 um 00:56 schrieb Jason Harris: The solution was posted in cocoa-dev. You can search it using cocoa.mamasam.com Here ist the relevant mail: ---------- Well, since nobody answered my post of yesterday, I've got my own hands dirty...   ;-) After some poking around, here is a new version of the method that makes windows ignoring Expose __and__ (Applause, please!) gets them back to normal behaviour: - (OSStatus)setSticky:(BOOL)flag if (flag) else Of course, like the other stuff, CGSClearWindowTags() has to be declared somewhere: extern OSStatus CGSClearWindowTags(const CGSConnection cid, const CGSWindow wid, int *tags, int Cheers, Andreas ________________________________________________________________________ __ Aksima Andreas Kummer Helene-Mayer-Ring 10/706 80809 Muenchen Germany Fon: ++49-89-15909290 Fax: ++49-89-15909289 Email: email@hidden ________________________________________________________________________ __</body>
  </mail>
  <mail>
    <header>Re: excluding a window from expose</header>
    <body>Unfortunately, I don't see a way to search the archives.  quartz-dev isn't listed on the search page and guessing doesn't work. Jason Harris [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: excluding a window from expose</header>
    <body>The archives are here the page (text &amp;amp; web pages) -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: Help needed with swapping issue around reading Jpeg with  CoreGraphics on 10.2.4 Server</header>
    <body>I note that you're releasing origImage but not data.  Are you retaining both of these in setImage? Nick</body>
  </mail>
  <mail>
    <header>Re: excluding a window from expose</header>
    <body>Hi Tyler, It has been discussed on the Cocoa-dev mailing list a few weeks ago. I didn't followed it in depth, so I'm not sure if it has been resolved. Take a look at &amp;lt;&amp;gt;. Eric ___________________________________________________________________ Eric Forget                       Cafederic</body>
  </mail>
  <mail>
    <header>excluding a window from expose</header>
    <body>Can anyone please explain how to create a window (preferably an NSWindow) that is excluded from being manipulated by exposi?  I wondered initially whether windows above (or below) a certain level might be excluded automatically. For example, the screensaver window and the desktop do not get manipulated. But alas, setting the window level to kCGScreenSaverWindowLevel (or kCGDesktopWindowLevel) had no beneficial effect. By the way, I didn't search the archives about this topic before posting because I couldn't find &amp;quot;quartz-dev&amp;quot; anywhere on the list archive search page (  ). Is it there under some other name, or is there some other way to search the quartz-dev archive, other than manually reading every previous thread? Thanks, Tyler</body>
  </mail>
  <mail>
    <header>Help needed with swapping issue around reading Jpeg with  CoreGraphics on 10.2.4 Server</header>
    <body>Hi We're having issues in .Mac production environment that started after I changed the code that generate thumbnails. I updated the code to read the JPG files using CoreGraphics instead of libjpeg in order to fix bugs we had we ColorSync not being respected, CMYK images, Grayscale images etc ... Since I've done that we started having memory issues. This is really weird. 6 instances of the process run on a machine, top shows that they stay between 15-30 RSIZE and between 130 - 160 VSIZE, but top shows that the free memory decrease steadily to the point were the machine has to be rebooted. It takes ~ 4 hours, and the box, when the app starts, has aroud 1.8 GB of RAM available. I've run leaks -cycles and it shows me leaks, but few of them. I attach a few of them. Of course these are the only process that run on that box. I would appreciate some qualified insider's look on the following code, and on the attached leaks reports. Moving to Panther isn't an easy option as this process are WebObjects 4.5.1 Objective-C. But bugs might have been fixed post 10.2.4 ? I would also appreciate any offer for a workaroud. Would eventually go to Quicktime layer myself to achieve the same goals could workaround some bugs ? Like in the DeveloperConnection example QTtoCG ? here is the init method that changed where I'm now using CG to get the data out of an image file. I'm doing the same with PNGs - initWithJPGFile:(NSString *)filename origImage = CGImageCreateWithJPEGDataProvider(provider, NULL,1, // setup some general info // create a context c = CGBitmapContextCreate([data mutableBytes],width,height,bitsPerComponent,bytesPerRow,cs,kCGImageAlpha // done.  release everything local // make sure it worked Thanks in advance, Benoit</body>
  </mail>
  <mail>
    <header>Re: Jaguar: Is the result of CGColorSpaceCreateDeviceRGB() a  cached object ?</header>
    <body>So it doesn't really matter if I call CGColorSpaceRelease(cs); or not ? Benoit</body>
  </mail>
  <mail>
    <header>Re: Jaguar: Is the result of CGColorSpaceCreateDeviceRGB() a  cached object ?</header>
    <body>Think of it as an optimization, rather than a behavior you can always rely on.  Some very frequently used invariant objects are kept around by the system.  The behavior may change in different releases. Your code has obtained a reference to a CGColorSpace object, and is responsible for releasing it.</body>
  </mail>
  <mail>
    <header>Re: Jaguar: Is the result of CGColorSpaceCreateDeviceRGB() a  cached object ?</header>
    <body>Yes; ColorSpaceRefs are ref-counted. --(jm)</body>
  </mail>
  <mail>
    <header>Jaguar: Is the result of CGColorSpaceCreateDeviceRGB() a cached  object ?</header>
    <body>Hi In a server (Jaguar) application with no UI, I keep creating colospace objects with CGColorSpaceCreateDeviceRGB(), but each time I get the same object back, is that expected ? Thanks, Benoit</body>
  </mail>
  <mail>
    <header>Re: Symbol font and quartz</header>
    <body>On Feb 17, 2004, at 7:37 PM, Andreas Mayer wrote: Here's a post from the carbon-dev list just a couple of days ago... On Feb 17, 2004, at 7:01 PM, Deborah Goldsmith wrote:</body>
  </mail>
  <mail>
    <header>Re: Symbol font and quartz</header>
    <body>Am 18.02.2004 um 01:17 schrieb stefano iacus: Hm. What's wrong? I can use special characters in TextEdit just fine. Are you sure the characters you want are even defined in Mac Roman encoding? Try to use Unicode instead. bye.  Andreas.</body>
  </mail>
  <mail>
    <header>Symbol font and quartz</header>
    <body>Hi all, I'm getting mad with displaying Symbol font graphics with quartz under Panther. I need essentially to print Integrals, summations symbols etc. Till Jaguar, my code works fine if I set the font with something like but under Panther there's no way to display such a font. I've seen that no Cocoa application (mine is a carbon one) like TextEdit (and even the font panel!) displays the font correctly. Is this font messed under Panther for some reason? Do you have some hints? stefano p.s. I apologize for cross posting.</body>
  </mail>
  <mail>
    <header>How expensive is CGContextSave/RestoreGState?</header>
    <body>We might have to call the Save/Restore pair quite a bit and I would like to know if those are expensive calls.   While I'm asking... how expensive is calling CGContextClip()? Thanks, -Mike</body>
  </mail>
  <mail>
    <header>Re: Drawing a Portion of an Image</header>
    <body>Ahhhhh... I guess I'd do the same thing with CGShading? Kind of a funky way to go about it. Seth Willits ------------------------------------------------------------------------ --- President and Head Developer of Freak Software - REALbasic Guru at ResExcellence - Webmaster for REALbasic Game Central - ------------------------------------------------------------------------ ---</body>
  </mail>
  <mail>
    <header>Re: Drawing a Portion of an Image</header>
    <body>You'd clip to the area you wanted to see and draw the image at full size.  For example: CGContextSaveGState CGContextClipToRect CGContextDrawImage CGContextRestoreGState Nick</body>
  </mail>
  <mail>
    <header>Drawing a Portion of an Image</header>
    <body>CGContextDrawImage will resize the image to fit within the rect. What I'd like to do is have it draw the image at the original size, but only draw part of it. How would I do this? Seth Willits ------------------------------------------------------------------------ --- President and Head Developer of Freak Software - REALbasic Guru at ResExcellence - Webmaster for REALbasic Game Central - -- Seth Willits ------------------------------------------------------------------------ ---</body>
  </mail>
  <mail>
    <header>Re: Quartz Drawing Weirdness</header>
    <body>Oh, I see what Nick meant. Awesome. That was the problem. Thanks for the help guys, Seth Willits ------------------------------------------------------------------------ --- President and Head Developer of Freak Software - REALbasic Guru at ResExcellence - Webmaster for REALbasic Game Central - -- A Bringer of Truth ------------------------------------------------------------------------ ---</body>
  </mail>
  <mail>
    <header>Re: Quartz Drawing Weirdness</header>
    <body>Your clearing fill is likely happening someplace you don't expect because you are not setting the origin of the rect that you are attempting to clear. So at runtime who knows the location that you will be filling and it will likely vary over time. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Quartz Drawing Weirdness</header>
    <body>I forgot to say the what is on the left is what it starts out as, and on the right is what it ends up as. That image is a splice of two separate screen shots. Seth Willits ------------------------------------------------------------------------ --- President and Head Developer of Freak Software - REALbasic Guru at ResExcellence - Webmaster for REALbasic Game Central - -- ------------------------------------------------------------------------ ---</body>
  </mail>
  <mail>
    <header>Re: Quartz Drawing Weirdness</header>
    <body>I'll assume the rectangle size is correct but you really should also set the origin.  FYI, it's easier if you post the code with the question rather than linking to it. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Quartz Drawing Weirdness</header>
    <body>The applicable code: The effect I'm seeing: I'm calling this function on every iteration of my game loop, however it will only actually draw anything if it needs to (ie, the score changes, you die, etceteras). More often than not, it initially draws normally, (in the actual function that draws everything else too, I often will get a white background instead of black) but the problem is that instead of erasing and drawing everything fresh, the score text is drawn on top of itself several times, and then after 5 or 6 times, it will draw correctly and the process starts all over. The code above is the simplest form in which I can reproduce the problem. I have no idea what is causing this, but it seems to me like Quartz isn't getting a chance to do everything it needs. Am I missing something? Thanks, Seth Willits ------------------------------------------------------------------------ --- President and Head Developer of Freak Software - REALbasic Guru at ResExcellence - Webmaster for REALbasic Game Central - ------------------------------------------------------------------------ ---</body>
  </mail>
  <mail>
    <header>Re: Porting from QuickDraw to Quartz</header>
    <body>On Feb 11, 2004, at 3:26 PM, Nick Nallick wrote: Yep, it was rhetorical. I'm trying to demonstrate how people coming to Quartz from QuickDraw might think. And with the documentation as it stands today, something this simple can take a while to track down. Bryan</body>
  </mail>
  <mail>
    <header>Re: Porting from QuickDraw to Quartz</header>
    <body>This may have been a rhetoric question, but I'll throw it out anyway. For bitmap and PDF context's there are specific ways to get the size but in general you can use CGContextGetClipBoundingBox. Nick</body>
  </mail>
  <mail>
    <header>Re: Porting from QuickDraw to Quartz</header>
    <body>I think this is a great idea. For example, today we needed to draw &amp;quot;page guides&amp;quot; -- the document window's content would be light shaded except for the page area. In QuickDraw we start this by calling GetPortBounds. But there is no CGGetContextBounds, so what do we use? Do we need to have information about the window associated with this context at our lower levels?  (By the way, it might be handy to think about adding properties to contexts ala the window, menu, control properties in Carbon -- it's very handy sometimes to be able to tie arbitrary data to an OS object.) Looking here: Quartz_2D_Ref/qref_main/APIIndex.html offered nothing, but we found CGContextGetClipBoundingBox in the headers which can do what we need in this case. But this is an example where explaining why there is no equivalent of GetPortBounds would help QuickDraw developers understand Quartz a lot better. Bryan</body>
  </mail>
  <mail>
    <header>Re: Documentation Needs</header>
    <body>This sample shows how to implement simple Quartz 2D replacements for the QuickDraw rounded rect and oval drawing functions. strokeOval, fillOval, strokeRoundedRect, and fillRoundedRect implement Quartz 2D equivalents to the QuickDraw oval drawing and rounded rect APIs, using CGRects instead of Rects. See DrawWindow() for sample usage. Dave</body>
  </mail>
  <mail>
    <header>Re: Documentation Needs</header>
    <body>Yeah I was aware of that, thanks, but I think if you are asking about overall documentation needs for Quartz there has to be a more cohesive feel to it with more of an emphasis on the QD programmer moving to it. Bryan</body>
  </mail>
  <mail>
    <header>Re: Documentation Needs</header>
    <body>Am 11.02.2004 um 17:37 schrieb Bryan Pietrzak: ovalteen.htm Manfred</body>
  </mail>
  <mail>
    <header>Re: Documentation Needs</header>
    <body>I'd go back to Inside Macintosh: QuickDraw and take a look at this style and approach. What's missing is simple, concise, practical examples... 1. For example, how can a QD developer replace the Round Rect calls? (someone really really needs to write a little QD &amp;quot;emulation&amp;quot; library &amp;quot;MoreQuartz&amp;quot; that can help this transition) 2. more examples on shaders, including custom shaders 3. examples of loading an image from the app bundle's resources folder and drawing it somewhere (these are real-life problems that developer will face) 4. PS to PDF converter 5. Marching Ants for 2004 -- how to show selection areas in the OS X way using overlays 6. examples on drawing 1 pt thick lines that look like 1 pt thick lines (and perhaps how to do so regardless of the contexts scale -- perhaps someone just wants to draw some lines around some interface element, but they don't want that magnified along with the 7. More examples demonstrating the differences in coordinate systems and how context scaling impacts that. For the non PS programmer, it's source of confusion I'm sure there's more, but off the top of my head, that's  start. I think the big thing is more practical examples ala the old IM. Bryan</body>
  </mail>
  <mail>
    <header>pure black becoming mixed color</header>
    <body>Hello! We have a publishing app which (among other things) reads print-ready PDF files and writes them out, combined in one big PDF page. The problem is with the black color in DeviceGray colorspace, which becomes color-managed and is not pure black any more (gets some non-zero CMYK values other than K). If the color is in CMYK color space, everything is ok, because CMYK colors do not change. Is there any way this can be prevented or corected (some kind of filter?) ? The problem is that just reading PDF and writing it back messes up the colors in document... izidor P.S. Also, the additional data such as OPI comments are just stripped away. It would be nice to be able to hook into Quartz and support reading/writing additional PDF data. In this way one could also decide whether to attach icc profile to a color without it or not...</body>
  </mail>
  <mail>
    <header>Minimum Font Smoothing Size</header>
    <body>I have an app, used for creating insurance forms, that draws a lot of small text into a CG context via ATSUI.  Some of this small text falls below the system's minimum text smoothing size in the Appearance preference panel (even at the minimum 4 point size).  When this happens there is a dramatic difference in appearance between this small text and the imperceptibly larger text next to it.  This difference is maintained even if I enlarge the image by scaling the CTM. I could be wrong, but I'm assuming this is a Quartz issue rather than an ATSUI issue because this is a system-wide preference. I would like to override this system-wide minimum in my app.  How can I do this? Thanks, Nick</body>
  </mail>
  <mail>
    <header>Anti-Aliasing Quality when Drawing PDF</header>
    <body>I noticed that Preview draws embedded bitmap images anti-aliased, but when I draw PDFs myself bitmap images are aliased. How do I control the anti-aliasing quality when drawing PDF? CGContextSetInterpolationQuality() on the context has no apparent effect. Manfred</body>
  </mail>
  <mail>
    <header>Re: How to avoid stretched screen mode?</header>
    <body>On Jan 13, 2005, at 11:33 AM, Shawn Erickson wrote: Oh I also meant to state that I am asking for a 1280 x 800 x 32 which on the config side of things has two display modes listed one with the stretched key set to a value of true and the other with no key (listing is built by getting and walking all display modes for the selected monitor). In fact on the config side of things the user is picking the exact same dictionary (contents wise) as the one we attempt to use on the game side of things. capacity = 11, pairs = (</body>
  </mail>
  <mail>
    <header>Re: How to avoid stretched screen mode?</header>
    <body>On Jan 13, 2005, at 11:03 AM, Mike Paquette wrote: Yes my CFShow is before my attempt to force things (note it is tagged as being immutable). FYI, I tried the key removal as a way to force things only because I couldn't stop stretched mode other ways, it was just dropped in temporarily. In other words the display mode I am firing down to switch mode, even when using the original one returned to me,  is not showing that it will use stretched mode yet it still does. Ok. As a side note my mutable copy of the original works the same as the original (note no key is found to be removed)... I guess the keys and &amp;quot;IOFlags&amp;quot; with the later two being used to map the dictionary to an IOKit mode/configuration. Does the values  for IOFlags give anyone a hit on stretched mode or not? The dictionary implies it isn't stretched because the stretched key-value isn't set. I may be hitting a bug in Quartz services then since I appear to working with a stretched mode however I cannot detect that at my level using the API as documented since the needed key-value isn't being set. Note when walking all supported display configurations I do see stretched and non-stretched configurations so it is setting the key for at least some of them (I wonder Quartz services has things backwards)? Anyway I am running 10.3.7 on a PM G5 2GHz (1st gen) with an Geforce 6800 GT and two 23&amp;quot; cinema displays attached (monitor &amp;quot;0&amp;quot; is my secondary display and monitor &amp;quot;1&amp;quot; is considered the main display as I have things wired and configured via monitor preferences).</body>
  </mail>
  <mail>
    <header>Re: How to avoid stretched screen mode?</header>
    <body>Also, you cannot create your own CFDictionary and pass it to CGDisplaySwitchToMode().  The CFDictionary passed to CGDisplaySwitchToMode() must be a member of the list returned by CGDisplayAvailableModes().  There's a list lookup that checks for this. The CFDictionary describing a display mode is an encoding of an internal IOKit structure (so it won't break horribly across releases), and we map the object to the IOKit internal data to apply the mode change.  If the CFDictionary originated from an IOKIt display mode description that does not put the display in stretched mode, then you'll get that unstretched mode.</body>
  </mail>
  <mail>
    <header>Re: How to avoid stretched screen mode?</header>
    <body>On Jan 13, 2005, at 10:13 AM, Mike Paquette wrote: I added a CFShow after I ask for the best mode I get the following which doesn't have mode is stretched key in it yet I still get stretched. capacity = 11, pairs = ( Nope no TV output support is wanted. Yeah I know... I guess you missed the fact that I am making a mutable copy of the dictionary returned. :-) It looks like they may be weighted to much towards that since I cannot stop them from doing it. :-(</body>
  </mail>
  <mail>
    <header>Re: How to avoid stretched screen mode?</header>
    <body>The  CGDisplayBestMode... functions are helpers intended to make simple requests fairly straightforward.  They can find entries in the list vended by CGDisplayAvailableModes() that meet some basic criteria, but they are not the ultimate in matching systems.  :-) You can call CGDisplayBestModeForParametersAndRefreshRateWithProperty() to obtain a typical display configuration with resolution and refresh rate that meet your needs.  Very often, for each such mode description that meets your specification, but has the kCGDisplayModeIsStretched property, there is another mode with the same values, but without that property.  (I cannot guarantee this, as I am but a software geek, and am not worthy to speak of the works of hardware wizards.) You can call CGDisplayAvailableModes() and inspect the complete list for modes that match the width, height, and refresh rate returned in the mode dictionary from CGDisplayBestModeForParametersAndRefreshRateWithProperty().  In your case, you'll also want to check for the presence of the key kCGDisplayModeIsSafeForHardware, and the absence of the key kCGDisplayModeIsStretched. If you are trying for a TV output, as via the S-Video connector, make sure the kCGDisplayModeIsTelevisionOutput property is present, and the refresh rate is appropriate for the user's TV.  NTSC uses a different rate than PAL or SECAM.  Since there's no monitor feedback (EDID data or similar information) from TV devices, I suggest that you always use a confirm dialog and a timer to revert the display mode on inaction on any modes marked kCGDisplayModeIsTelevisionOutput. Note that the CGDisplayBestMode... functions return a CFDictionaryRef, not a CFMutableDictionaryRef.  There's no guarantee that CFDictionaryRemoveValue() on a CFDictionaryRef won't just fail, or worse, crash your application. In general these functions are weighted to prefer stretch modes, so as to always fill the screen, and reduce complaints about those darn black bands (matte bands).</body>
  </mail>
  <mail>
    <header>How to avoid stretched screen mode?</header>
    <body>I am working on modernizing a game that uses mode switching to setup full game play. The following is an example of the code that we are trying to use to switch display mode as needed. We would like to avoid the use of stretched mode (stretching of the horizontal pixels to fill the display) and our configuration GUI is code to not offer stretched mode selection at this time when picking resolutions. This game is cross platform so as a result (at least currently) the configuration GUI cannot easily pass the display mode dictionary (I guess we could write it out as a plist) itself to the game application so instead we pass widths, height, bpp, and refresh rate. // Insure we have a reasonable display mode... displayConfig.displayMode = CGDisplayBestModeForParametersAndRefreshRateWithProperty( displayConfig.displayID, mybpp, mywidth, myheight, myrefreshrate, kCGDisplayModeIsSafeForHardware, // Make sure we don't use streched mode (trying to prevent it with following code)... displayConfig.displayMode = (CFDictionaryRef) CFDictionaryCreateMutableCopy( kCFAllocatorDefault, CFDictionaryGetCount(displayConfig.displayMode), CFDictionaryRemoveValue((CFMutableDictionaryRef)displayConfig.displayMod // Switch the monitor into the mode requested... CGDisplaySwitchToMode(displayConfig.displayID, So does a way exist to avoid stretched mode? Do I have to use something other then CGDisplaySwitchToMode? Or am I just missing something obvious?</body>
  </mail>
  <mail>
    <header>CreateStandardAlert / RunStandardAlert</header>
    <body>I am creating an alert box using CreateStandardAlert / RunStandardAlert from&amp;nbsp;&lt;SPAN class=003455905-07012005&gt;Dialog's filter proc&amp;nbsp;b&lt;SPAN class=003455905-07012005&gt;ut I am unable to see buttons and text on alert box. I see only a blank alert box.&amp;nbsp;Is there&amp;nbsp;anyone who have faced similar problem earlier &lt;FONT face=Verdana color=#000080 size=2&gt; &lt;FONT face=Verdana color=#000080 size=2&gt;Thanks, &lt;SPAN class=493391411-05012005&gt;Manish</body>
  </mail>
  <mail>
    <header>Re: Compositing a mask and an image</header>
    <body>On 5-Jan-05, at 1:01 PM, Nick Nallick wrote: Thanks to everyone for their input. I'll take a look at vImage. Thanks again, Jon -- Jonathan Johnson REAL Software, Inc. -- REAL World 2005 - The REALbasic User Conference March 23-25, 2005, Austin, Texas</body>
  </mail>
  <mail>
    <header>Re: Hardware requirements</header>
    <body>On Jan 5, 2005, at 2:29 PM, John Kerr wrote: I don't believe Apple has said anything publicly about that. Well, they've said that programmable shaders are required so that would certainly up the ante on the video card needs. I don't know that that information has been made available. But Apple has said that CI requires programable shaders, so it's a good bet that CI and Q2DE share the same requirements. fwiw, the only data for CI requirements can be found here if you haven't already seen it: Bryan</body>
  </mail>
  <mail>
    <header>Re: Hardware requirements</header>
    <body>On Jan 5, 2005, at 12:29 PM, John Kerr wrote: This is all under NDA still so if it isn't on Apple website then folks cannot say (unless an Apple employee chimes in). Additionally you are also sharing information about a product (your findings that you outlined) that you likely got under NDA (if not that is another issue) and hence could be considered in violation of the NDA.</body>
  </mail>
  <mail>
    <header>Hardware requirements</header>
    <body>Does anyone know the hardware requirements of Quartz2D Extreme? It seems it is greater than Quartz Extreme.  I find that a GeForce4 MX with 64MB won't use Quartz2D Extreme, but a GeForceFX5200 with 64MB does work. Are the requirements the same as for CoreImage?</body>
  </mail>
  <mail>
    <header>Re: Compositing a mask and an image</header>
    <body>On Jan 5, 2005, at 8:57 AM, Jonathan Johnson wrote: The CoreGraphics API doesn't currently export much in the way of &amp;quot;bitblit&amp;quot; operations.  You'll probably have to use pieces from Carbon and/or Cocoa or manipulate the raster data yourself.  One thing to remember is that CG expects alpha data to be premultiplied, whereas you're probably dealing with non-premultiplied data.  This is probably where you'd find vImage to be useful (as others have implied).</body>
  </mail>
  <mail>
    <header>Re: Compositing a mask and an image</header>
    <body>On Jan 5, 2005, at 7:57 AM, Jonathan Johnson wrote: You may want to look at using functionality provided by vImage since it would like be the fastest way to do this type of processing. Of course if a Quartz2D method exists I would use it instead since it likely would leverage vImage or similar code itself.</body>
  </mail>
  <mail>
    <header>Re: Compositing a mask and an image</header>
    <body>One method may be to use CGImageCreate.  Most of the parameters such as width, height, bitsPerPixel would be the same values as used for your &amp;quot;source&amp;quot; image.  You'd then also specify that the newly created image will have an alpha channel. I would then imagine that the data provider you pass to CGImageCreate would ultimately know how to access data from two input streams (source image and alpha image). Another option to explore is an image with pre-multiplied alpha.  That would definitely speed up rendering. You may want to also consider filing an enhancement request for a API that would effectively join a source and alpha into a single CGImage. I would think this would be limited to bitmap-based images. The QuickDraw-based version of one of my apps relied heavily on CopyMask.  I always had separate source and alpha images.  When I heard about the deprecation of QuickDraw, I first considered staying with Carbon and just using Quartz APIs.  But I didn't want to end up combining all my images at runtime especially since there was no convenient API. But now I've moved to Cocoa and all my images are either PDF with transparency effects or TIFF images with an alpha channel.  Thus, I never have to combine my images. If it's the case where you're in complete control of your images, you may want to consider combining them ahead of time. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Compositing a mask and an image</header>
    <body>If I have two CGImageRefs -- one representing the image and another representing the mask -- what's the best way to combine them into one image with an alpha channel? Thanks, Jon -- Jonathan Johnson REAL Software, Inc. -- REAL World 2005 - The REALbasic User Conference March 23-25, 2005, Austin, Texas</body>
  </mail>
  <mail>
    <header>Re: documentation problems</header>
    <body>On Nov 30, 2004, at 2:51 PM, Travis Heppe wrote:</body>
  </mail>
  <mail>
    <header>Re: documentation problems</header>
    <body>On Nov 30, 2004, at 4:51 PM, Travis Heppe wrote: Pattern space is the coordinate system into which you draw your pattern.  To put it another way, the pattern lives within its own coordinate system that is separate from all other spaces.  You establish the relationship between Pattern Space and the current &amp;quot;base space&amp;quot; when you call CGPatternCreate.  The affine transform you pass to that routine is used to map from pattern space to the &amp;quot;base space&amp;quot;. Usually the &amp;quot;base space&amp;quot; is the same thing as &amp;quot;user space&amp;quot; but does not necessarily have to be.  If you are drawing a PDF document, for example, and that PDF document contains a pattern, the transformation of the pattern will map to the &amp;quot;base space&amp;quot; of the PDF document you are drawing.  That way if you draw the PDF rotated, scaled, or otherwise, the CGPattern will be drawn relative to the coordinate system of the PDF itself, and not the User Space into which the PDF is being drawn. I guess I don't understand the effect you are trying to achieve.  Are you trying to replicate something similar to CGContextSetLineDash, but within a filled shape?  What do you mean by &amp;quot;diagonal dashes&amp;quot;? It sounds like you are trying to do the equivalent of a stroke dash a l√° CGContextSetLineDash...  How is what you are trying to do different from that?</body>
  </mail>
  <mail>
    <header>documentation problems</header>
    <body>In the &amp;quot;Painting stencil patterns&amp;quot; section of there are a few errors: The biggest problem I have with the document, however, is that it has no (as far as I have found) definition of the term &amp;quot;pattern space&amp;quot;.  To the best that I have been able to determine by trial and error, &amp;quot;pattern space&amp;quot; is simply the standard context with an appropriate positioning matrix, but no scale or orientation. I am trying (so far without luck) to create a stencil stroke pattern for a rotation.  In other words, diagonal dashes should not look longer or shorter than horizontal ones.  I was hoping that stroke patterns would get me there, but am I perhaps barking up the wrong tree? Quartz' pattern API seems oriented towards 2d patterns, and unsuitable for the kind of 1d patterns that orient along the line vector that I am trying to create.</body>
  </mail>
  <mail>
    <header>Re: poor rendering quality when printing pdf's that use	transparency	layers</header>
    <body>The rendering resolution was indeed 72 dpi in some usage cases of transparency layers. This is a known issue and will very likely be fixed in an upcoming 10.3 software update. A few other transparency layer related bugs will also be fixed in an upcoming software update. Can't say when it will be made public though. Attachment:</body>
  </mail>
  <mail>
    <header>Re: poor rendering quality when printing pdf's that use	transparency layers</header>
    <body>On Nov 25, 2004, at 9:37 AM, R. Scott Thompson wrote: the question i'm raising is &amp;quot;when should rasterization occur?&amp;quot;. transparency and shadows are a slightly different issue from transparency layers, they can be used without transparency layers.  my issue is that i'd like the rasterization of transparency layers to be delayed until render time. a 300 dpi image will be scaled down to 72 dpi.  that is what i meant. this isn't a shadowing issue but a transparency layer issue.  i don't have a problem with shadowing or transparency in pdf, it looks good with apple's pdf renderer and when printed it looks good with apple's rip.  my problem is with transparency layers, these look bad when printed or even when simply zoomed in in preview.app the print preview is going to generate pdf, for the most part the resolution of the images is going to be app dependent and should either match the resolution of the printer or the native resolution of the images themselves.  whichever makes sense.  however, if transparency layers are used everything is going to be rendered into a bitmap at 72dpi apparently and look bad. let me try to clarify what i was saying.  from my testing, it looks like when quartz creates a transparency layer it simply renders all drawing into a 72 dpi bitmap ref, and then composites that 72 dpi image onto the page using the transparency and shadowing parameters.  what i'm suggesting is drawing into a separate pdf xobject stream and embedding that stream.  thus, any embedded images (or text or vector graphics i imagine) would be preserved at full resolution.  this allows the quartz renderer to rasterize and composite that xobject at the target resolution that matches the output device.  if pdf is going to be rendered on-screen it has to be rasterized at some point.  it looks like transparency layers are rasterizing at document creation time.  i think this could be delayed to document rendering time. apple hacked shadowing into pdf and it looks good with their renderer even when printing.  i can live with that.  transparency layers are introducing an additional level of restrictions that look bad even when simply zooming in on the display, as well as when printing. it would vary according to the rip and i'm willing to accept that. that's the current situation. i just want to reiterate that shadows and transparency aren't the problem.  i can deal with the current situation there (which looks good when using quartz's rip and quartz pdf).  my issue is with transparency layers.</body>
  </mail>
  <mail>
    <header>Re: poor rendering quality when printing pdf's that use	transparency	layers</header>
    <body>Unfortunately rasterization unavoidable.  The soft drop shadows generated by Quartz 2D are images.   Even worse, depending on the output device, the printer may not support transparency.  The only way to reliably reproduce the effect is through some kind of rasterization. I don't know what you mean by &amp;quot;scaled down&amp;quot; but the images are most likely rasterized at 72 dpi because the shadow effect is primarily intended for on-screen use (which, by convention, is understood to be 72 pixels/inch on current systems... this may change). As an experiment, it might be interesting to see if a print preview of a saved to PDF, whose destination was a high-resolution ink-jet printer, uses the native resolution of the printer or just 72 DPI. Presumably you mean a transparency group XObject? But what would you put in the XObject stream?  Consider that Quartz is already recording your drawing commands into a PDF and it is not able to satisfy your request.  Trying to package drawing commands that are just as effective into an XObject is not going to buy you anything. I think the problem with your suggestion is that, XObject or not, there are no PDF operators for generating a soft drop shadow. You'd still be stuck having to put some kind of rasterization of the shadow into the PDF. You would also run into the problem that the same PDF would behave VERY differently on different printers.  Printing it to a low-resolution Ink-Jet printer might give you reasonable behavior.  If you took that same document to a 2400 DPI image setter it would suddenly generate a print stream that is something on the order 60 or 70 times larger and require much more memory to boot. I don't really think the shadow APIs are intended for print work. Transparency, in general, is a dicy prospect with regards to printing. Another ambitious user, for example, noted that shadings with transparency could cause problems when printing earlier. If the drop shadows are important to your content, then there are at least two approaches to consider. You could work with Quartz to rasterize the shadows at a higher resolution yourself. Just use the same technique the system is using... create an offscreen Context with enough pixels to represent your drawing and it's shadow at (say) 300 dpi resolution.  Set the scaling on that context to scale from 72 DPI to 300 DPI and draw your object into the offscreen, then use CGDrawImage to place it in the final document.  You're pulling the same trick... but at a higher resolution which may make the results more palatable. You could even make the resolution user-selectable.  That way if the user wanted to bloat their PDF with the shadow effect rendered at 2400 dpi resolution then they could do that.  That's the approach taken in FreeHand 11 for printing raster effects (with the additional caveat that they are all generated in an RGB color space and not properly color managed which pretty much makes them useless for anything on a real printer... but I digress). Another technique would be to generate the drop shadows as a &amp;quot;blend&amp;quot; of successively smaller objects with increasing alpha channels.  The first Object would be close to transparent.  The next, slightly smaller, would be more opaque.  So on and so forth until you get to something roughly the size of your object.  For simple shapes like a rectangle this would work pretty well.  If you have more complex shapes (curves and such) then it's going to be much harder to get it right.</body>
  </mail>
  <mail>
    <header>poor rendering quality when printing pdf's that use transparency	layers</header>
    <body>i've been using quartz 2D to generate pdf's with shadows and transparency effects, and it appears that when transparency layers are used in a pdf context, when that data is rasterized for printing (or rendering to screen) the transparency layers appeared to be rendered at a very low resolution (72 dpi i guess) and then scaled up.  this looks terrible when either printing or simply zooming in on an image in preview. i'd guess that the current implementation simply renders the pdf calls to a bitmap and then embeds that image.  one specific result of this is that high-res images are effectively scaled down to 72dpi at whatevever size the image is drawn into. i'd think that a better implementation would simply record the pdf commands into a pdf context, and then embed that pdf data as a x-object, and then when the pdf is rasterized for printing this would preserve full image fidelity. is this an accurate assessment of a known problem or am i simply using the api incorrectly?  are there any workarounds?</body>
  </mail>
  <mail>
    <header>Re: Reading pixels though translucent window</header>
    <body>If you're going to read the pixels straight from the screen, you may want to talk to DTS about a sample called glGrab that uses OpenGL to do just that. The sample used to be available on the ADC website but it comes and goes. DTS should either give you the sample for free or help you get your code working in exchange for a tech support incident. On Nov 24, 2004, at 5:43 PM, Jamie Hodkinson wrote:</body>
  </mail>
  <mail>
    <header>Re: Reading pixels though translucent window</header>
    <body>If there is no way to read the contents of windows underneath a window (and I can't assume anything about the windows underneath), is it possible to read the composite pixels from my window on top (assuming it is translucent?) - or will this have the same problem? Should I be reading pixels directly from the screen buffer?</body>
  </mail>
  <mail>
    <header>Re: Reading pixels though translucent window</header>
    <body>If you know, for some reason, that there's only a single window underneath your overlay (like for example if you created it deliberately to overlay that single window), you might be able to read its backing store directly. If you know that the underlying window is always your own window, you double-buffer it yourself. Dave Howell ProApps Engineering</body>
  </mail>
  <mail>
    <header>Re: Reading pixels though translucent window</header>
    <body>There is no mechanism to do this. The Quartz Compositor is going to composite all windows with the Source Over Porter-Duff composite operation. There really is no way to read the contents of all windows underneath a window.</body>
  </mail>
  <mail>
    <header>Reading pixels though translucent window</header>
    <body>Is there a simple way to read a pixel colour from the screen, though a translucent window? I'd like to create a 'filter' window, which is mainly translucent, except for certain parts, depending on the graphics underneath the window at that point, eg. if the window detects that the character 'a' is underneath it, it will highlight it in some way. I've looked in all the obvious places, but it is perhaps an unusual thing to be doing. Any help gratefully received.</body>
  </mail>
  <mail>
    <header>Re: gouraud triangles</header>
    <body>On Nov 24, 2004, at 1:52 PM, Travis Heppe wrote: The interpolation mechanism is context dependent.  You can provide a hint to the context as to what quality of interpolation you would like (CGContextSetInterpolationQuality) but I don't think you can dictate which mechanism the context uses.</body>
  </mail>
  <mail>
    <header>Re: gouraud triangles</header>
    <body>Thanks for the help. 3D hardware was indeed made to handle things like gouraud shading.  We **do** use opengl for display.  What I am trying to implement here is our hardcopy solution. On your suggestion, I'll try out the sampled image approach.  The docs don't say anything about how images are resampled when a scaling is applied.  Can I assume that it is bilinear?</body>
  </mail>
  <mail>
    <header>Re: gouraud triangles</header>
    <body>There is no way of creating general Gourad shaded triangles in Quartz 2D at the moment.  Quartz 2D currently only &amp;quot;understands&amp;quot; two types of shadings, linear and radial. While you can use linear gradients and transparency to reproduce some combinations of colors in a gourad shading, there is no way to reproduce that effect in the general case for all colors. Most notably, if you have three linearly independent colors (like red, blue, and green) then there is no way to reproduce a barycentric blend of those colors using linear gradients. like the ones in Quartz 2D. Your best bet is probably to use the sampled image approach.  If you want to generate the image as fast as possible, you could easily use OpenGL as the 3D hardware was MADE to handle this kind of shading.</body>
  </mail>
  <mail>
    <header>gouraud triangles</header>
    <body>I know I asked this question before, but I am still looking for an answer, and it is really important that I find something reasonable in this area. ----------------- What is the most appropriate API to implement gouraud triangles in Quartz2d (i.e. 3 point linear path, 3 independent colors, bilinear interpolation of color values).   I was looking at Axial and Radial shadings, but both seem to only have linear, not bilinear, interpolation. In postscript (language level 3), I believe it's something like &amp;quot;ShadingType 4&amp;quot;, but I haven't found the Quartz2d equivalent. ----------------- I was considering investigating: a) putting the vertex colors into a sampled image b) putting two linear shadings on top of each other, with the second being semi-transparent Some pointers from someone familiar with quartz could possibly save me from going down the wrong path.</body>
  </mail>
  <mail>
    <header>Re: OT?: Fixed Width fonts</header>
    <body>I should have mentioned.  If you want to monkey with the tables, you can use the Mac OS X font tools (duh) from fonts.apple.com. ftxdumperfuser - t hhea MPW ftxdumperfuser - t hmtx MPW</body>
  </mail>
  <mail>
    <header>Re: OT?: Fixed Width fonts</header>
    <body>On Nov 22, 2004, at 1:39 PM, R. Scott Thompson wrote: Thanks Scott, this gives me some places to look.  Seeing as MPW is an Apple font, maybe I'll see if Greg Branche can track someone down inside Apple to look at this ;) I tried, I really did! But I'm thoroughly addicted to using the MPW font in source code. :(</body>
  </mail>
  <mail>
    <header>Re: OT?: Fixed Width fonts</header>
    <body>I think what you're fighting against is whether or not the font will use fractional metrics. I have a vague, and potentially flawed, recollection that fixed width fonts do not use fractional spacing (because the fixed widths are usually integral?) Presumably the MPW font is some kind of FOND based font?  Bits 15 of the FOND's family flags denotes whether or not the font is a fixed width font.  You might de-rez the FOND and see what the value of that flag might be.  If it's not set to 1 then you could use Rez/DeRez to hack the font to include that bit and see if that makes a difference. (another alternative would be DumpFOND and FuseFOND at developer.apple.com/fonts/Tools/index.html). That's not to say for certain that the system will care about the flags field of the FOND (in general it doesn't care about FONDs any more) but it might be worth a shot. Beyond that you may be stuck having to look at the metrics tables within the font itself.  The 'hhea' table indicates how many metrics entries there are in the font and the 'hmtx' table contains each advance width.  If you edit the 'hhea' table and set it up to say there's only one metrics value, then change the 'hmtx' to provide a single left side bearing and advance width... then the font should be come monospaced.</body>
  </mail>
  <mail>
    <header>OT?: Fixed Width fonts</header>
    <body>I have a font that I used for development (the venerable MPW font). I've used it for ages in MPW, BBEdit and CodeWarrior. I'd like to use it in Xcode as well, but it gets anti-aliased in Xcode and looks terrible. And the problem goes away, but that's a bit of a hack, and doesn't allow my comments in Verdana 9 to be anti-aliased ;) It seems like the &amp;quot;Right Thing&amp;quot; would be for the OS to recognize the MPW font as a Fixed Width font (e.g. it does NOT appear in the Fixed Width collection in the Font panel) where (I assume) the AppleSmoothFixedFontsSizeThreshold preference would apply a different threshold. (I assume that's why Monaco looks ok in Xcode.) So, any advice on where to go for more information about making OS X realize that the MPW font is a fixed width font?</body>
  </mail>
  <mail>
    <header>Re: Text tracks</header>
    <body>On Nov 21, 2004, at 2:54 PM, CFlatow wrote: Try asking on one of the QuickTime lists.</body>
  </mail>
  <mail>
    <header>Text tracks</header>
    <body>I have just been using text tracks to annotate audio QT movies. I was using 42 point text which worked fine but when I changed the text size designation to 72 it still came out 42 pt. Does anyone know if there's a size limit on text tracks in QT movies? Thanks, Carl</body>
  </mail>
  <mail>
    <header>CGGLContext</header>
    <body>When quartz is operating on top of an OpenGL context, what OpenGL state affects the quartz drawing? glRasterPos? GL_DEPTH_TEST? GL_MODELVIEW matrix? glFrontFace? Conversely, do any quartz calls affect OpenGL state?</body>
  </mail>
  <mail>
    <header>Re: Converting PDF graphics to other formats</header>
    <body>I've converted my app to draw using Quartz, but I need to be able to &amp;quot;capture&amp;quot; parts of my windows into an in-memory &amp;quot;metafile&amp;quot; format. I used to do this with PICT, but in the brave new world, I am using PDF, which I capture by drawing into a CGPDFContext. (Using PDF for this might be my first mistake, but what's the alternative?) Now that I have a graphic in PDF format, I want to convert it to something else, like PNG. How do I do that? Back in the old days, I would use QuickTime's GraphicsExport facility, but that has numerous problems with PDF. Does it even support PDF?</body>
  </mail>
  <mail>
    <header>gouraud triangles</header>
    <body>What is the most appropriate API to implement gouraud triangles in Quartz2d (i.e. 3 point linear path, 3 independent colors, bilinear interpolation of color values).   I was looking at Axial and Radial shadings, but both seem to only have linear, not bilinear, interpolation. In postscript (language level 3), I believe it's something like &amp;quot;ShadingType 4&amp;quot;, but I haven't found the Quartz2d equivalent.</body>
  </mail>
  <mail>
    <header>Re: Output to PNG</header>
    <body>On Nov 17, 2004, at 12:27 PM, Tomas Franz√©n wrote: You're going to have to use an external agent to do the PNG writing.  A good tool in this case is QuickTime which has exporter components that can write to a large number of file formats, including PNG.  The basic QuickTime Export takes approximately 5 lines of code as I recall.  You can get fancier about it if you want to set the parameters of the export (say the compression settings of a JPEG) and QuickTime will even go so far as to put up an image compression dialog for you, allow the user to set the settings, and then give you back an exporter set up to carry out the user's wishes.  That takes quite a bit more than 5 lines of code, however. Scott</body>
  </mail>
  <mail>
    <header>Output to PNG</header>
    <body>How do I output a CGContext/CGImage to a PNG (or some other format) file? I tried using CGBitmapContextGetData, but the data seems just to be the raw bytes. Thanks. Tomas Franz√©n Lighthead Software I'm listening to Moby - Harbour</body>
  </mail>
  <mail>
    <header>Re: QuickDraw Behavior in VRAM</header>
    <body>Oh... well my question was not so much directly related to NewGWorld as it was me trying to learn a bit more about the way memory is shuffled back and forth between main RAM and the video card.  It seemed to me that NewGWorld would (presumably) be creating a texture of some sort... somewhere... and I was trying to figure out how it could change that texture since the QuickDraw commands probably don't have access to VRAM directly. (Can the CPU somehow map a range of VRAM like the video card can map a section of system RAM?) Perhaps a better question might be... If I have a texture and want to change a part of it... but that texture resides in VRAM, (i.e. not system RAM shared through the magic of AGP) then how doe the computer manage getting the changes back and forth? I would almost be willing to be, however, that I can answer my own question by learning  a bit more about OpenGL ;-)  I'll go read some more documentation.</body>
  </mail>
  <mail>
    <header>Re: QuickDraw Behavior in VRAM</header>
    <body>On Nov 16, 2004, at 1:16 PM, R. Scott Thompson wrote: (...) When create a GWorld in VRAM using the &amp;quot;useDistantHdwrMem&amp;quot; flag to NewGWorld, and I subsequently want to draw in that GWorld, how are my drawing commands transferred to VRAM? (...) As the comment in QDOffscreen.h says, the useDistantHdwrMem is just one of the many old flags which NewGWorld royally ignores on Mac OS X.</body>
  </mail>
  <mail>
    <header>QuickDraw Behavior in VRAM</header>
    <body>I know this is a Quartz list, but I think my question has a better chance here than anywhere else. When create a GWorld in VRAM using the &amp;quot;useDistantHdwrMem&amp;quot; flag to NewGWorld, and I subsequently want to draw in that GWorld, how are my drawing commands transferred to VRAM? Does LockPixels on such a GWorld allocate a temporary local GWorld where QuickDraw draws followed by the results being flushed to VRAM on the corresponding call to UnlockPixels... or is there something else going on?</body>
  </mail>
  <mail>
    <header>Re: Compressing Images in a PDF</header>
    <body>On Nov 15, 2004, at 7:06 PM, Derek Clegg wrote: This is simply not true.  Flate compression can only compress regular images, CG-generated images.  It is next to useless for actual photographs, and the vast majority of images in user files are photographs. I noticed this problem in the first releases of OS X and thought that this would be fixed ASAP, because it is just so obvious.  When it wasn't fixed by Jaguar, I took an afternoon to hack together PdfCompress. Marcel Weiher                Metaobject Software Technologies email@hidden        www.metaobject.com Metaprogramming for the Graphic Arts.   HOM, IDEAs, MetaAd etc. 1d480c25f397c4786386135f8e8938e4</body>
  </mail>
  <mail>
    <header>Re: Compressing Images in a PDF</header>
    <body>On Nov 15, 2004, at 12:01 PM, Nick Nallick wrote: Yes, that's correct.</body>
  </mail>
  <mail>
    <header>Re: [OT] Regressing to EPS</header>
    <body>On Nov 15, 2004, at 3:24 PM, R. Scott Thompson wrote: Nevermind.  I learned that ghostscript can do this for me if necessary. Sorry for the noise.</body>
  </mail>
  <mail>
    <header>[OT] Regressing to EPS</header>
    <body>I'm trying to submit some artwork to a printer and they expect the files to be delivered as EPS files.  Now it would seem to me that PDF is, for all practical purposes, a better format for submitting artwork than EPS and PDF/X should be another step along the way as well. Nevertheless, I'm curious to know if anybody has an idea for strategies to regress a PDF illustration back to an EPS file? My first thought was to write a script that ran the files through CUPS and collected the PS output then tagged that output with the required EPS comments and previews and such.  That doesn't sound like an overly egregious process.  But it would be quite a bit less egregious if someone knows of an Open Source tool (or similar) that could do it all for me :-)</body>
  </mail>
  <mail>
    <header>Re: Compressing Images in a PDF</header>
    <body>On Nov 15, 2004, at 12:06 PM, Derek Clegg wrote: Okay, thanks for the info.  I see what's happening.  I'm doing something similar with regard to Flate compression myself but in the case of JPEG I'm failing to pass that through to the PDF as JPEG so it's defaulting to Flate (which is considerably larger than the original JPEG).  It sounds like I should use a JPEG data provider for that case when going to PDF. In a related issue:  Does the JPEG data provider handle JPEG-2000 or just vanilla JPEG? Thanks, Nick</body>
  </mail>
  <mail>
    <header>How do Quartz event kCGEventSource... fields work?</header>
    <body>In my test app, I get expected values from the CGEventGetIntegerValueField() function for the kCGEventTargetProcessSerialNumber and kCGEventTargetUnixProcessID event fields. But I always get 0 for the kCGEventSourceUnixProcessID, kCGEventSourceUserData, kCGEventSourceUserID and kCGEventSourceGroupID event fields. How do I receive meaningful information for these fields, and what is its significance (i.e., what is meant by &amp;quot;event source&amp;quot; in this context)? Or are these field selectors simply not implemented? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: problem drawing transparent fill to NSImage</header>
    <body>I've changed things a bit where I'm now creating an NSImage in place of directly calling NSRectFill(). Oddly, I have ended up having to do two different compositing operations when I draw the image depending on the situation. In one situation I'm drawing the NSImage in an overlay view (transparent window) where I have to use NSCompositeSourceIn in order to see the view underneath properly, and then in another situation where I'm compositing this same NSImage over another NSImage I have to use NSCompositeSourceOver in order to see underneath properly.</body>
  </mail>
  <mail>
    <header>Re: How do you display CMYKA data with Quartz?</header>
    <body>Scott Thompson: There is no logical connection between alpha blending and additive colour. the term refers to the coefficient in linear blending, which can be applied just as well to subtractive as additive colour -- although in either case you have to convert to a linear colour space for the results to be consistent. That few if any printers support alpha blending is no reason to ignore the possibility of alpha blending in CMYK, unless you have a vested interest in maintaining the status quo. -- Jens Ayton Sed quis custodiet ipsos custodes?</body>
  </mail>
  <mail>
    <header>Re: How do you display CMYKA data with Quartz?</header>
    <body>On Mar 23, 2007, at 11:33 AM, Nick Nallick wrote: FreeHand has transparency effects and the like which use clipping paths, color manipulation, etc... to achieve transparency effects with CMYK colors.  I'm not trying to say that transparency in a CMYK universe is not useful.  From that perspective, I am being overly pedantic when I say that the Alpha channel doesn't make much sense in CMYK.  But in practical applications, the effect of an alpha channel would have to be accomplished through some other means when the ink hits the paper :-) Scott</body>
  </mail>
  <mail>
    <header>Re: How do you display CMYKA data with Quartz?</header>
    <body>On Mar 23, 2007, at 10:33 AM, Nick Nallick wrote: Following up my own thought... On Tiger or later, you could achieve the same result as a CMYKA image by combining a CMYK image with a gray image channeled through CGContextClipToMask.  Essentially your alpha channel becomes a clipping mask image. Nick</body>
  </mail>
  <mail>
    <header>Re: How do you display CMYKA data with Quartz?</header>
    <body>On Mar 23, 2007, at 10:18 AM, Scott Thompson wrote: Clearly it wouldn't be that much use as a final output format but I can imagine using it during a compositing phase.  People do pretty much the same thing now with clipping paths and the like. Nick</body>
  </mail>
  <mail>
    <header>Re: How do you display CMYKA data with Quartz?</header>
    <body>On Mar 23, 2007, at 10:45 AM, Shawn Erickson wrote: CMYKA does not make much sense really anyway.  CMYK is a printing format... subtractive colors on ink.  Alpha deals with light transmission (additive colors) which really only makes sense on computer screens.   I suppose you could come up with a similar concept to Alpha for subtractive colors, but how are you really going to implement it? In print you might achieve transparency through transparent inks (which would probably be hard to control)  or as it's done today -- through screening.</body>
  </mail>
  <mail>
    <header>CGEventSourceCounterForEventType() issue</header>
    <body>According to the Quartz Event Services Reference document, this code should return the number of user input events of all types (keyboard, mouse, or tablet) since the window server started: CGEventSourceCounterForEventType(CGEventSourceGetSourceStateID(eventSourceR ef), kCGAnyInputEventType) But it always returns 0. If I specify a specific event, such as kCGKeyDown, I get a reasonable result. Is this a known bug? Or is the documentation wrong? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: How do you display CMYKA data with Quartz?</header>
    <body>You have CMYK with alpha? If so I don't believe Quartz2D supports that natively. It supports CMYK without alpha IIRC. -Shawn</body>
  </mail>
  <mail>
    <header>How do you display CMYKA data with Quartz?</header>
    <body>I have some data stored in memory in a CMYKA format.  How can I display this to the screen with Quartz?  Do I have to convert it to RGBA?  Is there another way? Thanks, Tony _________________________________________________________________ Get a FREE Web site, company branded e-mail and more from Microsoft Office Live!</body>
  </mail>
  <mail>
    <header>Re: problem drawing transparent fill to NSImage</header>
    <body>On Mar 21, 2007, at 11:54 PM, john wrote:</body>
  </mail>
  <mail>
    <header>Re: What to do with kCGEventTapDisabledBy...?</header>
    <body>On Mar 22, 2007, at 6:54 AM, Bill Cheeseman wrote: Your callback should return the event that was passed in, or return NULL.  Do not release the event.  Returning a released event will produce the error you are reporting (as will overwriting the event's internal data, a less frequent problem). That will work just fine.   The system tracks the resources created around the callback and will release them as appropriate. The kCGEventTapDisabledByTimeout and kCGEventTapDisabledByUserInput events just carry the special event type.  The event timestamp is not set, as the event didn't actually pass through the usual event system process. When your callback returns from handling a kCGEventTapDisabledByTimeout event notification, by the way, the event tap should be re-enabled.  The notification is sent to let you know that the system determined that your tap was holding up event traffic and that events have bypassed your tap while it was unresponsive.   The notification is usually just seen while debugging an event tap.</body>
  </mail>
  <mail>
    <header>What to do with kCGEventTapDisabledBy...?</header>
    <body>What should an event tap callback function return when the triggering event type is kCGEventTapDisabledByTimeout or kCGEventTapDisabledByUserInput? I find that if I return an event of this type from the callback function, I get a system error when the system tries to post the event. If I return NULL from the callback when the event type is one of these values, everything seems to work correctly. Is this the right thing to do? (By the way, I notice that the event passed to the callback function with one of these event types appears to yield valid values for flags, location on the screen, and so on -- except that the timeout is always 0. Why doesn't one of these event types have a valid timeout value?) Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: problem drawing transparent fill to NSImage</header>
    <body>NSRectFill uses NSCompositeCopy which will blow away the pixels in the destination. Try using NSRectFillUsingOperation with NSCompositeSourceOver. Later, Guy</body>
  </mail>
  <mail>
    <header>problem drawing transparent fill to NSImage</header>
    <body>I'm drawing into NSImage by drawing another NSImage and then using NSRectFill() ot overlay a semi-transparent colour. The image is created fine if I don't draw the semi-transparent fill, but if I do then the fill covers the image and the image is not visible. - John</body>
  </mail>
  <mail>
    <header>CGEventKeyboardGetUnicodeString() count</header>
    <body>The reference document for CGEventKeyboardGetUnicodeString() says this, under &amp;quot;Discussion&amp;quot;: &amp;quot;When you call this function and specify a NULL string or a maximum string length of 0, the function still returns the actual count of Unicode But it always returns 0 for me. Is this code wrong? -- Or is it a bug in the function, or the documentation? I get the correct result (both the correct count and the string) when I do this: -- UniCharCount preCount = 10; // arbitrary buffer size returnString = [NSString stringWithCharacters:buffer Related question: is there a rule of thumb for the size of buffer I should use with this function -- to avoid having to call it twice, once to get the size and again to get the string? Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: Setting the color space for an entire PDF context/document</header>
    <body>I agree to your, more precise terminology. To be more precise about the task to be accomplished: The source PDF is in RGB colorspace with a single ICC printer profile, the same for all objects, and the destination PDF is supposed to be in RGB colorspace with a single ICC color profile, the generic RGB profile, for all objects. The Quartz-Filter is very simple to construct with the ColorSync Utility: crate a new filter, add one color management component, with the following parameters: assign profile, generic RGB profile, for all RGB-Data, to all objects. domain = PDF Workflows. If you apply the filter with ColorSync Utility to PDF with graphics, e.g. a cyan rectangle, text and a bitmap image, you see on the monitor that the bitmap graphics changes color, which is correct when you replace the profile, while the color of text and graphics stays the same. I verified with Voyeur and Acrobat, that the source PDF is RGB and there is, as far as I can tell, the same ICC color profile for all elements. When the I look at the destination PDF, or the output from the Quartz filter, I see different color profiles for different objects with Voyeur. objects. That's why I filed a bug. (radar:5074473). Thanks Thomas</body>
  </mail>
  <mail>
    <header>Re: Setting the color space for an entire PDF context/document</header>
    <body>On Mar 19, 2007, at 5:56 AM, Thomas Hartwich wrote: Before I start I should state that I may use slightly different terminology for color concepts.  I consider a color &amp;quot;space&amp;quot; to be defined by the type of components used to specify a color (e.g., RGB, CMYK, Lab, etc.), whereas a color &amp;quot;profile&amp;quot; is a transfer function to adjust your input components to output components within a color space (e.g., a ColorSync profile).  CG and PDF combine both of these concepts into the term colorspace. CGPDFContextCreate creates a CG context where drawing operations are recorded as native PDF operations.  This is considerably different than a more familiar CG bitmap context where drawing operations are converted to pixels.  When you draw vector graphics objects into any CG context the colorspace and profile have already been established for that context.  In this regard CG follows the PDF model which means the CG calls can be translated directly into PDF operations with virtually no semantic changes.  For example consider the following CG code fragment.  It loads an RGB profile, sets the context fill color to red from the loaded profile, and fills a rectangle with the resulting color.  If myContext in the code below is a PDF context the result will be a PDF file containing the color profile used for drawing and operations to set the fill color and draw a rectangle using a different syntax but the same semantics as what you see here. CMProfileRef colorProfile = LoadMyRGBColorProfile();	// e.g., sRGB, SMPTE-C, etc. CGColorSpaceRef colorSpace = CGColorSpaceCreateWithPlatformColorSpace profile used a different color space (e.g., gray or CMYK) we'd probably need a different component count Therefore, in answer to your question, CGPDFContextCreate doesn't contain a color space parameter because it can use any number of color spaces (and potentially multiple profiles within each color space).  The color used for any given drawing operation is the result of the current color space, color profile, and color components in effect at the time the drawing is performed.  Bitmap images are somewhat different because they can contain embedded color profiles. They may also be able to reference external profiles in a similar manner to the code above, but I'm a little fuzzy on the details. I don't have any experience with the filters.  Perhaps somebody else can provide feedback on that.  I would expect you'd be able to substitute one color profile for another, but probably not be able to change color spaces since they have different component counts. Nick</body>
  </mail>
  <mail>
    <header>drawHTMLTextInRect needs to have a transparent bkg...</header>
    <body>i'm using the python CoreGraphics binding to set some text onto an existing PDF document. I was originally using showTextAtPoint() to set this text, but this function has no built-in ability to wrap the text to a given width, so i wrote a little method that generates an html snippet and then uses drawHTMLTextInRect() to place the text onto the PDF. This all works very well. However, i cannot find any way to have the output of drawHTMLTextInRect() be on a transparent background (so i can see the PDF content through the whitespace of the text). I've tried explicitly setting the background in the HTML, but while i can set the background to any color, i cannot make it transparent. This is the latest snippet in question: Typewriter; font-size:24px; color:rgb(0,0,127)'&amp;gt;this is a bit of a michael geary</body>
  </mail>
  <mail>
    <header>Re: CGLayer performance problem</header>
    <body>On Mar 20, 2007, at 12:08 AM, David Duncan wrote: This is especially true if the layer is created using a nil context. Deferring CGLayer creation until I have a valid view context fixes my issue, and everything now works beautifully. Too much coding, too little sleeping.  :)  New frameworks do that to me....</body>
  </mail>
  <mail>
    <header>Re: CGLayer performance problem</header>
    <body>CGLayers are created optimized to the context that they are created from - using them with another context may not be as optimal. In general with view systems, the context that you get on any given draw event may not be the same type as you are given on another call, and if that happens the CGLayer will not necessarily be optimal. The actual implementation of a CGLayer depends on the context that it is associated with. There are many implementations of layers, as there are many types of CGContext. If you are unconcerned with potentially scaling (that is, you are willing to handle view size changes yourself) then you can instead create a CGBitmapContext to draw the background that is the size of the view, then create a CGImage to cache and draw that image. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>CGLayer performance problem</header>
    <body>I'm new-ish to Quartz, so perhaps I am misunderstanding CGLayers. Anyway: I have an NSView subclass that's animated by way of an NSTimer. Because the background image is time-consuming to draw, I decided to use a CGLayer.  It is my understanding that any drawing done into the layer context is cached (and thus, speedy to use later on in some other context).  However, I'm seeing little to no performance gain over drawing the background directly into the current CGContext. during NSView awakeFromNib: - create layer from current context - get the new layer context - set some context parameters - draw ~1000000 lines during the animation callback triggered by my NSTimer: - CGContextDrawLayerAtPoint(currentContext, CGPointZero, NULL) The animation is dog slow.  The thing is, if I reduce the number of lines drawn when creating the layer, my animation loop performance improves.  This suggests to me that CGLayer is more of a display-list type of construct.  Is this correct?  Should I be using some other Quartz context to achieve a very fast background render? Some profiling in Shark shows the my code spends most of it's time (97%) in CoreGraphics draw_line. Again, I'm new to Quartz so I suspect the problem is with my understanding.  But the Apple's documentation and _Programming with Quartz_ seem to suggest otherwise -- that the time-consuming part of drawing to the CGLayer happens once, and reuse is snappy.</body>
  </mail>
  <mail>
    <header>Re: CGGetEventTapList expected behavior?</header>
    <body>Well, I've been targeting the Finder. That explains everything, right? :) Thanks very much for your responses. I think I've got most of what I need to know about this pretty well in mind, now -- or at least some assurance that I'm on the right track. I'll try targeting some other applications to see whether they are less profligate with their window server connections. Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: CGGetEventTapList expected behavior?</header>
    <body>On Mar 19, 2007, at 4:48 AM, Bill Cheeseman wrote: CGEventTapCreateForPSN().  That's important... CGEventTapCreateForPSN() allows you to monitor and interact with all the CGEvents going to a particular process.  To do that, it needs to tap all communications channels established between that process and the window server. The window server tracks it's clients by the connections that they have established with the server.  Each connection represents a path events may take from the server to the application, and also represents ownership rights for resources allocated in the window server (windows, mostly) established using that connection. A process running on Mac OS X  may establish multiple connections.   This is done to support certain usage patterns, such as a single task or Unix process which runs multiple 'applications' within it's address space.  One could, for example, write a server process which runs multiple Java applets, each on it's own thread, with it's own windows and event streams using a connection to the window system for each thread. CGEventTapCreateForPSN() must place a tap on all connections between the process being tapped and the window server, and will route data from all these connections to the CFMachPortRef for your tap.   The tap callback function is passed a CGEventTapProxy, which is used when the callback generates new events to route them back to the appropriate communications channel. It sounds like the process that you are placing the event tap on may have established a large number of communications channels, or connections, to the window server.   That may indicate a problem with that program, as most applications only require one connection.</body>
  </mail>
  <mail>
    <header>Re: CGGetEventTapList expected behavior?</header>
    <body>My test app is a Cocoa app. It wraps a single event tap in a Cocoa class I call PFEventTap. The test app puts up a single window with 5 buttons: Install Event Tap, Remove Event Tap, Enable Event Tap, Disable Event Tap, and Log All Event Taps. It did it this way to ensure that each of my function calls is isolated in time and each function is called only once. Each of the buttons works correctly; that is, left or right clicking on a Finder icon works or doesn't work, as expected, after pushing one of the first four buttons. The Log All Event Taps button allows me to run CGGetEventTapList() at will, at any time, outputting the result as a Cocoa NSArray and logging it to console. Here's how I count and list the CGGetEventTapList() array: // Returns the count of all Event Taps installed on the system. Not all Event Taps included in the count are necessarily managed using PFEventTaps. // Covers the CGGetEventTapList function to return the count of installed Event Taps. // Returns an array of all Event Taps installed on the system. Each element of the array is a dictionary. Not all Event Taps in the array are necessarily managed using PFEventTaps. // Covers the CGGetEventTapList function to return the list of installed Event Taps. CGEventTapInformation *list = (CGEventTapInformation *)calloc(preCount, CGTableCount postCount; // should always equal preCount NSMutableArray *returnArray = [[[NSMutableArray alloc] NSDictionary *info = [NSDictionary dictionaryWithObjects:[NSArray arrayWithObjects:[NSNumber numberWithUnsignedInt:list[i].eventTapID], [NSNumber numberWithUnsignedInt:list[i].tapPoint], [NSNumber numberWithUnsignedInt:list[i].options], [NSNumber numberWithUnsignedLongLong:list[i].eventsOfInterest], [NSNumber numberWithInt:list[i].tappingProcess], [NSNumber numberWithInt:list[i].processBeingTapped], [NSNumber numberWithBool:list[i].enabled], [NSNumber numberWithFloat:list[i].minUsecLatency], [NSNumber numberWithFloat:list[i].avgUsecLatency], [NSNumber numberWithFloat:list[i].maxUsecLatency], nil] forKeys:[NSArray arrayWithObjects:@&amp;quot;eventTapID&amp;quot;, @&amp;quot;tapPoint&amp;quot;, @&amp;quot;options&amp;quot;, @&amp;quot;eventsOfInterest&amp;quot;, @&amp;quot;tappingProcess&amp;quot;, @&amp;quot;processBeingTapped&amp;quot;, @&amp;quot;enabled&amp;quot;, return nil; // error Here's how I install an event tap, after allocating one of my PFEventTap objects: - (id)initWithPid:(int)pid eventMask:(CGEventMask)mask appendAtTail:(BOOL)tailPlacement listenOnly:(BOOL)listenOnlyOption notificationDelegate:(id)delegate callbackSelector:(SEL)callback // Initializes a newly created PFEventTap object using the target application's process ID. The target application must be running. // Covers the CGEventTapCreateForPSN function. Key up and down events are available only if the &amp;quot;Enable access for assistive devices&amp;quot; setting in the Universal Access pane of System Preferences is turned on. // This method saves a pointer to the receiver and the incoming contextInfo (if any) in the CGEventTapCreateForPSN function's refcon parameter. When the system's event tap mechanism calls the callback function, it retrieves them from its refcon parameter and uses them, along with the information passed by the system in the callback function's other parameters, to create an NSInvocation object. It then uses the invocation object to invoke the client's Objective-C callback selector. The callback function obtains the receiver's notificationDelegate and callbackSelector from the receiver as it was saved in the CGEventTapCreateForPSN function's refcon parameter upon initialization here. This mechanism allows the client's callback selector to be written as an Objective-C method instead of a C function. // Use the -[PFEventTapManager eventMaskByAddingType:toMask:] utility method to build a mask by adding types. // Set up parameters for the CGEventTapCreateForPSN function call. CGEventTapPlacement placement = (tailPlacement) ? CGEventTapOptions option = (listenOnlyOption) ? kCGEventTapOptionListenOnly : 0x00000000; // the kCGEventTapOptionDefault constant is missing from CGEvent.h in Mac OS X 10.4 Tiger // Set up refcon parameter for the CGEventTapCreateForPSN function call. NSMutableDictionary *tempRefcon = [[NSMutableDictionary alloc] if (contextInfo) [tempRefcon setObject:contextInfo // Create and install the event tap. machPortRef = CGEventTapCreateForPSN(&amp;amp;psn, placement, option, mask, // Create the run loop source. eventSourceRef = CFMachPortCreateRunLoopSource(NULL, // Add it to the run loop. CFRunLoopAddSource(CFRunLoopGetCurrent(), eventSourceRef, // Set instance variables. notificationDelegate = [delegate retain]; // private callbackSelector = callback; // private registeredEventTapLocation = 3; // there is no constant for the case where the Event Type is created for a specific target application, but the system appears to assign 3 (which is the next available constant slot in the CGEventTapLocation enum). NSLog(@&amp;quot;\n\tclient path = %@&amp;quot;, [[NSBundle mainBundle] NSLog(@&amp;quot;\n\tclient pid = %@&amp;quot;, [NSNumber clientPid = [[NSNumber numberWithInt:[PFEventTap // The pendingEvent iVar is reinitialized in the callback function every time this Event Tap is triggered. //#ifdef PF_DEBUG NSLog(@&amp;quot;PreFab PFEventTap -initWithPid:eventType:appendAtTail:listenOnly:notificationDelegate:callback Selector:contextInfo: method failed to add run loop source for pid %d.&amp;quot;, //#endif //#ifdef PF_DEBUG NSLog(@&amp;quot;PreFab PFEventTap -initWithPid:eventType:appendAtTail:listenOnly:notificationDelegate:callback //#endif Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Setting the color space for an entire PDF context/document</header>
    <body>Hi, I would like to open an existing PDF document which has the same RGB color space for all objects in the document and write the contents out to an other PDF document into one other color space. I do not want to change the pixels or objects in the original document, just a change of the color space tag(s) for all of them. I read some documentation around creating PDF documents and found the function CGPDFContextCreate(), where I can set the mediabox and auxiliaryInfo. I also read the example in &amp;quot;Programming with Quartz&amp;quot;,p450. But no parameter to specify the color space for the destination document, unlike CGBitmapContext. I tried using a quartz filter, in the ColorSync Utility and using the python bindings, and assign the generic RGB profile to &amp;quot;allObjects&amp;quot; but that failed to change text and graphics objects. Is it a bug or a feature? I do not want to render to a CGBitmapContext, because I want to keep font and graphics information. I am out of ideas at that point and would appreciate any new hints and comments. Thanks in advance Thomas</body>
  </mail>
  <mail>
    <header>drawing a CIImage instead of rendering an openGL texture</header>
    <body>I've got some code that is based on the QTQuartzPlayer example, where a decompression session is initialized with ICMDecompressionSessionCreateForVisualContext, and then new frames are retrieved from the openGL context. I would like to optionally filter the output, before rendering it. Converting the frame to a CIImage, and filtering it is fine. If I I save off the filtered image, the output looks good. [mCIContext drawImage:highlightedImage atPoint:CGPointMake((int)(([self frame].size.width - imageRect.size.width) * 0.5), (int)(([self frame].size.height - imageRect.size.height) * 0.5)) // use integer coordinates to avoid interpolation Where mCIContext was initialised with the same openGLContext as the view. This doesn't seem to draw my filter output - instead my video just becomes really jerky. Can anyone give me any hints on how to render my CIImage correctly in place of the original frame? cheers, Martin</body>
  </mail>
  <mail>
    <header>Re: CGGetEventTapList expected behavior?</header>
    <body>Thanks, that was very helpful. I'm using CGEventTapCreateForPSN(), not CGEventTapCreate(). The debugger confirms that I am only calling it once. Nevertheless, whether immediately or long after creating the event tap, CGGetEventTapList() consistently shows me a large number of event taps, all identical except for the ID. Today, it's showing me 45 of them, including the one the system installs. Quitting a couple of other apps then running my test app again doesn't change this number, but from work session to work session it does seem to change somewhat, for no reason I can deduce. When I perform the indicated event (left or right click on a Finder icon), my callback function is called only once. When I call CGEventTapEnable() to disable the machPortRef, then log the CGGetEventTapList() function result again, I get the same number of event taps (45, currently). All of them now show that they are disabled. When I run your code to invalidate and release the machPortRef, then log CGGetEventTapList() again, all the event taps except the one installed by the system are gone. I have to conclude that it is a bug, probably in CGEventTapCreateForPSN(). Could there be any other explanation? Of course, these things usually turn out to be my own bug, after all, so I'll post some of my code in the next message, just in case somebody has the time to skim through it. Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: CGGetEventTapList expected behavior?</header>
    <body>There is a dependency between the CFRunLoopSource and the CFMachPortRef that produces an 'extra' retaincount on the CFRunLoopSource.   Before releasing the runloop source and the CFMachPortRef, call CFMachPortInvalidate() on the port.   This will switch off the event tap, and invalidate both the runloop source and the port.  It also drops the retain count on the source and the port by one, so the CFRelease will release the objects and not leak. // Stop! // Invalidate the CFMachPort, switching off the event tap // Remove the runloop source for the tap from the runloop CFRunLoopRemoveSource(CFRunLoopGetCurrent(), // Release the removed runloop source //Finally, release the event tap port. If your event taps are passive listeners, then no, other than consuming precious memory and kernel resources, there is no effect. If each tap is an active filter, then it will 'clog' the system's event flow until some internal safety mechanisms determine that there is a malfunction and disable it.  (It's re-enabled should it start responding again.) The 'stop' code above should shut down an event tap. From your previous mail: You may want to check to see if your code is being invoked multiple times.  35 identical event taps except for the ID would indicate that there are in fact 35 event taps. Each call made to CGEventTapCreate() will create one event tap, as reported by CGGetEventTapList().    Try using a debugger to break on calls to CGEventTapCreate(), to see if you are getting multiple calls in your app.</body>
  </mail>
  <mail>
    <header>Re: CGGetEventTapList expected behavior?</header>
    <body>I think I may have already answered some of my own questions, but I would appreciate confirmation: If I call CFMachPortInvalidate and CFRunLoopSourceInvalidate before releasing them, the CGGetEventTapList function no longer reports that they exist. I'm a novice at mach ports and run loop sources. Is this the correct technique? See no. 2, above. I'm still curious to know the answer, but no. 2, above, is easy enough to do, just in case. Subparts (2) and (3) apparently should be replaced by no. 2, above. Bill Cheeseman</body>
  </mail>
  <mail>
    <header>CGGetEventTapList expected behavior?</header>
    <body>Using a simple test application, I push a button to install an event tap using CGEventTapCreateForPSN to block the left and right mousedown events in the Finder. It works: clicking an icon on the desktop in the Finder does nothing. I push another button in my test application to disable the event tap, and clicks in the Finder work again. So far, so good. The trouble begins when I list the installed event taps. Before my test app quits, it calls CGGetEventTapList twice, once to log the count of installed event taps and again to log the list of installed event taps. These calls to CGGetEventTapList show that my test app installed 35 new event taps (all of them identical except for the ID), not the one event tap I expected. (There is also one event tap that the system apparently installed at boot time.) I've tried everything I can think of to remove these event taps, but they continue to be listed by CGGetEventTapList as long as my test app is still running. In fact, if I click the button to install the event tap again, I get 35 more event taps listed by CGGetEventTapList; push the button again, and I'm up to 115 installed event taps, and so on. When I run my test application again, two calls to CGGetEventTapList at the beginning show only the single event tap apparently installed by the system. I guess terminating the process automatically removes event taps, at least if they were installed by CGEventTapCreateForPSN. All this leads to several questions: 1. Is it expected that CGGetEventTapList would report 35 new event taps when I installed only one? Why? 2. Is it expected that CGGetEventTapList would still report the new event taps after I release the machPortRef and the eventSourceRef? Why? 3. Is there no way to remove event taps short of terminating the process? 4. Does it matter (i.e., do these listed event taps take up memory or slow the system or the process down)? 5. What is the recommended strategy? (I'm guessing: (1) never create the same event tap twice; (2) rely on terminating the process to remove all event taps for that process; (3) don't release the machPortRef and the eventsourceRef in the belief that it will remove the event taps, because it won't; (4) to turn an event tap off temporarily, just disable it, and re-enable it again later if it's needed again. But I'm only guessing.) Bill Cheeseman</body>
  </mail>
  <mail>
    <header>PDF -&amp;gt; BitmapContext: Howto avoid antialiasing artifacts without	loosing text antialiasing</header>
    <body>I am running into the problem that I get antialiasing artifacts when rendering a PDF page to a CGBitmapContext. I can disable antialiasing for the context, but then I get ugly and unreadable text. The artifacts I get are thin lines of boxes, which probably represent pdf objects and are clearly a result of antialiasing. I have checked the pdf documents with acrobat and the artifacts are also visible unless I turn of antialiasing for line art in the acrobat prefs. So my question is, is there a way to disable antialiasing for a context but keep the text antialiasing? Just the same way I can do with acrobat? Dominik</body>
  </mail>
  <mail>
    <header>Re: Completely work in the video card</header>
    <body>On Mar 14, 2007, at 5:40 AM, Mark Munte wrote: Beware, though, CIImage has no way to 'retain' the texture for you.  So, you will have to manually track the texture name and CIImage and not delete the texture until the last rendering operation using the CIImage is done. Additionally, the behavior is undefined if you modify the texture after giving it to the CIImage, so don't do that :)</body>
  </mail>
  <mail>
    <header>Re: Completely work in the video card</header>
    <body>Well, that just depends on how you set up OpenGL. If you do it all correctly, the CIFilters will be applied by the GPU. If the GPU runs out of resources, Open L / Core Image might fall back to an software renderer, I'm not sure how one would detect that, besides dramatic drop of framerate/increae of cpu :) I believe it is possible to disable the fallback using OpenGL pixel attributes (look for NSOpenGLPFAAccelerated and NSOpenGLPFANoRecovery), but I'm not sure. There are some good samples from Apple on OpenGL and texture uploads. Once you have OpenGL setup, applying CIFilters is really easy.</body>
  </mail>
  <mail>
    <header>Re: Completely work in the video card</header>
    <body>On 3/14/07, Yes: create the CIImage from an OpenGL texture. That means, take your buffer from main memory and upload it as a texture to an OpenGL context. Create your CIContext bound to the same OpenGL context, then create your CIImage from the texture and simply draw it to the GLContext&amp;#39;s</body>
  </mail>
  <mail>
    <header>Re: Completely work in the video card</header>
    <body>Yes: create the CIImage from an OpenGL texture. That means, take your buffer from main memory and upload it as a texture to an OpenGL context. Create your CIContext bound to the same OpenGL context, then create your CIImage from the texture and simply draw it to the GLContext's NSOpenGLView. For best performance, it is important to have the buffer in an optimal format, so you avoid any swizzling from the OpenGL driver when the data is uploaded. There are lots of old posts on this on the OpenGL list, take a look there.  For even better performance upload the buffer to the texture using OpenGL Pixel Buffer Objects (PBO).</body>
  </mail>
  <mail>
    <header>Re: ImageIO and read-only Exif tags</header>
    <body>On Mon, 5 Feb 2007 18:13:36 +0100, I'm very pleased to notice that this particular problem with GPS tags not being saved to JPEG files has been resolved in Mac OS X 10.4.9.  Way to go, Apple. -- marco -- It's not the data universe only, it's human conversation. They want to turn it into a one-way flow that they have entirely monetized. I look at the collective human mind as a kind of ecosystem. They want to clear cut it. They want to go into the rainforest of human thought and mow the thing down.</body>
  </mail>
  <mail>
    <header>Completely work in the video card</header>
    <body>Is there any way to create a CIImage object directly from the video card buffer? More precisely, I mean, if someone want to use the core image filter to some image which is currently in the video buffer to create some ripple/copy machine/swap efforts, etc, one of the way is first to read the image in the video buffer to the memory, then use some API to create the CIImage object, pass the object as the parameter to the CIFilter object, Then show it. 1.there are some data in the memory presenting a picture 2.copy the data to the main memory (in contract to the video card memory) 3.to use the data above to create the CIImage object. 4.create the CIFilter using the CIImage 5.get the result image processed by the CIFilter 6.display the result image You can see if the resolution of the screen is very big, (for example of me, mine is 2048 x1152), you will suffer the performance degrade for you have to move the huge data from the video to the memory then back. Anyone knows how to create the CIImage directly from the video card buffer? Thanks for reply.</body>
  </mail>
  <mail>
    <header>Re: Quartz-dev Digest, Vol 4, Issue 46</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Problem: 16-bit TIFF's and Image IO with	Float	CGBitmapContextCreate</header>
    <body>I finally solved my issue by reading TIFF's with QuickTime. similar to this example:</body>
  </mail>
  <mail>
    <header>Re: CGEventTapInformation</header>
    <body>Well, yes, I can see how it would be useful to Apple to have an internal tracking number. But you expose it publicly in the CGEventTapInformation, which suggests that you think there may be some external utility in it, too. But it isn't made available to me when I create an event tap, so it isn't easy for a developer to use in any way that seems obvious to me. I guess you could consider this a feature suggestion, but I'm hardly far enough into event taps to appreciate all the nuances. Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: Removing event taps</header>
    <body>I'm now releasing the machPortRef explicitly when my app quits normally. The accumulation of old event taps seems to be a result of hitting the Terminate button in Xcode, instead of quitting my test app normally. When I reboot, the excess event taps go away, of course. Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: CGEventTapInformation</header>
    <body>On Mar 13, 2007, at 8:57 AM, Bill Cheeseman wrote: That is an internal tracking value which will persist as long as your event tap persists. When you create an event tap, you can provide a (void *) 'refcon' referring to any identifying data you want to associate with a particular tap, such as a data structure to establish a filtering context: CG_EXTERN CFMachPortRef CGEventTapCreate(CGEventTapLocation tap, CGEventTapPlacement place, CGEventTapOptions options, CGEventMask eventsOfInterest, CGEventTapCallBack callback, The callback prototype shows that this 'refcon' is passed to the callback function: typedef CGEventRef (*CGEventTapCallBack)(CGEventTapProxy proxy, Use this 'refcon' to hold any data related to your event tap, such as identifying information and state.</body>
  </mail>
  <mail>
    <header>Re: Removing event taps</header>
    <body>On Mar 13, 2007, at 9:02 AM, Bill Cheeseman wrote: Your tapping process may not have exited, but instead may be lingering around in an odd state.  When a process that has placed an event tap does actually exit, the window server gets a port death notification for the Mach port associated with the event tap, and will remove any remaining references to the tap.</body>
  </mail>
  <mail>
    <header>CGEventTapInformation</header>
    <body>I have a specific question about Event Taps (see Quartz Event Services): The CGEventTapInformation data type, a struct, includes a uint32_t field called &amp;quot;eventTapID&amp;quot;. What is this supposed to be used for? The reason I ask is that there doesn't seem to be any way to get an Event Tap's ID when I create it, but only when I get a list of all Event Taps already installed. Thus, there doesn't seem to be any way to identify which Event Tap in the list of installed Event Taps is mine (except by examining the tappingProcess pid, and then I'm not sure exactly how I would distinguish among several Event Taps that I installed). -- PLEASE NOTE MY NEW E-MAIL ADDRESS: Bill Cheeseman - email@hidden</body>
  </mail>
  <mail>
    <header>Removing event taps</header>
    <body>After experimenting for a couple of days, I see that there are over 100 event taps installed on my computer, most of them bearing my test application's pid. How do I remove them? I'm guessing that I should have removed them before quitting each session with my test app, by releasing the machPortRef that I keep around after creating and installing an event tap. But how do I deal with them if I neglected to do that or if, say, my test app crashed? -- PLEASE NOTE MY NEW E-MAIL ADDRESS: Bill Cheeseman - email@hidden</body>
  </mail>
  <mail>
    <header>Event Taps and CGEventRef in Cocoa</header>
    <body>I'm playing around with Event Taps and CGEventRef, for use in a Cocoa application. Initially, I thought I would like to convert back and forth between CGEventRef and NSEvent for convenience in manipulating flags and the like. They don't seem to be toll-free bridged, so I assume I would have to create lookalike events the hard way. Is this a bad idea? Are there any special issues I should be aware of? (After a couple of days, I'm starting to think that CGEventRef has enough special features that I should forget about NSEvent in this context. But I'm still curious about thoughts anyone might have.) -- PLEASE NOTE MY NEW E-MAIL ADDRESS: Bill Cheeseman - email@hidden</body>
  </mail>
  <mail>
    <header>Re: Capture drawing commands</header>
    <body>It might also be nice to be able to catch it for SQA regression testing, just as in ye olden days we would capture the serial instructions to graphics terminals. ____________________________________________________________________________________ TV dinner still cooling? Check out &amp;quot;Tonight's Picks&amp;quot; on Yahoo! TV.</body>
  </mail>
  <mail>
    <header>Transferring a CIImage to another machine for display</header>
    <body>I have a CIImage that was obtained from an sequence grabber. I would like to encapsulate this in an NSData object, and send this over the network to another computer, for display. However, when I unarchive the NSData on the other side, and either try and render the CIImage into an NSCoreImageView, or to convert it into an NSImage, I get a crash. I know that the original CIImage is fine, because I can render it locally with no problem. If I convert the CIImage into a NSImage before sending, and then archive and despatch the NSImage, that works fine (but sucks CPU, and seems to leak memory). If not, is there any efficient way to convert from the CIImage to a portable representation. I'm just going to be displaying it in an NSImageView or NSOpenGLView at the other end. cheers, m.</body>
  </mail>
  <mail>
    <header>Re: Custom image formats</header>
    <body>Hi, When we are at it, let me also once again express that I find it extremely limitating that the number of images in a image file needs to be specified at creation time. In nearly any use-case of mine I do not know that number in advance (e.g. the image come from a ADF copy machine, lab microscopes and the like) and I just want to fill a TIFF no matter if the user has placed 6 or 66 pages into the machine. I also tried to specify 0, -1 and 999999 as count in: CGImageDestinationCreateWithURL but neither combination worked out. -1 and 0 where NSLog'ed as invalid and with 999999 the class complained at: CGImageDestinationFinalize about missing images. I certainly would have expected such a limitation in an hobbyist open source project, but we are talking about what is suppost to be the next gen ImageIO on Mac OS X here ... That said we are back to our own in-house image io library due this (and other) limitations. Yours, -- Ren√© Rebe - ExactCODE GmbH - Europe, Germany, Berlin +49 (0)30 / 255 897 45</body>
  </mail>
  <mail>
    <header>Re: Custom image formats</header>
    <body>David is correct; this facility isn't available in ImageIO.  But your feedback is well understood and appreciated. In addition to your post here I'd recommend also filing a feature request so we can have the details on record. (&amp;amp; please send me the radar ID offline) Are you working around this by converting your format to one supported by ImageIO or going a different route altogether (Quicktime, etc)? Allan ___________ Allan Schaffer 2D &amp;amp; 3D Graphics Evangelist email@hidden</body>
  </mail>
  <mail>
    <header>Re: Variable Parameters in a Path</header>
    <body>On Dec 30, 2006, at 4:19 PM, Gordon Apple wrote: To vary the width over a stroked path you'd need to write your own stroke engine.  Quartz allows a subset of the graphics defined in PDF so the only way to vary color parameters within a single object is via the shader mechanism.  However I don't think that will work for what you're describing, so you'd have to create multiple objects. Nick</body>
  </mail>
  <mail>
    <header>Variable Parameters in a Path</header>
    <body>&amp;gt; Send Quartz-dev mailing list submissions to Has anyone found a way to vary the context state by element in a Bezier path?  What I am specifically looking for is a way to record (I can do that) and then draw (what I'm asking) paths of varying width, color, darkness, alpha, whatever, in response to pen stroke on a pressure sensitive tablet. It would be nice to have one path (possibly a subclass of NSBezierPath if done in Cocoa) that would allow capturing and drawing everything as a single path between mouse down and mouse up. A somewhat more difficult issue would be capturing time (or time differential) per point for replay rendering. Is drawing every segment separately the only way to do this to? -- Gordon Apple Ed4U Little Rock, AR email@hidden</body>
  </mail>
  <mail>
    <header>transforming/releasing/appending CGPDF...</header>
    <body>I am trying to emulate w/ Quartz what we used to do in CFM w/ QD &amp;amp; rsrc PICTs. Reading the docs, I understand the recommendation about gathering resources inside PDFs. So I went ahead and used Apple's examples and built a program that allowed us to convert thousands of rsrc graphic elements to multipage PDFs. So far so good. I can read/write to/fro bitmap/window/pdf contexts freely. This function's rotation arg is supposedly in int degrees, but it only seems to accept X = 90*N, N=0,1,2.. Otherwise, it crashes. Are we not allowed to give it an arbitrary value? If not, what are the alternatives? When accessing a PDF file, we end it with this. Before that, we seek a particular page (CGPDFPageRef) in that file. Exactly at what point are we supposed to free the CGPDFPageRef? Freeing it causes crash for some reason. 3) how do I _append_, _delete_ or _insert_ a CGPDFPageRef to/fro an existing PDF file? TIA, -Shin</body>
  </mail>
  <mail>
    <header>Extending Quartz</header>
    <body>I realise that I'm at the edge of what Quartz can do, and as the author of Lineform (), a drawing and illustration app, have different requirements to most people. But I still want to push Quartz further. Specifically I know Apple's rendering engine can at least cope with various features of PDFs  that I can't access. Soft masks (not the bitmap kind), although their rendering seems slow. PDF gradient types 4-6 Shadings with better control, specifically, not the current PDF type 0 function with 8 samples, but a type 3 stitched function of type 2 linear functions I can generate PDF data myself, but I don't want to replace my entire quartz backend with a bespoke system. It'd be great to see these features in Quartz, but I'm not holding my breath. What would be nice is to have some way to pass my own PDF data through Quartz untouched. Quartz parses then regenerates any included PDF data (usually rasterising anything it doesn't like) at the moment. I realise this is fairly unlikely, but if anyone knows of any way I can augment Quartz, or has any other suggestions. Let me know. regards -- will</body>
  </mail>
  <mail>
    <header>Re: Converting Postscript File to PDF</header>
    <body>You're right, thanks. That solved my problem. But brought back an old problem. I forgot why I chose NOT to use CGPDFPageGetDrawingTransform until now. CGPDFPageGetDrawingTransform has a bug (others have called it a feature) where it will scale the page DOWN, but it will not scale it up. So if I want to draw the page larger than 100%, it does not enlarge the page. I can't think of why it would be made to behave this way. So I ended up doing the transformations on my own. I didn't include a rotation transformation since I didn't plan on rotating the page. I was unaware that there is a /rotate value in a PDF page (as Dave Gelphman mentioned in another message) -- email@hidden</body>
  </mail>
  <mail>
    <header>Re: Creating a CGImage with a blurred edge</header>
    <body>CGRect rect = CGRectMake(0, 0, [self extent].size.width, [self [self getShapeContext];		// gets a cached context for this object [self drawShape];			// draws this object into the context image = createImageFromContext(_context, rect.size.width, rect.size.height, TRUE, TRUE);  // convert to CGImageRef initialImage = [CIImage imageWithCGImage:image];									  // convert to CIImage CIContext *c = [CIContext contextWithCGContext:_context options: nil];					  // create CIContext for filter to operate in CIFilter *f = [CIFilter filterWithName:@&amp;quot;CIGaussianBlur&amp;quot;];								  // create filter [f setValue:[NSNumber numberWithFloat:_edgeWidth] I may then composite the blurred result with the initialImage to have only the edge blurred and not the entire shape, but I'll play around with it a bit first to see how the various shapes work when blurred. Thanks again for your reply.  I knew there had to be a simpler method to achieve what I wanted.</body>
  </mail>
  <mail>
    <header>Re: Converting Postscript File to PDF</header>
    <body>This sounds like you are not respecting the /Rotate key for each page. The Rotate key specifies the degrees of clockwise rotation you should apply prior to displaying the page. If you were to use PDFKit to display your PDF documents I would expect you to get the same results as Preview. If you are using CoreGraphics directly then you need to obtain the value for the Rotation for each page and transform the context appropriately prior to drawing. The &amp;quot;Programming with Quartz Book&amp;quot; by Bunny Laden and me has discussion and code examples for handling this, going back to 10.0. Hope this helps, David On Nov 28, 2006, at 4:45 PM, Gerry Beggs wrote:</body>
  </mail>
  <mail>
    <header>Re: Converting Postscript File to PDF</header>
    <body>On Nov 28, 2006, at 5:45 PM, Gerry Beggs wrote: How does the page look when converted by Preview?  If Preview is getting it right it probably means you need to draw the resulting PDF using the transform returned from CGPDFPageGetDrawingTransform(). Nick</body>
  </mail>
  <mail>
    <header>Converting Postscript File to PDF</header>
    <body>I'm trying to support reading Postscript files in my program. I use Quartz's ability to convert Postscript to PDF so that I can display the Postscript file. I'm having a problem converting some Postscript files to PDF. The converted PDF file gets rotated 90 degrees in my program. However, if I open the file in Preview, it displays correctly. Postscript files generated by by the system through the Print dialog window (Save PDF as Postscript) convert and display correctly in my program. I haven't tried this with other sources, but the postscript files I'm having problems with are generated by Adobe Pagemaker 6.5.1. Any ideas what I might be doing wrong here? Why would Preview display it correctly, but my program doesn't? I don't do any rotation transformations on the graphics context. -- email@hidden</body>
  </mail>
  <mail>
    <header>Re: Creating a CGImage with a blurred edge</header>
    <body>On Nov 27, 2006, at 8:14 PM, Chip Coons wrote: One way to do this in Tiger would be to create an image containing a copy of the shape in white on a black background and perform a gaussian blur on it.  Then use the blurred image as a clipping mask via CGContextClipToMask().  You could also do something similar in CoreImage. Nick</body>
  </mail>
  <mail>
    <header>Re: Exporting 16-bit-per-component files from CoreImage</header>
    <body>As I reported there: I managed once to get 16 bits output from CoreImage, but I had to resort to a private method.</body>
  </mail>
  <mail>
    <header>Re: Exporting 16-bit-per-component files from CoreImage</header>
    <body>I think that you are out of luck.  I say this because I'm out of luck. The tack that I took was to give CoreImage floating point data (made fp from 16-bbp) from the start.  As you know, a floating point image can be any color depth.  So I didn't give CoreImage a chance to even guess.  But guess it did.  It truncated the range of values in the histogram from black to white.  When I converted back to 16bpp from the floating point, there were many gaps in the histogram. My methodology was as follows: -&amp;gt;  convert 16 image data to floating point CGDataProviderCreateWithData CGImageCreate -&amp;gt; make a ciimge CIImage	*cImg = [CIImage imageWithCGImage: cgImg options: ciOptionsDict] -&amp;gt; do some coreimage stuff -&amp;gt; allocate some memory for rendering the ciimage (context) CGColorSpaceRef colorSpaceRef = CGColorSpaceCreateWithName(kCGColorSpaceGenericGray); //single channel image CGContextRef outputCGImageContext = CGBitmapContextCreate(outputData, width, height, bitsPerComponent, bytesPerRow, colorSpaceRef, kCGBitmapByteOrder32Host | kCGImageAlphaNone | NSDictionary *outputContextOptions = [NSDictionary dictionaryWithObjectsAndKeys: [NSNumber numberWithBool: NO], kCIContextUseSoftwareRenderer, colorSpaceRef, CIContext *ciContext = [CIContext contextWithCGContext: [ciContext drawImage: outImg atPoint: CGPointZero I get float data, but it has been truncated as I mentioned.  So therefore it is no good to me.  I post this because maybe you can see my error or just give you a few more resources to experiment with. -- ---- Bruce Johnson email@hidden</body>
  </mail>
  <mail>
    <header>Creating a CGImage with a blurred edge</header>
    <body>I'm trying to blur the edge of an arbitrary shape drawn using Quartz.  Ideally, the edge would have a gaussian fall-off in color/ alpha, with the width of the blur being defined by the user. I've got a rudimentary solution, scaling the CGContext down as I progressively increase the alpha component of the drawing, but I can't help but think there is a better method of solving this. Any thoughts or suggestions would be appreciated.</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate and its data</header>
    <body>On Nov 27, 2006, at 12:19 PM, Steve Green wrote: IMO, the bitmap context is inconsistent with the rest of CG in this regard.  For example, there's no way to provide a data deallocator the way you can with a data provider.  The way I work around this is to create a &amp;quot;bitmap release&amp;quot; wrapper function that checks the retain count of the bitmap context before releasing it.  If the retain count is one I get the bitmap's data pointer and dispose of it.  The limitation to this is that you have to assume a particular allocation scheme is always used to create and dispose the buffer (e.g., malloc and free). Nick</body>
  </mail>
  <mail>
    <header>CGBitmapContextCreate and its data</header>
    <body>I've been using CGBitmapContextCreate for several days now and I would just like to confirm what I think I know about the data pointer.  It's quite clear from the documentation that the data pointer is not owned by the context.  That design decision works well in most cases however it's causing a problem in my specific instance. Generally speaking I should be able to create an object, pass it to some other code and release the object.  The other code might have retained the object, and I shouldn't have to care.  If it did retain the object, I would be effectively transferring ownership of the object to that other code.  As it pertains to bitmap contexts, I am being forced to care.  Who free()s the data, and when?  Even worse, the other code in question only has knowledge of CGContextRef.  It's never heard of bitmap contexts. Since I control both halves of the equation in my scenario, I can easily code around the problem but it's ugly to do so.  What (if any) are the possible workarounds for this problem? I've considered messing with the allocator context on the bitmap context to also free the bitmap data but I don't see any supported way to do that.  I've also investigated &amp;quot;deriving&amp;quot; from CGBitmapContext but I don't see how to do that either.  Any thoughts? ~S</body>
  </mail>
  <mail>
    <header>Re: Most efficient way to move images between processes</header>
    <body>And in fact it was that easy.  Not only did it work, but it worked the first time ;)</body>
  </mail>
  <mail>
    <header>Re: Most efficient way to move images between processes</header>
    <body>Thanks for confirming my suspicions.  As for mutex waiting, that's exactly what I am doing via a posix semaphore.  I am open to any better ideas in that arena too, but it's off topic and probably not appropriate for this list.  Please feel free to contact me directly. ~S</body>
  </mail>
  <mail>
    <header>Re: Most efficient way to move images between processes</header>
    <body>On Nov 22, 2006, at 7:02 AM, Steve Green wrote: You might try looking at the Cocoa distributed objects architecture. In particular the classes NSConnection and NSDistantObject.</body>
  </mail>
  <mail>
    <header>Re: Most efficient way to move images between processes</header>
    <body>Hi, You definetly do not want to pass PNG, compression (and decompression) does waste way too much time. If you avoid compression, TIFF could be ok - but something more low-level definetly wastes less time. For high data rates a shared memory architecture is definetly a pro but remeber to use some form of notification without latency (but no busy waiting on some mutex). Yours, -- Ren√© Rebe - ExactCODE - Berlin (Europe / Germany) +49 (0)30 / 255 897 45</body>
  </mail>
  <mail>
    <header>Re: Most efficient way to move images between processes</header>
    <body>On 22 Nov 2006, at 15:02, Steve Green wrote: I know nothing about shared memory, but you can certainly create a CGBitmapContext to draw into a block of memory you've allocated and a CGImageRef which reads its data from a block of memory, so it should be easy :-)</body>
  </mail>
  <mail>
    <header>Most efficient way to move images between processes</header>
    <body>I am writing an application that manipulates images in a background application.  I have a client app that can attach and optionally observe the images in near real time.  My question is this.. what is the most efficient way to architect this? My initial idea is something like this.  Get each image as TIFF data and write the data into shared memory.  The client app could easily reconstruct a TIFF image from TIFF data and display it.  Maybe PNG is better?  While that will probably work fine, I can't help but think that there's a much better way.  In my ideal world, I would be able to construct a graphics context such that the background app draws directly into shared memory.  Then on the client side, somehow create an image from that shared memory and display it with. Any pointers, suggestions, or even words of encouragement would be greatly appreciated. Thank you..</body>
  </mail>
  <mail>
    <header>Re: Quartz-dev Digest, Vol 3, Issue 249</header>
    <body>How can I do that without CGS? Message: 2 Date: Fri, 17 Nov 2006 23:45:27 -0800 Subject: Re: Quartz-dev Digest, Vol 3, Issue 248 Cc: email@hidden 10.4 because But why do you need the cursor's image? Most of the time you just need to set the cursor to whatever you want it to be and don't really care about the image that is actually used. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan -- Cotton Chen</body>
  </mail>
  <mail>
    <header>Re: Re: Quartz-dev Digest, Vol 3, Issue 248</header>
    <body>Efficient&amp;nbsp;screen&amp;nbsp;casting&amp;nbsp;is&amp;nbsp;one&amp;nbsp;reason&amp;nbsp;you&amp;nbsp;may&amp;nbsp;want&amp;nbsp;to&amp;nbsp;know&amp;nbsp;the&amp;nbsp;current&amp;nbsp;cursor&amp;nbsp;image.-Shawn David Duncan &amp;nbsp;&amp;nbsp;&amp;nbsp; I want to get the cursor's image, and have read lot's of discussion in ADC's mailing list. &amp;nbsp;&amp;nbsp;&amp;nbsp; Is there any API could accomplish the the purpose desired? I mean to get the mouse cursor's image. &amp;nbsp;&amp;nbsp;&amp;nbsp; Than you so much.</body>
  </mail>
  <mail>
    <header>Re: Quartz-dev Digest, Vol 3, Issue 248</header>
    <body>&amp;nbsp;&amp;nbsp;&amp;nbsp; Than you so much. email@hidden Send Quartz-dev mailing list submissions to &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; email@hidden To subscribe or unsubscribe via the World Wide Web, visit or, via email, send a message with subject or body 'help' to You can reach the person managing the list at &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; email@hidden When replying, please edit your Subject line so it is more specific Today's Topics: &amp;nbsp;&amp;nbsp;1. About CGSGetGlobalCursorData (Cotton Chen) &amp;nbsp;&amp;nbsp;2. Re: About CGSGetGlobalCursorData (David Duncan) ---------------------------------------------------------------------- Message: 1 Date: Fri, 17 Nov 2006 18:19:38 +0800 Subject: About CGSGetGlobalCursorData To: Message-ID: Content-Type: text/plain; charset=UTF-8; format=flowed Hello, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;I recently used CGSGetGlobalCursorData in my Carbon application. But it show an error in link phase. &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Shell I include any framework or library ? And does Tiger still support this API? &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Thanks a lot. Cotton ------------------------------ Message: 2 Date: Fri, 17 Nov 2006 09:11:58 -0800 Subject: Re: About CGSGetGlobalCursorData To: Cotton Chen &amp;lt;email@hidden Cc: Content-Type: text/plain; charset=US-ASCII; delsp=yes; format=flowed The CGS prefix means that is an Apple Private SPI. They are not supported, and can change at anytime. What are you trying to accomplish that you believe you need to use an SPI? -- David Duncan Apple DTS Quartz and Printing email@hidden ------------------------------</body>
  </mail>
  <mail>
    <header>Re: About CGSGetGlobalCursorData</header>
    <body>On Nov 17, 2006, at 2:19 AM, Cotton Chen wrote: The CGS prefix means that is an Apple Private SPI. They are not supported, and can change at anytime. What are you trying to accomplish that you believe you need to use an SPI? -- David Duncan Apple DTS Quartz and Printing email@hidden</body>
  </mail>
  <mail>
    <header>About CGSGetGlobalCursorData</header>
    <body>Hello, I recently used CGSGetGlobalCursorData in my Carbon application. But it show an error in link phase. Shell I include any framework or library ? And does Tiger still support this API? Thanks a lot. Cotton</body>
  </mail>
  <mail>
    <header>CIImage and 2D LUTS</header>
    <body>Of the stock filter/image units that I see, there is only the &amp;quot;colorcube&amp;quot; filter, which to my understanding performs more like a 3D lut. At the end of my ciimage processing chain I need to apply a 2-d look up table to the image.  This LUT is calculated programatically so it cannot be &amp;quot;emulated&amp;quot; with other ciimage filters. It would be something similar to vImage's vImageInterpolatedLookupTable_PlanarF functionality. thanks in advance. -- ---- Bruce Johnson email@hidden</body>
  </mail>
  <mail>
    <header>Re: Handling multiple CGLayers</header>
    <body>On Nov 15, 2006, at 4:11 PM, Hemant Balakrishan wrote: Depending on what context you use when you create the layer, the computer may store a lot of information.  In Tiger, for example, I believe that each layer maintains a full pixel map for the content of the layer.  I suspect you're chewing up a lot of memory with lots of offscreen pixel maps for each layer. Quartz 2D is really fast.  Did you try just redrawing everything (within the bounds of the object being moved) and checking the performance of that first? If you still need a performance boost, what you might be able to do is create two special layers when the user gets ready to move an object.  Into one layer you draw everything underneath the object that is moving, and on the other you draw everything that is above it.  Then when the user moves the object, you draw the &amp;quot;underneath layer&amp;quot;, the &amp;quot;item being moved&amp;quot; and finally the &amp;quot;overlay layer&amp;quot;. Those special layers need only exist for the duration of the move operation.</body>
  </mail>
  <mail>
    <header>Handling multiple CGLayers</header>
    <body>My application requires me to draw several objects on a canvas and then the user get to move a groups of selected objects or manipulate the dimensions of a single object. I'm trying to create a list of Layers (one per object) and reuse the layer if the corresponding object is not moved or modified (this saves some time). For the objects that are moved/modified I try to redraw the layer, store it and then use it. For some reason storing these layers between redraws seems to be space intensive and eventually slows down the application. The other issue is with resolution, when I zoom in the stored layers do not scale up to the current resolution.</body>
  </mail>
  <mail>
    <header>Re: Display a NSView into a CGContextRef</header>
    <body>If you use the other advices, setting the current context, don't try to save the &amp;quot;old current context&amp;quot;, just do: Note that your NSGraphicsContext flipped parameter has to be equal to your view isFlipped value. -[NSView displayRectIgnoringOpacity:inContext:]; which is much cleaner and avoid doing all the &amp;quot;current context&amp;quot; voodoo. -- Julien</body>
  </mail>
  <mail>
    <header>Re: Display a NSView into a CGContextRef</header>
    <body>I haven't tried this, but you could probably do something like this (untested) code: void DrawNSViewWithContext(CGContextRef cgContext, BOOL flipped) NSGraphicsContext *gc = [NSGraphicsContext Jason On Nov 13, 2006, at 5:54 AM, laurent grangier wrote:</body>
  </mail>
  <mail>
    <header>Re: Display a NSView into a CGContextRef</header>
    <body>On Nov 13, 2006, at 4:54 AM, laurent grangier wrote: Is the hosting application Cocoa based? Is the call into your plugin from the main thread? -[NSGraphicsContext graphicsContextWithGraphicsPort:flipped:] // -[NSView lockFocusIfCanDrawInContext:] // note not sure this does anything at the moment... With that said I am note sure how you would position things in that context or what Cocoa framework initialization would be needed for this to work (need to get AppKit attached to the window server).</body>
  </mail>
  <mail>
    <header>Forcing CIContext not to interpolate images?</header>
    <body>When drawing, it seems that CIContext always interpolates images. Is there any workaround for this? Is this a bug anyway? It seems that CIContext has no interpolation setting at all. Using CIPixellate filter might help, but what if accurate pixel conversion is needed (CIPixellate is innnacurate?) ? Regards, Aidas</body>
  </mail>
  <mail>
    <header>Display a NSView into a CGContextRef</header>
    <body>Hello, I'm developing a plugin and I can get a CGContextRef (CoreGraphics) from the main application. My question is how can I display a Cocoa NSView on the CGContextRef ? Is it possible ? To be sure you understand my question, here is an other explanation: the main application gives me a CGContextRef pointer (it is the only graphical element I receive from it). I want to display Cocoa NSView on it. Thanks for your answers, Laurent Grangier</body>
  </mail>
  <mail>
    <header>Re: pixel data from CGImageRef</header>
    <body>The only recommended way to do this is to create a bitmap context with a memory buffer that you specify and draw the CGImageRef to it using CGContextDrawImage. Now you have a copy of the data in the buffer you specified. No other official way to do this.</body>
  </mail>
  <mail>
    <header>Re: Question regarding a custom image unit</header>
    <body>I don't see you setting the class? Also is the ivar that you pass to the kernel of NSNumber type? Check out the CIDemoImageUnit in /Developer/Examples/Quartz/Core Image for guidance.</body>
  </mail>
  <mail>
    <header>Re: Question regarding a custom image unit</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: View flipping effect</header>
    <body>On 5 Nov 2006, at 19:14, Thomas Hartwich wrote: I have a Core Image kernel which will do the flipping effect if you have the images for the front and back of the thing to be flipped. You just pass in the time value. I'm not sure if you're trying to flip the whole window or just part of its content. If it's the whole window, that's more tricky because you need to get a screen shot of the desktop under the window and include that in your transition. Email me if you want the kernel and a Quartz Composer project which demonstrates it.</body>
  </mail>
  <mail>
    <header>View flipping effect</header>
    <body>Hi, I have been looking through quite a few Quartz examples, but failed to find a solution for the following task: I have a regular window with a content view in Cocoa and I want to present a flipping effect like a dashboard widget, when the user hits a control, e.g. a button in the view. The front view presents some buttons and the backside some others. So far I have not found an example or a core image effect doing this kind of flipping. I also looked in to NSAnimation without success. Do you have any pointers to some conceptual information or even sample code how to achieve this? Thanks in advance Thomas Hartwich</body>
  </mail>
  <mail>
    <header>QCView with composition and output keys not available</header>
    <body>I'm loading a Quartz Composition in q QCView in Cocoa and when I start the rendering the first time, I can get the list of Output Keys, but not the value of a specific key. Here is the code: [circleGraphic loadCompositionFromFile:pointerPath]; NSArray NSLog(@&amp;quot;All output keys %@&amp;quot;,[circleGraphic outputKeys]); NSLog (@&amp;quot;Output key value (should be a string): '%@'&amp;quot;,[circleGraphic 2006-10-25 20:18:02.841 TestApp?[1217] Output key value (should be a string): '' 2006-10-25 20:18:02.841 TestApp?[1217] Output key value (should be a string): 'Excellent Value' Where &amp;quot;Excellent Value&amp;quot; is the proper value stored in the Composition's output key. It seems like as long as the composition is not rendering the values appear empty, and just starting the rendering before may not leave enough time... not too sure. What would be the right way to do this? It looks like there are notifications, should I use those? I tried to, but the notification doesn't seem to get posted... - Renaud</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate/ImageIO weirdness</header>
    <body>Am 16.04.2007 um 08:56 schrieb Ken Tozier: ... which boils down to (rounded) 4&amp;quot; @ 72 dpi. Everything &amp;quot;works as designed&amp;quot;, you need to fiddle with Quark internals after importing. Regards, Tom_E</body>
  </mail>
  <mail>
    <header>CGBitmapContextCreate/ImageIO weirdness</header>
    <body>I'm making these simple on-the-fly placeholder images for ads and  am getting this really weird scaling issue. I think I'm probably not choosing pixel formats correctly, but  after testing all the ones that seemed to make sense, I'm stuck. What's happening is really bizarre, the smaller I define an image, the larger it appears when imported into a Quark picture box. I can open them in Photoshop and the sizes are correct but Photoshop is a dedicated image processing application so maybe it's compensating for odd pixel definitions. For example, here's a matrix of defined sizes vs sizes when the images are imported into Quark Defined size			Import size		Percentage 130 x 72		=&amp;gt;		273 x 160		210% 270 x 504	=&amp;gt;		154 x 288		57% 688 x 715	=&amp;gt;		277 x 288		40% CGContextRef CGUtilCreateBitmapContext(int inPixelsWide, int inPixelsHigh, int inBytesPerPixel, void *inBuffer) context = CGBitmapContextCreate (inBuffer, inPixelsWide, inPixelsHigh, 8,      // bits per component inPixelsWide * inBytesPerPixel, colorSpace, // I really have no clue what to choose here but // I tried several and all give the same result - (void) initPlaceholderImage CGContextRef				context				= CGUtilCreateBitmapContext(width, CGImageDestinationRef		destRef				= CGImageDestinationCreateWithURL // Cocoa draw commands are simpler so wrap CGContext in an NSGraphicsContext NSGraphicsContext			*nsContext			= [NSGraphicsContext graphicsContextWithGraphicsPort: context // create the placeholder image // write img to a tiff file // import image into box If I define an images to be</body>
  </mail>
  <mail>
    <header>Re: CGLayer performance problem</header>
    <body>Forgive me for asking, but what is the value of a CGLayer if it has to be re-rendered every time it is used under a new context (for example... every time a window is updated). So far in my experience with CGLayer has been awful.  For those trying to use it like a canvas (ie: your document is 5000 X 7000 but your window is 640 X 480) forget it!  Even after clipping to the destination CGContext the performance is terrible. You might as well write code that simply draws on demand. -- Rick Steele Wildsync Systems L.L.C.</body>
  </mail>
  <mail>
    <header>Bindings, and Core Image</header>
    <body>I'm having a bit of a problem, using bindings with a core image filter that I'm working on. The filter is actually constructed as a composite of several other filters, which it uses bindings to keep in sync. The problem I'm having is that changes to one of it's parameters (in this case, inputOrigin) are not reflected by calls to outputImage. inputOrigin is bound to a user interface element, which allows the effect to be positioned on the image. What I would expect to have happen is that when inputOrigin is changed, it would trigger a change notification for outputKeyColor, which would then trigger a change notification for outputImage. What actually happens is that the change notification for outputKeyColor occurs, but the notification for outputImage does not. + (void) initialize [self setKeys: [NSArray arrayWithObjects: @&amp;quot;inputImage&amp;quot;, @&amp;quot;inputOrigin&amp;quot;, // ... nil] [self setKeys: [NSArray arrayWithObjects: @&amp;quot;inputImage&amp;quot;, // ... @&amp;quot;outputKeycolor&amp;quot;, nil] // ... [self setKeys: [NSArray arrayWithObjects: @&amp;quot;inputImage&amp;quot;, @&amp;quot;inputOrigin&amp;quot;, // ... @&amp;quot;outputKeycolor&amp;quot;, nil] The notification for outputImage then occurs, but at the wrong time - before outputKeyColor has been resolved. Currently, outputImage is using setValue: forKey: to connect all of the filter stack's input and output images - this is because core image throws an exception if I bind to a filter's outputImage without first having established all of its inputs. Ron Aldrich Software Architects, Inc.</body>
  </mail>
  <mail>
    <header>Re: CGLayerCreate's context argument</header>
    <body>I worked around this problem (once... in a small app) by keeping meta- information beside the layer that kept track of the context in which the layer was created.  For example, when the layer was created to draw to the main display I kept a flag that says &amp;quot;this layer is good for the screen&amp;quot;.  Then, when printing, I would ignore the layer and do the drawing directly. This is similar to your &amp;quot;Give me a blob&amp;quot; idea except that I kept that information myself.</body>
  </mail>
  <mail>
    <header>Re: CGLayerCreate's context argument</header>
    <body>As for speeding up redraw, I saw in my test while evaluating the API that the raster case is very different from the non-raster one. In the raster case, I found that using CGImages really speed things up, but with a curious limitation I saw on a macmini ppc : if the CGImage does not match the CGColorSpace of the device, there is a big performance hit right after 24 blocks of 128x128 images are drawn. Amortizing the color space conversion take something like 80ms the first time the CGImage are cached, and then less than 10ms to reuse it, but in the &amp;gt;24 case, the 80ms is never amortized, which makes the redraw from very fluid to very jerky (hope it is the right word) So if I profile manually the images (which require more work), I can draw those images almost instantly (drawing a big image into tiled images). Which makes moving the image on the screen almost instant, even when the original file is something like 45 MB of data. What is weird is that if I open the same image in the Preview app from Apple (which should know a lot about performance issue), the dragging of the image is dog slow, it takes like 1s to redraw each time a move is made. This very special case to say that... The API does pretty good things on the functionnality part. But when it comes to performance, I've found no other solution than creating a new project to test different scheme for doing the same thing, which may/would change the architecture of your app if you're porting it. In fact you've got to know the limitations of it and unfortunately this is not documented. Somehow the knowledge of the limitation of the API in term of performance must be included in your code and &amp;quot;embed&amp;quot; the API. This requires more work, but this makes a huge difference in the end.</body>
  </mail>
  <mail>
    <header>Re: HIViewSetBoundsOrigin (Zoom bug)</header>
    <body>On Apr 11, 2007, at 11:34 AM, Hemant Balakrishnan wrote: I don't think we have any existing reports of this, so your next step should be to file a bug with a sample app that shows the problem, and we can investigate. It doesn't sound offhand like something you'll be able to work around, though.</body>
  </mail>
  <mail>
    <header>CGLayerCreate's context argument</header>
    <body>I've been converting existing code from using bitmap-based drawing caches to using CGLayers. In general, the conversion goes smoothly and improves performance, which is great. But there's a stumbling block I've run into each time, and that is the 'context' argument to CGLayerCreateWithContext(). There are two general problems I run into repeatedly when using CGLayers, one small and one large. The small one is the necessity of having an initialized CGContext at the instant I want to create the CGLayer. This makes it awkward to pre-cache drawing I know I'm about to do, but it can be worked around. The larger problem is the apparent impossibility of determining whether a given CGLayer is &amp;quot;compatible&amp;quot; with a given CGContext. If I'm going to store complex drawing in a CGLayer so I can re-use it, then I need some way to know whether I should re-use a given layer in a given situation. If my document is being printed to PDF, or rendered to a bitmap, or if the window is moved to a screen with different properties (different GPU hardware, e.g.), or the display's properties such as color depth or resolution are changed, then I need to know to create a new CGLayer rather than re-using an existing one. As a result, right now CGLayers are only really useful for drawing something multiple times within a given view, not for speeding up re- draw, except in some limited situations. What I would really like to see is some API to extract the salient information from a CGContext into an opaque blob, which could be stored and compared to other blobs, and later used to create a CGLayer without direct reference to the CGContext. (Perhaps it could be passed in using the currently-unused 'auxiliaryInfo' argument to CGLayerCreateWithContext().) Then, when drawing, I could get the blob for the CGContext I'm drawing to, compare it to the blobs used to create my various cache CGLayers, and either select the appropriate cache or create a new one. There would probably need to be a CGLayerAreBlobsEquivalent() call, rather than just using CFEquality on the blobs, because some layer attributes --- such as gstate parameters --- do not affect compatibility. In an ideal world, this blob would actually be a CFDictionary, some of whose keys would be private (internal hardware info, for example) but some of whose keys would be public (CTM, native color space, context type) or even modifiable, if that makes sense.</body>
  </mail>
  <mail>
    <header>Threaded Caching CIImage and GraphicContext</header>
    <body>we have a problem using NSGraphicContext/CGGraphicContext in threads. One the one (main) thread, we use the context to draw normal stuff. There we also draw semi-transparent CIImages, so we added CGContextBeginTransparencyLayer and CGContextEndTransparencyLayer calls. Also some CGContextClipToRect calls are needed while drawing. This works normally without problems. But than we added another thread to cache the created CIImages. This thread creates a bitmap representation using and uses [[nsContext CIContext] drawImage:] to draw to this bitmap rep. While doing this, everything works fine, but in not determinable circumstances the application crashs with such a stack trace: Thread 0 Crashed: 0   &amp;lt;&amp;lt;00000000&amp;gt;&amp;gt; 	0xfffeff18 objc_msgSend_rtp + 24 1   com.apple.QuartzCore           	0x94298078 -[CICGContextImpl setOwner:] + 40 2   com.apple.QuartzCore           	0x94316c00 -[CIContext invalidate] + 44 3   com.apple.QuartzCore           	0x94316b9c -[CIContext dealloc] + 36 4   com.apple.CoreFoundation       	0x907eace8 CFDictionaryRemoveValue + 492 5   com.apple.AppKit               	0x937a5358 -[NSCGSContext dealloc] + 36 6   com.apple.AppKit               	0x937a5320 - [NSWindowGraphicsContext dealloc] + 76 7   com.apple.AppKit               	0x937a529c -[_NSViewGState dealloc] + 56 8   com.apple.Foundation           	0x92939968 NSPopAutoreleasePool + 536 9   com.apple.AppKit               	0x93702d34 -[NSApplication run] + 544 10  com.apple.AppKit               	0x937f387c NSApplicationMain + 452 Any ideas where a error could be? I tried to retain the context while drawing and release it after drawing, but this has no effect.</body>
  </mail>
  <mail>
    <header>(Solved) Extracting a pixel's color from a  CIImage</header>
    <body>On Apr 10, 2007, at 11:54 AM, Jason Harris wrote: colorspace, in order to get the behavior I wanted.  Also, in order to get floating point data which was endian swapped correctly, I had to specify kCGBitmapFloatComponents | kCGImageAlphaPremultipliedLast | kCGBitmapByteOrder32Host for the CGBitmapInfo parameter to CGBitmapContextCreate. I've attached source code to an a CIImage class extension (Based loosely on your sample), in the hope that others will find it useful. Attachment: Ron Aldrich Software Architects, Inc.</body>
  </mail>
  <mail>
    <header>Re: Extracting a pixel's color from a  CIImage</header>
    <body>On Apr 10, 2007, at 12:45 PM, Ron Aldrich wrote: It's probably worth noting that a CIImage doesn't contain pixels, it contains instructions to render pixels into a CIContext.  I believe (but I might be wrong) that a CIImage also doesn't have an associated color space.  It uses the colorspace of the CIContext (which usually uses the colorspace of the CGContext used to create it). One thing you could try is to disable the CIContext's colorspace as follows: NSDictionary* contextOptions = [NSDictionary dictionaryWithObjectsAndKeys:[NSNull null], CIContext* context = [CIContext contextWithCGContext:imageContext Nick</body>
  </mail>
  <mail>
    <header>Extracting a pixel's color from a  CIImage</header>
    <body>I'm looking for a way to extract a pixel's color from a CIImage, in whatever color space the CIImage happens to be using.  (Basically, I need to do some analysis on an image which is not suitable for a kernel function, and extract a &amp;quot;Color of interest&amp;quot; from the image, which will then be fed into a kernel filter.) Currently, it appears that the values I'm getting are being modified in some way when I render them to an offscreen buffer as follows: NSMakePoint([inputOrigin X], [inputOrigin Y]), NSZeroSize theBitmapContext = CGBitmapContextCreate (theBitmapData,           // data theBoundsRect.size.width,  // width theBoundsRect.size.height, // height 8,                       // bits per component theBytesPerRow,          // bytes per row theColorspace,           // color space NSGraphicsContext* theGraphicsContext = [NSGraphicsContext graphicsContextWithGraphicsPort: theBitmapContext NSGraphicsContext* theSavedContext = [[NSGraphicsContext NSRectFill(NSMakeRect(0, 0, theBoundsRect.size.width, [theImage drawInRect: NSMakeRect(0, 0, theBoundsRect.size.width, theBoundsRect.size.height) fromRect: theBoundsRect operation: NSCompositeSourceOver I would assume that because I'm using CGColorSpaceCreateDeviceRGB() to create the color space, that I'm causing the color conversion at that point. So, first - what color space should I be using for the bitmap context, in order to prevent any conversion of the colors? And third, how would I get the pixels in 32-bit float format, instead of 8 bit fixed? Ron Aldrich Software Architecs, Inc.</body>
  </mail>
  <mail>
    <header>Re: CoreImage Filter Definitions</header>
    <body>I does give me a good definition of each mode (the basic ones at least).  So, Thanks! I have a test bench in Quartz Composer that I'm using to experiment with the various filters and I'm seeing very odd things when I give them pixel values out of the range 0.0-&amp;gt;1.0. For example: A Grayscale ramp going from 0.0 to 2.0 is blended with an image which has 1.0 for a pixels, using the Multiply Blend Mode. The result is a ramp from -1.0 to 1.0, when obviously I would expect the same ramp that I put in. In this case the Multiply patch (which I assume maps to CIMultiplyCompositing) does what I expect (and what my reading of the PDF spec says). Unfortunatley that's about the only filter that has a more understandable sibling.</body>
  </mail>
  <mail>
    <header>Re: ImageIO bug when exporting CGImage to data handle?</header>
    <body>On Apr 10, 2007, at 6:46 AM, Dinge Raphael wrote: NDA. I can't post any of the APIs. I'm sure there must be a way, just can't find it at the moment... Ken</body>
  </mail>
  <mail>
    <header>Re: ImageIO bug when exporting CGImage to data handle?</header>
    <body>On Tue, 10 Apr 2007 05:13:44 -0400, PICT files differ from in-memory PICT handles in that they have a 512-byte header (containing no useful information). Perhaps ImageIO is including this extra junk to the beginning of your CFMutableDataRef.  When you copy the CFMutableDataRef to a Handle, try skipping the first 512 bytes. -- marco -- It's not the data universe only, it's human conversation. They want to turn it into a one-way flow that they have entirely monetized. I look at the collective human mind as a kind of ecosystem. They want to clear cut it. They want to go into the rainforest of human thought and mow the thing down.</body>
  </mail>
  <mail>
    <header>CoreImage Filter Definitions</header>
    <body>I'm writing some simple compositing software using CoreImage to do the work. I'm finding myself writing my own versions of some of the shipping CI filters because I can't find a good mathematical definitions of the ones that ship with Tiger. Does anybody know where I might find this information? All I can find is the (rather wooly) descriptions of the intended behaviour in the Core Image Filter Reference Document. The alternative is doing 'black box' analysis on them, but that's slow, any difficult to get all corner cases. C = A / (1.0 - B) : B  &amp;gt; 0.0 &amp;amp;&amp;amp; B &amp;lt;= 1.0 C = A             : Otherwise Obviously only some Filters lend themselves to that kind of description, but I'm only really looking for the CICategoryCompostiteOperation filters, and possibly CICategoryColorAdjustment &amp;amp; CICategoryColorEffect. Of course, source code would be best, but I'm assuming I'm not going to get that. Paul Attachment:</body>
  </mail>
  <mail>
    <header>Re: ImageIO bug when exporting CGImage to data handle?</header>
    <body>This is why I think that maybe the data of the PICT gets written differently from ImageIO when writting to memory or writing to disk. Probably could you test this, just looking if the PICT written to disk has resource forks. This makes me remember the old days :) When I was a kid, Quark would take PICT file to put into image blocks, and the neat thing and the use of PICT, is that a PICT could contain quickdraw commands, allowing Quark to interpret not only raster informations, but more general drawing operations to handle texts and lines it could convert to valid PS. (but this was a mess, and I knew only one external program that would generate valid PICTs for Quark to produce a PS valid for those old RIPs) Is the Quark SDK public or are you on NDA ? I would be curious to see how it has changed, and maybe if there is a simple solution to your problem (given that putting raster data in a quark document should be made simple on the dev part I agree...)</body>
  </mail>
  <mail>
    <header>Re: ImageIO bug when exporting CGImage to data handle?</header>
    <body>In my case, there are no forks of any kind. The images are built, in- memory, from ad information  (dimensions, color mode, client name, etc...) and passed off to Quark. They never get written to disk (except as embedded data in a Quark document) Unfortunately, after further reading, it looks like Quark may have replaced this simple &amp;quot;set a handle&amp;quot; method with a tortuous process involving literally dozens of functions. The new way is much more powerful, but the learning curve looks like several weeks worth of concerted effort, just to put an image in a box. Ken</body>
  </mail>
  <mail>
    <header>Re: ImageIO bug when exporting CGImage to data handle?</header>
    <body>From what I remember, I had a lot of problem with PICTs to use directly into Quartz. As far as I remember, this was a problem with resources. Here I would logically except that the mutable data ref only holds the data fork of the PICT, and maybe for some reason is this different when you specify an URL, where the engine could also add resource forks ? In that case, Quark would only support PICT with resources, and it would fail. Maybe should you try (at least to see) with TIFF files or any other data fork based format supported by both Quark &amp;amp; Quartz.</body>
  </mail>
  <mail>
    <header>ImageIO bug when exporting CGImage to data handle?</header>
    <body>I'm generating images on the fly using CGBitmapContexts to create CGImages which are then converted to picts using a CGImageDestinationRef. What I'm finding is that when I pass these pict handles off to Quark, it doesn't like them. It sees that they have data in them and act as if all is working but it then fails silently, not rendering the picts in the specified boxes. If I export the exact same image to a URL, I'm able to open the picts through the import dialog. Has anyone else encountered difficulties with ImageIOs export to CFMutableDataRef functionality? Ken</body>
  </mail>
  <mail>
    <header>Re: inter-process drawing of a CGContext</header>
    <body>This is more an outcome of the X11 port of the logic where they don't have to do any of this messing around, and their question why do you have to do that stuff? The hassle is that we have to draw, tell the plugin in the other process what was drawn, wait for it to say it's finished drawing to the screen, handle the case where it does not response, then continue the cycle. On Apr 9, 2007, at 4:46 PM, Christopher Hunt wrote: -- ======================================================================== === Corporate Smalltalk Consulting Ltd. ======================================================================== ===</body>
  </mail>
  <mail>
    <header>Re: inter-process drawing of a CGContext</header>
    <body>Cheers, -C</body>
  </mail>
  <mail>
    <header>some issus about CIImage object.</header>
    <body>Hi list. I am very interested in the core image and opengl programming.So when I read the document about the core image, I discovered there is a method which create the core image object with opengl texture. - initWithTexture:(unsigned long) An OpenGL texture. Because CIImage objects are immutable, the texture must remain unchanged for the life of the image object. See the discussion for more information.cs for images that don't contain color data (such as elevation maps, normal vector maps, and sampled function tables).It is abosutely useful for my application, for I want to save my opengl sence to the texture and then create the CIImage object. After that, I can&amp;nbsp; apply the CIFilter to the CIImage object to create some visual effort. However , without enough example , I don&amp;#39;t know the exact step to accomplish this task. I really need some guidance. If some master here are willing to help me , we can have a further discussion. Thank you all in advanced. Really appreciate for you help, Cheers.</body>
  </mail>
  <mail>
    <header>Re: inter-process drawing of a CGContext</header>
    <body>&amp;quot;You might want to check the archives for this... I think the conclusion I did however tackle changing things but need to allocate 16MB + 20 bytes and other list messages then imply shmget has to be some multiple of 4MB plus then other issues, plus then the example shmgets I used gaved invalid parm, or other types of errors. So I think I'll leave things alone, ship, and gather more input. -- ======================================================================== === Corporate Smalltalk Consulting Ltd. ======================================================================== ===</body>
  </mail>
  <mail>
    <header>Re: inter-process drawing of a CGContext</header>
    <body>On Apr 7, 2007, at 7:38 PM, John M McIntosh wrote: You could consider switching to shmget instead of mmap to share memory between processes. I was using mmap for something similar, but couldn't get the performance needed until I switched to using the shared memory manager. It's a little harder to synchronize, but the performance gains are well worth it. HTH, -Jon -- Jonathan Johnson email@hidden REAL Software, Inc. REAL World 2007 Conference May 9 - 11 in Austin, Texas</body>
  </mail>
  <mail>
    <header>Re: inter-process drawing of a CGContext</header>
    <body>I'm afraid not. A CGContextRef is only valid in the process of it's creator, so you cannot share it directly with another process. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>inter-process drawing of a CGContext</header>
    <body>I have this interesting problem where I need to move the data out of a CGContext in a headless application to a CGContext  belonging  to a browser window. The headless application is forked off by a browser plugin btw, and is fed the netscape api UI events, and responds back with drawing events. Right now I&amp;quot;m using a CGBitmapContextCreate  to share the bitmap data area which mmap() to share the data pointer between the two processes. The shared memory is updated by doing a CGContextDrawImage along setting top,left,bottom,right co-ordinates and the plugin then uses a CGBitmapContextCreate, CGBitmapContextCreateImage, and CGContextDrawImage to draw the bits into the browser's CGContext. Usage of offset into data, and calculation of rowbytes allows me to create subimages out of the larger drawing context. I had used the CGImageCreateWithImageInRect originally but need this to run under 10.3 also. However I'm wondering if there is a way to draw directly from one process into another process's CGContext and avoid all this hassle. -- ======================================================================== === Corporate Smalltalk Consulting Ltd. ======================================================================== ===</body>
  </mail>
  <mail>
    <header>Re: Color space equivalent to	CGColorSpaceCreateWithName(kCGColorSpaceGenericRGB)</header>
    <body>On Apr 5, 2007, at 1:25 PM, Mark Munte wrote: That's the color space documented as being equivalent to generic RGB in the Technical QA on the subject:</body>
  </mail>
  <mail>
    <header>Re: Core Image Fun House badly broken?</header>
    <body>on 3-04-2007 5:13 email@hidden said the following: I tried with several images, including the four sample images that CIFH prompts for opening when launched. -- Andrea &amp;quot;XFox&amp;quot; Govoni AIM/iChat/ICQ: email@hidden Yahoo! ID: xfox82 Skype Name: draykan PGP KeyID: 0x212E69C1 Fingerprint: FBE1 CA7D 34BE 4A53 9639  5C36 B7A0 605F 212E 69C1</body>
  </mail>
  <mail>
    <header>Re: Core Image Fun House badly broken?</header>
    <body>on 3-04-2007 4:39 Ricky Sharp said the following: I tested CIFH on an eMac G4 1,42GHz with an ATI Radeon 9600. I didn't try the builded from source version because I'm referring to the binary shipped with the last Xcode Tools (2.4.1). However I'll try to compile the project to see if it makes any difference. -- Andrea &amp;quot;XFox&amp;quot; Govoni Codice Wii: 0294 5787 0060 3021 Contest of the week: Da quale film √® tratta la frase seguente? Il pi√π veloce a rispondere ricever√† una salvietta ufficiale di Multiplayer.it! AIM/iChat/ICQ: email@hidden Yahoo! ID: xfox82 Skype Name: draykan PGP KeyID: 0x212E69C1 Fingerprint: FBE1 CA7D 34BE 4A53 9639  5C36 B7A0 605F 212E 69C1</body>
  </mail>
  <mail>
    <header>Re: Color space equivalent to	CGColorSpaceCreateWithName(kCGColorSpaceGenericRGB)</header>
    <body>Can someone confirm that kCGColorSpaceGenericRGB is equivalent or not to the ICC profile at /System/Library/ColorSync/Profiles/Generic RGB Profile.icc ? Since CGColorSpace is an opaque type I have found no way to query it's values - or am I missing something obvious?</body>
  </mail>
  <mail>
    <header>ROI and convolution</header>
    <body>kernel vec4 convolution3x3(sampler src, float f1, float f2....) ... ... //same for the 8 pixels remaining This works fine, but now I'd like to change the size of the mask from 3 to, say, 9. I could write another kernel, but I don't think that's the right solution. I read on the &amp;quot;Core Image programming guide&amp;quot; that I could supply a ROI function which looks like : - (CGRect) regionOf:(int)samplerIndex destRect:(CGRect)r userInfo:obj float n = [obj floatValue];   //size of the mask But I can't find any example for how to rewrite my kernel according to this. I guess I could do something more or less like : kernel vec4 convol(sampler src, sampler mask). But I don't see how after. G.</body>
  </mail>
  <mail>
    <header>Re: Rectangle, Rotating and Clipping</header>
    <body>Thank you Jon J. for your offer and Scott T. for your response. It looks like my call to CGContextClip(myContext) within my draw routine is the culprit. I'm struggling a bit with the clipping rectangles required, but deleting this call gives me what I am looking for. Thank you again and sorry for the noise. -- Thomas C.</body>
  </mail>
  <mail>
    <header>Re: Event types 19 and 20?</header>
    <body>On Apr 3, 2007, at 7:22 AM, Bill Cheeseman wrote: Seems I remember something about this from WAY back on the Carbon-Dev list.  I can't effectively search for them at lists.apple.com because 19 is such a popular number :-) As I recall, Eric Schlegel responded and said that there are some undocumented, internal events that get posted to the carbon event queue.  He suggested that the person who posted the question simply pass them on to the other handlers in the Carbon Event stack.  As I recall they were related to the window server... but I don't remember exactly what they were all about.</body>
  </mail>
  <mail>
    <header>Re: Rectangle, Rotating and Clipping</header>
    <body>If you're using the built in Graphics calls, I wouldn't expect them to work with custom rotation performed via declares. However, if you're using CoreGraphics calls directly to draw the rectangle, I'm not sure exactly what's going wrong. It appears if in addition to rotating you're also scaling. If you'd like to send me your code off-list, I could take a quick gander. HTH, -Jon -- Jonathan Johnson email@hidden REAL Software, Inc. REAL World 2007 Conference May 9 - 11 in Austin, Texas</body>
  </mail>
  <mail>
    <header>Re: Rectangle, Rotating and Clipping</header>
    <body>On Apr 2, 2007, at 10:43 PM, Thomas Cunningham wrote: It looks like you, or potentially Real, are scaling the canvas so that the rectangle always lies within the frame of the canvas.</body>
  </mail>
  <mail>
    <header>Event types 19 and 20?</header>
    <body>When I set an event tap to monitor all events, I see events with types 19 and 20 from time to time (I'm monitoring the Finder).  I can't find these defined anywhere. Can somebody point me to a header file? The closest I can come is IOLLEvent.h, which says immediately after NX_KITDEFINED (15), &amp;quot;There are additional DPS client defined events past this point.&amp;quot; I assume &amp;quot;DPS&amp;quot; is Display PostScript. This file dates back to 1988 (!!!) with a 1991 note referring to a dpsclient/event.h file, which I can't find. Is this note obsolete? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: Rectangle, Rotating and Clipping</header>
    <body>Am 03.04.2007 um 05:43 Uhr schrieb Thomas Cunningham: I think you should rephrase your question. Maybe it's just me, but it isn't at all clear 1. what you are trying to do 2. what you expect to happen vs. what really happens. And you should also post the code in question. Otherwise I don't think anyone will be able to help. You may want to read: Andreas</body>
  </mail>
  <mail>
    <header>Re: Rectangle, Rotating and Clipping</header>
    <body>So anyone have any suggestions on where I might look for errors in my code so solve this clipping problem? Thank you. -- Thomas C.</body>
  </mail>
  <mail>
    <header>Re: Core Image Fun House badly broken?</header>
    <body>On Apr 2, 2007, at 9:39 PM, Ricky Sharp wrote: The error you're seeing most likely indicates that, for some reason, the application couldn't create a CISampler from the image you provided.  Perhaps a different image might work? Brendan Younger</body>
  </mail>
  <mail>
    <header>Re: Core Image Fun House badly broken?</header>
    <body>On Apr 2, 2007, at 9:23 PM, Andrea XFox Govoni wrote: Works fine for me.  Dual 2GHz G5, 10.4.9.  Clean build of FunHouse with Xcode 2.4.1.  Graphics card is ATI 9800 Pro. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Core Image Fun House badly broken?</header>
    <body>Is Core Image Fun House broken? I have a PowerPC Core Image enabled Mac with Mac OS X 10.4.9 and Xcode Tools 2.4.1 installed and when I try to apply an Image Unit nothing happens and I get errors in console.log like this: 2007-04-01 04:08:50.667 Core Image Fun House[458] CIColorInvert: negative: nil value for argument #0 (src) I got discordant feedbacks about the issue (someone says it works, someone says it doesn't) and I'd like some more confirms prior to submit a bug report. -- Andrea &amp;quot;XFox&amp;quot; Govoni AIM/iChat/ICQ: email@hidden Yahoo! ID: xfox82 Skype Name: draykan PGP KeyID: 0x212E69C1 Fingerprint: FBE1 CA7D 34BE 4A53 9639  5C36 B7A0 605F 212E 69C1</body>
  </mail>
  <mail>
    <header>Re: Meaning of kCGTabletEventTangentialPressure?</header>
    <body>That's very helpful information. Thanks for the reference. -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: Meaning of kCGTabletEventTangentialPressure?</header>
    <body>On Apr 2, 2007, at 09:23 , Bill Cheeseman wrote: Tangential pressure is usually the value that is controlled by the scroll wheel on an airbrush. NxtGenImpGuideX.pdf Cheers, Dave</body>
  </mail>
  <mail>
    <header>Re: Meaning of kCGTabletEventTangentialPressure?</header>
    <body>In the Core Graphics API, there are two separate pressure fields for tablet pointers, 'pressure' and 'tangential pressure'. I believe you're talking about the first. I'm asking about the second. Bill</body>
  </mail>
  <mail>
    <header>Re: 3D rendering?</header>
    <body>On Feb 19, 2007, at 4:19 PM, Jeff Heyob wrote: Doing 3D graphics in a 2D graphics library is challenging (not hard, but challenging).  There are many, many resources on the web that can help you do that. If you don't want to concern yourself with the picky details of 3D projection and such, however, OpenGL will be more than happy to handle the details for you.  There are numerous OpenGL tutorials available on the web as well.  I would recommend checking them out.</body>
  </mail>
  <mail>
    <header>3D rendering?</header>
    <body>Hi, I've done a little Quartz 2D by converting some QuickDraw data plotting routines. Would there be any sample code for doing 3D data plots in Quartz? Specifically, I've got polar coordinates of angle, radius and height that I need to display in 3D. Thanks in advance for any help. Jeff</body>
  </mail>
  <mail>
    <header>building a spectrogram plot</header>
    <body>I would like to build a spectrogram plot.&amp;nbsp; A 2D plot with Every update to the plot removes the oldest row of frequency data, moves all remaining rows up one row and adds a new row of data at the bottom.&amp;nbsp; I was hoping that someone who really knows this graphics stuff could give me a direction to take to create a nice efficient plot using the latest and greatest API.&amp;nbsp; I've done work with custom views and drawing to image ref's and stuff so I could get something to work but I'm not sure that would be very efficient.&amp;nbsp; There seems to be so many way's to put pixels on the screen that I'm a little overwhelmed with the options.</body>
  </mail>
  <mail>
    <header>Re: Problem: 16-bit TIFF's and Image IO with	Float	CGBitmapContextCreate</header>
    <body>Did you set the kCGImageSourceShouldAllowFloat key when you loaded the image from ImageIO? You can replace this with aBitmapInfo = aBitmapInfo | kCGBitmapFloatComponents | kCGBitmapByteOrder32Host -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Extended display, not a mirror, from single headed graphics card</header>
    <body>On Feb 16, 2007, at 4:13 AM, selvaganesan wrote: I asked this question a few weeks ago.  The answer seems to be that you have to create an IOKit driver which overrides IOFrameBuffer. This will allow you to create a virtual display on an extended desktop (i.e., not a mirror). Nick</body>
  </mail>
  <mail>
    <header>Problem: 16-bit TIFF's and Image IO with Float	CGBitmapContextCreate</header>
    <body>I'm trying to read a 16-bit tiff (per color component so 48 bit total per pixel) I'm drawing the result in CGBitmapContextCreate that's floating point: #ifdef __i386__ aBitmapInfo = aBitmapInfo | kCGBitmapFloatComponents | #else aBitmapInfo = aBitmapInfo | kCGBitmapFloatComponents | #endif // Let's alloacte the memory for our bitmap. mContext = CGBitmapContextCreate (	mBitmapDataPtr, theSize.width, theSize.height, mBitmapColorChannelDepth, mRowBytes, aColorSpaceRef, but it has been working fine for me EXCEPT that the result image looks like that it has been gone through some kind of 8-bit pipeline and there is a lot of color detail missing. The only solution I'm seeing now is that I have to go back in using QuickTime for doing my image reading. Is this expected? I know that in the Leopard online slides they mention that 16-bit support will be added is this reason?</body>
  </mail>
  <mail>
    <header>CGContextDrawImage only works first time</header>
    <body>right now I ran into the next puzzle. Image drawing works as expected in an alternative code path where images are rendered into an CGBitmapContext.  With an CGGLContext, only the first image after launching is drawn. The following calls take the full image decode time but the result has only the background erased without any decoded data. Image and contexts are fresh created with CVOpenGLBufferCreate, CGLCreateContext and  CVOpenGLBufferAttach (and all available error codes checked...) Some logs are: [5813]  loading image PICT7437.MRW [5813]  Created for image 0xe739de0 buffer 0xe7487d0, cgl_ctx 1f39000, cg ctx e7d8b40 (w=3000 h=1992) [5813]  loading took 1.843286 sec [5813] loading image PICT7441.MRW [5813] Created for image 0xe7dac70 buffer 0x3fcef0, cgl_ctx 1ed2200, cg ctx e7dd040 (w=3000 h=1992) [5813] loading took 1.853324 sec CGContextRef ctx = CGGLContextCreate ( pbufferctx, CGSizeMake( width, height ), NSLog( @&amp;quot;Created for image %@ buffer %@, cgl_ctx %x, cg ctx %x (w=%i // draw an red background to see if the buffer is touched at all CGContextSetRGBFillColor( ctx , 1.,0.,0.,1. ); // 1. draws always CGContextDrawImage(ctx, rect, _image); // 2. works only the first time invoked.. CGContextSynchronize(ctx); // otherwise, CGContectClear didn't have any effect. //// (un)locking the pbuffercontext prevents drawing</body>
  </mail>
  <mail>
    <header>Extended display, not a mirror, from single headed graphics card</header>
    <body>It's also possible to have a virtual screen that can represent more than one physical display. Is it possible to extended  the display, not a mirror, from a single headed graphics card? Thx in advance, selva</body>
  </mail>
  <mail>
    <header>Re: attaching CVOpenGLBufferRef to CGContextRef [solved]</header>
    <body>Am 15.02.2007 um 23:27 schrieb Thomas Engelmeier: Just for the archives: the missing link is one needs to use CVOpenGLBufferCreate, CGLCreateContext and then CVOpenGLBufferAttach instead of CGLSetPBuffer. Regards, Tom_E</body>
  </mail>
  <mail>
    <header>attaching CVOpenGLBufferRef to CGContextRef</header>
    <body>has anyone managed to attach the buffer of an CVOpenGLBufferRef to a CGContextRef? I try to draw to an Quartz Composer usuable GL pbuffer and there is something missing to get the CGLPBufferObj out of an CVOpenGLBufferRef. Regards, Tom_E</body>
  </mail>
  <mail>
    <header>Re: Painting individual pixels to a view</header>
    <body>You might want to take a look at Core Image (http:// developer.apple.com/macosx/coreimage.html) for this as well. That will give you very fast pixel manipulations. Martin email@hidden</body>
  </mail>
  <mail>
    <header>Re: Painting individual pixels to a view</header>
    <body>On Feb 12, 2007, at 11:54 AM, Scott Thompson wrote:</body>
  </mail>
  <mail>
    <header>Re: Painting individual pixels to a view</header>
    <body>On Feb 12, 2007, at 4:13 AM, Ken Tozier wrote: If you will forgive me, I'd like to be very pedantic and exact for a second... There is no direct way for Quartz 2D to manipulate individual pixels.  There is a very good reason for that, Quartz 2D is a resolution independent drawing library and really has no concept of a &amp;quot;pixel&amp;quot; in it's drawing model. Even when you're talking about images, Quartz 2D &amp;quot;sees&amp;quot; them rectangular arrays of color samples. The mapping to &amp;quot;pixels&amp;quot; is done when the image (or any other graphic) is drawn. Now having provided a very academic argument,  in the real world your best bet for working with pixels in Quartz 2D is to create a CGImage and manipulate that CGImage.  So long as you don't rotate or scale that CGImage when you go to the display, it should map 1:1 to pixels (provided, of course, that you are also not dealing with a scaled GraphicsImaging/RN-ResolutionIndependentUI/index.html&amp;gt; ) Your code will have to concern itself with efficiently scrambling the color samples in a color buffer.  To get those bits onto the screen, then you can create a CGImage from the pixel buffer and ask Quartz to blast it to the display.  Keep in mind, however, that the data buffers of a CGImage are supposed to be immutable so you can't re-use the same CGImage if you make changes the your image buffer. Scott</body>
  </mail>
  <mail>
    <header>Re: Painting individual pixels to a view</header>
    <body>.I pressed Send a bit too quickly: That will work provided you don't have any scanline padding. Jerry</body>
  </mail>
  <mail>
    <header>Re: Painting individual pixels to a view</header>
    <body>On 12 Feb 2007, at 10:13, Ken Tozier wrote: Just create a new CGImage sharing the same data but with a different bytesPerRow and draw it.</body>
  </mail>
  <mail>
    <header>Painting individual pixels to a view</header>
    <body>I'm playing around with image scrambling and one idea was to create an image class where the pixels wrap in the containing view when it's resized in exactly the same way text wraps in a text view. I looked at NSBitmapRepresentation and it has a &amp;quot;bitmapData&amp;quot; method that lets you access the picture data directly, but what I can't figure is how exactly to paint individual pixels to the screen as efficiently as possible. The only thing that even comes close would be to define thousands (or millions) 1 x 1 pixel rects and paint a color into them but this sounds very expensive processor wise and I was wondering if there was a fairly efficient Quartz way to render colored pixels. Ken</body>
  </mail>
  <mail>
    <header>Re: Disable interpolation on a CIFilter</header>
    <body>Hey, I'm still having this problem.  I'm linking to two images.  The first, scanline.png is the image I am trying to tile and crop.  The second, screenshot.png, is that image tiled and scaled to an image that is twice the size.  You can see the interpolation artifacts: the 4 individual tiles, and then each tile has two triangles.  Again, any help would be appreciated. [2]:</body>
  </mail>
  <mail>
    <header>Re: Seeing Alpha after CIFilters</header>
    <body>Thanks Frank, Yep -- looks like this gets overwritten -- so no joy. Does CI Crop introduce a lot of overhead (i.e., when compared to writing a CI Sampler filter)? I could just automatically set this up behind the scenes as the output of the QuickTime movie. Best, mark -- ======================================================================= Mark Coniglio             |  email@hidden TroikaTronix              | Isadora¬Æ                  | |  (interactive media processing environment) =======================================================================</body>
  </mail>
  <mail>
    <header>Seeing Alpha after CIFilters</header>
    <body>I am sourcing some CIFilters from a QuickTime movie, and seeing the black pixels on the edge of my image drawing as green in the final rendering. I'm rendering it into an OpenGL context using code quite similar to that seen under contextWithCGLContext:pixelFormat:options: at I'm going from QT Movie -&amp;gt; CI Torus Lens Distortion -&amp;gt; CI Crop -&amp;gt; Open GL Draw. When the lens of the torus distortion goes past the edge of the image, I see the green pixels. I reckon this has something to do with the fact that YUV black is RGB green -- see an image at If you render the image on top of a checkerboard of gray/white squares, you'll see that the green portion is at least partially transparent. (See Now, if I switch the blend mode setting to GL_SRC_ALPHA instead of GL_ONE, the green pixels go away -- I'm sure this is because the alpha of the green areas is zero. But this causes other problems with layering when using GL_SRC_ALPHA. Finally, if I put a CI Crop immediately after the QuickTime movie (before the torus) and limit the size of the frame to the same resolution as the QuickTime movie, the problem goes away, even when using glBlendFunc (GL_ONE, GL_ONE_MINUS_SRC_ALPHA). I'm assuming those edge pixels get limited by the crop. Is there a way to force those edge pixels to black/zero alpha without using the crop? Best, Mark -- ======================================================================= Mark Coniglio             |  email@hidden TroikaTronix              | Isadora¬Æ                  | |  (interactive media processing environment) =======================================================================</body>
  </mail>
  <mail>
    <header>Does Quartz support 16bits RGB565 bitmap image?</header>
    <body>Hi all, &amp;nbsp;&amp;nbsp;&amp;nbsp; I got a problem to stretch a 16-bit RGB565 bitmap image. ( the source image is generated by other application) &amp;nbsp;&amp;nbsp;&amp;nbsp; The steps are, read the image data and set to a context by using CGBitmapContextCreate( bitmapbuffer,w ,h,5 , 2*w, &amp;nbsp;&amp;nbsp;&amp;nbsp; Then use CGBitmapContextCreateImage to create an image, finally draw the image to a new context with the stretched size. &amp;nbsp;&amp;nbsp;&amp;nbsp; But I can&amp;#39;t get the correct resized image. &amp;nbsp;&amp;nbsp;&amp;nbsp; Is there any way to stretch 16-bits RGB565 bitpmap? -- Cotton Chen</body>
  </mail>
  <mail>
    <header>Re: Best way to draw text from secondary thread.</header>
    <body>Mark, This is more of a Cocoa issue so the cocoa-dev list would be a better forum for these sorts of questions.  That said, I would suggest not drawing in another thread, and instead use -[NSRunLoop addTimer:forMode:] to have your timer fire during user actions.</body>
  </mail>
  <mail>
    <header>Best way to draw text from secondary thread.</header>
    <body>Hi, I am currently drawing text to a NSTextView, about 5 times per second, from the main thread through an NSTimer. I would like to move this drawing to another thread, so that user actions (like moving sliders) do not prevent/block the text from being written. Thanks, Mark</body>
  </mail>
  <mail>
    <header>Re: Memory Leak with CIContext?? (Still Broken)</header>
    <body>On Feb 6, 2007, at 6:57 AM, Mark Coniglio wrote: I would try downloading 10.4.9 developer preview if you can and see if that fixes it (sorry not in the defect loop on this one so I don't know if it would be)... if not you likely will have to wait till 10.5 for a fix. I know of no work around other then minimizing context creation in certain situations and instead reuse them. If that is simply not possible consider pushing this logic off into a second executable... that secondary executable could periodically get recycled (exit and relaunch) to clear out the leaked contexts. Note in my usage scenario I can reuse contexts for a long time so I never had to dig deeply for a work around... so some way may exist.</body>
  </mail>
  <mail>
    <header>Re: Memory Leak with CIContext?? (Still Broken)</header>
    <body>And the original bug is unreproducible in 10.4.8. Problem is that that bug only came with a few lines of code snippets. None had a small sample application demonstrating the problem.</body>
  </mail>
  <mail>
    <header>Re: Memory Leak with CIContext?? (Still Broken)</header>
    <body>I reported it on Dec 19th. RADAR 4891009 -- it was marked as a &amp;quot;duplicate&amp;quot;, so clearly I'm not the only one with this issue. Thanks, Mark -- ======================================================================= Mark Coniglio             |  email@hidden TroikaTronix              | Isadora¬Æ                  | |  (interactive media processing environment) =======================================================================</body>
  </mail>
  <mail>
    <header>Re: Memory Leak with CIContext?? (Still Broken)</header>
    <body>On Feb 5, 2007, at 7:35 PM, Mark Coniglio wrote: Your best bet of getting some attention for your issue is to report a bug at bugreport.apple.com.  Include your sample application and a good description of the issue.  Report back here with the radar number and repeat your plea for help.</body>
  </mail>
  <mail>
    <header>Re: More CIImage drawImage woes...</header>
    <body>I had actually thought about this:, and had set the input CIImage to nil prior to the view being disposed, which released the CIImage and left it with nothing to draw. But this still crashed. I was able to solve the problem as follows: create the NSOpenGLView when I need the thumbnail. When I'm no longer need it, hide the view and remove it from the superview, but retain it so that it doesn't get disposed. If I need it again, add it back to the superview and draw. Finally, when the window that contains the thumbnail is disposed, release the NSOpenGLView. This is not crashing. On another note, what about this situation: let's say that the QuickTime movie I'm playing is destined for a window that is not on the same graphics card as the window with the preview. Because the textures will be on the wrong card, trying to display the thumbnail as described above will break, no? (I can't test this on my MacBookPro because it has just one graphics card.) Thanks a lot, Mark -- ======================================================================= Mark Coniglio             |  email@hidden TroikaTronix              | Isadora¬Æ                  | |  (interactive media processing environment) =======================================================================</body>
  </mail>
  <mail>
    <header>Re: More CIImage drawImage woes...</header>
    <body>You have to make sure that the view stays alive as long as you have CIImages and with that CVImageBuffers (in this case OpenGL textures) in flight, as they require the OpenGL context to be valid. AutoreleasePools can be quite a bit dangerous in those cases.</body>
  </mail>
  <mail>
    <header>Create OpenGL Texture from CIImage</header>
    <body>I'd like to render the output of a CIImage using OpenGL, so that I can control the transparency more easily (see previous post on that) and also have glColor() affect the colorization of the image, etc., etc. I've seen some references here about creating a PBuffer context and rendering to that. Is there sample code on how to do this? Best Wishes, Mark -- ======================================================================= Mark Coniglio             |  email@hidden TroikaTronix              | Isadora¬Æ                  | |  (interactive media processing environment) =======================================================================</body>
  </mail>
  <mail>
    <header>More CIImage drawImage woes...</header>
    <body>I am playing converting frames from a QuickTime movie into CIImage using initWithCVImageBuffer. These frames are fed to one or more CIImage filters, and then rendered to the main output window. This all works fine. Now I'm trying to draw a thumbnail of one of the CIImage filters to my user interface window. Because of the seemingly unsurmountable memory leak bug when rendering to a CIImage to a bitmap, I switched my approach to attempt to render into a small NSOpenGLContext which I create on the fly, and the create a CIContext from that. (All of my code for this is based on the VideoView.m code in the Video Viewer example -- see The drawing works fine in both the preview and main window -- but as soon as I dispose the view, the following crash occurs when the AutoReleasePool pops: Thread 0 Crashed: 0   &amp;lt;&amp;lt;00000000&amp;gt;&amp;gt; 	0x00000000 0 + 0 1   com.apple.CoreFoundation       	0x9080a50c _CFRelease + 188 2   com.apple.CoreFoundation       	0x9080dcda __CFArrayReleaseValues + 451 3   com.apple.CoreFoundation       	0x9080a50c _CFRelease + 188 4   com.apple.QuartzCore 0x93f84381 CVImageBacking::prepareToRemoveFromPool() + 33 5   com.apple.QuartzCore 0x93edf3bb CVBufferBacking::releaseUsage() + 49 6   com.apple.QuartzCore 0x93edf36b CVOpenGLTexture::finalize() + 25 7   com.apple.CoreFoundation       	0x9080a50c _CFRelease + 188 8   com.apple.CoreFoundation 0x908213d7 __CFDictionaryDeallocate + 460 9   com.apple.CoreFoundation       	0x9080a50c _CFRelease + 188 10  com.apple.QuartzCore           	0x93ec556b release_provider + 90 11  com.apple.QuartzCore           	0x93ec547b tiled_bitmap_finalize + 159 12  com.apple.QuartzCore           	0x93ebab45 fe_release + 414 13  com.apple.QuartzCore           	0x93ec52f8 image_buffer_finalize + 124 14  com.apple.QuartzCore           	0x93ebab45 fe_release + 414 15  com.apple.Foundation           	0x925d9a0f NSPopAutoreleasePool + 530 16  com.troikatronix.isadora 0x00049deb CAutoReleasePool::~CAutoReleasePool [in-charge]() + 45 (CAutoReleasePool.mm:24) ... Is this because the main output window and the user interface window are two different Open GL contexts? (Even though they are on the same screen?) Do I need to use a shared context for the main output window and the preview window? Really, I'd be happy to use the bitmap solution if I could get it working. But anything that would allow me to show the thumbnail would be great. Thanks in Advance, Mark -- ======================================================================= Mark Coniglio             |  email@hidden TroikaTronix              | Isadora¬Æ                  | |  (interactive media processing environment) =======================================================================</body>
  </mail>
  <mail>
    <header>Re: Memory Leak with CIContext?? (Still Broken)</header>
    <body>Re: Memory Leak with CIContext?? (Still Broken) I'm trying this and getting a massive memory leak. This bug can be demonstrated in the Apple sample</body>
  </mail>
  <mail>
    <header>CGDisplayCapture and display sleep</header>
    <body>Can anyone confirm that CGDisplayCapture prevents a display from sleeping? I'm guessing that this is the case as I'm entering full screen mode (full screen gl context) and my &amp;quot;pmset -a force displaysleep 1&amp;quot; doesn't work when I'm in fullscreen mode. Additionally using the sys prefs UI to sleep the display after 1 min of inactivity doesn't appear to work either. All is fine if I'm not in full screen mode. Kind regards, Christopher</body>
  </mail>
  <mail>
    <header>ImageIO and read-only Exif tags</header>
    <body>Hi all, Is there a list of Exif tags that can be successfully modified in, or added to, an image file using CGImageDestination APIs? It seems to me that some Exif tags, notably the GPS family of tags, are treated as read-only by the ImageIO framework, i.e., they are properly read from existing images, but cannot be saved.  However, this doesn't seem to be documented anywhere. -- marco -- It's not the data universe only, it's human conversation. They want to turn it into a one-way flow that they have entirely monetized. I look at the collective human mind as a kind of ecosystem. They want to clear cut it. They want to go into the rainforest of human thought and mow the thing down.</body>
  </mail>
  <mail>
    <header>Re: Simple question about parameter sizes</header>
    <body>Didn't see this mentioned on that page, but in gcc if an enum has a value that causes it to not fit in an int, then the enum becomes a long, so 8 bytes. That of course is a bad thing from a binary compatibility standpoint. Ali On Jan 31, 2007, at 3:24 , Wim Lewis wrote:</body>
  </mail>
  <mail>
    <header>Re: Caps lock in event taps doesn't send flags changed upon release</header>
    <body>On 1/31/07, NX_NUMERICPADMASK does not mean what you think it means.&amp;nbsp;&amp;nbsp;It indicates the keystrokes originate from the numeric keypad area of a keyboard, and does not reflect the state of the Num Lock switch. Arrow keys are considered part of the numeric keypad area. This flag is present to allow applications to differentiate between key characters from the main keyboard matrix and ientical characters from the numeric keypad matrix, such as the number keys and math</body>
  </mail>
  <mail>
    <header>Re: PDF - Drawing pages with 1-bit images is all black - default	background colour?</header>
    <body>Thanks for confirming that.  I guess it's upto the application to decide the background colour, and for most 'document' viewers that would be white. --Simon</body>
  </mail>
  <mail>
    <header>Re: PDF - Drawing pages with 1-bit images is all black - default	background colour?</header>
    <body>Re: On Mar 31, 2006, at 8:35 AM, Simon Liu wrote: There is no notion of an initial background color in PDF documents. If there are portions of the drawing that are clear then whatever content the clear portions cover will continue to show through. The first page of the PDF document you reference contains image masks which by definition have a portion of them that are clear or partially transparent.</body>
  </mail>
  <mail>
    <header>PDF - Drawing pages with 1-bit images is all black - default	background colour?</header>
    <body>Using CGContextDrawPDFPage on some PDFs renders pages mostly black. Example PDF Link [336kb] To display correctly, I need to first fill the drawing context with white (or some other colour).  I notice the example code in CGPDFVIewer does this.  Is this the default background colour for PDFs?  Or is the default colour stored in the PDF? I've used Voyeur to examine the PDF and haven't found anything to suggest background colour.  I did notice that many pages contain 1-bit images encoded with /CCITTFaxDecode filter. Any pointers appreciated.  Thanks. Regards, Simon</body>
  </mail>
  <mail>
    <header>Best Method to force lazy evaluation to render now</header>
    <body>I do several operations in my app that require saving/restoring parts of the image. Typically undo/redo and pasting brush strokes with blending modes. In order to force rendering I use the method below. I just render onto a 16x16 CGContext. Is this the code below a correct/efficient solution? Any other way to do this? It seems to work pretty well here. Just wondering... -(void) forceRender:(CIImage*) cimg dirtyRect:(CGRect) dRect if (_miniTempBitmap==nil) _miniTempBitmap = [[[NSBitmapImageRep alloc] autorelease] initWithBitmapDataPlanes:NULL pixelsWide:width pixelsHigh:height bitsPerSample:8 samplesPerPixel:4 hasAlpha:YES isPlanar:NO colorSpaceName:NSDeviceRGBColorSpace CGContextRef cgContext = [[NSGraphicsContext CIContext *coreContext = [CIContext contextWithCGContext:cgContext //We can be sure the Image is rendered //ONLY when drawing to the context Attachment:</body>
  </mail>
  <mail>
    <header>Re: Thumbnail NSImage from CIImage?</header>
    <body>The timing of this post is very interesting as I'm tackling a similar issue. I'm not sure that I can do this kind of thing with built-in Mac OS technologies, but I'm presently pursuing a path with QuickTime. I have an image of about 500MB. In short I believe that I have to use a file format like JPEG2000 or MrSID. These formats allow you to specify a &amp;quot;Region Of Interest&amp;quot; in a file and an output resolution. These codecs are built to handle large files. MrSID is a bit of an issue for me at the moment as LizardTech's SDK requires GCC3, whereas we're using GCC4 of course (I could build a small library to factor out the LizardTech code I suppose). 1. Set up a graphics importer for a JPEG2000 file 2. Set the importer's source rect to my region of interest 3. Set the importer's destination rect to my opengl viewport's dimensions 4. Create a graphics exporter for a raw RGB format (I'm going to ultimately make a GL texture of it after some more filtering) 5. Associate the exporter with the importer from (1) 6. Do export to a memory handle CIImage though is great for playing with images in VRAM. There's no encoding/decoding in this technology though (I think) so QT really should be what you're looking at any how. Cheers, -C</body>
  </mail>
  <mail>
    <header>I need a programmer for some quartz coding</header>
    <body>I have an app which displays images. I need someone who can do some coding to add some new features to the code. It's probably a relatively small job.</body>
  </mail>
  <mail>
    <header>Re: Thumbnail NSImage from CIImage?</header>
    <body>Unfortunately we don't have any sample code showing this so you will have a bit of a learning curve ahead of you. Dealing with images of the size you mention is not easy under any performance consideration and definitely stretches the envelope of the CI to NSImage API. A lot of apps dealing with high resolution images actually have seperate threads or processes dealing with thumbnail generation for display purposes. Each of them solving the problem in a different way depending on the constrains and requirements under which they have been conceived. So the only suggestion I can give you is to investigate different routes like creating thumbnails from your source images or reading back rendering results from screen through pbuffers.</body>
  </mail>
  <mail>
    <header>Re: Thumbnail NSImage from CIImage?</header>
    <body>I'm not conversant with OpenGL, which is what I understand is needed to go the NSOpenGLView route. I used NSImageViews just for implementation simplicity, but I'm open to other avenues. You said, &amp;quot;You could render your image scaled into a pbuffer and read it from there.&amp;quot; Could you point out a reference or two for this approach? This sounds like it would involve the least amount of rearchitecting. On Mar 29, 2006, at 1:20 PM, Frank Doepke wrote:</body>
  </mail>
  <mail>
    <header>CIFilter and CALayer</header>
    <body>I'm using a custom CIFilter to add a distortion effect to a layer-backed view but I'm seeing some strange behaviour. As I resize the filter's inputImage extent doesn't always correlate to the layer's bounds. Specifically, as I make the view smaller the input image extent stays the same until the view gets larger again. This means my filter doesn't render correctly because it has a dependency on the size of the input image (via samplerSize). I tried playing around with this in Quartz Composer and I found that turning on Show Advanced Input Sample Options makes the problem go away. But I can't get the same behaviour in code by using the sampler settings that appear when you turn this setting on. Can anyone help with this or explain what is going on? I guess my next step is to try to rewrite filter to not be dependent on the sampler size. Thanks in advance, Dion</body>
  </mail>
  <mail>
    <header>CGDisplayRegisterReconfigurationCallback with event taps?</header>
    <body>I'm using mouse/keyboard event taps in a console c++ application. I need to detect when the display configuration changes. I've been trying to follow this: But I can't seem to get it to work. When I change the resolution on my desktop I don't recieve any notification. I'm pretty sure there's a little more to it than the example provides. Luke</body>
  </mail>
  <mail>
    <header>Re: Inverse of circle clipping?</header>
    <body>Add a circle and a rectangle that encloses the area you want to draw. Then use EOClip to clip, and draw your content. If you don't have a natural bounds for the rectangle, you can use CGContextGetClipBounds() to get a suitable bounding box. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Inverse of circle clipping?</header>
    <body>Apple's PathDemo shows you how to clip to a circle.  Is it possible to do the inverse and clip to everywhere but the circle? thanks Jeff</body>
  </mail>
  <mail>
    <header>Mouse event taps, and hiding the mouse cursor globally</header>
    <body>I've written a console app that blocks the mouse movement events and prints the delta X and delta Y of the mouse cursor to the console. At first when I returned NULL in my call back I thought it wasn't working because the mouse was still moving... but I found that it was working, just not as expected. When my application was active applications/objects did not respond to the mouse going over it, like the Dock, close button, etc... This is actually perfect because I can block the mouse click events and it's almost as if the cursor wasn't there. There is just one thing missing... I need to be able to hide the mouse cursor itself globally. Here is my code so far: CGEventRef mouseEventCallback(CGEventTapProxy proxy, CGEventType type, CGEventRef event, void *refcon) if (type != kCGEventMouseMoved) int deltaX = (int) CGEventGetIntegerValueField(event, int deltaY = (int) CGEventGetIntegerValueField(event, int main(void) mouseEventTap = CGEventTapCreate(kCGHIDEventTap, kCGHeadInsertEventTap, 0, (1 &amp;lt;&amp;lt; kCGEventMouseMoved), mouseEventCallback, if(!mouseEventTap) mouseRunloop = CFMachPortCreateRunLoopSource(kCFAllocatorDefault, CFRunLoopAddSource(CFRunLoopGetCurrent(), mouseRunloop, (Any other comments on how to improve this code would also be appreciated, like properly handling closing the application, etc...) For those wondering what I'm trying to do: I'm writing an application like Synergy2 or Teleport that will allow me to remotely control another computer. When the mouse leaves the screen it must disappear as if it went from one monitor to another. Hence the question of trying to hide the mouse cursor. This application has no window what so ever. I need to be able to do this no matter what application is active, and without affecting the active application. This program completely runs in the background. Another thing I'm not quite clear on: When the application gets to &amp;quot;CFRunLoopRun();&amp;quot; how do I do &amp;quot;other things&amp;quot; once it gets to that point? What is the &amp;quot;correct way&amp;quot; without interrupting the taps? And how do I correctly terminate the application (cleanly)? Do I use atexit perhaps? I'd like to make this compatible with 10.4 and above. If I have to, 10.5 and above. Luke</body>
  </mail>
  <mail>
    <header>How does one find if an HIGroupBox is &amp;quot;Primary&amp;quot;?</header>
    <body>One attribute of an HIGroupBox is that it is &amp;quot;Primary&amp;quot; or &amp;quot;Secondary&amp;quot;. When &amp;quot;HIObjectPrintDebugInfo(myGroupBox)&amp;quot; is called, it correctly prints out either &amp;quot;Primary   : Yes&amp;quot; or &amp;quot;Primary  : No&amp;quot;.  However, there does not appear to be a defined constant or field reference defined in Scott -----</body>
  </mail>
  <mail>
    <header>Can't set alpha to zero in CGImage</header>
    <body>I'm sorry to ask such a basic question, but I'm a newbie to CGImage.  I'm using Xcode 1.5 on Mac OS X 10.3.9 to make an Objective-C, doc app. My Objective I have a RGB bitmap with no alpha.  I make a new CGImage with alpha, and copy all the RGB colors making one of them transparent. The Problem My problem is in assigning zero to the byte  that holds the alpha value for the color I want to make transparent (ie 255, 255, 254).  The alpha value should be at *(p2 + 3), and I use *(p2 + 3) in a loop to assign the alpha value.  However, when I write . Two Questions When you pre-multiply the RGB values by a zero alpha, the values go to zero.  So why isn't the pixel black? What value can I use to assign 0 to the alpha component at *(p2 + 3)?Here's part of the code: *(p2 + *(p2 + Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Solved - Re: A Tail of Two Bitmaps</header>
    <body>It happens to *everybody*, no matter how many years you've been typing code. You should've seen how long it took us to find this a bunch of years back: Yes, the order is src-dst (not that incredibly goofy C dst-src order:). Nothing like changing the value of 0 and whatever else is after that constant string in the TOC. :) _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Solved - Re: A Tail of Two Bitmaps</header>
    <body>The structure I was using to initialize the dataProvider had a bug where the size of the allocated block was never set. Yes, embarrassed am I.</body>
  </mail>
  <mail>
    <header>Re: A Tail of Two Bitmaps</header>
    <body>If alphaInfo is kCGImageAlphaPremultipliedFirst and I try to draw the resulting image into a rectangle that is smaller than the image, I get junk. If alphaInfo is kCGImageAlphaNoneSkipLast, I can draw into a smaller rectangle without problem. csRenderBitmap::csRenderBitmap( csCount width, csCount height ) try //   kCGImageAlphaNoneSkipLast //::fprintf( csLog::Out(), //					 &amp;quot;  alphaInfo        = %d\n&amp;quot;, //					 this-&amp;gt;fWidth, this-&amp;gt;fHeight, bitsPerComponent, bitsPerPixel, bytesPerRow, alphaInfo //	uses malloc to allocate the block CGContextRef	context = ::CGBitmapContextCreate( this-&amp;gt;fImageData.pointer(), this-&amp;gt;fWidth, this-&amp;gt;fHeight, bitsPerComponent, //kCGImageAlphaNoneSkipLast kCGImageAlphaPremultipliedLast kCGImageAlphaPremultipliedFirst catch( const csException&amp;amp; ex ) this-&amp;gt;setContext( null, false );		// just to be sure. CGImageRef csRenderBitmap::createImage() if( null == this-&amp;gt;getContext() ) if( null != this-&amp;gt;fImage ) try provider		= ::CGDataProviderCreateWithData( null, size_t bitsPerComponent = ::CGBitmapContextGetBitsPerComponent( size_t bitsPerPixel = ::CGBitmapContextGetBitsPerPixel( size_t bytesPerRow = ::CGBitmapContextGetBytesPerRow( CGImageAlphaInfo alphaInfo = ::CGBitmapContextGetAlphaInfo( //::fprintf( csLog::Out(), //					 &amp;quot;  alphaInfo        = %d\n&amp;quot;, //					 width, height, bitsPerComponent, bitsPerPixel, bytesPerRow, alphaInfo this-&amp;gt;fImage = ::CGImageCreate(	width, height, bitsPerComponent, bitsPerPixel, bytesPerRow, ::CGBitmapContextGetColorSpace( this-&amp;gt;getContext() ), alphaInfo, provider, null, true, kCGRenderingIntentDefault catch( const csException&amp;amp; ex )</body>
  </mail>
  <mail>
    <header>Re: A Tail of Two Bitmaps</header>
    <body>On May 31, 2005, at 3:03 PM, Mark Morrill wrote: Have you verified that the values returned by CGBitmapContextGet... are returning the same values you used to create the context?  (seems they should be... but maybe there is a bug there?) It might help if you went ahead and expanded ...basic info... and ...basic info from context... to actual code. Where is the image buffer comming from?  Is it a buffer you have created yourself (e.g. with malloc) or is it potentially a QuickDraw GWorld.</body>
  </mail>
  <mail>
    <header>Re: A Tail of Two Bitmaps</header>
    <body>Further information... When using kCGImageAlphaPremultipliedFirst, the image is garbage when it is scaled down. Mark Hi, I'm wanting to create a CGImageRef from a CGBitmapContextRef - which is pretty straight forward. However, I'm having a bit of a problem... I want to use an alpha channel but I'm only getting good results by not using an alpha channel. Basically I'm doing this: CGImageRef	image	= ::CGImageCreate(	... basic info from context ..., dataProvider, null, true, The &amp;quot;basic info&amp;quot; is block of data that assumes 32 bits per pixel. The colorspace is CGColorSpaceCreateDeviceRGB The &amp;quot;basic info from context&amp;quot; are the CGBitmapContextGet... calls. The dataProvider is created with the block of data used to create the context. When theAlphaInfo is kCGImageAlphaPremultipliedFirst (or last), drawing the image into a context with a size other than the image's size will sometimes produce garbage on the screen, sometimes it will produce results that suggest that the row bytes is wrong. When theAlphaInfo is kCGImageAlphaNoneSkipLast (32bits per pixel), the drawing lacks the alpha channel but it always draws right. Thoughts? I'm still using 10.3</body>
  </mail>
  <mail>
    <header>A Tail of Two Bitmaps</header>
    <body>Hi, I'm wanting to create a CGImageRef from a CGBitmapContextRef - which is pretty straight forward. However, I'm having a bit of a problem... I want to use an alpha channel but I'm only getting good results by not using an alpha channel. Basically I'm doing this: CGImageRef	image	= ::CGImageCreate(	... basic info from context ..., dataProvider, null, true, The &amp;quot;basic info&amp;quot; is block of data that assumes 32 bits per pixel. The colorspace is CGColorSpaceCreateDeviceRGB The &amp;quot;basic info from context&amp;quot; are the CGBitmapContextGet... calls. The dataProvider is created with the block of data used to create the context. When theAlphaInfo is kCGImageAlphaPremultipliedFirst (or last), drawing the image into a context with a size other than the image's size will sometimes produce garbage on the screen, sometimes it will produce results that suggest that the row bytes is wrong. When theAlphaInfo is kCGImageAlphaNoneSkipLast (32bits per pixel), the drawing lacks the alpha channel but it always draws right. Thoughts? I'm still using 10.3 Mark _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Which PDF version is supported in Tiger?</header>
    <body>Tiger supports PDF version 1.6. If you find a PDF that does not display correctly, then please file a bug report.</body>
  </mail>
  <mail>
    <header>Which PDF version is supported in Tiger?</header>
    <body>Are CGContextDrawPDFPage() and co. in CGOntext.h supposed to work with a particular version of PDF? If yes then which one? Particularly which PDF version is supported in Tiger? The header files seem not to include such an information. Thanks JJ ___________________________________________________________ Yahoo! Messenger - NEW crystal clear PC to PC calling worldwide with voicemail</body>
  </mail>
  <mail>
    <header>CVPixelBufferCreateWithBytes returning -6680</header>
    <body>Hi, I'm currently trying to convert  a sequence grabber based on the old sgdataproc example to use the newer quartz/quicktime model. As there currently does not seems to exist much conceptual documentation, i have delve into it, head first. But already my first attempt to create a CVPixelbuffer fails: CVPixelBufferCreateWithBytes (NULL, config.xsize, config.ysize, k32RGBAPixelFormat,  //assuming I get this with a raw codec in the sequence grabber, but since there is no length parameter, the memory should be interpreted as such a buffer. callBackData, config.xsize * 4, // 8 bit *4 NULL, //no callback NULL, //callback reference NULL, //Pixelbuffer attributes First question: Is this function the equivalent to NewGWorldFromPtr, that I assume it to be ? Second question: What the heck does the error code 6680 mean. It is not listed in the CoreVideo result codes yet... Third question: Do the parameters look reasonable to you ? Fourth question: Is there example code I might have missed ? Thanks, Martin</body>
  </mail>
  <mail>
    <header>Re: Core Image and CICrop</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Using Quartz to draw in a Custom Column</header>
    <body>Hi I have created a custom column and I am trying to use Quartz to do the drawing. I have created a context from the current port and I understand about the co-ord issues between Q and QD. However I am unable to get my coord transformation to work correctly. The port appears to be that of the column, so the context's x co-ord matches the columns' (everything is aligned correctly on the left and right).  However the y coords appear too low (in keeping with the coord mismatch having the origin on the bottom left rather than top left) and the drawing starts from the bottom and proceeds upwards. My transformation uses the height of the port to push the y coord up but it doesn't get it pushed far enough, on opening the custom column 'cells' are two cells below where they should be. When I open a container the custom cells progress up the screen and out of the top of the databrowser view. All suggesting the origin needs to be flipped, but I am unclear on to proceed. Aidan</body>
  </mail>
  <mail>
    <header>Re: Serious performance issue when windows overlap with Quartz Composer or OpenGL windows</header>
    <body>My guess was that it was related to some of the discussion here, but I felt it would be good to report the issue from a more &amp;quot;user&amp;quot; than &amp;quot;expert&amp;quot; perspective. After all, I am experiencing theses issues with Apple apps (Quartz Composer and Finder) and not just with my own apps so it is not something where I can do things right or wrong or take a particular side in the heated debate. To me as a user and as a developer of apps that sometimes have to be placed above someone else's OpenGL window I am very much concerned with this huge performance hit. That is while I filed the bug too. david.</body>
  </mail>
  <mail>
    <header>Re: Serious performance issue when windows overlap with Quartz	Composer or OpenGL windows</header>
    <body>This is an issue on Tiger when a window overlaps an OpenGL context. It appears to be a bug in the beam sync that Tiger enables by default, because if you disable beam sync in Quartz Debug everything works fine again. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Serious performance issue when windows overlap with Quartz Composer or OpenGL windows</header>
    <body>I have been noticing that my G5 DP 2.5 Ghz slows down to extremes when I have a Quartz Composer Viewer window open and another window partly overlaps it. The Finder feels like 10.0 on a G3 266Mhz (or actually worse) in such cases. I also see it with certain game windows, for example, when Kill Monty () is used in windowed mode. Even when the game is not being played (it is showing the high scores page)  and a Finder window partly overlaps my machine slows down. On 10.3.8 doing the exact same has no negative impact on speed. I have filed a bug rdar://4133576 including Shark profiles, Spin Control detected hangs and a short screen movie. Note that I did not fiddle with any settings this is a clean 10.4 install updated to 10.4.1 david.</body>
  </mail>
  <mail>
    <header>Final rendering</header>
    <body>Hello, Please any one can clarify the point : Whether final rendering on the screen is being done be the quartz2D, quartz Compositor or graphcis kext. If quartz/quartz Compositor is performing the final rendering then what is the role of Graphics kext on the system .</body>
  </mail>
  <mail>
    <header>Re: Core Image performance</header>
    <body>On May 23, 2005, at 7:20 PM, David Rokeby wrote: I've had similar results on high-end hardware.  I have some effects code that runs in Altivec through vImage and I reimplemented it in CoreImage.  I've found that the CI solution runs at about half the speed of the vImage solution on a G5-2.5GHz/DP with a GeForce 6800 Ultra card while using more CPU time. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Re: Core Image performance</header>
    <body>On May 23, 2005, at 9:04 PM, Darrin Cardani wrote: Read :</body>
  </mail>
  <mail>
    <header>Re: Core Image performance</header>
    <body>You need to use an accelerated context to get hardware acceleration, I think. I believe the docs describe how to get such a context. My understanding is that the 5200 is a real dog and that the CPU is actually faster than the 5200. (That's what I have also, unfortunately.) Darrin -- Darrin Cardani - email@hidden President, Buena Software, Inc. Video, Image and Audio Processing Development</body>
  </mail>
  <mail>
    <header>Core Image performance</header>
    <body>I have been exploring Core Image recently and have been a little surprised at the slowness of it. I am working on a 2 x 2 gig G5 with nvidia GeForce fx 5200 and a 1 gHz power book with nvidia GeForce fx 5200 go. System profiler tells me that my video cards support CoreImage. I have been poking around with OpenGL profiler and as far as I can tell, little or nothing is being accelerated by the GPU. No programs are listed under resources and there are no calls that appear to be program related. The speed appears to be what I would expect from floating point RGB based Altivec code. My own altivec code is faster pretty much across the board. I have tried encapsulating a very simple addition kernel in an Image Unit without interface to force it to be nonexecutable, but I am still getting the same performance. I am only making the simplest texture access so it is not because of a disallowed texture call. I am also not seeing the kind of advantage I was expecting from the aggregation of filters into a single program. The second filter doubles the load of the first filter, where I was expecting the second filter to be much less of a drain as it should have been combined with the first, reducing various unnecessary processing steps (which also indicates to me that everything is being rendered by the CPU. Are there other factors being factored into decisions about whether to use the GPU or CPU? Am I missing some setting that enables GPU processing? Was I just expecting too much out of the FX5200?</body>
  </mail>
  <mail>
    <header>Re: Transitioning from QuickDraw to Quartz</header>
    <body>On May 23, 2005, at 3:44 PM, Scott Fraser wrote: If you, and the rest of the list will forgive shameless self- promotion, you might also look at: Quartz 2D and QuickDraw have very, very different drawing models. You certainly can't do a call-by-call translation.  Depending on what you are drawing, you may have to do more, or less work.  Can you give us some idea of what kinds of things you are having to draw? They're really separate &amp;quot;steps&amp;quot;.  AppleEvents is quite different from Carbon Events and the two different event models can, and should coexist in some applications.  Carbon Events tends to operate &amp;quot;inside&amp;quot; an application while AppleEvents are sent between applications. As for Quartz and Carbon Events, there is no inherit dependence between these two technologies.  You can adopt them independently of one another. Carbon proposes that you replace your use of the old-style Control Manager with the more-up-to-date HIView mechanism.  If you do that then Apple recommends that you replace your QuickDraw code with Quartz because the Quartz drawing will play nicely with the HIView compositing view mode.  However, even in that case the use of the HIView model and Quartz are not tightly coupled.  You can use HIView with QuickDraw as well. Unequivocably, Yes.  You will find folks on this list that can help you convert QuickDraw code to Quartz code. Unfortunately, this is not the right place to ask about transitioning to Carbon Events or adopting HIView.  For those tasks you would probably be better off asking on Carbon-Dev. That depends on what you are trying to replace.  If you have a custom control, I would say work at the HIView level.  If you are trying to change an entire window and don't want to change your event model, then you can use things like QDBeginCGContext/QDEndCGContext to replace things at the granularity of a window's update event. As a general rule, I would expect to use QDBeginCGContext/ QDEndCGContext a lot, unless you are planning to replace controls with HIViews. What are you planning to do with the event model?  Are you interested in switching to Carbon Events?  Are you interested in adopting HIView?  Each of these questions can have some impact on the order in which you do your conversion. As a first step, you might start with the &amp;quot;Update Event&amp;quot; handling and use QDBeginCGContext/QDEndCGContext to just change the way you draw the contents of a window. Yes.  WaitNextEvent is implemented under the hood using the Carbon Event Manager.   Whenever you call WaitNextEvent, the computer tries to handle as many Carbon Events as there are event handlers. WaitNextEvent will only return if it runs into a Carbon Event that is unhandled, and which it can convert back into an old-style EventRecord.  I believe that WaitNextEvent will also return if its sleep time expires.  Under the WaitNextEvent  model that would return a NULL event.  However, one of the first tasks for any WaitNextEvent application that wants to play on Mac OS X as a well-behaved citizen would be to replace the null even processing with Carbon Event timers so that your machine can spend most of it's time blocked.</body>
  </mail>
  <mail>
    <header>Re: Transitioning from QuickDraw to Quartz</header>
    <body>On May 23, 2005, at 1:44 PM, Scott Fraser wrote: I'm in the same boat and have yet to make the transition.  The last couple of WWDCs had a session on this very topic, and will have one again at WWDC this year.  If you don't have a chance to go to WWDC then I would try to buy a set of DVDs from last year's WWDC.  I see them every now and then on ebay.  Just search for WWDC. How difficult it is to port from QuickDraw depends on how much of QuickDraw you are using and especially how much interaction there is with your graphics.  For example if you are just rendering some graphics, say like a chart or graph, and don't require user interaction your task may not be to difficult.  However, if you are doing interactive graphics it will be a lot more difficult.  QuickDraw has an XOR mode that will let you complement graphics on and off.  Quartz has no such thing.  In Quartz you must paint the entire image if you wish to remove something.  This is countered by Quartz now having overlays. So if you are trying to interactively place something graphic you would do so by drawing it on an overlay, which can be cleared and redrawn at the new location as the object moves. For a particular window which you are drawing graphics into, you will one to replace all of it and one time.   If you are talking about replacing each of the dialogs with Carbon/Nib based windows you will also want to do the complete (individual) dialog at one time.  We have 150 dialogs and about 80 of them have been converted to Nib based windows.  Because WNE can exist with Carbon based windows you don't need to do all of them at the same time.  However, you would not want to have a older style dialog and try to replace just a few controls on it.</body>
  </mail>
  <mail>
    <header>Re: Transitioning from QuickDraw to Quartz</header>
    <body>It would be to your advantage to also be on the Carbon-Dev list. Things like the HIView model, while likely to be understood by many of us here, is not the topic of this list but are essential to properly moving to Quartz. Generally CGContexts will be given to you, but at what level you get them depends on how you transition. If you do a quick and dirty transition, you will get them at the window level. If you do a more thorough conversion you will get them at the Control/View level. None that I know of, but I'm sure someone has some favorite method that they have used in the past... Yes, you can mix and match carbon event handlers and WNE, although this is more a topic for Carbon-Dev. The grand prize for that work is when you get rid of WNE all together and use RunApplicationEventLoop () instead. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Transitioning from QuickDraw to Quartz</header>
    <body>Hello, I'm new to this list (and OS X programming in general) but it seemed the place to start.  I hope you can help. My employer, a small 10 person company, has an application that was ported from OS 9 CodeWarrior C++ to OS X Carbon Xcode.  I've been tasked to transition the app from the existing QuickDraw drawing environment to Quartz.  The Apple documentation describes the Quartz environment well enough.  The documents of interest appear to be: &amp;quot;Upgrading to the Mac OS X HIToolbox&amp;quot;		Upgrading_HIToolbox.pdf &amp;quot;Quartz Primer&amp;quot;							QuartzPrimer.pdf &amp;quot;Quartz 2D Programming Guide&amp;quot;			drawingwithquartz2d.pdf Management believes that a simple one-to-one replacement of QuickDraw calls with Quartz calls is sufficient.  &amp;quot;Just encapsulate all the QuickDraw calls in a class object, and then replace the QuickDraw calls one at a time.&amp;quot;  However, the documentation suggests that this is not only not desirable, but not even possible.  Most of the documentation assumes that one replaces the entire QuickDraw and Apple Event model, with a nib-based Carbon Event model.  However, management doesn't want to take that big of a bite all at once. 0.	Is this the right list to ask about transitioning from QuickDraw? 1.	At what level of granularity does it make sense to replace QuickDraw calls?  Each window?  Each control?  Each region/path?  The need to obtain and release a CGContext suggests a larger granularity, like a window. 3.	Can Carbon event handlers coexist with QuickDraw WaitNextEvent model? Any suggestions would be greatly appreciated. Scott Fraser -----</body>
  </mail>
  <mail>
    <header>Re: Rendering HTML into CGBitmapContextRef</header>
    <body>This is, by in large, true.  You can easily create a GWorld and a CGBitmapContext that both share the same memory. For the sake of posterity, however, I should mention that it is also pretty easy to use QuickTime to create a GWorld that doesn't work with QuickDraw!  For example, you can use QuickTime to create a GWorld that is CMYK (for importing CMYK images), but you cannot draw into that GWorld using QuickDraw. Even if you create an ARGB offscreen pixel map that is shared by a GWorld and a CGBitmapContext drawing into it with QuickDraw may not be what you want.  In particular, QuickDraw treats things as XRGB, not ARGB and you may run into trouble when QuickDraw sets the &amp;quot;X&amp;quot; to 255.</body>
  </mail>
  <mail>
    <header>Re: Rendering HTML into CGBitmapContextRef</header>
    <body>On May 20, 2005, at 1:25 PM, Scott Thompson wrote: As I understand it, an Offscreen GWorld and a CGBitmapContextRef are analogous, and there is no reason that they cannot both point to the same underlying memory.  Use QTNewGWorldFromPtr to create your GWorld and give it the same memory as your CGBitmapContextRef.  The fact that Quartz and QuickDraw use different locations for (0,0) may be slightly problematic. Gen</body>
  </mail>
  <mail>
    <header>Re: Coalesced Updates &amp;amp; QuartzDebug</header>
    <body>That reminds me: I haven't upgraded yet and thus can't check, but is there a way in Tiger to get the refresh rate for an LCD, rather than assuming 0 means 60 Hz? I remember long and vicious arguments about this on mac-games-dev before Panther was released, but what it boils down to is: even if LCD updating doesn't work the same way as CRT updating, it would be useful to be able to get the optimum update rate for a given device without ever having to assume an arbitrary 60 Hz. -- Jens Ayton</body>
  </mail>
  <mail>
    <header>Re: Rendering HTML into CGBitmapContextRef</header>
    <body>On 20-May-05, at 16:04, Scott Thompson wrote: small note overlooked unread, hours of scratching head at last, time for bed</body>
  </mail>
  <mail>
    <header>Re: Rendering HTML into CGBitmapContextRef</header>
    <body>On May 20, 2005, at 4:08 PM, Mark Morrill wrote: WebView has no particular dependence on Tiger.  IIRC WebView should work back to 10.2.8 or 10.2.9 with Safari installed. If you want sample code that uses this technique... look in the CarbonWeb sample code (I have mine at /Developer/Examples/WebKit/ CarbonWeb).  The code in TWebWindow uses this technique to print HTML pages through Quartz 2D. Scott</body>
  </mail>
  <mail>
    <header>Re: Rendering HTML into CGBitmapContextRef</header>
    <body>Hmm... yet another goodie to compel me shell out for Tiger ;) I so need to get it. Yes. I hang my head in shame for not already having it. Okay. I'm not going to use the old stuff, but going with WebView, keeping an eye on the Tiger.</body>
  </mail>
  <mail>
    <header>Re: Coalesced Updates &amp;amp; QuartzDebug</header>
    <body>Looks like Eric pointed you to it already. We're hoping to document the feature in an upcoming Technical Q&amp;amp;A, but I'd though I'd add some information that may be useful: The three options are : 1) Disable Beam Synchronization This disables all beam syncing in the system, including the Coalesced Update feature. So you will see visual tearing, but increased performance because your drawing calls will no longer block waiting for the previous flush to complete. Use this mode for benchmark testing to see what the total throughput of your code is (disregarding the the display flushing).  There is currently no way to disable just the Coalesced Update feature only. 2) Automatic Beam Synchronization This is the default for Tiger. In this mode, beam syncing is enabled for window geometry changes (also true in Panther) and the Coalesced Update feature is also enabled. The latter is new to Tiger and will result in the window server coalescing display updates across applications into a single update to the display waiting for a vertical blank of the display. This is tied to the display refresh rate and is also applicable to LCD monitors (which really update at 60Hz). This results in better system performance, but can have side- effects to applications that attempt to flush faster than the refresh of the display. Note that only mach-o applications linked on Tiger will initiate coalesced updates. CFM and apps built pre-Tiger will not initiate a coalesced update, but if another application has initiated a coalesced update for a vbl, then the original app's update will also be coalesced. 3) Force Beam Synchronization This will force coalescing of updates across all applications. Developers can use this to check the performance impact of the feature without waiting to link their application on Tiger. Right now, the first time the app tries to flush faster than the display refresh rate or ends up spending more that 50% of it's time waiting for the vbl will have a message logged to /var/log/windowserver.log. The message is of the format : &amp;quot;Application &amp;quot;SuchAndSuch&amp;quot; is being throttled by update coalescing.&amp;quot; This message will only be output once per application. Developers should monitor this log to check to see if their application is being impacted by coalesced updates or not.</body>
  </mail>
  <mail>
    <header>Re: Rendering HTML into CGBitmapContextRef</header>
    <body>On May 20, 2005, at 1:25 PM, Scott Thompson wrote: In Tiger, you can use the poorly named method + (NSGraphicsContext *)graphicsContextWithGraphicsPort:(void *) to create and draw into an NSGraphicsContext backed by an arbitrary CGContextRef.  This avoids the step of creating PDF just to re-render to your bitmap context.  See NSGraphicsContext.h for more details.</body>
  </mail>
  <mail>
    <header>Re: Rendering HTML into CGBitmapContextRef</header>
    <body>On May 20, 2005, at 1:42 PM, Mark Morrill wrote: WebView is still your best bet. HTML Rendering Library is very out of date and is no longer being actively maintained. It's there for compatibility with existing apps only.</body>
  </mail>
  <mail>
    <header>Re: Rendering HTML into CGBitmapContextRef</header>
    <body>I thought of that. But the part of the code that wants to do the rendering is already based on Quartz and is suppose to be cross platform... I want to avoid too much Cocoa. BitmapContext -&amp;gt; image. But, I'm also worried that the HTML Rendering Library might be out of date. On 20-May-05, at 14:25, Scott Thompson wrote:</body>
  </mail>
  <mail>
    <header>Re: Rendering HTML into CGBitmapContextRef</header>
    <body>On May 20, 2005, at 1:06 PM, Mark Morrill wrote: Your best best is probably to use a WebView.  All NSViews implement dataWithPDFInsideRect: which can return a PDF document for whatever is drawn in that view.  You could then draw that PDF into your BitmapContextRef.</body>
  </mail>
  <mail>
    <header>Rendering HTML into CGBitmapContextRef</header>
    <body>I've looked and I've come up with nothing... yet. Often, the answer will appear after posting to a list :) For example, after starting writing this post, I found &amp;quot;HTML Rendering Library&amp;quot;. This looks like it will let me render into a offscreen gworld but, I'd rather skip that step. What I want to do is to take a bit of html and render it into a CGBitmapContextRef (or CGContextRef for that matter.)</body>
  </mail>
  <mail>
    <header>Re: Filter the drawing contents</header>
    <body>Quartz does not provide any bottleneck support like QuickDraw. Maybe there are other ways to accomplish what you are doing.</body>
  </mail>
  <mail>
    <header>Filter the drawing contents</header>
    <body>Hello, I want to develop a filter that can filter all the drawing calls ie all of the drawing calls that  are being gnerated  should go through our filter. While I was going through the Quartz help  I figured out functions like CopyBits etc that are drawing functions. What I have assumed is that drawing calls that are generated by the application , will call some of the APIs of quartz library. So        we can trap those calls and filter the contents of the buffer and further pass the buffer to the lower layer. eg If I can trap the call of CopyBits that is generated from OS , modify the source buffer which is one of the parameter  and then further pass the call further  on  ... Whether the above task is possible.  IF yes then where to place our filter. Any sample code etc can help us .</body>
  </mail>
  <mail>
    <header>Re: User Spaces vs. Generic Spaces</header>
    <body>It depends on your point of view. For some of us, it is the only thing of interest :-) OK. That sounds like a reasonable approach. Of course the misuse of DeviceXXX has been going on since PostScript was invented, and UnCalibratedXXX sounds like another round in the shell game of hiding Device colorspace, and then adding another path to get at the _real_ device colorspace. That sounds like it will work.</body>
  </mail>
  <mail>
    <header>Re: CGImageRef to CIImage -options dictionary -single channel images</header>
    <body>So what combination do I use for a single channel, floating point image.  According to the docs: Colorspace Gray 128 bpp, 32 bpc, kCGImageAlphaNone|kCGBitmapFloatComponents This would indicate that the in memory data needs to be laid out in an RGBA fashion even though it is greyscale in nature (only by the absence of color) right?  32(bpc) * 4(channels) = 128(bitsperPixel) My greyscale images need to be 32 bitPerPixel, 32 bitsPerChannel but I guess that there is no way to create a CGBitmapContext with just that data.  Is it correct then that I would need to &amp;quot;pad&amp;quot; out the memory buffer to fulfill the 128bpp/32bpc requirement? -- ---- Bruce Johnson email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGShading in PDFs</header>
    <body>Sorry for the inconvenience, Derek</body>
  </mail>
  <mail>
    <header>Re: Re: Re: CGImageRef to CIImage -options dictionary -single	channel images</header>
    <body>That QA document is out-of-date now. Review the following... -Shawn</body>
  </mail>
  <mail>
    <header>Re: Re: CGImageRef to CIImage -options dictionary -single channel	images</header>
    <body>/* Pixel formats. */ Thank you for the link, the list indicates that the CGContextRef formats take 8 bitsPerComponet.  My float data is 32 bitsPerComponets *and* one of the flags for CGBitmapContextCreate is kCGBitmapFloatComponents. Does anyone know what CGBitmapContextCreate is doing? The data buffer that I give it is sized to fit an image of type float. So what I should be able to do for a single channel image is: void *outputData = calloc(bytesPerRow, height); //sized for float component 32 bitsPerComponet CGColorSpaceRef colorSpaceRef = CGContextRef outputCGImageContext = CGBitmapContextCreate(outputData, width, height, bitsPerComponent, bytesPerRow, colorSpaceRef, kCGBitmapByteOrder32Host | kCGImageAlphaNone | -- ---- Bruce Johnson email@hidden</body>
  </mail>
  <mail>
    <header>CGShading in PDFs</header>
    <body>I've found that when rendering a CGShading to a PDF that the sampling of gradient function is extremely low, often in some cases missing out entire bands of colors. Other than rasterising the shading does anyone know of a solution? thanks -- Will</body>
  </mail>
  <mail>
    <header>Help: error rendering to OGL EXT_framebuffer_object (MacBook)</header>
    <body>I'm having trouble rendering offscreen to a OGL texture attached to an OGL frame buffer object. My code is based upon the sample code provided on &amp;quot;OpenGL Programming Guide for Mac OS X&amp;quot;, chapter &amp;quot;Drawing Offscreen: rendering to a Framebuffer Object&amp;quot;. The call to glCheckFramebufferStatusEXT is returning GL_FRAMEBUFFER_INCOMPLETE_ATTACHMENT_EXT but I can't find the reason for the error. Some preparations done... GLuint				_outputTexture // this is the texture attached to the framebuffer: the render target! ... - (void)processFrame //do some drawing preparations if (_needsReshape) if (NSIsEmptyRect([self visibleRect])) else glViewport(0, 0, (GLsizei)(frame.size.width), (GLsizei) glOrtho(NSMinX(bounds), NSMaxX(bounds), NSMinY(bounds), NSMaxY //setup &amp;amp; upload the input texture glTexImage2D(GL_TEXTURE_RECTANGLE_EXT, 0, GL_RGBA8, _width, _height, //set the framebuffer as the rendering target, attached to the output texture glTexImage2D(GL_TEXTURE_RECTANGLE_EXT, 0, GL_RGBA, _width, _height, glFramebufferTexture2DEXT(GL_FRAMEBUFFER_EXT, if (status != GL_FRAMEBUFFER_COMPLETE_EXT) NSLog(@&amp;quot;&amp;gt;ERROR WITH //render to the framebuffer + texture //...some rendering code that user the input texture //set the screen as rendering target //render to the screen //...some rendering code that user the output texture</body>
  </mail>
  <mail>
    <header>CGImageRef to CIImage -options dictionary -single channel images</header>
    <body>I'm trying to get a handle on what the options dictionary needs to look like when calling: + (CIImage *)imageWithCGImage:(CGImageRef)image options:(NSDictionary *)d the docs state that the dict can specify either a colorspace, pixel format or both. (I guess) For colorspace I assume that the dictionary would look like [....dictionaryWithObjectsAndKeys: (a CGColorSpaceRef) whose key is kCIImageColorSpace] But the CIFormat constants are integers.  I assume that I would wrap those up in an NSNumber and set that in the dictionary as the object. I what the key would be that CIImage would recognize isn't readily apparent. What would the options dictionary look like if I want to specify a CIFormat of: kCIFormatRGBAf I need to loft an image created and stored in memory to a CIImage. The image in question is a single channel, greyscale image of type float. I get problems when I'm creating the CGContextRef from which I'll use to create my CIContext for eventual offscreen drawing.  I keep getting errors (during runtime) in my CGBitmapContextCreate stating that the various combinations of pixel format types that I'm trying aren't compatible. The only thing that I've gotten to work is sending a 4 channel color image of type float using: CGColorSpaceRef colorSpaceRef = CGContextRef outputCGImageContext = CGBitmapContextCreate(outputData, width, height, bitsPerComponent, bytesPerRow, colorSpaceRef, kCGBitmapByteOrder32Host | kCGImageAlphaNoneSkipLast When I try: CGColorSpaceRef colorSpaceRef = CGContextRef outputCGImageContext = CGBitmapContextCreate(outputData, width, height, bitsPerComponent, bytesPerRow, colorSpaceRef, kCGBitmapByteOrder32Host | kCGImageAlphaOnly (single channel ??) Right now I'm assuming that CIImage will only work on ARGB or RGBA images regardless. as always, thanks in advance. -- ---- Bruce Johnson email@hidden</body>
  </mail>
  <mail>
    <header>Re: CVDisplayLinkStop causes deadlock</header>
    <body>Great, thanks.  I've got it all working, now.</body>
  </mail>
  <mail>
    <header>Re: CVDisplayLinkStop causes deadlock</header>
    <body>The way how you go about these things is that you create a lock for the OpenGL context. This lock should not be held by anything that is unrelated to the OpenGL rendering (like stopping the display link).</body>
  </mail>
  <mail>
    <header>Re: CVDisplayLinkStop causes deadlock</header>
    <body>But I need the lock to ensure only 1 thread accesses the OpenGL context.  The lock is blocked in the callback because the main thread is messing with OpenGL.  I've only got 1 lock, so it should not get deadlocked.  Are you suggesting that CV calls must not be included in the OpenGL lock?  That's what seems to be the fix, but it is not apparent from the Core Video documentation.  Essentially, CVDisplayLinkStop() blocks until the callback is complete.  I can imagine an implementation of CVDisplayLinkStop() where this is not the case.</body>
  </mail>
  <mail>
    <header>Re: CVDisplayLinkStop causes deadlock</header>
    <body>The&amp;nbsp;display&amp;nbsp;link&amp;nbsp;will&amp;nbsp;only&amp;nbsp;stop&amp;nbsp;once&amp;nbsp;the&amp;nbsp;callback&amp;nbsp;has&amp;nbsp;exited&amp;nbsp;which&amp;nbsp;you&amp;nbsp;are&amp;nbsp;blocking&amp;nbsp;with&amp;nbsp;your&amp;nbsp;lock.</body>
  </mail>
  <mail>
    <header>CVDisplayLinkStop causes deadlock</header>
    <body>I'm writing a custom OpenGL view that uses the Core Video display link to drive animation, rather than a timer.  I have an NSRecursiveLock I use to make sure all CV and OpenGL calls are serialized.  I also want the view to support full screen, so when the view changes from windowed mode to full screen, I go through the following steps: * Lock display lock (an NSRecursiveLock) * Stop display link * Switch to full screen OpenGL context, and make full screen * Start display link * Unlock display lock I'm getting deadlocks when I try to stop the display link, in step #2.  Here are the backtraces of the two deadlocked threads during the deadlock: #0  0x9002c2e8 in semaphore_wait_signal_trap () #1  0x90030dcc in pthread_cond_wait () #2  0x92943bc0 in -[NSRecursiveLock lock] () #3  0x00002e50 in -[DDCustomOpenGLView openGLContext] #4  0x000037d0 in -[DDCustomOpenGLView currentOpenGLContext] #5  0x00003e54 in -[DDCustomOpenGLView(Private) drawFrameInternal] #6  0x00003c04 in myCVDisplayLinkOutputCallback ( #7  0x94279db0 in CVDisplayLink::performIO () #8  0x9427d0bc in CVDisplayLink::runIOThread () #9  0x9002bc28 in _pthread_body () #0  0x9002c2e8 in semaphore_wait_signal_trap () #1  0x90001b30 in pthread_mutex_lock () #2  0x94279fcc in CVDisplayLink::stop () #3  0x00003fe0 in -[DDCustomOpenGLView(Private) enterFullScreen #4  0x000036ac in -[DDCustomOpenGLView setFullScreen:] #5  0x00005cf4 in -[MyController setFullScreen:] [ ... 20 more AppKit frames ... ] It seems CV already has it's own lock, which it locks for the duration of the CV callback, and is causing a circular wait condition.  Since I didn't see this documented in the CV reference, should I not use my own lock around CVDisplayLinkStop()?  That would seem to break the circular lock dependency.  Or am I doing something else fundamentally wrong here?</body>
  </mail>
  <mail>
    <header>Re: Obtaining the CGContextRef</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>CGEventCreateFromData: line 1858 Internal inconsistency</header>
    <body>I want to record a series of CGEvent's, store them to a file then play them back later. I tried to use the CGEventCreateData() / CGEventCreateFromData() pair of functions to serialize/deserialize my events. But I got strange behaviours. , where event is a CGEventRef received by an eventTap callBack function Moreover the event created by CGEventCreateFromData contains incorrect data, but the original event is correct. Antoine Baudoux</body>
  </mail>
  <mail>
    <header>pixel data from CGImageRef</header>
    <body>I have a CGImageRef that was obtained via CGDataProviderCreateWithData and CGImageCreate (not via a CGBitmapContext) Further along in the program I would like to access the raw pixel data of the CGImageRef without digging for the original data buffers used to create the CGImageRef. I know that I can get the data from a CGContext via CGBitmapContextGetData but I don't see an equivalent function for CGImageRef. The only thing that I came up with,  was: (I think from CocoaDev) CGImageDestinationRef destCG = CGImageDestinationCreateWithData((CFMutableDataRef)imageData, but this is either assuming or forcing the data to conform to kUTTypeTIFF when that isn't necessarily the case. Any other thoughts? -- ---- Bruce Johnson email@hidden</body>
  </mail>
  <mail>
    <header>Re: If CIImage is immutable...</header>
    <body>Yes, use CIAffineTransform, or as a learning exercise, you could write your own kernel: kernel vec4 flipH( sampler image )</body>
  </mail>
  <mail>
    <header>Re: NSImage to CVPixelBufferRef problems</header>
    <body>I would also be interested in knowing why my function does not work. I think I have followed the suggestions from other mails (except for the TIFFRepresentation).</body>
  </mail>
  <mail>
    <header>NSImage to CVPixelBufferRef problems</header>
    <body>I've been searching through the archive and I have found many mails rearding the subject issue. They have helped me in getting the function I send, the problem is that it doesn't seems to work. The idea is to draw an NSImage in a CGBitmapContext. I have used CreateCGImageFromData(NSData* data) from the technical note (http:// developer.apple.com/technotes/tn2005/tn2143.html), and passing the TIFFRepresentation from NSImage to it. After I do that, it seems to get BytesPerRow and BitsPerComponent right, but when I use it with Quartz Composer I don't see anything. I have tried to write random data to the BaseAddress of PixelBuffer and I see a something (obviously bad) in my composition, so I guess this is a conversion from NSImage to PixelBuffer instead of a QC thing. - (CVPixelBufferRef) pixelBufferFromNSImage:(NSImage*) theImg CVReturn result = CVPixelBufferCreate (NULL, [theImg size].width, [theImg size].height, k32ARGBPixelFormat, NULL, if (result == kCVReturnSuccess) NSRect imageRect = NSMakeRect(0.0, 0.0, imgSize.width, CGImageSourceRef sourceRef = CGImageSourceCreateWithData ((CFDataRef) [theImg TIFFRepresentation], if (sourceRef) cgImage = CGImageSourceCreateImageAtIndex (sourceRef, 0, NSLog (@&amp;quot;Width: %f - Height: %f&amp;quot;, imgSize.width, NSLog (@&amp;quot;Bits per component: %d&amp;quot;, NSLog (@&amp;quot;Bytes per row: %d&amp;quot;, CGImageGetBytesPerRow CGContextRef bitmapContext = CGBitmapContextCreate (data, imgSize.width, imgSize.height, CGImageGetBitsPerComponent (cgImage), CGImageGetBytesPerRow (cgImage), CGImageGetColorSpace (cgImage), CGContextDrawImage (bitmapContext, *(CGRect *)</body>
  </mail>
  <mail>
    <header>Creating synthetic drag operations</header>
    <body>I'm trying to programmatically generate a drag operation. Ideally I would like to be able to: - Control drags directly (mouse down, mouse drags, mouse up from code) - Toggle mouse down status, so that movements of the hardware mouse can generate mouse drags. I've been using the CGRemoteOperation.h API (the older version, I need 10.3.9 support) to generate mouse clicks and keyboard events. That all works. I'm having trouble with the mouse drags. In the code below I register to track the mouse movements and follow the mouse movements of the hardware mouse with synthetic mouse drag events (This is needed, because the mouse buttons of the real and the virtual mouse don't seem to be 'or-ed' for dragging). I experience the following problems: - selection rectangle doesn't disappear on mouse up. - when I'm dragging files, the calling dragToggle doesn't end the drag operation. - really only 'works' (somewhat) in the Finder. One additional complication is that my program is controlled from a keyboard. The user presses a device that is recognized by the system as a keyboard. My program registers a hotkey to respond to that user action. In the hotkey handler I generate the mouse drag events... Is there a chance that this setup could work, or is there a conflict between hotkey and CGRemoteOperation.h? I've also read about IOHIDPostEvent, but I've been unable to locate that function on ADC or in IOKit. code: ----- //  mouse tracking support static void stopMouseTracking(void) static OSStatus mouseCallback(EventHandlerCallRef inHandlerCallRef, EventRef inEvent, void *isSynthetic) //  a mousedown from the hardware mouse: end drag operation //... *(BOOL *)isSynthetic = NO;  //  Ignore synthetic events. //  generate a slightly offset drag event to //  make the screen reflect the drag operation static void startMouseTracking(void) //  register for mouse events InstallEventHandler(GetEventMonitorTarget(), handlerUPP, GetEventTypeCount(kEvents), kEvents, &amp;amp;synthetic, void dragToggle(void) // Permit all local events during the interval after we post an event CGSetLocalEventsFilterDuringSuppressionState (kCGEventFilterMaskPermitAllEvents, // Permit all local events during a remote control mouse drag CGSetLocalEventsFilterDuringSuppressionState (kCGEventFilterMaskPermitAllEvents, //  mouse down/up CGPostMouseEvent(getCurrentMousePosition(), false, 1, ! //  track the mouse if this is the start of a drag, void moveMouse(CGPoint delta) //  new postion // Permit all local events during the interval after we post an event CGSetLocalEventsFilterDuringSuppressionState (kCGEventFilterMaskPermitAllEvents, // Permit all local events during a remote control mouse drag CGSetLocalEventsFilterDuringSuppressionState (kCGEventFilterMaskPermitAllEvents, //  move mouse</body>
  </mail>
  <mail>
    <header>Extremly poor Color Space conversion in Quartz compared to lcms</header>
    <body>I am still thinking that I must be doing something wrong, but I am getting very disappointing results when trying to convert a CMYK image to RGB and then back to CMYK. - Input image is in EuropeISOCoatedFOGRA27 CMYK color space. - Create a CGBitmapContext with ECI-RGB Color Space - draw the input image in this context - create an CGImageRef from that context - create a new CGBitmapContext with EuropeISOCoatedFOGRA27 color space - draw the rgb image in that context - create an CGImageRef from that context The result is pretty bad. Either I am doing something seriously wrong here or it's because of Quartz lack of black point compensation. - Input image is in EuropeISOCoatedFOGRA27 CMYK color space. - create a CGBitmapContext with EuropeISOCoatedFOGRA27 color space - draw the image in this context - Create a CGBitmapContext with ECI-RGB Color Space - create a lcms transform from EuropeISOCoatedFOGRA27 to  ECI-RGB - convert the bitmap buffer from the EuropeISOCoatedFOGRA27 context using the lcms  transform and write the result into the ECI-RGB context's bitmap buffer - do the same thing the other way round Dominik</body>
  </mail>
  <mail>
    <header>Core Video + CIImage + CIContext drawing</header>
    <body>I am not sure if this is the right list, so please let me know if I need to subscribe to a different list... I have an application that uses quicktime movies for mixing and displaying images into openGL views.  Basically a VJ application. Currently, here is what I am doing (roughly): Track A has a dedicated preview view associated with it. Track B has a dedicated preview view associated with it. There is a small preview mix view in the main window also. There is a full-screen mix view on a secondary display. All of the &amp;quot;preview&amp;quot; and &amp;quot;mix&amp;quot; views are of the same class.  They use callbacks to get the CIImage to be displayed.  This all works fine. Each view has its own OpenGL context (sharing a &amp;quot;main&amp;quot; context) and use a shared pixel format. When I create the QT movies (via QTOpenGLTextureContextCreate and NewMovieFromProperties) I use the shared &amp;quot;main&amp;quot; context for both movies (If I do not I get some massive memory leaks, even when I use a new context that shares the &amp;quot;main&amp;quot; context). A single (currently, I have tried each channel having it's own display link too) display link is called that renders both tracks and then does the mixing and outputting on the display. Everything works fine as far as the rendering goes.  But there is a small (~1M per 15 seconds) memory leak in the CIContext drawImage method. Here is what I have been able to trace down.  Here is a psuedo code of the display link render method: If I move my &amp;quot;perform mixing&amp;quot; and &amp;quot;mix/fullscreen display&amp;quot; methods before channelB renders, the memory leak goes away.  As best I can tell, because the two channels are using the exact same openGL &amp;quot;main shared&amp;quot; context, when a second render happens it screws up some of the texture mapping and confuses CIContext's drawImage so that it thinks it needs to create a new texture backing object (which it never frees).  As I said, when I create two separate openGL contexts for the video link channels to use (but still have them share the main shared context) the leak gets MUCH worse, like in the 2M/sec range. I would assume there is a way to use CV and display links to render more than 1 quicktime file and then be able to display (wether mixed or not) whichever one you want into a single openGL context on screen.  Is there something I am missing or is it just not possible to do what I want to do? My only other solution (which is a bit of a pain for down-the-road use) is to somehow draw each channel at a specific transparency in all 3 views before I render the second channel.  But that seems like such a hack for something that (as I understand it) should work. Daniel</body>
  </mail>
  <mail>
    <header>Re: Howto determine color space from an Image</header>
    <body>I think what you have should be sufficient then. Recall you can use the same color space with multiple images/bitmaps, some with alpha and some without. If the color space knew about alphas too, then this would require them to modified whenever they are used with an image or bitmap context that used a different alpha variant than the color space knew about. This isn't the case however, all such contexts share the color space directly without modification. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Howto determine color space from an Image</header>
    <body>On 12.10.2006, at 14:33, David Duncan wrote: I need to do some manual color space translation in case the image is cmyk. Yep, sorry forgot this in my code, My understanding is that if I have an RGB image with an alpha channel (RGBA or ARGB) that CGColorSpaceGetNumberOfComponents will return 4 and not 3. But that was just an assumption.</body>
  </mail>
  <mail>
    <header>Re: Howto determine color space from an Image</header>
    <body>Do you need to know just to inform a user, or for some other reason? You don't need to check for Alpha here, Color spaces don't deal with alpha values themselves. I'm not certain what bitmapInfo is here (I suspect it's a CGBitmapInfo value, but you don't show that) but it should be unnecessary as well. But note that you can also get num==1 for Grayscale and num==3 for RGB or L*a*b* color spaces. Differentiating RGB from L*a*b* with the current APIs could be difficult at best. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Howto determine color space from an Image</header>
    <body>I need to find out what color space an images uses. I have not found a good solution for it. --&amp;gt; CMYK --&amp;gt; RGB This is somewhat lame and probably not accurate in all cases. Are there any better options?</body>
  </mail>
  <mail>
    <header>Re: problem with disabling antialiasing?</header>
    <body>My observation has been that Quartz seems to do the same pixel coverage calculations regardless of whether antialiasing is enabled or disabled.  However, when antialiasing is disabled Quartz has the highly annoying behavior of fully covering any pixel which has coverage &amp;gt; 0 (even if it's incredibly small).  This is completely incompatible with every other graphics library we use and has caused me nothing but headaches since I added Quartz drawing support a few years ago. A good example of this problem is drawing 2 transparent polygons which share a common edge.  Any sane non-antialiasing graphics library would cover each pixel along the edge for either one polygon or the other.  Quartz (with antialiasing disabled) covers each of the pixels along the edge for *both* polygons, resulting in a thick dark line between the two.  Here's an example: import os from CoreGraphics import * ctx = CGBitmapContextCreateWithColor(100, 100, CGColorSpaceCreateDeviceRGB(), (0, 0, 0, 0)) ctx.setShouldAntialias(0) ctx.setRGBFillColor(1, 0, 0, .5) ctx.moveToPoint(0, 0) ctx.addLineToPoint(70, 0) ctx.addLineToPoint(30, 100) ctx.addLineToPoint(0, 100) ctx.closePath() ctx.fillPath() ctx.setRGBFillColor(0, 0, 1, .5) ctx.moveToPoint(100, 100) ctx.addLineToPoint(100, 0) ctx.addLineToPoint(70, 0) ctx.addLineToPoint(30, 100) ctx.closePath() ctx.fillPath() ctx.writeToFile(&amp;quot;/tmp/a.png&amp;quot;, kCGImageFormatPNG) os.popen(&amp;quot;/usr/bin/open -a Preview /tmp/a.png&amp;quot;) Rob Raguet-Schofield (rob ra gA skO fEld)</body>
  </mail>
  <mail>
    <header>Re: problem with disabling antialiasing?</header>
    <body>That's what I figured.. that's very unfortunate.  Even more strangely, a line from (20,20.5) to (20,20.5) (that is, from and to the same point), yields no output with antialiasing enabled (as I would expect), but yields a single pixel with it disabled.  The rounding error seems pretty significant, because even in the one point wide line case, the resulting area of coverage should cover one pixel exactly; there should be nothing to cause rounding issues in adjacent pixels. - Vlad</body>
  </mail>
  <mail>
    <header>Re: problem with disabling antialiasing?</header>
    <body>On Oct 10, 2006, at 11:17 PM, Jens Ayton wrote: In the case of Quartz 2D he is drawing a 1 point wide line starting at the left edge of the point square to the right edge of the point square with the line centered in the point square vertically. This should map to one pixel on the display device assuming identify transform (and standard origin) between user space and device space.... at least in theory based on how Quartz 2D defines it coordinate space (the center of the point square is the center of the point not one of its corners). Likely with antialiasing off some artifact of the math (floating point) used in the line render is resulting in an extra pixel to be painted ... with antialiasing enabled the amount of bleed over into the adjacent pixel with likely not trigger much if any of that pixel to be colored. -Shawn</body>
  </mail>
  <mail>
    <header>Re: problem with disabling antialiasing?</header>
    <body>Vladimir Vukicevic: [...] A pixel is a point sample. A one px long line which is one px wide is a one px square. Your co-ordinates position this square so that it precisely intersects two pixels: *     *     *     * +-----+ |     | *     *     *     * |     | +-----+ *     *     *     * -- Jens Ayton Sed quis custodiet ipsos custodes?</body>
  </mail>
  <mail>
    <header>problem with disabling antialiasing?</header>
    <body>ends up drawing a 1-pixel red line for the first stroke, but as soon as antialiasing is disabled for the blue path, that becomes a 2-pixel line, covering pixels 20 and 21.  This seems bogus, as a 1-width line from 20,20.5 -&amp;gt; 21,20.5 with CGLineCapButt should exactly cover 1 pixel, no? Thanks, - Vlad</body>
  </mail>
  <mail>
    <header>Re: CGImageSource and NULL CFImage</header>
    <body>On Oct 7, 2006, at 11:08 AM, Alexander Cohen wrote: CGImageSource doesn't handle PDF data. It only handles bitmap images. You want to use CGPDFDocument.</body>
  </mail>
  <mail>
    <header>Re: InvertRect (was Everything except...)</header>
    <body>Hot damn! Thanks, Apple. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>InvertRect (was Everything except...)</header>
    <body>Under 10.4 you can use blend modes to do invert. Check out: Ross.</body>
  </mail>
  <mail>
    <header>Re: Draw CGImage to CVImageBuffer</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Everything except (was Re: Help with smooth animation)</header>
    <body>On Oct 9, 2006, at 9:12 AM, Steve Mills wrote: Well, I think the reason is that calls like InvertRect and XOR drawing are device-specific operations (i.e. they only make sense for bitmap devices... those with pixels) while Quartz 2D is a device independent drawing library. I've liked the idea of a secondary library, not necessarily Quartz 2D, that can manipulate pixel maps and such when you know you are working with a bitmap device. I suspect that you are expected to implement things like XOR drawing and invert rect calls with Core Image filters, but that seems a bit heavy handed. Scott</body>
  </mail>
  <mail>
    <header>Re: QDBeginCGContext and flipping coordinates</header>
    <body>You basically do the same thing. Here's my routine for this: CGRect TransformContextToFlipRect(CGContextRef c, CGRect destBox) Don't forget to save and restore the context state if you plan on drawing other things after drawing the image. And please don't post to multiple mailing lists with the same message. Post to the one that makes sense. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: QDBeginCGContext and flipping coordinates</header>
    <body>On Oct 9, 2006, at 11:08 AM, Alexander Cohen wrote: The same transform will switch the coordinates back from left to right hand coordinates.  However if what you're trying to do is draw an image it's probably easier just to use HIViewDrawCGImage() and let it take care of the coordinates. Nick</body>
  </mail>
  <mail>
    <header>Re: QDBeginCGContext and flipping coordinates</header>
    <body>On 9-Oct-06, at 12:52 PM, Nick Nallick wrote: I just noticed that i asked the wrong question. Once that is done, all images i draw are updside down. I know Quartz draws images along the y axis and the y axis is down now so the images is coming out as it should. But what transforms do i need to apply so the image is draw the other way around, like i want it to come out. Alex</body>
  </mail>
  <mail>
    <header>Re: QDBeginCGContext and flipping coordinates</header>
    <body>On Oct 9, 2006, at 10:23 AM, Alexander Cohen wrote: To convert from right-hand coordinates (Quartz) to left-hand coordinates (QuickDraw) you need to know the height of the coordinate space.  The transformation is: Nick</body>
  </mail>
  <mail>
    <header>QDBeginCGContext and flipping coordinates</header>
    <body>How would i go about transforming a CGContextRef i got from QDBeginCGContext so that the coordinates are aligned the same way as in QD ( Top-Left and y pointing downward ). Alex</body>
  </mail>
  <mail>
    <header>Colormanagement: FOGRA27 -&amp;gt; Device RGB -&amp;gt; FOGRA27 mess up</header>
    <body>I need to get the following process get to work with the best possible result for maintaining color correctness. I am receiving some images with  a Europe ISO Coated FOGRA27 color space. For processing reasons I need to convert the color space to RGB, do some processing and then convert the color space back to Europe ISO Coated FOGRA27. My process seems to work fine with other color spaces like Generic CMYK or Adobe Photoshop 5 Default CMYK, but completely messes up with Europe ISO Coated FOGRA27. - read an image with Europe ISO Coated FOGRA27 color space - create a CGBitmapContext with Device RGB color space (from the docs this should be the way to go to maintain color correctness for offline rendering) - create a new image from that CGBitmapContext and the original image - do some processing ( in test case do nothing) - create a CGBitmapContext with Europe ISO Coated FOGRA27 color space - create a new image representation from that CGBitmapContext and the processed image - write out the processed image Unfortunately the colors of the processed image not even come close to the original image. I also tried to use the Device CMYK color space for testing purposes, but the results have been similar. I also tried to change the rendering intent, but that did not make any difference at all. I also noticed that Preview.app displays images with Europe ISO Coated FOGRA27 color space quite differently to Adobe Photoshop. So my question is, am I trying to do something which obviously can't work at all, do something wrong or is there something wrong with Quartz color management? Dominik</body>
  </mail>
  <mail>
    <header>Re: Everything except (was Re: Help with smooth animation)</header>
    <body>On Oct 9, 2006, at 8:12 AM, Steve Mills wrote: and bitblt (i.e., CopyBits)</body>
  </mail>
  <mail>
    <header>Everything except (was Re: Help with smooth animation)</header>
    <body>Well, except inverting existing pixels. I sure miss that sometimes, like when I had to add a grouped overlay window to draw handles and other object manipulation things into and all the work that went along with that, just to replace simple calls to InvertRect or xor drawing. I realize that you can't invert/xor with PostScript, but why cripple onscreen drawing by omitting it from Quartz? I've always been curious about that. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: CGImageSource and NULL CFImage</header>
    <body>There was just a question about this a few days ago so you might want to check the list archive.  ImageIO is for bitmap images, but PDF is a vector image format.  You should use the PDFDocument/PDFPage APIs to deal with it.</body>
  </mail>
  <mail>
    <header>Re: &amp;quot;Programming with Quartz&amp;quot; book is out</header>
    <body>Re: On Dec 15, 2005, at 6:17 AM, Nick Nallick wrote: Nick is correct, however the book does mention that Cocoa apps can create EPS data from their NSView drawing and provides references to the related information. This is documented in: ApplicationKit/ObjC_classic/Classes/NSView.html#//apple_ref/occ/instm/ NSView/dataWithEPSInsideRect: Hope this helps, David</body>
  </mail>
  <mail>
    <header>Re: &amp;quot;Programming with Quartz&amp;quot; book is out</header>
    <body>On Dec 15, 2005, at 6:46 AM, Felix Schwarz wrote: I believe it covers the use of EPS in Quartz.  Specifically the use of a CGImageRef with an EPS data provider and the conversion to PDF. Unfortunately Quartz doesn't support EPS data creation. Nick</body>
  </mail>
  <mail>
    <header>Re: &amp;quot;Programming with Quartz&amp;quot; book is out</header>
    <body>Looks like a very cool book! I'll order it as soon as it's available at amazon.de. One question, though: Does &amp;quot;Chapter 17 Supporting PostScript and EPS Data&amp;quot; cover the creation of EPS data or &amp;quot;just&amp;quot; the conversion from (E) PS to PDF data?</body>
  </mail>
  <mail>
    <header>Re: &amp;quot;Programming with Quartz&amp;quot; book is out</header>
    <body>Re: On Dec 14, 2005, at 6:07 AM, Stephen Chu wrote: Glad you are interested. To my knowledge, there isn't something like that available at this time.</body>
  </mail>
  <mail>
    <header>Re: &amp;quot;Programming with Quartz&amp;quot; book is out</header>
    <body>On Dec 13, 2005, at 12:29 PM, David Gelphman wrote: Looks like a great book. Any discount coupon we can use? :) --</body>
  </mail>
  <mail>
    <header>Re: Capturing a screenshot using Quartz - is it the right API  touse?</header>
    <body>I now have a nice grin on my face having managed to get the sample code working very nicely in a simple app (in the wee hours this morning) - so that you for all of the help provided so far. The post mentioned &amp;quot; ... and a fast asynchronous path using the texture read mechanism, which is a bit more complex, but works well for repeated capture operations, such as screencasting or recording a movie from the screen&amp;quot;. This is something I've very interested in pursuing, but I'm really green when it comes to OpenGL.  Should I ask this question on a different list, or can someone here point me at the right functions that I should research? Again, thanks for the help so far. (I'm having fun coding again!) Regards, Neil Clayton</body>
  </mail>
  <mail>
    <header>&amp;quot;Programming with Quartz&amp;quot; book is out</header>
    <body>I'm pleased to say that the book &amp;quot;Programming with Quartz&amp;quot; is now shipping from the publisher, Morgan Kaufmann. Here's a link to information about the book on Morgan Kaufmann's website: My understanding is that other book sellers are also taking orders and will ship them as soon as they have the books in stock.</body>
  </mail>
  <mail>
    <header>Re: Determining usability of CIFilters</header>
    <body>If you don't have an OpenGL view, it is up to CoreImage to create one if it can and it will determine if it runs in software or not. This means, you don't control it. So it is better for you to create your own OpenGL context. I cannot comment on what the Dock is doing.</body>
  </mail>
  <mail>
    <header>graphicsContextWithBitmapImageRep: with flipped view (continued with sample code)</header>
    <body>Hello here is the mail I send a few days ago on cocoa-dev list. This time I add a small example app the illustrate my point I'm trying to capture a view to create a CIImage I want to use with some CoreGraphics filters. This method works fine when I try to capture the all view or if the view is not clipped by a scrollview. But as soon as I try to capture the visible portion of a view that is clipped I don't get every thing I ask for. The top or the bottom is empty but what is drawn is at the right position. My view is flipped so I tried to convert [self visibleRect] but then the position of the elements of my view are wrong and I still get those horizontal lines at the top or the button of the image, depending of where I scroll the view. doawnlod this file to get the code If someone have an idea about this, that would be nice to let me know. regards mat.</body>
  </mail>
  <mail>
    <header>Re: Capturing a screenshot using Quartz - is it the right API  touse?</header>
    <body>Thank you. That looks like it'll be very, very useful. Regards, Neil Clayton</body>
  </mail>
  <mail>
    <header>Re: Determining usability of CIFilters</header>
    <body>First off, this requires having a valid OpenGL context. I can get the current graphics context with NSOpenGLContext, but I have no guarantee that one will actually exist. I don't have an OpenGL view so I'm not creating any. Secondly, I checked and the Dock is not using this method. In fact, the Dock isn't linking to any gl* methods. Any idea what the Dock *is* doing? Because whatever it's doing is what I want to do. -- Kevin Ballard Yahoo! Mac Engineer email@hidden Phone: (408) 349-6693 Cell: (408) 660-5028 Yahoo! ID: k_r_ballard</body>
  </mail>
  <mail>
    <header>Re: Capturing a screenshot using Quartz - is it the right API to	use?</header>
    <body>on 12/12/05 7:00 PM, Neil Clayton at email@hidden wrote: I was interested to see that it used QuickDraw and was wondering if a) it's really fast and b) if not, what might be a faster/better method to get the frames such that I could then use them to create a movie. --- Neil Clayton On 12 Dec 2005, at 23:06, douglas a. welton wrote: On Dec 12, 2005, at 4:05 PM, Neil Clayton wrote: I'm not sure if this is the right place to ask, since I'm just beginning with Mac development (cocoa primarily). I'm interested in capturing a screenshot continuously, in order to create a movie - in much the same was that SnapXPro (Ambrosia) does.? I've looked at the quicktime API's, but they seem to capture only from video devices (from what I can see). I figured that I could probably capture frame by frame, evey 'n' ms, as long as I could get fast access to the screen. Is quartz a reasonable place to start? or can someone tell me if I'm barking up the wrong tree and should be using some other API? If quartz is going to be the fastest way to get frames such that I can add them to a movie - where do I start? The docs seem to indicate ( Any help is appreciated. Thank you. --- Neil Clayton ?_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list? ? ? (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Capturing a screenshot using Quartz - is it the right API to	use?</header>
    <body>I've used glReadPixels for similar purposes. glReadPixels(yourRect.origin.x, yourRect.origin.y, yourRect.size.width, yourRect.size.height, GL_BGRA, GL_UNSIGNED_INT_8_8_8_8_REV, You'll need to flip the lines vertically, as this returns image data with the reverse y direction. The relationship between Rect and NSRect: Rect uses 4 values indicating leftmost, rightmost, topmost, bottommost coordinates, while NSRect has the bottom left coordinate as the origin alongside a size attribute.  From a Rect to an NSRect should mean: NSRect nsrectVersion = NSMakeRect(r.bottom, r.left, r.right - r.left,</body>
  </mail>
  <mail>
    <header>Re: Determining usability of CIFilters</header>
    <body>It would be nice to test, but I don't know if I can. Can you give me pointers on testing for ARB fragment programming capability? Thanks, Kevin -- Kevin Ballard Yahoo! Mac Engineer email@hidden Phone: (408) 349-6693 Cell: (408) 660-5028 Yahoo! ID: k_r_ballard</body>
  </mail>
  <mail>
    <header>Re: Determining usability of CIFilters</header>
    <body>Try creating a hidden window, then run your filter in it and see what frame rate it got, then set flags in your app accordingly. Not sure if this is the BEST way, but is A way.</body>
  </mail>
  <mail>
    <header>Re: Capturing a screenshot using Quartz - is it the right API to	use?</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Determining usability of CIFilters</header>
    <body>There are a number of different factors that come into play: image size, filters being used, etc. The one big difference that you can track is GPU vs. CPU by checking for the ARB fragment programing capability. But to state again, just because a filter runs in software, doesn't mean that it is slow. So you would have to check with your use cases on different hardware. You might be surprised how well filters run in software.</body>
  </mail>
  <mail>
    <header>Re: Determining usability of CIFilters</header>
    <body>So is there any heuristic I can use to determine if the effect will be rendered on the GPU or the CPU? I need it to look good, and so I want to disable it completely if it's not going to be smooth. -- Kevin Ballard Yahoo! Mac Engineer email@hidden Phone: (408) 349-6693 Cell: (408) 660-5028 Yahoo! ID: k_r_ballard</body>
  </mail>
  <mail>
    <header>Re: Determining usability of CIFilters</header>
    <body>Apple chose to disable that for usability reasons I suppose. Running the ripple on low end hardware might drop enough frames to not look good. I know an a Rev A iMac G5 the widgets in Dashboard still ripple, so Apple must have decided the G5 was fast enough to handle it.</body>
  </mail>
  <mail>
    <header>Re: Determining usability of CIFilters</header>
    <body>If that's the case, then how come the Dashboard doesn't ripple on lower-end graphics cards? For example, on my laptop at home it doesn't, but on my desktop it does. That's the distinction I'm going for - I want to be able to detect the same way the Dock does for when to use my effect, with the same requirements that the Dock is using. -- Kevin Ballard Yahoo! Mac Engineer email@hidden Phone: (408) 349-6693 Cell: (408) 660-5028 Yahoo! ID: k_r_ballard</body>
  </mail>
  <mail>
    <header>Re: Determining usability of CIFilters</header>
    <body>If a filter cannot run on the give GPU, CoreImage will resort to rendering in software. There is now way for you to determine, if a filter cannot be run on a GPU beside the general distinction of CoreImage capable graphics hardware (fragment programmable).</body>
  </mail>
  <mail>
    <header>Re: Determining usability of CIFilters</header>
    <body>As far as I'm aware, all filters should be able to render in software mode. I don't think any filters require specific hardware to run.</body>
  </mail>
  <mail>
    <header>Determining usability of CIFilters</header>
    <body>I'm trying to programmatically determine whether a given CIFilter is usable on the current graphics hardware, but I can't find any definitive answer on how to do this. In Quartz Composer I can check for given OpenGL extensions and hardware acceleration, which I imagine is how I'm supposed to do this, except I'm not using Quartz Composer and I don't know what combination of extensions/acceleration is necessary for my filter. Is there any good way to figure this out, or am I going to have to try and find computers with different graphics cards to test with? And even if I do the testing, how am I supposed to detect the extensions/acceleration availability? Thank you, Kevin Ballard -- Kevin Ballard Yahoo! Mac Engineer email@hidden Phone: (408) 349-6693 Cell: (408) 660-5028 Yahoo! ID: k_r_ballard</body>
  </mail>
  <mail>
    <header>Re[2]: QuickTime, Core Video,	OpenGL and rendering frames outside display link callback</header>
    <body>That exactly what I have done now (thanks to Frank yesterday for pointing out my total misunderstanding on how the display link works ;)). It turns out that the flicker was totally unrelated to this anyway (or so it appears now), it was infact a rouge glRotatef in the wrong place at the wrong time :) I feel like such a fool :P I have it all running now, thanks everyone for your help and pointers! Craig FD&amp;gt; Quite the opposite - you need to do your rendering in the FD&amp;gt; callback so that the display link can adjust the framerate, if FD&amp;gt; your rendering takes more than one VBL FD&amp;gt; Frank FD&amp;gt; You should not be doing any rendering inside of the CVDisplayLink FD&amp;gt; callback, I don't think- you should just be updating your FD&amp;gt; CVImageBufferRef from your texture context (QTVisualContextRef).</body>
  </mail>
  <mail>
    <header>Re: QuickTime, Core Video,	OpenGL and rendering frames outside display link callback</header>
    <body>Many thanks for the clarification.  Wasn't sure why this was being done in the example code so I didn't handle things this way. David</body>
  </mail>
  <mail>
    <header>[Image Capture] Problem with TWPT_GRAY</header>
    <body>If I set the PixelType to TWPT_RGB and the bitdepth to 8, I get the image, exactly the same for TWPT_BW and the bitdepth at 1. But when I set the pixel type to TWPT_GRAY with the bitdepth at 8 I don't receive the callback with the image object. -- Jeff GRANG</body>
  </mail>
  <mail>
    <header>Re: QuickTime, Core Video,	OpenGL and rendering frames outside display link callback</header>
    <body>I'm not sure why your video is flickering, but I've also got my code working based on QTCoreVideo101 and the Core Video programming guide. You should not be doing any rendering inside of the CVDisplayLink callback, I don't think- you should just be updating your CVImageBufferRef from your texture context (QTVisualContextRef).  If there's a new frame tell your NSOpenGLView that it needs updating and then somwhere inside your drawRect: method, the following code will give you access to the latest quicktime frame as a gl texture. CVOpenGLTextureGetCleanTexCoords(currentFrame, bottomLeft, Hope this helps, David</body>
  </mail>
  <mail>
    <header>Re[4]: Core Video rendering frames outside display link callback</header>
    <body>Ah... I think I get what you are saying now. The display link will run as fast as it needs to keep up the refresh rate of the monitor (i.e. v-sync), and is in fact unrelated to video... it can be used as my main render thread for the OpenGL scene, and would even be appropriate as a render loop for OpenGL even if I didn't have want to have video? I was working under the assumption that the display link would only make the callback at frames per second of the video.. which I guess was a stupid assumption to make as at no point in time have I told the display link specifically about the video file! Cheers! Craig FD&amp;gt; Are you rendering to the screen? Because in that case the display FD&amp;gt; link will drive you to render as fast as makes sense (when you try to FD&amp;gt; render faster than the refresh rate of the display you just tax your FD&amp;gt; app unnecessarily - a big NO NO). FD&amp;gt; Frank</body>
  </mail>
  <mail>
    <header>Re: Re[2]: Core Video rendering frames outside display link callback</header>
    <body>Are you rendering to the screen? Because in that case the display link will drive you to render as fast as makes sense (when you try to render faster than the refresh rate of the display you just tax your app unnecessarily - a big NO NO).</body>
  </mail>
  <mail>
    <header>Re[2]: Core Video rendering frames outside display link callback</header>
    <body>Hi Frank, Thanks for the reply. Unfortunately, in this instance I can't restrict myself to render just from the display link thread. As I have an OpenGL scene which must be rendered as fast as possible, the video is just being embedded into the scene as a texture assigned to one of many objects within the scene - the other objects need to be updated and rendered independent of the video, in fact sometimes video isn't playing in the scene at all. Since this is the case I guess the display link callback is the wrong way to go about this. I was hoping to use the display link callback to prepare the next frame as an OpenGL texture ready for the main render thread to use. This is what I've been attempting to do, but it gives me this flicker. I have seen an example (QTValuePak) of drawing a QuickTime movie into a GWorld and then using UpdateMovie()/MovieTask() along with glSubTexImage2D to update the frame texture rendering faster than the frame rate of the video, it appears to render as fast as the v-sync of the monitor which is what I'm after. I'm going to attempt this approach instead.. unless of course Core Video can emulate such an approach? Cheers. Craig FD&amp;gt; You need to refactor your code to render just from the display link FD&amp;gt; thread, otherwise you will not get any proper rendering. The whole FD&amp;gt; purpose of the display link is to properly drive the rendering at the FD&amp;gt; appropiate frame rate. FD&amp;gt; Frank</body>
  </mail>
  <mail>
    <header>Re: Core Video rendering frames outside display link callback</header>
    <body>You need to refactor your code to render just from the display link thread, otherwise you will not get any proper rendering. The whole purpose of the display link is to properly drive the rendering at the appropiate frame rate.</body>
  </mail>
  <mail>
    <header>Core Video rendering frames outside display link callback</header>
    <body>Hi, I am attempting to use QuickTime (NewMovieFromProperties(..)) and Core Video to get video frames and then rendering these frames into my main OpenGL scene (i.e. onto a quad along with other OpenGL objects being animated/rendered). I am rendering the main OpenGL scene as fast as possible using the main application thread (while loop with [NSApp nextEventMatchingMask...] and using the display link callback to obtain the video frames. I do have this &amp;quot;working&amp;quot;, based on the documentation and samples on the website, but the video flickers like crazy on playback. Is it a requirement of Core Video to render the CVOpenGLTextureRef video frame inside the display link callback, and only flush the OpenGL buffer on each new video frame? From what I can see all the samples appear to do this? This is how I am using core video, but it gives me the flickering on playback Instance variables of interest.. I call this method in the display link callback (this is how I get me video frames)... - (CVReturn) getFrameFromTime:(const CVTimeStamp*) syncTimeStamp // Autorelease pool for this thread // Check for new frames if(QTVisualContextIsNewImageAvailable(m_qtTextureContext, syncTimeStamp)) if(nextFrameIdx &amp;gt;= 3) // Get the new frame texture // Update index Then in my main render thread, I render the frame like so... - (void) renderFrame . . Use 'currentFrame' to get texture target/id, texture bind, render to quad etc.. . . // Give time to QT for frame rendering Do I need to copy the data from the current CVOpenGLTextureRef (which is just a CV pixel buffer?) and update an OpenGL texture using glTexSubImage2D(..) which I have already created? Is the CVOpenGLTextureRef is being destroyed after its first display, is this why I get the flicker? It seems unlikely, since I check if currentFrame is NULL and it never occurs... Any pointers, samples, documentation etc on how I can get Core Video to render outside the display link callback without the flicker would be greatly appreciated. Thanks. Craig</body>
  </mail>
  <mail>
    <header>Re: CV extract pixel data...</header>
    <body>You can use nil, it would return the current frame of the movie. Is the movie playing or are you using setMovieTime? Also, as you are not using QTKit make sure that you call MoviesTask.</body>
  </mail>
  <mail>
    <header>Re: CV extract pixel data...</header>
    <body>Ok, now I still have the problem that the call to QTVisualContextIsNewImageAvailable indicates that there are no images available. I'll just post my source code here, maybe sbdy could have a short look at it... if(QTVisualContextIsNewImageAvailable(theVisualContext,nil)) ... What could be wrong here?? - Mathias</body>
  </mail>
  <mail>
    <header>Re: CV extract pixel data...</header>
    <body>You are on the right track. If you get a frame from QTVisualContextCopyImageForTime you have to first lock the base address using CVPixelBufferLockBaseAddress. And once you are done with the PixelBuffer, make sure to unlock the base address again. I hope this helps Frank On Dec 6, 2005, at 7:31 AM, Mathias Mueller wrote:</body>
  </mail>
  <mail>
    <header>CV extract pixel data...</header>
    <body>Hello List, I'm trying to extract pixel data (eg bitmap data) from individual frames of a QT Movie. I think I need to use CV, because QTKit and NSImage would be too slow (I've tried it)... So, I wrote some source code that creates a pixel buffer context, creates a new movie from properties, and then for each frame runs QTVisualContextTask, QTVisualContextIsNewImageAvailable, QTVisualContextCopyImageForTime and CVPixelBufferGetBaseAddress. My first question is do I need to pass a timestamp value and does the frame grabbing have to happen in an EventHandler? If so, that might explain why CVPixelBufferGetBaseAddress(CVImageBufferRef) returns NULL. I hope that somebody out there can help me... Thanks in advance, - Mathias</body>
  </mail>
  <mail>
    <header>CIExposureSample weirdness</header>
    <body>The /Developer/Examples/Quartz/Core Image/CIExposureSample code example uses a custom NSView subclass (CIExposureView) to render into. Building the example out of the box, upon init, drawRect: gets invoked once as a result of loading the nib, and once per each value change of the slider. That's expected. However, if I change CIExposureView to inherit from NSImageView or NSOpenGLView, drawRect: gets invoked a second time at startup, once as a result of nib loading but also once from [NSApplication run]. This second invocation occurs while the first invocation is still executing, causing potential multithreading problems. If I then change CIExposureView back to a NSView subclass (as it was originally), it still invokes drawRect: twice at startup. (I'm modifying CIExposureView.h&amp;quot; &amp;quot;@interface CIExposureView: NSView&amp;quot;, and then dragging the modified file from the Xcode browser window into the Interface Builder MainMenu.nib window to update it, then doing a clean + build.) #0	0x0000927c in -[CIExposureView drawRect:] #1	0x936c7628 in -[NSView _drawRect:clip:] #2	0x936c6be8 in -[NSView _recursiveDisplayAllDirtyWithLockFocus:visRect:] #3	0x936c9930 in _recursiveDisplayInRect2 #4	0x9076c954 in CFArrayApplyFunction #5	0x936c6cfc in -[NSView _recursiveDisplayAllDirtyWithLockFocus:visRect:] #6	0x936c9930 in _recursiveDisplayInRect2 #7	0x9076c954 in CFArrayApplyFunction #8	0x936c6cfc in -[NSView _recursiveDisplayAllDirtyWithLockFocus:visRect:] #9	0x936c61b0 in -[NSView _recursiveDisplayRectIfNeededIgnoringOpacity:isVisibleRect:rectIsVisible RectForView:topView:] #10	0x936e6e14 in -[NSThemeFrame _recursiveDisplayRectIfNeededIgnoringOpacity:isVisibleRect:rectIsVisible RectForView:topView:] #11	0x936bfe24 in -[NSView _displayRectIgnoringOpacity:isVisibleRect:rectIsVisibleRectForView:] #12	0x936b5118 in -[NSView displayIfNeeded] #13	0x936b4f88 in -[NSWindow displayIfNeeded] #14	0x936c0478 in -[NSWindow _reallyDoOrderWindow:relativeTo:findKey:forCounter:force:isModal:] #15	0x9369b8bc in -[NSIBObjectData nibInstantiateWithOwner:topLevelObjects:] #16	0x937725a8 in old_loadNib #17	0x93687588 in +[NSBundle(NSNibLoading) _loadNibFile:nameTable:withZone:ownerBundle:] #18	0x936de994 in +[NSBundle(NSNibLoading) loadNibFile:externalNameTable:withZone:] #19	0x9376e80c in +[NSBundle(NSNibLoading) loadNibNamed:owner:] #20	0x9376e5ac in NSApplicationMain #21	0x00009180 in main at main.m:13 Second invocation of drawRect: #0	0x0000927c in -[CIExposureView drawRect:] #1	0x936c7628 in -[NSView _drawRect:clip:] #2	0x936c6be8 in -[NSView _recursiveDisplayAllDirtyWithLockFocus:visRect:] #3	0x936c61b0 in -[NSView _recursiveDisplayRectIfNeededIgnoringOpacity:isVisibleRect:rectIsVisible RectForView:topView:] #4	0x936c6778 in -[NSView _recursiveDisplayRectIfNeededIgnoringOpacity:isVisibleRect:rectIsVisible RectForView:topView:] #5	0x936c6778 in -[NSView _recursiveDisplayRectIfNeededIgnoringOpacity:isVisibleRect:rectIsVisible RectForView:topView:] #6	0x936e6e14 in -[NSThemeFrame _recursiveDisplayRectIfNeededIgnoringOpacity:isVisibleRect:rectIsVisible RectForView:topView:] #7	0x936bfe24 in -[NSView _displayRectIgnoringOpacity:isVisibleRect:rectIsVisibleRectForView:] #8	0x936b5118 in -[NSView displayIfNeeded] #9	0x936b4f88 in -[NSWindow displayIfNeeded] #10	0x936b4e34 in _handleWindowNeedsDisplay #11	0x9075cccc in __CFRunLoopDoObservers #12	0x9075cf6c in __CFRunLoopRun #13	0x9075ca0c in CFRunLoopRunSpecific #14	0x931831e0 in RunCurrentEventLoopInMode #15	0x931827ec in ReceiveNextEventCommon #16	0x931826e0 in BlockUntilNextEventMatchingListInMode #17	0x93681904 in _DPSNextEvent #18	0x936815c8 in -[NSApplication nextEventMatchingMask:untilDate:inMode:dequeue:] #19	0x9367db0c in -[NSApplication run] #20	0x9376e618 in NSApplicationMain #21	0x00009180 in main at main.m:13</body>
  </mail>
  <mail>
    <header>Re: Question about Compositing CIFilters</header>
    <body>Le 5 d√©c. 05 √† 04:52, huaixing zhang a √©crit : [...] &amp;quot;Source Over&amp;quot; is probably more appropriate, actually. (Source Atop will crop your bigger image to the size of the smaller, which I think is not what you want). You should translate one of your images (most likely the smaller one), using the &amp;quot;imageByApplyingTransform:&amp;quot; method of CIImage.</body>
  </mail>
  <mail>
    <header>Question about Compositing CIFilters</header>
    <body>Hi All Since I am new to Core Image, I have a small question about composting two images using Core image. Here I have 2 images one is bigger, another is samller. I wanna take the small one as the foreground and put it on the center of the bigger one, which is considered as background image, using &amp;quot;Source Atop Compositing&amp;quot;. My problem is I don't know how to set the smaller one's position to the center of the bigger one. Should I specify some parameter or do some transformation? Regards Huaixing</body>
  </mail>
  <mail>
    <header>Re: CoreVideo -&amp;gt; CoreImage -&amp;gt; CoreVideo</header>
    <body>You're right! And Core Image does support that. Just make a CIImage from your source texture, set that as the input for a CIFilter, create a CIContext with a destination pbuffer or GLcontext, and draw the output CIImage of the CIFilter into that CIContext. There's no reason to read the results into memory unless you want to examine the pixels with the CPU. Dave Howell Apple Pro Apps Attachment:</body>
  </mail>
  <mail>
    <header>Re: CoreVideo -&amp;gt; CoreImage -&amp;gt; CoreVideo</header>
    <body>Thanks!  Your suggestions were a gigantic huge help over Apple's meager documentation on this issue :) Your description worked perfectly. It seems like there should be some way to apply the result of the Core Image filter operation to a texture without having to use glReadPixels(), which seems to be kinda slow since it's bringing the contents of the context framebuffer into main memory. David</body>
  </mail>
  <mail>
    <header>Overlay window that follows the cursor causing crashes</header>
    <body>A carbon application I'm writing occasionally has a composited, overlay-styled window follow the cursor's movements.  The window contains a single HIView control for drawing and is hidden whenever this isn't happening, and then when its supposed to be following the cursor, it's shown then relocated with MoveWindow each time a kEventMouseMoved event is received.  Pretty simple.  The contents of the window is just some outlined and filled shapes with a transparency layer for a shadow drawn using basic Quartz 2D function calls. The application will run fine for a while, with the window following the cursor, but then after anywhere between 20 minutes and a few hours the program will just crash.  There doesn't seem to be any rhyme or reason as to how long it takes, or what has to happen to cause the crash.  Checking crash reports reveals that the program just crashes somewhere as quartz tries to draw the window, but never consistently in the same place or at a traceable point in my kEventControlDraw handler function.  The only consistent thing is that it crashes right as the window is supposed to be drawn after being shown - I've never observed it crashing while the window was following the cursor.  (This makes sense since it shouldn't redraw the window whenever its moved.) If I comment out all the code in my kEventControlDraw handler function the crashes stop (but of course then nothing is seen following the cursor) so the error *might* be in there somewhere.  Here's the code: OSStatus CursorWindowDraw(EventHandlerCallRef myHandler, EventRef event, void *userData) status = GetEventParameter (event, kEventParamCGContextRef, // Various drawing routines happen here.  They all look like the following, but draw different triangles: // error handling here.  This error never comes up though. Here's an excerpt from a typical crash report from when the program crashes with this bug: Thread 0 Crashed: 0   com.apple.CoreFoundation       	0x907c7360 __HALT + 0 1   com.apple.CoreGraphics         	0x903cecb8 doClip + 188 2   com.apple.HIToolbox            	0x9330d8bc HIView::RecursiveDrawComposited(__HIShape const*, unsigned long, HIView*, CGContext*, unsigned char) + 384 3   com.apple.HIToolbox            	0x9330d6e0 HIView::DrawComposited(short, OpaqueGrafPtr*, __HIShape const*, unsigned long, HIView*, CGContext*) + 576 4   com.apple.HIToolbox            	0x93427ee0 FlushWindowObject(WindowData*, void**, unsigned char) + 548 5   com.apple.HIToolbox            	0x9319eac8 FlushAllWindows() + 204 6   com.apple.CoreFoundation       	0x9075cccc __CFRunLoopDoObservers + 352 7   com.apple.CoreFoundation       	0x9075c9f4 CFRunLoopRunSpecific + 244 8   com.apple.HIToolbox            	0x931831e0 RunCurrentEventLoopInMode + 264 9   com.apple.HIToolbox            	0x931827ec ReceiveNextEventCommon + 244 10  com.apple.HIToolbox            	0x931c7ca4 AcquireNextEventInMode + 72 11  com.apple.HIToolbox            	0x931c7a94 RunApplicationEventLoop + 132 12  program.name                 	0x0000cd20 main + 484 (main.cpp:223) 13  program.name                 	0x00002c88 _start + 344 (crt.c:272) 14  program.name                 	0x00002b2c start + 60 Does anyone have any ideas what's causing this?  Any help would be greatly appreciated! - Brian bjk02 @at@ hampshire .dot. edu</body>
  </mail>
  <mail>
    <header>Re: Using multiple CPU's for CoreImage?</header>
    <body>It is not easily answered in general if your multithreading idea really helps. There are a lot of factors that come into play, like the complexity of the filter chain and the size of the images. But in general you need to create a CI context per thread. Besides that there is nothing special to it besides making sure that you properly thread safe your resources. As for running multiple software renderers: Core Image already takes advantage of SMP, so the load should be distributed pretty evenly among the CPUs.</body>
  </mail>
  <mail>
    <header>Using multiple CPU's for CoreImage?</header>
    <body>I'm in a situation where I have to do bulk image conversion using CoreImage. It's not unreasonably slow, but I'd like to get some extra speed. I was wondering if it's possible to thread so that if the user was running on a multiple CPU system I could send one image to be rendered on GPU and have the other unused processors render images separately in a threaded situation, thus allowing (for example on a dual core G5 2.5) 4 images to be rendered concurrently, one on the GPU and 3 more on the 3 remaining cores. As far as I'm aware, every filter can be run in software mode. Is there a way to do this? Am I just not understanding something about CoreImage that would make this impossible? I'm doing about 600 images in one go, and threading would be really nice especially on the coming Intel systems with HyperThreading. Apologies if I have a severe lack of understanding somewhere. I understand CoreImage from an API standpoint but I just know the basics of it's inner workings. -Colin</body>
  </mail>
  <mail>
    <header>Re: About DrawPicture() and GetPicture()</header>
    <body>On Nov 30, 2005, at 9:42 AM, Scott Thompson wrote: Note that the techniques Scott refers to above are for Carbon applications that want to do their drawing at print time ONLY to a CGContext and not use QuickDraw at all during printing. In Tiger we added a much more straightforward way for Carbon applications to accomplish this. The sample print loop code in the / Developer/Examples sample code for printing shows how to do this in a way that works in both Tiger and earlier versions. See: and look for the routines MyPMSessionBeginCGDocument and MyPMSessionGetCGGraphicsContext and how they are used. Hope this helps, David</body>
  </mail>
  <mail>
    <header>Re: Colors in Q2D</header>
    <body>And doesn't all this make the point that floats are better in a world where color matching and rendering affects end pixel values? (I think it does.  I'm like that.) --Matt -- We sincerely apologize for yesterday.  It was eaten by squirrels.</body>
  </mail>
  <mail>
    <header>Re: Colors in Q2D</header>
    <body>On Apr 30, 2006, at 12:46 PM, James W. Walker wrote: Going from the byte value to float... 0 = 0.0 255 = 1.0 A value of 50% gray (0.5) would give an integer value of 127.5 (midpoint of 0..255).  But when using integers, you'd either have to select 127 (slightly darker than 50% gray) or 128 (slightly lighter than 50% gray). Granted, the hardware is going to have to use integral values at which point 0.5 * 255 = 127.5 (which then, depending on rounding rules, would yield 127 or 128).  Perhaps the system would be smart enough to use a dithered pattern of 127, 128, 127, 128, ... to give the appearance of a value of 127.5 :) FYI, Photoshop will convert 50% gray into an RGB of (128, 128, 128). Illustrator converts it into an RGB of (127, 127, 127) ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: Colors in Q2D</header>
    <body>On Apr 30, 2006, at 10:21 AM, Matt Deatherage wrote: I'm not convinced.  While it's true that there are 256 values, the fact that there are an even number of values means that there is no value that is exactly in the middle, so you will never get exactly 0.5.  Also, if you use a denominator of 256, then there are 257 possible (integer) numerators. My answer to the OP's question would be: yes, just compute a float by division, but you can do it in one line: Attachment:</body>
  </mail>
  <mail>
    <header>Re: Colors in Q2D</header>
    <body>This, in fact, is no kind of problem whatsoever.  Your math is shifted. There are not 255 possible values for an 8-bit component, there are 256 values:  0 through 255.  (Zero is #1, One is #2, and 255 is #256.)  What you are describing as the value &amp;quot;127/255&amp;quot; should really be &amp;quot;128/256&amp;quot;, and is easily expressible as a float:  namely, &amp;quot;0.5&amp;quot;. In general, though, your question hints that you're thinking of color in terms of absolute pixel values.  Quartz does not - a &amp;quot;color&amp;quot; to Quartz is a combination of intensities for various components in a a color space, either device-specific or device-independent.  If you're thinking of colors in HTML-like 0-255 components, you may be thrown off when ColorSync renders them through color matching to achieve results.  An RGB CGColor of [0.5, 0.0, 0.0] is &amp;quot;50% red,&amp;quot; and depending on your device's color profile and your CGContext's rendering intent, that may or may not work out to pixel values of (127,0,0). Pretty much by definition, there's no fraction with &amp;quot;256&amp;quot; as the denominator that can't be accurately represented by a float, but if you think of CGColor in terms of matching exact pixel values, you're likely headed for trouble.  The archives of this list have much more on the topic, as do the developer documentation and (I'm sure) David Gelphman's book. -- &amp;quot;Unix geeks are the Amish of the computer world.&amp;quot;  -- Jens Alfke</body>
  </mail>
  <mail>
    <header>Colors in Q2D</header>
    <body>I took a look today at representing colors in Q2D and was wondering how for example am I going to represent the colour 127/255 0/255 0/255. The problem is that 127/255 cannot be represented with an exact number. So should I write in the code something like: and then use c for the Red component? Also is there a tool which converts from hex representation to quartz representation? Is it just a matter of computing a float using division? Kind regards, Milen Dzhumerov Email: email@hidden</body>
  </mail>
  <mail>
    <header>Re: Cannot draw a shading after several hours</header>
    <body>On 30 Apr 2006, at 01:24, Simon Fraser wrote: Thanks a lot, that fixed the problem. This is a bit off-topic but although I have turned off ZeroLink in the build configuration the project still compiles with the typo. If I switch to release config then it outputs a linker error. I'm a bit confused why's that. Thanks. Kind regards, Milen Dzhumerov Email: email@hidden</body>
  </mail>
  <mail>
    <header>Re: Cannot draw a shading after several hours</header>
    <body>On Apr 29, 2006, at 3:08 pm, Milen Dzhumerov wrote: ^^^^ CGContextRestoreGState ZeroLink sometimes causes more trouble than it's worth. I'd advise turning it off, so that errors like this are caught at link time.</body>
  </mail>
  <mail>
    <header>Cannot draw a shading after several hours</header>
    <body>I've been struggling to draw a shading for several hours while using the sample code from the ADC Ref lib. I have a custom NSView and here's the complete drawRect: method: [CODE] CGContextRef myContext = [[NSGraphicsContext currentContext] [/CODE] I've copied the functions static void myCalculateShadingValues (void *info, const float *in, from the sample listings are here's my void myPaintAxialShading (CGContextRef myContext, CGRect bounds) function: [CODE] startPoint = CGPointMake(0, height/2); // 2 endPoint = CGPointMake(width, height/2);// 3 CGColorSpaceRef colorspace = CGColorSpaceCreateDeviceRGB();// 4 CGFunctionRef myShadingFunction = myGetFunction(colorspace);// 5 CGShadingRef shading = CGShadingCreateAxial (colorspace, // 6 startPoint, endPoint, myShadingFunction, CGContextDrawShading (myContext, shading);// 12 CGColorSpaceRelease (colorspace);// 13 GContextRestoreGState (myContext); // 14 [/CODE] When I try to run the code the debugger outputs: [OUTPUT] ZeroLink: unknown symbol '_GContextRestoreGState' CocoaMy has exited due to signal 6 (SIGABRT). [/OUTPUT] Just after it enters the myPaintAxialShading function. Kind regards, Milen Dzhumerov Email: email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGPathContainsPoint returns a bad value</header>
    <body>Yes, there's definitely a bug in Tiger with some paths and some points.  Sadly, there's no easy way to characterize which paths and which points, however.</body>
  </mail>
  <mail>
    <header>CGPathContainsPoint returns a bad value</header>
    <body>I believe that other folks have seen this before too, but I just ran into it myself. I've got a closed path and a point that is outside of the convex hull the path :-(</body>
  </mail>
  <mail>
    <header>Re: Quartz 2D + Cocoa</header>
    <body>On Apr 28, 2006, at 12:48 PM, Milen Dzhumerov wrote: The great thing about CoreGraphics and HIView/NSView's is that it doesn't really matter. As long as you're painting into the right context ([NSGraphicsContext currentContext] inside of drawRect:), it'll properly clip. But yes, if you want to see everything you're drawing, stick to the bounds :) HTH, -Jon -- Jonathan Johnson REAL Software, Inc.</body>
  </mail>
  <mail>
    <header>Re: Quartz 2D + Cocoa</header>
    <body>On 28 Apr 2006, at 18:35, Frank Doepke wrote: Then I should not draw anything with coordinates bigger than the bounds width and height, is that right? Kind regards, Milen Dzhumerov Email: email@hidden</body>
  </mail>
  <mail>
    <header>Quartz 2D + Cocoa</header>
    <body>I'm planning to implement my own NSView-derived control and I will be using Quartz 2D to draw it. There is one thing I cannot understand: how would I make sure that everything I draw is displayed on the control? For example how do I know the width and height of the context so I can make sure that I'm drawing from the beginning to the end of the allocated space. For example, let's suppose I draw a rectangle from (0, 0) and (0, 10) to (50, 0) and (50, 10). Now how do I know if I have gone out of bounds on the NSView or I have just drawn only half of the allocated space for the NSView rectangle. width is not supposed to exist but I need a way, for example, to start from (0, 0) and cover the whole area of the control with a particular gradient so I need to know the starting and ending points. Kind regards, Milen Dzhumerov Email: email@hidden</body>
  </mail>
  <mail>
    <header>Re: Quartz2D image drawing performance</header>
    <body>On Apr 27, 2006, at 12:11 PM, Michael Bernardo wrote: It is cached in vram in the Quartz 2D Extreme implementation in Tiger (which you can enable through Quartz Debug for testing). Not sure what the status of Q2DX is in recent 10.4.x updates. yes</body>
  </mail>
  <mail>
    <header>Re: Quartz2D image drawing performance</header>
    <body>So all the Quartz doc that says the CGLayer *might* be cached in VRAM is actually post-Tiger wishful thinking? :-) This makes sense to me now. I couldn't see a change in VRAM usage using CGLayers no matter what I tried. Did you mean to say &amp;quot;if the scale factor _does_ change&amp;quot; ? That's what I suspected. After doing some more research I found the same results (i.e. everyone is using OpenGL for this type of animation) Thanks for the insight, it was very helpful. -M</body>
  </mail>
  <mail>
    <header>Re: Quartz2D image drawing performance</header>
    <body>A few things: - Using CGLayers does provide performance improvements as you can see from the QuartzCache example, but in Tiger, CGLayers are not hardware accelerated so will not be cached in VRAM. So everything is in software which is why shark shows you the time spent in CG. - Quartz does cache resulting cache image, but if the scale factor doesn't change, it has to throw away the previous cached result and re-create it at the new zoom level and perform image interpolation again. The ARGB32_image_mark routines are already vectorized and going as fast as they can in software. - The fact that you drew your source image into a bitmap context, effectively caches the source image because you avoid the cost of color correction on each frame (so you are already doing the right thing here). - As has been suggested, OpenGL will be the fastest way to achieve the performance you need for your particular case. When Quartz 2D Extreme is ready (please don't ask me more about this, :-) I'm no longer on the quartz team and do not know the status of the feature, but I'm sure there will be more info about this at this year's WWDC), the source image will be cached on the GPU and should give you the best performance, but until then OpenGL is the answer for best performance. Almost all the high performance Ken Burns type affects that I know of in various applications all utilize OpenGL. - Also the coalesced update feature will have impact on your frame rate, in that your animation rate will be one of 60, 30, 20, 15, 12, ... as each frame will coalesced with the next vbl. So you will not see frame rates between 30 or 60. It will be one or the other depending on how much work is being done per frame. When drawing in a loop you are avoiding the flush operation and just measuring the performance of the draw image. As soon as you draw one frame per drawRect, you do participate in the window server composition process. haroon</body>
  </mail>
  <mail>
    <header>Re: Quartz2D image drawing performance</header>
    <body>The scaling ratio has effects on the performance due to the sampling although some of your observations sound suspicious. I would suggest to break it down into a small app and file a bug.</body>
  </mail>
  <mail>
    <header>Re: Quartz2D image drawing performance</header>
    <body>I've tried several variations. At first, I was always using the original 2Kx2K image. Scaling this to a small destination (under 500x500) achieved 30fps easily. I started to drop frames when the destination rectangle approached 600x600. Then I changed the code to pre-scale the 2Kx2K image to something smaller - 500x500. Using this smaller image as the source, I was able to scale to a slightly larger destination, but still easily started dropping frames if the destination approached 700x700. Even starting with a very small initial image (100x100), drawing became too slow to maintain 30fps when the destination size gets large. -M</body>
  </mail>
  <mail>
    <header>Re: CIImage from texture -- which GL context?</header>
    <body>I will send this request to the tech doc department. Thanks for pointing it out.</body>
  </mail>
  <mail>
    <header>Re: CIImage from texture -- which GL context?</header>
    <body>Thanks!  I was hoping that there might be some SPI retain/release mechanism for textures that you guys could use in CIImage.  I'll just do the bookkeeping myself, then :) For the future, it would seem to reduce the occurrence of hard-to- diagnose errors if CIImage could do this for you, but at a minimum, this should be spelled out in the documentation. On Apr 26, 2006, at 11:10 AM, Frank Doepke wrote:</body>
  </mail>
  <mail>
    <header>Re: CIImage from texture -- which GL context?</header>
    <body>When using a texture to create a CIImage, the texture must be valid in the CIContext that is used to draw the CIImage into. That means the texture must be created in the CGLContext from which the CIContext is created from or the context it was created in must be shared with the CGLContext that the CIContext is based on. Textures do not have a retain and release mechanism so it is not possible for CIImage to either retain texture as you suggested and you don't want to pay the price of a copy on each CIImage creation.</body>
  </mail>
  <mail>
    <header>Re: Fast screen shots for VNC like functionality...</header>
    <body>What I figured must be taking place. Anyway I hadn't realized that you can attach your own gl context to a screen that you didn't capture and by way of doing that gain read ability. My needs fall more in the screen casting realm and we have the needs to support 10.3.9 and later on PPC and Intel. I believe their are several discussions in the archives for this list ... now that I better understand that OpenGL can be used in my case I will go review them more closely. Thanks, -Shawn</body>
  </mail>
  <mail>
    <header>CIImage from texture -- which GL context?</header>
    <body>(Logged as part of Radar #4527730)... The CIImage -initWithTexture:... documentation doesn't specify what GL context is used for the CIImage.  Does CIImage 'bind' itself to the current context at creation time?  Does it use the texture in whatever context is current when it is drawn (which would mean it would produce different results)? It would be really nice if I could create a CIImage from a texture name and then be able to deallocate the texture (with the CIImage retaining a copy of it), but this is presumably prohibited by the immutability clause.</body>
  </mail>
  <mail>
    <header>Re: Fast screen shots for VNC like functionality...</header>
    <body>That winds up doing programmed I/O reads over the PCI bus from the framebuffer.  That can be very slow. OpenGL can help here, as it includes the ability to convert pixel formats, and can even deal with hardware-specific pixel reading problems such as microtiling, or getting the correct framebuffer when full screen buffer swapping (the GL version of page flipping) is in effect.  OpenGL will also use a DMA transfer for reading the framebuffer content, where available. OpenGL provides both a synchronous read path suitable for simple screen captures using glReadPixels, and a fast asynchronous path using the texture read mechanism, which is a bit more complex, but works well for repeated capture operations, such as screencasting or recording a movie from the screen. Here's a simple mechanism that can be used to read a rectangle from a display to a CGImageRef, as an example to help you get started.  It's more comment than code: /* *  glGrab.c */ /* IMPORTANT:  This Apple software is supplied to you by Apple Computer, Inc. (&amp;quot;Apple&amp;quot;) in consideration of your agreement to the following terms, and your use, installation or modification of this Apple software constitutes acceptance of these terms.  If you do not agree with these terms, please do not use, install or modify this Apple software. In consideration of your agreement to abide by the following terms, and subject to these terms, Apple grants you a personal, non-exclusive license, under Apple\xd5s copyrights in this original Apple software (the &amp;quot;Apple Software&amp;quot;), to use and modify the Apple Software, provided that if you redistribute the Apple Software in its entirety and without modifications, you must retain this notice and the following text and disclaimers in all such redistributions of the Apple Software. Neither the name, trademarks, service marks or logos of Apple Computer, Inc. may be used to endorse or promote products derived from the Apple Software without specific prior written permission from Apple. Except as expressly tated in this notice, no other rights or licenses, express or implied, are granted by Apple herein, including but not limited to any patent rights that may be infringed by your derivative works or by other works in which the Apple Software may be incorporated. The Apple Software is provided by Apple on an &amp;quot;AS IS&amp;quot; basis.  APPLE MAKES NO WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION THE IMPLIED WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE, REGARDING THE APPLE SOFTWARE OR ITS USE AND OPERATION ALONE OR IN COMBINATION WITH YOUR PRODUCTS. IN NO EVENT SHALL APPLE BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) ARISING IN ANY WAY OUT OF THE USE, REPRODUCTION, MODIFICATION AND/OR DISTRIBUTION OF THE APPLE SOFTWARE, HOWEVER CAUSED AND WHETHER UNDER THEORY OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF APPLE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. */ /* * perform an in-place swap from Quadrant 1 to Quadrant III format * (upside-down PostScript/GL to right side up QD/CG raster format) * We do this in-place, which requires more copying, but will touch * only half the pages.  (Display grabs are BIG!) * * Pixel reformatting may optionally be done here if needed. */ static void swizzleBitmap(void * data, int rowBytes, int height) while ( top &amp;lt; bottom ) /* * Save and swap scanlines. * * This code does a simple in-place exchange with a temp buffer. * If you need to reformat the pixels, replace the first two bcopy() * calls with your own custom pixel reformatter. */ /* * Given a display ID and a rectangle on that display, generate a CGImageRef * containing the display contents. * * srcRect is display-origin relative. * * This function uses a full screen OpenGL read-only context. * By using OpenGL, we can read the screen using a DMA transfer * when it's in millions of colors mode, and we can correctly read * a microtiled full screen OpenGL context, such as a game or full * screen video display. * * Returns a CGImageRef.  When you are done with the CGImageRef, release it * using CFRelease(). * Returns NULL on an error. */ CGImageRef grabViaOpenGL(CGDirectDisplayID display, CGRect srcRect) CGColorSpaceRef cSpace = CGColorSpaceCreateWithName CGLPixelFormatAttribute attribs[] = kCGLPFAFullScreen, kCGLPFADisplayMask, 0,    /* Display mask bit goes here */ 0 if ( display == kCGNullDirectDisplay ) /* Build a full-screen GL context */ if ( pixelFormatObj == NULL )    // No full screen context support if ( glContextObj == NULL ) bytewidth = width * 4;                // Assume 4 bytes/pixel for now bytewidth = (bytewidth + 3) &amp;amp; ~3;    // Align to 4 bytes bytes = bytewidth * height;            // width * height /* Build bitmap context */ if ( data == NULL ) CGLClearDrawable( glContextObj );    // disassociate from full screen CGLDestroyContext( glContextObj );    // and destroy the context bitmap = CGBitmapContextCreate(data, width, height, 8, bytewidth, cSpace, /* Read framebuffer into our bitmap */ glFinish();                /* Finish all OpenGL commands */ glPixelStorei(GL_PACK_ALIGNMENT, 4);    /* Force 4-byte alignment */ /* * Fetch the data in XRGB format, matching the bitmap context. */ glReadPixels((GLint)srcRect.origin.x, (GLint)srcRect.origin.y, width, height, GL_BGRA, GL_UNSIGNED_INT_8_8_8_8_REV, /* * glReadPixels generates a quadrant I raster, with origin in the lower left * This isn't a problem for signal processing routines such as compressors, * as they can simply use a negative 'advance' to move between scanlines. * CGImageRef and CGBitmapContext assume a quadrant III raster, though, so we need to * invert it.  Pixel reformatting can also be done here. */ /* Make an image out of our bitmap; does a cheap vm_copy of the bitmap */ /* Get rid of bitmap */ /* Get rid of GL context */ CGLClearDrawable( glContextObj );    // disassociate from full screen CGLDestroyContext( glContextObj );    // and destroy the context /* Returned image has a reference count of 1 */</body>
  </mail>
  <mail>
    <header>Fast screen shots for VNC like functionality...</header>
    <body>I am working on a VNC like application that has the need to read pixels from one or more frame buffers (depending on the number of screens). This application doesn't capture (as in take over) the screen like you do in a game but runs in background grabbing pixels (plan to bound read back based on CGScreenRefreshCallback and CGScreenUpdateMoveCallback). I have seen many posts on this list talking about using OpenGL... FBO, etc. to do screen shot type operations but it isn't clear to me if that can work unless you have captured the screen (I could easily be wrong). Anyway I put together the following little tool to help me test the one obvious way of doing screen pixel read back but I am finding the performance to be to slow for my needs. Main display: 1920 x 1200 x 32 (rowBytes: 8192, lineBytes: 7680) Frame buffer: 9.38 MiB (9830400 Bytes), Address: 0x2018000 Starting unclamped memcpy test, 10 framebuffer copies... Total Time: 22.490628 s (22490628002 ns) Average Time: 2.249063 s (2249062800 ns) That is 2.25 seconds to read just my main screen. Any pointers one what I am doing wrong or a better way to do this? void logDisplayInformation(DisplayInfo* displayInfo) printf(&amp;quot;Main display: %ld x %ld x %ld (rowBytes: %ld, lineBytes: %ld)\n&amp;quot;, displayInfo-&amp;gt;displayWidth, displayInfo-&amp;gt;displayHeight, displayInfo-&amp;gt;displayPixelBits, displayInfo-&amp;gt;displayRowBytes, #define ALIGN(PTR, ALIGNMENT) \ (BytePtr)(((UInt32)(PTR) + (ALIGNMENT) - 1) &amp;amp; ~((ALIGNMENT) - 1)) int main (int argc, const char * argv[]) frameBuffer.bufferSize = displayInfo.displayHeight * frameBuffer.bufferPtr = malloc(frameBuffer.bufferSize + frameBuffer.bufferPtr = ALIGN(frameBuffer.bufferPtr, printf(&amp;quot;Frame buffer: %.2lf MiB (%ld Bytes), Address: %p\n&amp;quot;, frameBuffer.bufferSize / (1024.0 * 1024.0), frameBuffer.bufferSize, printf(&amp;quot;Starting unclamped memcpy test, %ld framebuffer copies...\n&amp;quot;, size_t lineBytes = displayInfo.displayWidth * //        memcpy(frameBuffer.bufferPtr, //               frameBuffer.baseAddress, UInt64 nanoSeconds = printf(&amp;quot;  Total Time: %lf s (%lld ns)\n&amp;quot;, nanoSeconds / 1000000000.0, printf(&amp;quot;Average Time: %lf s (%lld ns)\n&amp;quot;, (nanoSeconds / 1000000000.0) / i,</body>
  </mail>
  <mail>
    <header>separate audio-band and audio signal smoothing</header>
    <body>I¬¥m experimenting with QC and audio input ... what i like to do is to separate audio-bands from the audio input patch ... when u move the cursor over the &amp;quot;Spectrum&amp;quot; point on the audio-input-patch u see the values of the passed &amp;quot;audio frequency bands&amp;quot;, but how can i separate them?  for exapmle: i just want to pass through Band1! so when i¬¥m done with this i want to smooth the signal! because the signal is a kind of flickering - but i want for example a straight punch from a bass-signal Any suggestions would be great kind regards Markus Hirschl source visuals, AUSTRIA _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Quartz2D image drawing performance</header>
    <body>On Apr 25, 2006, at 2:42 PM, Michael Bernardo wrote: Using CGLayers, even with small initial images e.g. 200x200 pixels, seems to have no effect. I found that the OpenGL Driver Monitor tool allows you to monitor the &amp;quot;Current Free Video Memory&amp;quot; parameter. That claimed I had around 10M of free VRAM. Watching that metric while loading images with my test app didn't seem to change the amount of free VRAM. If anyone is interested in looking at the code, email me and I'll send you a copy of my test app.</body>
  </mail>
  <mail>
    <header>Re: Quartz2D image drawing performance</header>
    <body>Doh. Of course, you're right. For some reason I was thinking 8bpp. So, obviously the original image won't be cached. However, even starting with a 500x500 image I was seeing the same performance. (Not using CGLayers) I haven't yet tried creating a CGLayer with a with a smaller initial image. I'll give that a shot to see if it makes a difference. -M</body>
  </mail>
  <mail>
    <header>Re: Quartz2D image drawing performance</header>
    <body>Um, unless you are displaying a grayscale image, you'll actually use up ~18MB if stored as RGB or ~24MB if stored as RGBA... -- ______________________________________________________________________ Michael Sweet, Easy Software Products           mike at easysw dot com Internet Printing and Document Software</body>
  </mail>
  <mail>
    <header>Re: Quartz2D image drawing performance</header>
    <body>The image is scaled when drawn. I'm trying to draw a zooming animation - each step of the animation scales the destination rectangle. Does Quartz try to cache the resulting scaled image, or the original un-scaled version? I'm starting with a 3072x2040 image, which by my quick calculation should be 5.98MBytes. This should fit in the 32MB of VRAM of the Mini, unless the OS is already consuming most of it. If the scaled version is cached, then it doesn't really help for this type of animation, since the scaled version is only drawn once for that specific frame. Do any tools exist to inspect VRAM utilization?</body>
  </mail>
  <mail>
    <header>Re: Quartz2D image drawing performance</header>
    <body>Ah I am missing your original post for some reason. Yup. You can more directly measure time using those and system trace. Ok. Is this reused CGImage created at the target resolution from your larger image or is this image scaled when drawn. I am just surprised that a CGLayer (assuming you reused one of those when you tried it) would not have a radical bump in performance (unless the image is just to big to fit in VRAM on the system you are testing on). System trace lets you see system call history, sign posts, and threads in a timeline. It provides a different way of looking at the same information, both time profile and system trace are good to use. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Quartz2D image drawing performance</header>
    <body>I've already run a Time Profile with Shark. You can find the results in the original post. (90% of the sampled time is in CoreGraphics, in the two calls ARGB32_image_ARGB32 and ARGB32_image_mark) I will try the System Trace. Did you mean to say &amp;quot;after _adding_ CHUD sign posts ...&amp;quot;? I'm creating and re-using a single CGImage instance for all drawing. Doesn't Time Profile provide the same answers? How is a system trace different? I'm triggering a setNeedsDisplay with an NSTimer scheduled at the target framerate. Are you referring to the latency between the setNeedsDisplay and the drawRect calls? Again, a small destination rectangle has no problems achieving the target framerate. It only becomes a problem when  I increase the size of the destination, which seems to indicate to me that the bottleneck is the actual drawing. -M</body>
  </mail>
  <mail>
    <header>Re: Quartz2D image drawing performance</header>
    <body>I would fire up shark and use the system trace capability, ideally after CHUD sign posts to your code. This will allow you to understand just how much time is spent in CG doing the drawing, etc. It also isn't clear to me how you are managing your CGImages / CGLayers... are you creating them once (or not often) and drawing the same image instance several times? With system trace you can understand if memory allocation (creating a new CGImage), image scaling, color correction, or simply fill rate are your biggest issues. Also it may just be latency in how often your drawRect is called. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Quartz2D image drawing performance</header>
    <body>I was afraid of that too. I was able to squeeze out a few more fps by ensuring that the destination rect had integral bounds. I glossed over the fact that CGRect uses floats - my scaling calculations were leaving fractional pixel coordinates in the destination rectangle. Now this makes the animation a bit &amp;quot;jumpier&amp;quot; since it's no longer anti-aliasing the bounds, but I can now get to a 650x650 destination rect before dipping below 30fps. It makes the animation generally uglier and doesn't provide a big peformance win, so I'm not sure it's worth it. On a lark I started digging through the sample code on Apple's site and came across the &amp;quot;Quartz Cache&amp;quot; sample app. I was surprised to find that this sample gets MUCH better performance. There are a few differences from my app though: - It measures frames/sec in an inner-loop contained in the drawRect call. I.e. it doesn't actually animate anything - It doesn't set any options on the context. My app is setting the interpolation mode to 'kCGInterpolationNone' - It draws the image at the same size repeatedly (512x512 pixels) Setting the interpolation mode actually makes a big difference in performance. Without setting the interpolation mode, the sample app gets ~350 CGContextDrawImage calls per second. Setting the interploation mode to kCGInterpolationNone drops that to around 125! This seems very counter-intuitive to me. What's going on? My only guess is that by setting the interploation mode, it causes Quartz not to cache the image? I then tried the same test in my app. I removed the call to CGContextSetInterpolationQuality, and put a loop in my drawRect that executed my drawing code. I was able to get similar performance - around 300 CGContextDrawImage calls per second. Then I tried actually changing the size of the destination rect in each iteration of the loop to see if this had any effect on the cache. I scale the destination rect from 100% to a 1x1 pixel rect. Even better - over 1000 CGContextDrawImage calls/second! Probably due to the fact that I'm drawing a lot more smaller images. The bad news is that I can only get this level of performance in a loop that doesn't leave drawRect. If I only draw one frame per drawRect call, then I'm back to bad performance. (Even worse than my original numbers, since interpolation is still on) Can anyone explain this drastic difference in performance? Why is drawing in a loop so much better? I'm guessing the drawing calls are somehow queued before being sent to the compositor? As it stands now, it seems that Quartz2D is not very good for doing real-time animations of large images on G4 hardware. I'd love for someone to prove me wrong since Quartz is such a nice API and I'm not too thrilled about having to go directly to OpenGL. -M</body>
  </mail>
  <mail>
    <header>Re: Quartz2D image drawing performance</header>
    <body>On Apr 24, 2006, at 9:29 AM, Michael Bernardo wrote: It sounds like you're stuck on the fill rate then.  The only thing I can suggest to get a better fill rate would be to move the fill rate to hardware.  If you've tried layers and they didn't help, then your next best bet is Core Video and OpenGL I'm afraid.</body>
  </mail>
  <mail>
    <header>Is quartz the right place to do this task?</header>
    <body>&amp;nbsp; I want to draw some scripts on the destop but I can still select icon ,double click it on the destop. Is quartz&amp;nbsp;API the&amp;nbsp;right one to do this? Or is there a better way? &lt;br clear=all&gt;‰ΩøÁî®‰∏ñÁïå‰∏äÊúÄÂ§ßÁöÑÁîµÂ≠êÈÇÆ‰ª∂Á≥ªÁªü‚Äï   &lt;a href="http://g.msn.com/8HMACNCN/2749??PS=47575" target="_top"&gt;MSN Hotmail Get 2 months FREE*.</body>
  </mail>
  <mail>
    <header>Re: CoreImage filters on CMYK image</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGPDFStringCopyTextString and Line Breaks</header>
    <body>Yes,  this is a bug in Tiger. There's no way to do this in Tiger. There isn't one: PDF strings (assuming they aren't encoded in Unicode) use the &amp;quot;PDFDocEncoding&amp;quot;, which is described in the PDF specification. The closest match is probably kCFStringEncodingWindowsLatin1 (from CoreFoundation ‚Äî I'm not sure what it is in NS terms).  It's not very hard, though, to map from the PDFDocEncoding to Unicode yourself.</body>
  </mail>
  <mail>
    <header>CGPDFStringCopyTextString and Line Breaks</header>
    <body>I am using Quartz to read text annotations from a PDF file.  I used Acrobat to add the annotations, which have multiple lines, separated by '\r' character. I read the annotations in with CGPDFDictionaryGetString, which returns a CGPDFStringRef.  When I look at the raw characters of the CGPDFStringRef using CGPDFStringGetBytePtr, the '\r' characters are preserved.  However, when I use the function CGPDFStringCopyTextString to convert the CGPDFStringRef to a CFString, the function replaces the '\r' characters with spaces. Converting the raw characters returned by CGPDFStringRef with + [NSString stringWithCString: encoding:] preserves the line breaks, using NSMacOSRomanStringEncoding for the encoding parameter. 1) Is there some way to control the behavior of CGPDFStringCopyTextString, i.e. not convert the '\r\ characters to spaces? 2)  What would the best value for the encoding parameter in stringWithCString, given that I'm using PDF strings? Stewart Milberger Megaptera, Inc. (www.megaptera.com)</body>
  </mail>
  <mail>
    <header>Re: Quartz2D image drawing performance</header>
    <body>&amp;gt; If I understand what you are trying to do, you are taking a large image (2000x2000) Actually, no - I should have been clearer. The performance numbers I quoted are from drawing the entire image bounds at a scaled destination, with the same aspect ratio. E.g. drawing the entire 2000x2000 image in a 500x500 destination. As I mentioned in the original post, the size of the initial image doesn't seem to have a large impact. Drawing a smaller source image to the same size destination achieves similar results. Drawing to a very small destination (E.g. 100x100) is very fast. I easily get ~60fps, even from the large 2000x2000 source image. It gets hard to maintain 30fps at around a 550x550 destination. -M</body>
  </mail>
  <mail>
    <header>Re: Quartz2D image drawing performance</header>
    <body>On Apr 23, 2006, at 11:40 PM, Mike Bernardo wrote: First of all, thanks for looking at my book :-) If I understand what you are trying to do, you are taking a large image (2000x2000) and then panning around the image at some animation rate. The first thing that strikes me is that you are trying to redraw the entire destination each time.  A key graphic enhancement on just about any system is drawing only as much as you need to.  In that vein, it seems that if you are scrolling the image around in the image, you should be able to move a portion of the existing image, and then just redraw the newly exposed portions. In fact, this can be handled by the system for you if you are clever.  Since you are using Cocoa, I would look at using an NSClipView instead of handling all the drawing yourself.   Your document will be responsible for drawing it's 2000x2000 image in the &amp;quot;document view&amp;quot; associated with your clip view.   I believe the clip view itself will handle most of the scrolling and invalidating on your behalf. For best performance, you should look into using &amp;quot;getRectsBeingDrawn:count:&amp;quot; so you can copy over just those portions of the big image you need to redraw.</body>
  </mail>
  <mail>
    <header>(no subject)</header>
    <body>From: 	  email@hidden Subject: 	Quartz2D image drawing performance Date: 	April 24, 2006 12:19:26 AM EDT To: 	  email@hidden I'm trying to create a real-time image animation in the spirit of the &amp;quot;Ken Burns effect&amp;quot; using Quartz 2D and I'm running into some performance problems just getting images to draw fast enough. I'm starting with a 2000x2000 pixel image obtained through CGImageSourceCreateThumbnailAtIndex. I get an equal sized, color-corrected version of the image by copying it to a CGBitmapContext with a colorspace matching the monitor's profile. (Using the snippet in Scott's book) I'm drawing the color-corrected CGImageRef at 30 frames per second into a scaled destination rectangle via CGContextDrawImage using NSImageInterpolationNone. The animation is triggered with a repeating NSTimer which just ends up calling setNeedsDisplay:true on the view. The maximum size of the destination rectangle before I start dropping frames is around 550x550. This is on a 1.42Ghz G4 Mini, OS 10.4.6. I can get the images to around 800x800 if I drop the frame rate to 20 fps, but then of course the animation gets choppy. I tried using CGLayers, and also drawing via a CIImage, (both with scaled destination rectangles), and I get the same performance. Dropping the size of the original image doesn't seem to help either. Starting with a 800x800 image starts to drop frames at a 700x700 destination image @30fps. The gating factors seems to be the size of the destination, and the speed of the redraws. Should I expect better performance than this with Quartz2D? What could I be doing wrong? (These number seem pretty weak, even for a Mini) Obviously better performance is possible via direct OpenGL, but I'd like to avoid that if I can. In case it provides any insight, Shark tells me I'm spending 90% of my time on these two stacks: 65.5%	65.5%	CoreGraphics	ARGB32_image_ARGB32 0.0%	65.5%	CoreGraphics	ARGB32_image_mark 0.0%	65.5%	CoreGraphics	ARGB32_image 0.0%	65.5%	libRIP.A.dylib	ripd_Mark 0.0%	65.5%	libRIP.A.dylib	ripl_BltImage 0.0%	65.5%	libRIP.A.dylib	ripc_RenderImage 0.0%	65.5%	libRIP.A.dylib	ripc_DrawImage 0.0%	65.5%	CoreGraphics	CGContextDelegateDrawImage 0.0%	65.5%	CoreGraphics	CGContextDrawImage 0.0%	65.5%	ImageZoomer	-[ImageZoomerView drawRect:] 24.4%	24.4%	CoreGraphics	ARGB32_image_mark 0.0%	24.4%	CoreGraphics	ARGB32_image 0.0%	24.4%	libRIP.A.dylib	ripd_Mark 0.0%	24.4%	libRIP.A.dylib	ripl_BltImage 0.0%	24.4%	libRIP.A.dylib	ripc_RenderImage 0.0%	24.4%	libRIP.A.dylib	ripc_DrawImage 0.0%	24.4%	CoreGraphics	CGContextDelegateDrawImage 0.0%	24.4%	CoreGraphics	CGContextDrawImage 0.0%	24.4%	ImageZoomer	-[ImageZoomerView drawRect:]</body>
  </mail>
  <mail>
    <header>Re: CoreImage Interfilter Pixel Values</header>
    <body>You might want to pick up a copy of the OpenGL extensions guide, which discusses ARB_fragment_program in depth. I'm sure there are other references available - but this is a good one and discusses semantics and limitations of fragment program behavior.  For those kernels executed on the GPU, these docs are relevant.  For those kernels executed on Altivec, I'm sure they are simply using vector float as the native data type, which would imply uniform behavior across all the channels. On Jun 24, 2005, at 8:35 AM, Nick Nallick wrote:</body>
  </mail>
  <mail>
    <header>CoreImage Interfilter Pixel Values</header>
    <body>I'm building a multi-pass CoreImage filter that invokes two kernels in series.  The second kernel depends on the fact that the output from the first kernel may contain floating-point pixel values greater than 1.0.  This seems to work fine for the RGB components but alpha seems to behave differently.  It appears the alpha component is being clamped to a smaller value than the others (perhaps 1.0).  Is alpha indeed treated differently than the RGB components, and if so is there anything I can do to stop this from happening? Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: CGImageCreate providers invoking</header>
    <body>On Jun 24, 2005, at 9:53 AM, Tor Langballe wrote: The data provider will be used for the life of the image.  That's what the last parameter (the one you're setting to NULL) is for.  It can be used to release the memory when the image has finished with it.</body>
  </mail>
  <mail>
    <header>CGImageCreate providers invoking</header>
    <body>provider = ::CGDataProviderCreateWithData(NULL, data, &amp;lt;size&amp;gt;, What I can't seem to find adequate documentation for is: Is the provider used (once only) during CGImageCreate? If not I have to keep my data lying around, which is a pain, so I wonder if there's a way to flush it. Or is the provider something that randomly might get called if the graphics card gets full and I has to re-load the bitmap or something? Thanks, Tor Langballe -------------------------------------------------------- CUTTING EDGE TECHNOLOGIES Bogstadveien 5B, 0355 OSLO, Norway Fax: +47 2256 4770</body>
  </mail>
  <mail>
    <header>pattern vs. dash</header>
    <body>Can anyone tell me which would result in a faster stroke of a complex path, using a pattern or a dash? I'm changing our marching ants from QD to Quartz. The QD code xor'd 8 different patterns progressively. each time. This results in very odd animation when running on a 450MHz G3. The bottom third or so of the path draws correctly each time (marching along nicely), but the upper 2/3 or so will sort of jump around, or seem to just toggle between 2 different dash phases. Would stroking with a pattern be any faster? I'll be drawing the pattern to mimmic the QD patterns, which were 45¬∞ 4-pixel alternating black/white lines, so I'll probably use something like this as my pattern draw code: BeginPath, (MoveToPoint, LineToPoint) * 3, StrokePath. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: CIVideoDemoGL sample</header>
    <body>Frank, it is on the dual G5 but not on the iMac. I've tried both versions but see no difference in CPU usage. Should the sample in theory use less CPU on my Radeon 9800? Or should I can the idea of having the video's time code in the movie at a decent frame rate and continue to use an external text field? chris</body>
  </mail>
  <mail>
    <header>Re: Requirements for CIKernel acceleration</header>
    <body>Yes, ARB_fragment_program is the extension required to accelerate kernel programs. There is also the restriction that programs must be short enough to fit within the limits of the current GPU (for example, most ATI cards only support 64 arithmetic instructions per program)</body>
  </mail>
  <mail>
    <header>Re: Releasing a Layer and It's Context</header>
    <body>You should not release the context.  This is a bug in the documentation which we should fix.  It would be helpful if you filed a bug.</body>
  </mail>
  <mail>
    <header>Re: CGShadingCreateAxial retains CGFunctionRef?</header>
    <body>A CGShading does, in fact, retain its CGFunction.  The documentation should mention this, and it would be helpful if you filed a bug requesting that it do so.</body>
  </mail>
  <mail>
    <header>Requirements for CIKernel acceleration</header>
    <body>Does ARB_fragment_program support roughly correlate to the ability to accelerate programs written in the CIKernel language?  Is there some other method (other than timing an actual run of the kernel) that can be used to make some guess about the hardware support of a given filter tree?</body>
  </mail>
  <mail>
    <header>Re: CIVideoDemoGL sample</header>
    <body>On the FX 5200 Core Image will run in software - depending on the filter being used and the movie (codec and size) this can still be fast enough for realtime playback. Are you using the sample code from the Tiger installation or the latest one that was made available in WWDC timeframe on the ADC website?</body>
  </mail>
  <mail>
    <header>What is a good card to purchase for QC ?</header>
    <body>As in it will be primed for when apple releases it's own version of hardware GLSL, whenever that may be ...</body>
  </mail>
  <mail>
    <header>Re: CIVideoDemoGL sample</header>
    <body>Hi, short update on the dual G5: after disabling the display on the decklink card the sample runs correctly on the G5. But it's still all CPU (which would probably be because of the FX 5200). chris</body>
  </mail>
  <mail>
    <header>Re: CIVideoDemoGL sample</header>
    <body>Hi Frank, no. I'm running on a dual G4 with 1.25 GHz and 1 gig of RAM. On my bosses' iMac G5 the sample app can't even run with a decent frame rate (that would be 25 frames per sec for us) but I think it only has an NVidia GeForce FX 5200. On my colleagues Dual G5 (also with an NVidia GeForce FX 5200) the app shows only a black screen though his system might be corrupt as it includes an Aurora as well as a Decklink capture card installed in PCI slots. chris</body>
  </mail>
  <mail>
    <header>Drawing with an image mask</header>
    <body>Hello, All the documentation I've read for image masks seems to say that I can draw with an image mask and get the fill color drawn in the unmasked areas. Here is what I'm reading: I've looked through the mailing list archives and what I read there seems to say the same thing. So I take this to mean I can do something roughly like so: and I figure I should get a yellow version of my mask painted. But when I try this all I ever get is black. Am I missing something? I get the feeling at this point that I'm doing something stupid or not doing something important. If it helps understand better what I'm trying to do, I'm doing this for a small drawing tool and I don't want to have to create a new bitmap every time I want to change the color of the brush. I'd be thankful for any help. Thanks, -S</body>
  </mail>
  <mail>
    <header>Re: Releasing a Layer and It's Context</header>
    <body>On Jun 22, 2005, at 2:05 PM, Nick Nallick wrote: Definitely.... either a BAD bug in CG that breaks the CF Copy/Create/Get semantics for this API. Or incredibly wrong documentation.</body>
  </mail>
  <mail>
    <header>Re: Releasing a Layer and It's Context</header>
    <body>On Jun 22, 2005, at 12:37 PM, Scott Thompson wrote: That's a good point, but I'll then submit that the documentation is misleading.  From the description of CGLayerGetContext() at http:// developer.apple.com/documentation/GraphicsImaging/Reference/CGLayer/ cglayer_reference/chapter_1.2_section_5.html To me this implies I should release the context. Nick</body>
  </mail>
  <mail>
    <header>Re: Releasing a Layer and It's Context</header>
    <body>On Jun 22, 2005, at 1:10 PM, Nick Nallick wrote: Thought we never called release on &amp;quot;Get&amp;quot; APIs -- only on Create and Copy APIs? CFMemoryMgmt/Concepts/Ownership.html &amp;quot;If you create, copy, or retain a Core Foundation object, you alone are responsible for releasing it when you no longer need it. If you did not directly create or copy the object, you do not own it and should not &amp;quot;Important: Never release an object that you don‚Äôt own, such as an object returned from a Get function. Doing so could have serious and</body>
  </mail>
  <mail>
    <header>Re: Releasing a Layer and It's Context</header>
    <body>On Jun 22, 2005, at 1:10 PM, Nick Nallick wrote: The routine CGLayerGetContext is a &amp;quot;Get&amp;quot; routine.  You are not responsible for releasing something that comes from a &amp;quot;Get&amp;quot; method (only &amp;quot;Copy&amp;quot; or &amp;quot;Create&amp;quot;).</body>
  </mail>
  <mail>
    <header>Releasing a Layer and It's Context</header>
    <body>The following code crashes in CGLayerRelease when provided a basic window context (i.e., inContext).  Can anybody tell me what's wrong here?  According to the documentation, the caller is responsible for releasing the context returned by CGLayerGetContext().  FWIW, it doesn't make any difference if I draw into the layer or not. . CGLayerRef layer = CGLayerCreateWithContext(inContext, Exception:  EXC_BAD_ACCESS (0x0001) Codes:      KERN_INVALID_ADDRESS (0x0001) at 0x63657350 Thread 0 Crashed: 0   &amp;lt;&amp;lt;00000000&amp;gt;&amp;gt;     0x63657350 0 + 1667593040 1   com.apple.CoreGraphics           0x90412af8 contextDelegateFinalize + 44 2   com.apple.CoreFoundation         0x9072a8d4 _CFRelease + 240 3   com.apple.CoreGraphics           0x904043b8 contextFinalize + 96 4   com.apple.CoreFoundation         0x9072a8d4 _CFRelease + 240 5   com.apple.CoreFoundation         0x9072a8d4 _CFRelease + 240 Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: Reference Counting in CIImage</header>
    <body>On Jun 22, 2005, at 10:18 AM, John Harper wrote: Apparently the crash is caused when the layer is released an extra time, however it doesn't look like CoreImage is the culprit.  I get the same crash if I release the layer without calling CI at all. I'll post that in a new thread. Nick</body>
  </mail>
  <mail>
    <header>Re: Drawing Core Image into a PDF Context</header>
    <body>On Jun 21, 2005, at 8:39 PM, Scott Thompson wrote: thanks, this is definitely a CI bug, we will look into it.. I'll let you know if we find a workaround..</body>
  </mail>
  <mail>
    <header>Re: Reference Counting in CIImage</header>
    <body>That's correct (for now, the behavior may change later). However, I don't see why this would cause a crash - the CIImage doesn't reference the CGLayer after -imageWithCGLayer returns</body>
  </mail>
  <mail>
    <header>Re: Dialog background image</header>
    <body>On Jun 21, 2005, at 9:50 PM, Simon Harms wrote: - you should specify a subject for your message - since this is a Carbon question, it's more appropriate for the carbon-dev mailing list than for quartz-dev Your request that the solution use neither Quickdraw nor HIView is very limiting. You almost have to use one or the other; not using either doesn't leave you much room for a solution. - create a compositing window - embed an HIImageView into the compositing window's content view. Position it at the bottom of the window's view hierarchy. - load the background image into memory using CGImageCreateWithJPEGDataProvider - set the HIImageView's image to be the CGImage that you've loaded It's important to use a compositing window, because in a non- compositing window, the controls in the window will erase their content areas before drawing, and that will leave blocky white splotches on your picture. It's possible to avoid this for some controls by patching out the QD bottlenecks for the port while the control is drawing, but not easy, so we highly recommend compositing mode.</body>
  </mail>
  <mail>
    <header>Reference Counting in CIImage</header>
    <body>I've been cleaning up some experimental CoreImage code and I've found that I crash when releasing a CGLayerRef used to create a CIImage. Further experimentation seems to indicate that CIImage imageWithCGImage: retains the provided image but CIImage imageWithCGLayer: doesn't retain the provided layer.  Can anyone confirm or refute this? Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: Drawing Core Image into a PDF Context</header>
    <body>On Jun 21, 2005, at 9:39 PM, Scott Thompson wrote: FWIW this works fine for me, although I did find that I had to use a CGImageRef instead of a CGLayerRef when creating my source CIImage.</body>
  </mail>
  <mail>
    <header>Re: CIVideoDemoGL sample</header>
    <body>Sorry, but I seem to have hit a gmail shortcut when I didn't want to. Here's the complete message. Hi all, I'm trying to create a movie view based on the CIVideoDemoGL sample. My goal is to be able to include the movie's time code within the movie view. Now, while toying around with the sample code I noticed that it uses around 50% of both my CPUs while Quicktime uses about 10% when playing a movie. I deleted the effect filter from the sample but CPU usage remains high. I had a look at OpenGL profiler but couldn't find any proof that the GPU is actually used when running the sample app. I also tried specifying that the GPU be used according to the technical Q&amp;amp;A 1416: ciContext = [[CIContext contextWithCGLContext:(CGLContextObj)[[self openGLContext] CGLContextObj] pixelFormat:(CGLPixelFormatObj)[[self pixelFormat] CGLPixelFormatObj] options:[NSDictionary dictionaryWithObjectsAndKeys: (id)colorSpace,kCIContextOutputColorSpace, (id)colorSpace,kCIContextWorkingColorSpace, [NSNumber numberWithBool:NO],@&amp;quot;kCIContextUseSoftwareRenderer&amp;quot;,nil But as the sample creates its CIContext from an OpenGL context I don't know if that will work. Oh and I am using an ATI Radeon 9800 Pro. So, any hints on how to have the GPU do some work or would you suppose that it is actually used? Thanks chris</body>
  </mail>
  <mail>
    <header>CIVideoDemoGL sample</header>
    <body>Hi all, I'm trying to create a movie view based on the CIVideoDemoGL sample. My goal is to be able to include the movie's time code within the movie view. Now, while toying around with the sample code I noticed that it uses around 50% of both my CPUs while Quicktime uses about 10% when playing a movie. I deleted the effect filter from the sample but CPU usage remains high. I had a look at OpenGL profiler but couldn't find any proof that the GPU is actually used when running the sample app. I also tried specifying that the GPU be used according to the technical Q&amp;amp;A 1416: ciContext = [[CIContext contextWithCGLContext:(CGLContextObj)[[self openGLContext] CGLContextObj] pixelFormat:(CGLPixelFormatObj)[[self pixelFormat] CGLPixelFormatObj] options:[NSDictionary dictionaryWithObjectsAndKeys: (id)colorSpace,kCIContextOutputColorSpace, (id)colorSpace,kCIContextWorkingColorSpace,</body>
  </mail>
  <mail>
    <header>NSView relative CGPattern</header>
    <body>Hello, I am trying to use Core Graphics calls from within a COCOA application to draw in an NSView. I have been using the CGPattern calls to create and draw a pattern in the view following the apple documentation. However, when the window in which the view is located resizes, the pattern moves as if it screen relative, while all the other CG content drawn on top of the pattern moves as you would expect - relative to the window. This seems very strange, but also occurs in the only other example of using NSPattern I could find: My draw method to make the referenced code work is as follows: CGContextRef myContext = [[NSGraphicsContext drawPatternInStrokedCircle(myContext, *(CGRect*) If you run this (on Tiger 10.4.1 in my case) and resize the window which contains the view, the stars move out of sync with the circle. The documentation seems to make no reference to this sort of behaviour. Any light you could shed on this would be most welcome! Many Thanks __________________________________________________ Do You Yahoo!? Tired of spam?  Yahoo! Mail has the best spam protection around</body>
  </mail>
  <mail>
    <header>(no subject)</header>
    <body>This is probably a very simple question. I am currently trying to set a jpeg(don't mind using a pdf or other format) as the background image for a dialog I am writing in carbon and I just cannot work out how to do this I have looked at the API's and example code but nothing seems to help I am trying to do this for a splash screen I want the code to be both backwards and forwards compatible (so no quick draw and no hiview) which makes things complictated Any help would be very much appreciated Simon Harms</body>
  </mail>
  <mail>
    <header>Re: Drawing Core Image into a PDF Context</header>
    <body>OK... I put my code that tries to draw a CIImage into a PDF context back together again and I'm getting the same result.  The resulting PDF is black where the image is supposed to be.  The same CIImage drawn to a window works just fine. The original application was a carbon application using Core Image (that's why there's an explicit autorelease pool for example. There's really nothing remarkable (that I know of) about the Beach image.  It's just a picture that I took with my digital camera and exported at 640x480.  If I adjust the application so that it draws in a Carbon window (inside of an HIView's kEventControlDraw handler into the context passed to me by the OS), the same DrawBlurredImage routine draws a blurry version of my image. If you have any thoughts about why I can't dump the image into a PDF context.  Please let me know. CGImageRef CreateSourceImage() CFURLRef sourceImageURL = CFBundleCopyResourceURL CGImageSourceRef imageSource = CGImageSourceCreateWithURL CGImageRef sourceCGImage = CGImageSourceCreateImageAtIndex void DrawBlurredImage(CGContextRef outputContext, const CGRect &amp;amp;viewBounds) // Create an autorelease pool to catch stray objects NSAutoreleasePool *autoreleasePool = [[NSAutoreleasePool alloc] // Obtain the source image and create a CIImage with that set of pixels CGRect sourceBounds = CGRectMake(0, 0, CGImageGetWidth // We've done with the source image. // Create a filter and assign the parameters CIFilter *gaussianBlur = [CIFilter filterWithName: [gaussianBlur setValue: [NSNumber numberWithFloat: 10] forKey: // extract the resulting CIImage from the blur filter. // Create a CIContext on our graphics context and render the image into it. CIContext *outputCIContext = [CIContext contextWithCGContext: [outputCIContext drawImage: result inRect: viewBounds fromRect: // release the autorelase pool void DrawPDFVersion() CFURLRef destinationURL = CFURLCreateWithFileSystemPath(NULL, CFSTR(&amp;quot;/Users/scott/Desktop/BlurredBeach.pdf&amp;quot;), kCFURLPOSIXPathStyle, CGContextRef outputContext = CGPDFContextCreateWithURL int main(int argc, char* argv[])</body>
  </mail>
  <mail>
    <header>outlineRoot returns NULL when creating PDFDocument with initWithData</header>
    <body>The subject says it all. If I use initWithURL to create a PDFDocument and then call outlineRoot to get the outline, everything is fine. If I instead use initWithData, then when I call outlineRoot, NULL is returned. Anybody know why? Is this a bug?</body>
  </mail>
  <mail>
    <header>Re: Core Image &amp;lt;-&amp;gt; SVG</header>
    <body>If you look at the FunHouse sample code (/Developer/Examples/Quartz/ Core Image/FunHouse), you can see an example how one can archive a XML description of a Core Image rendering with the filters. This is not a SVG but feel free to wrap your SVG exporter around it.</body>
  </mail>
  <mail>
    <header>Re: Core Image &amp;lt;-&amp;gt; SVG</header>
    <body>I'm fuzzy on the details but maybe that could be avoided at least part of the time. Have you looked at SVG's filters ()? You might be able to use them to composite 6 back into the rest (at least sometimes). There's a cult community for SVG that's on email@hidden and irc.freenode.net#svg . I don't know that much has been done with Core Image there, but it's a cult with many ramifications. You might also want to check out the WebKit project where people are noodling about integrating KSVG. -- Robin Berjon Senior Research Scientist Expway,</body>
  </mail>
  <mail>
    <header>Style and Matrix transformation</header>
    <body>Hi I want to draw text with style and matrix transformation. this fonction work but then I'm not able to use the style. If I use ATSU routine I got the style but err = ATSUSetLayoutControls return err is kATSUInvalidAttributeTagErr = -8799 Calling ATSUSetLayoutControls with kATSUImposeWidthTag return also kATSUInvalidAttributeTagErr Calling ATSUSetLayoutControls with kATSUCGContextTag return also noErr I've try to call the fonction with Mach0 but the result is the same. Is there something I've forgot when initialisating? Any idea.? Thanks Vincent</body>
  </mail>
  <mail>
    <header>Re: Core Image &amp;lt;-&amp;gt; SVG</header>
    <body>On Jun 21, 2005, at 10:59 AM, Ken Tabb wrote: I've looked into SVG as it relates to Quartz in the past and I haven't found much activity.  Apart from a few basic cases I think CoreImage would be best exported as a bitmap.  There are just too many ways to express things beyond the scope of SVG.  For that matter, there doesn't seem to be anything out there to use as a graphics exchange format for CI so there's not even a standard way of exchanging these things between apps via copy/paste, etc.</body>
  </mail>
  <mail>
    <header>Core Image &amp;lt;-&amp;gt; SVG</header>
    <body>I checked the archives for the list to see if this had been asked before, but couldn't spot anything obvious (or even semi-obvious). I am working on an app that I'd like to use Core Image in. I'd also ideally like to be able to export as SVG (scalable vector graphics) if possible, in addition to the native file format. Looking at the SVG format specification, there are a lot of situations where an Image Unit's parameters could be expressed in the XML of an SVG document (eg. gradient fills). However, from my initial reading of the SVG format, it seems like I'd have to do a walk through each object's Image Units, and if it's one that is expressible in XML, do that, and if it can't be expressed in XML - either because the format doesn't cover that type of effect, or because I don't know what the Image Unit does (eg. 3rd party Image Units installed by end users, rather than Apple's default Image Units or my Image Units) - then export a bitmap render for the object, with rendered Core Image output... which of course is neither scalable or vector based, but aside from that, would be very suitable for inclusion in an SVG document!! Furthermore if (for example) object 3 in my document has 10 image units, and the 6th image unit to be applied has an effect that can't be expressed as SVG XML, then I'd effectively have to output steps 1-6 inclusive as the bitmap, in which case only steps 7-10 would be 'editable' by the user in another SVG app (as the first 6 would be hardwired into the bitmap). Not to mention file sizes etc. which would presumably grow exponentially per the number of objects that have to be output as bitmaps. Has anyone done any work with Core Image &amp;lt;-&amp;gt; SVG... is there a cult community in existence who I have yet to tap into? I understand that Core Image is basically bitmap based, and SVG is basically vector based, but there are definitely some cross overs where the two could talk. Please tell me if I'm barking up the wrong tree here... thanks in advance for any feeders, Ken - - - - - - - - - - Dr. Ken Tabb Mac &amp;amp; UNIX Developer - Health &amp;amp; Human Sciences Machine Vision &amp;amp; Neural Network researcher - School of Computer Science University of Hertfordshire, UK</body>
  </mail>
  <mail>
    <header>Missing API functionality in PDFKit?</header>
    <body>(I sent this to the Cocoa-dev list, and realized that the Quartz list is probably a better choice, so sorry if you have received this before...) I have been experimenting with the PDFKit, and have run into some roadblocks.  Either there is some missing functionality, or I am missing something.  So far, specifically: 1.) PDFAnnotationButtonWidget - You can find out what controlType a button widget is (kPDFWidgetUnknownControl, , kPDFWidgetPushButtonControl, kPDFWidgetRadioButtonControl, kPDFWidgetCheckBoxControl) using 2.) You can get the parentID of the PDFAnnotationButtonWidget, but there is no way to set it.  There is also no way to find out in the current document what the current maximum object id is, so you aren't stepping on a current object id.  (From what I can see, if you make a couple of PDFAnnotationButtonWidgets of type kPDFWidgetRadioButtonControl, they need to have the same parentID for the radio buttons to function radially (i.e. exclusively selected).) 3.) For a PDFAnnotationFreeText, the setColor: (defined in PDFAnnotation) sets the background color.  I could not find a method that sets the text color of this class. 4.) Is there any way (now or planned) to get the path objects on a pdf page?  (i.e. to be able to detect the paths that draw a line or a box on an existing page). Just a few questions / items I have run into.  Will probably find more, but wanted to get feedback on these items.</body>
  </mail>
  <mail>
    <header>Re: Drawing Core Image into a PDF Context</header>
    <body>No, it should work with any context, what kind of CIImage are you using?</body>
  </mail>
  <mail>
    <header>Re: New to quartz;	avoiding triple-buffering when porting win32 code?</header>
    <body>I suppose I should also note, since no one else has yet, that when (if) Apple enabled Quartz 2D Extreme by default, you may see significant advantages to Quartz over QuickDraw.  Even in Tiger you should see Quartz outperforming QuickDraw in some (most?) cases. Refer to &amp;lt;&amp;gt; for more details.  Note that for some things, such as where you are trying to push a whole lot of pixels (e.g. a 800x800 square, in one of their tests) Quartz 2D Extreme is *236* (two-hundred and thirty- six) times faster than [software] Quartz 2D.  Unfortunately Ars haven't benchmarked QuickDraw on the same test, but I dare say there's just no way possible you could match that performance in any software implementation. I think the main thing holding back Quartz 2D Extreme, however, is accuracy.  I tried using it for a few days on my Powerbook, and there were significant (and consistent) graphical glitches in numerous places.  In addition, the performance improvement in most cases was negligible.  In fact, I think the Ars article mentions that in some cases performance actually decreases. So, while I may seem to have digressed, my point is that Quartz is probably going to be faster in the long run, if it isn't already, but don't necessarily hold your breath on huge improvements (e.g. Quartz 2D Extreme). Wade Tregaskis (AIM/iChat, Yahoo &amp;amp; Skype: wadetregaskis, ICQ: 40056898, MSN, audio/video iChat &amp;amp; email: email@hidden, Jabber: email@hidden) -- Sed quis custodiet ipsos custodes?</body>
  </mail>
  <mail>
    <header>Drawing Core Image into a PDF Context</header>
    <body>Upon trying to draw a CIImage into a PDF context, I was surprised to see that area of the context show up completely black. Is it the case that you can only render a CIImage into a pixel-based context?</body>
  </mail>
  <mail>
    <header>Re: New to quartz;	avoiding triple-buffering when porting win32 code?</header>
    <body>On Jun 17, 2005, at 8:28 AM, Simon Finne wrote: For many operations Quartz may be faster than QuickDraw.  For others, it is not.  It depends on what you are trying to draw. For this application, however, the speed of your engine may be eclipsed by the triple buffering scheme.  The application would have to move main memory from your back buffer to the windows back buffer and then from the window's back buffer to the video card.  That seems like an awful lot of memory copying. Exactly. You can use the &amp;quot;lockFocus&amp;quot; method of an NSView to activate its graphics context (i.e. [myView lockFocus].  You can then draw into the view and when you are done you should call [myView unlockFocus]. This is generally frowned upon because the system can, potentially, synchronize the calls to drawRect with whatever else may be happening in the system. It is much more efficient to do ALL of your drawing in the drawRect method.  If something outside of that routine needs the display to be updated, you should make a note of what needs to be drawn and call [myView setNeedsDisplayInRect: someRect] (or any of the setNeedsDisplay routines).  At that point the system will make note of the fact that your view needs drawing and will ensure that drawRect is called at the most appropriate time. Since your application seems to be doing some animation... another option is to let your engine run and return to the event loop without drawing or calling setNeedsDisplay.  You can then set up an event timer that fires at some reasonable time (say 30 times a second) which calls the setNeedsDisplay on your view.  Only when you get that message, then, would you collect your graphics and draw them.</body>
  </mail>
  <mail>
    <header>New to quartz; avoiding triple-buffering when porting win32 code?</header>
    <body>I've searched the list archives and googled the last couple of days, but haven't found an answer to this problem which I however assume is rather trivial. I'm porting a win32 application which has the following basic structure: mainloop: poll &amp;amp; handle events draw changes to window backbuffer draw changes in backbuffer to screen Actually the port has been done for a while, it was started on Jaguar and finished on Panther, but because my own benchmarks gave the result that QuickDraw was the most efficient way to draw graphics I was simply using an NSQuickDrawView and then porting was rather straight-forward. Now in Tiger it however seems that Quartz has surpassed QuickDraw in speed (is this correct?). My problem with switching to Quartz (this problem was there also in the port using QuickDraw, but I'd like to correct it now) is that I would like to avoid having an &amp;quot;extra backbuffer&amp;quot;, as each window already has it's own backbuffer it feels unnecessary to have another backbuffer allocated by the application into which all drawing is first done which is then copied to the window's backbuffer. Basically my question is: Is there any way to draw directly to the backbuffer of the window, outside of the drawRect method in my NSView? The problem is that while handling events the application will need to update graphical elements, and I want to avoid having an extra backbuffer. Regards, Simon</body>
  </mail>
  <mail>
    <header>how to display pdf onmacos 9</header>
    <body>i found PDF Kit only use macos X 10.4 and later. and CGPDFDocumentCreateWithURL(and other) only use macos X 10.0 and later. but i must read and display some pdf file on macos 9, using codewarrior 8. how can i do? thanks. &amp;nbsp;&amp;nbsp;Ê≥®ÂÜåÁôªÂΩïÁΩëÊòìÈÇÆÁÆ±ÔºåÊØèÂë®ÈÉΩÊúâÊÉäÂñúÔºå50ÂÖÉÁé∞ÈáëÂà∏ÂÆåÂÖ®ÁôΩÈÄÅÔºåÁÇπÂáªÊü•ÁúãÊ¥ªÂä®ËØ¶ÊÉÖÔºÅ</body>
  </mail>
  <mail>
    <header>Re: CGContextSetLineWidth / performance issues</header>
    <body>On Jun 16, 2005, at 11:17 AM, nick briggs wrote: OK... the other optimization that Apple suggests is only putting in as many lines as you strictly need.  If your trace is 800 pixels wide, then 800 segments seems reasonable.  If your window is only 100 pixels wide, however, trying to squeeze 800 points in there may be more than you need.  You might be able to filter the data. If you're displaying an oscilloscope display, another thing you're likely to run into is the need to separate the data capture loop from the display loop.  If these two things are tightly coupled, you can run into problems where the refresh rate of the display can limit the speed at which you capture voltage data. Scott</body>
  </mail>
  <mail>
    <header>Re: CGContextSetLineWidth / performance issues</header>
    <body>CGContextStrokeLineSegments does improve performance significantly... for my tiger users</body>
  </mail>
  <mail>
    <header>Re: Wrong colors when applying CoreImage filters</header>
    <body>Displaying the image is not the goal at the moment. Applying a CoreImage filter is one step of many processing operations. The output as unsigned char* is what is passed to the next process. Regards JJ Send instant messages to your online friends</body>
  </mail>
  <mail>
    <header>Re: Wrong colors when applying CoreImage filters</header>
    <body>Thank you for your suggestion but my data is ARGB. As it seems from the CIImage.h that ARGB format is the only choice we have when working with 8 bit data, RGBA was excluded right from the beginning. Regards JJ Send instant messages to your online friends</body>
  </mail>
  <mail>
    <header>Re: Problems with running texture-base CIImage through	custom	CIFilter</header>
    <body>On May 30, 2006, at 11:28 AM, Frank Doepke wrote: filed as #4567067.</body>
  </mail>
  <mail>
    <header>Re: Problems with running texture-base CIImage through	custom	CIFilter</header>
    <body>Its probably easiest, if you file a bug to track this down. Please attach your system profile and the sample code. On May 30, 2006, at 12:21 AM, Timothy Wood wrote:</body>
  </mail>
  <mail>
    <header>Wrong colors when applying CoreImage filters</header>
    <body>Hi everyone, My first test of CoreImage does not seem to go well. I the camera simulation movement seems correct however there is an inexplicable (to me of course) shift of colors: black and red appear blue, green becomes pink, etc. The image is an RGB with an alpha constant everywhere (value 255). What am I doing wrong please? Many thanks JJ ================= size_t imageWidth, imageHeight; // Known NSBitmapFormat bitmapFormat = NSBitmapFormat( NSAlphaFirstBitmapFormat + // set ioData here as interleaved ARGB. CGColorSpaceRef colorSpaceRef = CIImage* inCIImage = [[CIImage alloc] initWithBitmapData: [NSData dataWithBytesNoCopy: ioData length: bitmapByteCount freeWhenDone: NO] bytesPerRow: bytesPerRow size: CGSizeMake( imageWidth, imageHeight) format: kCIFormatARGB8 colorSpace: CIFilter* motionBlur = [CIFilter filterWithName: [motionBlur setValue: [NSNumber numberWithFloat: [motionBlur setValue: [NSNumber numberWithFloat: CIImage* outCIImage = [filter valueForKey: // Create a bitmap rep NSBitmapImageRep* bitmapImageRep = [ [NSBitmapImageRep alloc] initWithBitmapDataPlanes: NULL pixelsWide: imageWidth pixelsHigh: imageHeight bitsPerSample: bitsPerSample samplesPerPixel: samplesPerPixel hasAlpha: YES isPlanar: NO colorSpaceName: NSDeviceRGBColorSpace bitmapFormat: bitmapFormat bytesPerRow: bytesPerRow bitsPerPixel: (bitsPerSample*samplesPerPixel) // Draw on the bitmapImageRep NSGraphicsContext* nsgContext = [NSGraphicsContext CGContextRef cgContext = (CGContextRef)[nsgContext CIContext* myCIContext = [CIContext [myCIContext drawImage: outCIImage atPoint: CGPointZero fromRect: CGRectMake( 0, 0, imageWidth, unsigned char* ioData = (unsigned // When I do the drawing from ioData here I get this strange color shift! Send instant messages to your online friends</body>
  </mail>
  <mail>
    <header>Re: Problems with running texture-base CIImage through custom	CIFilter</header>
    <body>On May 29, 2006, at 11:49 PM, Timothy J. Wood wrote: I should mention that this was on a GF6600 on a 10.4.6 G5. Running on a MBP, I get a message on the console: CoreImage: ROI is not tilable: DIVY DOD [-419.876,-629.814 839.752x1259.63] ROI [-27,-354 28x41] RGBA_14 APPLY csync_curve0 DOD [-419.876,-629.814 839.752x1259.63] ROI [-27,-354 28x41] RGBA_14 APPLY matrix_ DOD [-419.876,-629.814 839.752x1259.63] RGBA_14 AFFINE [2.09938 0 0 2.09938 0 0] DOD [-419.876,-629.814 839.752x1259.63] RGBA_14 APPLY null DOD [-200,-300 400x600] RGBA_14 AFFINE [1 0 0 1 -200 -300] DOD [-200,-300 400x600] ARGB_8 IMAGE CIImage:0x361160 DOD [0,0 400x600] ARGB_8 APPLY csync_curve0 DOD [-419.876,-629.814 839.752x1259.63] ROI [-27,-334 28x21] RGBA_14 0xbfffa2b8 I'd previously tried setting a ROI selector, but it never got called.  Anyway, I think this error message would be useful if it a reason _why_ it isn't tilable, what I might do about it, etc.</body>
  </mail>
  <mail>
    <header>Problems with running texture-base CIImage through custom CIFilter</header>
    <body>I've narrowed down a problem I'm having to the following: - CIImage built from a rectangle texture - Translate via -imageByApplyingTransform: - Run through a CIFilter that does nothing (looks up input, returns) - Transform again via CIAffineTransform. - render output to GL When I render, my texture comes up in all manner of crazy permutations in my real app (which actually tries to do something in its filter :). In the test app, they are a little tamer, but still hokey, even with a filter that is doing nothing. If I skip my null filter, then all draws reasonably.  If I use a non-texture based image, all is fine (say, from a CGLayer).  The null filter in my test case looks like the following (full app source is @interface NullFilter : CIFilter @end static NSString *KernelSource = return [self apply:k, src, kCIApplyOptionDefinition, [src</body>
  </mail>
  <mail>
    <header>Re: DisplayLink setup on macbook pro fails...</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Using screen image as input to CoreImage / CoreVideo</header>
    <body>On May 26, 2006, at 2:25 AM, Ken Tabb wrote: Depending on the machine you're on (i.e. Svideo out on a PowerBook), you could mirror the output as video and feed it back in via FireWire as DV... --&amp;gt; Michael B. Johnson, PhD --&amp;gt;  (new &amp;amp; improved!) --&amp;gt;  (alum) --&amp;gt; MPG Lead --&amp;gt; Pixar Animation Studios</body>
  </mail>
  <mail>
    <header>Using screen image as input to CoreImage / CoreVideo</header>
    <body>is there an easy way to use the image from the monitor (eg. your desktop) as input to a CoreImage / CoreVideo plugin? I can't see an obvious way, the closest I could muster was to use the screencapture command line tool to get successive images, and use that sequence. Of course that route is a bit choppy. Suffice it to say I'm reasonably familiar with CoreImage, but not at all with CoreVideo (yet). Thanks in advance for any tips / light you can shed, Ken - - - - - - - - - - Dr. Ken Tabb Mac &amp;amp; UNIX Developer - Health &amp;amp; Human Sciences Machine Vision &amp;amp; Neural Network researcher - School of Computer Science University of Hertfordshire, UK</body>
  </mail>
  <mail>
    <header>Re: How to fix glGrab.c for Intel machines? (Was: Re: Fast screen shots for VNC like functionality...)</header>
    <body>Thanks, that is indeed it. I knew there had to be some simple way, but I am afraid I had little understanding of this constants... david.</body>
  </mail>
  <mail>
    <header>Re: How to fix glGrab.c for Intel machines? (Was: Re: Fast screen	shots for VNC like functionality...)</header>
    <body>I only scanned the code, but it looks like if you replace the GL_UNSIGNED_INT_8_8_8_8_REV with GL_UNSIGNED_INT_8_8_8_8 in the glReadPixels call, it will be swapped. -:sigma.SB</body>
  </mail>
  <mail>
    <header>Re: How to fix glGrab.c for Intel machines? (Was: Re: Fast screen shots for VNC like functionality...)</header>
    <body>I understand that there are bytes to be swapped, however, my question is what the best way would be, without loosing all the speed advantages of the glGrab.c Mike posted about a month ago. I am hoping there is a way without manually swapping those bytes :-)</body>
  </mail>
  <mail>
    <header>Re: Writing a CIKernel that returns a rectangle?</header>
    <body>A filter can only produce an image, and you would have to render this image into a bitmap context or read the pixels from the OpenGL context.</body>
  </mail>
  <mail>
    <header>Writing a CIKernel that returns a rectangle?</header>
    <body>is it possible to get something else than pixels out of a CIFilter? I am thinking about finding the bounds of the non- transparent zone in a CIImage. Currently the only solution I found was to draw onto an nsbitmaprep and scan the pixels from there. Could I do this without transferring the image from the GPU to the main memory? Is there a way that a CIKernel could access some static values that could then be returned by the CIFilter? Thanks, Jacques</body>
  </mail>
  <mail>
    <header>Re: How to fix glGrab.c for Intel machines? (Was: Re: Fast screen shots for VNC like functionality...)</header>
    <body>Hi guys, Just swap the alpha and blue channels. Problem solved. -Chilton</body>
  </mail>
  <mail>
    <header>How to fix glGrab.c for Intel machines? (Was: Re: Fast screen shots for VNC like functionality...)</header>
    <body>-- originally posted a few days ago, but now changed the title to better match the question Your code sample works pretty well, except for one thing. On Intel machines the colors look pretty awkward. I am sure some swapping needs to take place, but not being a quartz/opengl specialist, I am not sure what the best approach would be. Would you mind indicating what would need to be done to make the sample code work correctly on both Intel and PowerPC? david.</body>
  </mail>
  <mail>
    <header>Re: How to create a custom cursor using Carbon ?</header>
    <body>I was experiencing the same problem. I found out that Cocoa cursors will only show up (in a Carbon app) after having created a Cocoa window. The following code (called once at startup) will do the trick: it to work. I ended up faking a cursor using a little overlay window. show up.</body>
  </mail>
  <mail>
    <header>Re: Using flters inside a .qtz</header>
    <body>OK I know. But what I am interested in is to let the user create the processing chain in Quartz Composer because it provides a GUI for that, then load that chain from code. But this has been answered in the QC list and it is not possible at this time.</body>
  </mail>
  <mail>
    <header>Re: CoreVideo stopping after 1 second</header>
    <body>Thanks a lot ! That's it... I had a tight loop that probably prevents MoviesTask() to be called in the Cocoa run loop. Calling MoviesTask() in my own loop fixes the problem.</body>
  </mail>
  <mail>
    <header>Re: Using flters inside a .qtz</header>
    <body>Sounds like you just want to use CoreImage image units directly. A Quartz Composer document is, as I understand it, a composition of image units and settings that any application can load. There is no reason you can't compose image units in your own code and drive the processing yourself. See here for starters: -- -Corey O'Connor</body>
  </mail>
  <mail>
    <header>Re: CoreVideo stopping after 1 second</header>
    <body>Just from the top of my head, that sounds like that MoviesTask() is not getting called. On May 24, 2006, at 1:18 PM, Dr. William H.A. Beaudot wrote:</body>
  </mail>
  <mail>
    <header>CoreVideo stopping after 1 second</header>
    <body>Hi, I added CoreVideo capability in my app based on the QTCoreVideo101 example. Everything seems to work fine except movie playback stops exactly after 1 second, that is after the number of frames specified as the movie frame rate. This problem occurs for all movies, that otherwise play ok with QTCoreVideo101. I pinpointed the source of the problem to the QTVisualContextIsNewImageAvailable call that returns NO after exactly 1 second. Any idea why this happens and how to fix it??? Thanks in advance,</body>
  </mail>
  <mail>
    <header>Re: Efficient drawing OpenGL vs. CoreImage</header>
    <body>On May 20, 2006, at 1:27 PM, Federico Tessmann wrote: There is a good chance that your screen flushes are running afowl of the screen refresh mechanism on 10.4.  On 10.4, if you explicitly flush one of your buffers and then try to draw to it again before the system has fully flushed it to the display then the system will block your application.  Your timer interval of 50Hz sounds like it might be just close enough to the refresh rate of the monitor that it's giving you fits. You might look into combining your OpenGL solution with Core Video. Core Video is the best bet you have for providing animation at a fixed refresh rate.</body>
  </mail>
  <mail>
    <header>Re: How to create a custom cursor using Carbon ?</header>
    <body>On May 23, 2006, at 2:01 AM, Graeme Gill wrote: It might seem this way at first, but in general this is not a problem. There is going to be an impedance mismatch between cross platform code and any platform specific services you need.  Moveover if the service is platform specific  on one platform, it is likely to be specific to any other platforms as well (i.e. unless you use OpenGL, the graphics libraries of modern OSes are all different API). Typically, the operating system services you have to call through Objective-C are prime examples of this (many represent technologies that don't even exist on other platforms)... even if the calls themselves were straight C routines, they wouldn't do you any good whatsoever on another platform. If you want to create a cross platform API you will typically wrap that code in your own abstraction layer.  Since the service is likely to be platform specific on every platform you're going to need a &amp;quot;Horrible Translation Layer&amp;quot; on every platform anyway.  Calling Objective-C in the Macintosh implementation of that layer is requires very little in the way of logistics.</body>
  </mail>
  <mail>
    <header>Re: How to create a custom cursor using Carbon ?</header>
    <body>On May 23, 2006, at 8:29 AM, Graeme Gill wrote: Hmm, so it sounds like your app's main thread is just blocked waiting for some reply from hardware, and you're not processing user input events at all. In that case, my suggestion wouldn't apply. You can't use a separate thread for monitoring the events I suggested, since they wouldn't be sent to that event. It sounds like Terminal is becoming the active application when you click the mouse button. In that case, the cursor becomes under control of Terminal at that point, and your application no longer has any control over its visibility. Does it matter very much whether the cursor becomes visible when you click in Terminal? If, during normal use, you won't be activating any other apps, then you probably don't care whether the cursor becomes visible anyways.</body>
  </mail>
  <mail>
    <header>Re: How to create a custom cursor using Carbon ?</header>
    <body>Thanks, this seems a pretty reasonable approach. Unfortunately the application is not arranged to service events once the window has been painted with a test color. This doesn't cause problems, because the windows class and attributes make it have a fixed position, it is over any other window, and doesn't need to be accessed by the user at all. The process is waiting for the instrument to reply, or the user to hit a key on stdin indicating an abort. I not sure I want to add a separate thread to service window events to track the cursor, although I may look into how complicated it would be. Sorry, I wasn't very specific. The cursor disappears as soon as my application starts (it having done a HideCursor()), so if I want to abort the application I have to select the terminal window it was started from, so I click the mouse button, and the cursor appears again. Probably the pointer wasn't located in the application window when it was clicked, which presumably explains why the cursor appears - the HideCursor() only applied to the application thread, not other threads (I'm guessing. The HideCursor() documentation doesn't explain these sort of details, but the lack of any arguments is suspicious) &amp;gt; I didn't see that problem  for No controls, it's just a color test window. There are no other GUI elements, as the application is controlled from a terminal. Graeme Gill.</body>
  </mail>
  <mail>
    <header>Re: How to create a custom cursor using Carbon ?</header>
    <body>On May 23, 2006, at 12:01 AM, Graeme Gill wrote: Since Carbon does not support automatic cursor switching, let's consider approaches that allow you to do it manually. It does seem to me that you could implement this quite easily using two event handlers for kEventWindowCursorChange and kEventMouseMoved. Install these on the application event target. When you get the CursorChange event, check the window parameter in the event; if it's the window that should hide the mouse, then hide it with HideCursor(). When you get the MouseMoved event, you'll know that the mouse is not over one of your windows, so you can show the cursor with ShowCursor(). I wrote a quick test app that does this and, at a first approximation, it seems to work pretty well. You mentioned in a previous message that when you clicked in the window, the cursor became visible again. I didn't see that problem for normal clicks in my test window, but I was just using standard HIToolbox controls. What kind of content does your window use - standard controls, or your own custom content?</body>
  </mail>
  <mail>
    <header>Re: Fast screen shots for VNC like functionality...</header>
    <body>Your code sample works pretty well, except for one thing. On Intel machines the colors look pretty awkward. I am sure some swapping needs to take place, but not being a quartz/opengl specialist, I am not sure what the best approach would be. Would you mind indicating what would need to be done to make the sample code work correctly on both Intel and PowerPC? david.</body>
  </mail>
  <mail>
    <header>Re: How to create a custom cursor using Carbon ?</header>
    <body>This seems like a really bad move to me. There's a bigger world for applications than just MAC OS X, and confining full functionality to a language that's effectively platform specific, seems like an attempt at &amp;quot;lock in&amp;quot;. Since it's easy to call C code from C++ and (presumably) Objective C, not to mention lots of other languages (ie. scripting languages), it would seem to me that the sensible approach is to create base system API's to be C compatible, and base other language bindings on top of that. The alternative for cross platform development is probably a horrible translation layer, where Objective C API's are encased in C, so that they can be invoked from other languages. Needless to say, if that is the only alternative, full MAC OS X support will probably lag behind, and there will be fewer applications for the platform. It's not that complicated. For my particular window, I want to make sure that the cursor can't get in front of it. So either the cursor should be hidden whenever its in my window, or it should switch to a transparent cursor. Having the window manager switch the cursor is what I'd like, since I don't want to have to create a thread to service window events. Graeme Gill.</body>
  </mail>
  <mail>
    <header>Re: How to create a custom cursor using Carbon ?</header>
    <body>On May 22, 2006, at 9:37 PM, Graeme Gill wrote: OK, but keep in mind that Objective C is probably the most common language being used today to provide new system services in Mac OS X. It will become more and more difficult to build a fully savvy Mac OS X application without using some Objective C-based APIs. It would behoove all Mac OS X developers to become familiar with Objective C. Could you describe again exactly what you're trying to accomplish? Maybe I've misunderstood your goals. You mentioned setting a custom cursor per window, but you also mentioned wanting to hide the cursor. I'm not quite sure how these intersect.</body>
  </mail>
  <mail>
    <header>Re: How to create a custom cursor using Carbon ?</header>
    <body>Cross platform compatibility, historical reasons, and simplicity. If I mix in any other language, it will be C++ to be able to use WxWidgets, again for cross platform compatibility. No, but I was hoping that setting an invisible cursor in QD might operate differently. It doesn't. Sounds overly complicated for something so simple on other platforms, but I assume this is more flexible.</body>
  </mail>
  <mail>
    <header>Re: How to create a custom cursor using Carbon ?</header>
    <body>On May 22, 2006, at 7:47 PM, Graeme Gill wrote: Any particular reason? You can compile your C code with the ObjC compiler and thereby call ObjC APIs, and if your code really isn't compatible with ObjC (say you use variables named &amp;quot;id&amp;quot;), then you could just put the ObjC code into a separate source file. So if you're just trying to hide the cursor, is there any reason you can't just call HideCursor or CGDisplayHideCursor? We will keep that in mind (although it's more likely we'll provide an API that allows you to set a cursor as an HIView attribute. You could always pass the window's root view to cover the etnire window.)</body>
  </mail>
  <mail>
    <header>Re: How to create a custom cursor using Carbon ?</header>
    <body>Thanks for the suggestions. As far as I can tell, NSCursor is Cocoa, so no good for my C only code, and I tried QD, but it doesn't really work. Yes the cursor initially disappears (from the whole screen, which is not what I want), but if the user then clicks the mouse, it comes back again. Given this, I'm not sure if there's any point in trying to implement the trickier business of tracking the cursor, and changing it when it's over my window using QD. It's into diminishing returns for me at this point. There are enough headaches in trying to figure out how to use IOI2CSendRequest to keep me busy. A higher level interface for controlling the monitor settings would be a good thing too, but I shouldn't complain too much, at least there is an interface (unlike win32!), and at least it's associated with the display (unlike Linux!). I do hope the new Carbon API is a little smarter, and allows setting cursors as a window attribute ...</body>
  </mail>
  <mail>
    <header>Re: How to create a custom cursor using Carbon ?</header>
    <body>I'm not sure, but I think I just completely failed to get the cursor to show up. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: How to create a custom cursor using Carbon ?</header>
    <body>If you mean QDRegisterNamedPixMapCursor, it doesn't work on all Macs, so I considered that unacceptable.  And I needed cursors bigger than 16x16, so I couldn't use older QuickDraw calls. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: How to create a custom cursor using Carbon ?</header>
    <body>On May 22, 2006, at 10:22 AM, James W. Walker wrote: I certainly have no inside information, but I'd say that in this case you're almost certainly going to be fine using the deprecated QuickDraw calls.  If it's a concern the future safe thing to do would be to add a runtime check and use the new API for Leopard (once that's available). Nick</body>
  </mail>
  <mail>
    <header>Re: How to create a custom cursor using Carbon ?</header>
    <body>On May 22, 2006, at 9:22 AM, James W. Walker wrote: Interesting. Do you remember what problems you ran into?</body>
  </mail>
  <mail>
    <header>Re: How to create a custom cursor using Carbon ?</header>
    <body>On May 22, 2006, at 7:56 AM, Eric Schlegel wrote: I once tried using NSCursor in a Carbon app, but was unable to get it to work.  I ended up faking a cursor using a little overlay window.</body>
  </mail>
  <mail>
    <header>Re: How to create a custom cursor using Carbon ?</header>
    <body>It appears not.  Not yet, at any rate.  (Ah, I see Eric responded already.)  Here's the sequence of calls we use: NewGWorld() with the desired dimensions, probably with bits per pixel == 32. The trickiest part for us, actually, was making this per-window, as Eric mentioned.  The technique I used was to set the name of the current cursor to be used on the window using SetWindowProperty, and then to check this property and respond appropriately during each mouse event at any window.  (Note that it may be wise for you to use QDIsNamedPixMapCursorRegistered at this time if you may tear down the cursor before removing it from all windows using it.)</body>
  </mail>
  <mail>
    <header>Re: How to create a custom cursor using Carbon ?</header>
    <body>On May 22, 2006, at 6:10 AM, Graeme Gill wrote: There is no Carbon API for setting a cursor specifically for a window. The existing Carbon API only supports setting a cursor for an entire process. It's up to you to set the cursor appropriately as the mouse moves into and out of your window; this would usually be done using a mouse tracking region (HIViewNewTrackingArea) or a kEventMouseMoved or kEventWindowCursorChange Carbon event. There is also currently no undeprecated Carbon API for creating a custom cursor. We plan to add Carbon API for this task in Leopard. Until then, we recommend using either the deprecated Quickdraw API, or NSCursor.</body>
  </mail>
  <mail>
    <header>How to create a custom cursor using Carbon ?</header>
    <body>After searching for some considerable time on developer.apple.com, I'm unable to locate the Carbon API's for setting a custom cursor for a window. Deprecated QuickDraw functions, and Cocoa functions I've found, but not Carbon. Do they exist ? Unfortunately the cursor I'm after isn't available as one of the standard themed cursor (I need an invisible cursor for my window, so it can't block the window contents). I was unable to locate a Window attribute that would place the window over the cursor. CGDisplayHideCursor() does hide the cursor for the whole screen, but it comes back again if you click with the mouse etc., and it's awkward being without a cursor for other windows. What I'm after is the Carbon equivalent of X11's XCreatePixmapCursor(), or Win32's SetClassLong(..GCL_HCURSOR..). Graeme Gill.</body>
  </mail>
  <mail>
    <header>Using flters inside a .qtz</header>
    <body>Is it possible to use the filters inside a .qtz file without rendering (no use of QCRenderer renderAtTime:) ? What I'd like to have is a sequence of filters in a .qtz, lets say Gaussian, followed by (connected to the input of) Sepia, publishing the first filter input and last filter output. Then open the .qtz as a QCRenderer and feed the first filter a CIImage and get the output CIImage for rendering with CIContext's drawImage: So far, this does not seem to be possible. Is it ? Do I have to render using QCRenderer ? Do I have to render in a pixel buffer and load it as a gl texture in a new CIImage or is there a shorter route (QCRenderer uses CIImage internally...) ? Raphael</body>
  </mail>
  <mail>
    <header>Re: Efficient drawing OpenGL vs. CoreImage</header>
    <body>CoreImage sits on top of GL and, I think, GL shaders. Thus if you use CI the functionality of CI is going to rapidly speed up your development time. CI provides a higher level imaging API than GL itself. So, choose GL only if you don't need the associated CI functionality (which is significant). Actually I don't imagine that CI adds a great deal to slowing performance at all... Threads don't necessarily increase performance. Unless you can identify some true areas of parallelism then threads won't help. Only one thing can ever draw to a gl context at any one time. Additionally, unless you have multiple CPUs, threads might not pay off for you either. Finally GL (and I suspect CI) is not thread-safe so you'll have to mutex it all yourself. Sounds like threads are not required here. Mutexes. However a common strategy is to handle drawing just in the main thread (also commonly referred to as the UI thread). I don't really see this relationship - input is different to output - I don't see the two things being associated with each other strictly speaking. I suspect that your juddering can be cured with double buffering and flush synchronisation with display refreshes. There's a lot on this subject out there. Lookup Opengl animation. Cheers, -C</body>
  </mail>
  <mail>
    <header>Re: CoreImage Accelerated or not How do I detect it?</header>
    <body>Yes another option would be to setup your state in such a manner that when you are ready to render your primitives you would call: CGLGetParameter(CGLGetCurrentContext(), kCGLCPGPUVertexProcessing, CGLGetParameter(CGLGetCurrentContext(), kCGLCPGPUFragmentProcessing, Which would tell you whether the GPU will process (or is currently processing) via the programmable vertex and or fragment pipelines. Max Rupp OpenGL / Core Video Programming email@hidden</body>
  </mail>
  <mail>
    <header>Working with PDFlib; and slamming the wall</header>
    <body>Greetings: I've downloaded pdflib verson 6.0.1 from pdflib.com.   What I got was some demo and doc files.   I'm assuming the libpdf.a library is installed somewhere globally. Nevertheless, I've created a test (testC++) routine via XCode 2.1 containing the 'hello.cpp' and 'hello.hpp' source with links to the pdflib.cpp, pdflib.hpp and pdflib.h The following error was encountered during the link part of the build: [Session started at 2005-07-21 16:20:43 -0700.] ZeroLink: unknown symbol '_PDF_get_api' 1) How do I solve this? and 2) I notice that I have the libpdf.a file on disk (via spotlight). Does the suffix '.a' mean a static library? 3) Where would be a good place to place 3rd-party static libraries and their associated header files? Thanks for your help!</body>
  </mail>
  <mail>
    <header>CoreImage Resolution Independence</header>
    <body>My application is editing CoreImage effects at screen resolution (i.e., 72 dpi) and outputting them at a higher final resolution (e.g., 300 dpi).  For output I'm supplying source images at the final resolution, but I'm unsure how to scale the other CI filter parameters.  I've played with two options for this but neither is completely satisfactory. Option 1)  Scale the input CIImage(s) down to screen resolution using imageByApplyingTransform, apply the filters exactly as in the screen case, and scale the result back up.  This provides somewhat better results than using screen resolution inputs but the image processing is clearly happening at screen resolution and the final result is fuzzy. Option 2) Scale each filter's &amp;quot;distance&amp;quot; parameters types (e.g., distance, rectangle, etc.) when applying the filters.  This does the image processing at the correct resolution but some parameters don't scale correctly.  For example the &amp;quot;cross width&amp;quot; parameter of &amp;quot;star shine&amp;quot; filter is apparently being applied in two dimensions so it needs to be scaled by the square of the scale factor to get the correct result. Surely this isn't an unusual requirement.  Can anybody give me any advice or suggestions? Thanks, Nick</body>
  </mail>
  <mail>
    <header>ColorSpace problems</header>
    <body>Hi, right now I am trying to write an app that extract images out of PDF files. I use the &amp;quot;CGColorSpaceCreate*&amp;quot; functions to create the approrpiate colorspace but some of these function give me some trouble. (1) CGColorSpaceCreateIndexed : The resulting image is grayscale - no colors. // Only the definitions - all these variables hold valid values int       lastIndex, bpc; // bpc = bits per component // Only a grayscale image CGColorSpaceRef cspace = CGColorSpaceCreateIndexed (CGColorSpaceCreateDeviceRGB(), lastIndex, CFDataGetBytePtr CGDataProviderRef dprov = CGDataProviderCreateWithData(NULL, CFDataGetBytePtr(data), CGImageRef newImg = CGImageCreate(width, height, bpc, bpc * comp, width * comp * (bpc / 8), cspace, kCGImageAlphaNone, dprov, NULL, FALSE, : and then pass &amp;quot;DeviceRGB&amp;quot; as colorspace then it works. Whats the problem? (2) CGColorSpaceCreateICCBased : The &amp;quot;range&amp;quot; isn't in PDFs - anly alternative besides just using the alternative Colorspace or N? Thanks alot, Patrick -- Patrick Gleichmann (</body>
  </mail>
  <mail>
    <header>Re: Crash converting CVOpenGLTeexture to NSImage (using CIImage)</header>
    <body>Allow me to restate my problem: I want to capture the result of a sequence of Core Image filters into main memory (where I will do a bunch of image processing things), but I don't want to make any modifications to the image that the user will actually see. So I draw the original, untouched image to the CIContext I created from an NSOpenGLView, and then get the result of the filtering operations into &amp;quot;some other&amp;quot; context in main memory for the additional processing. The only other way I can think of is to render the filtered image into a pbuffer, then read that back into main memory to do my work. It sounds reasonable, but it's not really the one-line approach that drawing the image into a NSBitmapImageRep-backed CIContext appears to be.</body>
  </mail>
  <mail>
    <header>URLs supported by e.g. CGDataProviderCreateWithURL</header>
    <body>Can routines like CGPDFDocumentCreateWithURL which accept CFURLs as parameters accept &amp;quot;any old URL&amp;quot; or are they limited to using only file URLs? If they can handle generic URLs, is the file at the destination read when the data provider is created, or when the program first asks for the data?</body>
  </mail>
  <mail>
    <header>Re: Crash converting CVOpenGLTeexture to NSImage (using CIImage)</header>
    <body>If you know that you want to touch the pixels in main memory, use a QTPixelBuffer visual context instead.</body>
  </mail>
  <mail>
    <header>Re: Crash converting CVOpenGLTeexture to NSImage (using CIImage)</header>
    <body>This is precisely what we need. Our principal usage of the decompression session is to view the live video live. I guess initWithCVImageBuffer: would also fetch from the video memory if given a CVOpenGLTexture. If you want to get all the images in main memory, may be you should use QTPixelBufferContextCreate instead of QTOpenGLTextureContextCreate. Jean-Alexis Montignies</body>
  </mail>
  <mail>
    <header>How to get a image representation of the PDFView with text selected</header>
    <body>Hi All I need to draw a image representation of the PDFView. Seems NSPDFImageRep can do this job by sending the pdf data to it. However, I wanna draw the PDFView after some texts are selected (highlighted), which means the selected (highlighted) text should also appears as being selected in the image. Can anyone tell me how to do this? Thanks. Huaixing</body>
  </mail>
  <mail>
    <header>Re: QuickDraw to Quartz ... Point &amp;amp; RgnHandle in Carbon Events?</header>
    <body>Someone pointed out to me offline that you might possibly mean here to &amp;quot;undo&amp;quot; the swap once, ever. The context that you get from a kEventControlDraw event should not be considered permanent. The context can only really be considered valid while in your event handler. Any changes that you make to the context will not be permanent. Also, as you mentioned, other events will be passing you coordinates like mouse locations, etc, which will require conversion to your preferred coordinate system. I think that some of this was mentioned in an excellent WWDC presentation this year. Anyone remember the details of which one? ----- David McLeod email@hidden</body>
  </mail>
  <mail>
    <header>Re: Crash converting CVOpenGLTeexture to NSImage (using CIImage)</header>
    <body>It would be much nicer to get the frame data before it gets sent to the video card, rather than read it back from video memory once it's been uploaded. Since both our code is crashing inside the implementation of CVPixelBuffer (&amp;quot;A pixel buffer stores an image in main memory&amp;quot;), I'm hopeful that this will be both possible and faster than this OpenGL read-back approach. Your approach works fine, but hopefully we'll get a real solution to our problem shortly.</body>
  </mail>
  <mail>
    <header>Re: QuickDraw to Quartz ... Point &amp;amp; RgnHandle in Carbon Events?</header>
    <body>Great. Great. Not that it has much to do with your context's CTM unless you are porting your QD code and don't want to flip all of your drawing calculations. Yup. There is a CGContextDrawImage (which HIViewDrawCGImage is wrapped around), but there isn't a CGDrawThemeTextBox. HIThemeDrawTextBox does a lot on the inside (theme font/shadow lookups, layout caching, etc) and can deal with either orientation of your context. You might want to stick with it. ----- David McLeod email@hidden</body>
  </mail>
  <mail>
    <header>Re: QuickDraw to Quartz ... Point &amp;amp; RgnHandle in Carbon Events?</header>
    <body>Recap time for me. a) I'm comfortable with (0.0f, 0.0f) as the bottom-left b) I don't want to use QD ever again c) I &amp;quot;undo&amp;quot; HIView coordinate swap when first created, d) I never call either of HIViewDrawImage or HIViewDrawThemeTextBox but always their Quartz equivalents (I suspect: CGDrawImage and CGDrawThemeBox) then I can forget about swapping the coordinate system, except when I want to do it for my own purpose, using standard trigonometric transformation relying on the standard trigonometric math model. Many thanks. -- Jean-Fran√ßois Brouillet email@hidden</body>
  </mail>
  <mail>
    <header>Re: QuickDraw to Quartz ... Point &amp;amp; RgnHandle in Carbon Events?</header>
    <body>OK, that's only been what... ten years I've been saying that wrong in my head?  And for almost all of those ten years, only I heard it!  lol</body>
  </mail>
  <mail>
    <header>Re: QuickDraw to Quartz ... Point &amp;amp; RgnHandle in Carbon Events?</header>
    <body>On Jul 19, 2005, at 4:23 PM, Bryan Pietrzak wrote: Nope... Read your PostScript Red Book.  It's &amp;quot;Current Transformation Matrix&amp;quot;.</body>
  </mail>
  <mail>
    <header>Re: QuickDraw to Quartz ... Point &amp;amp; RgnHandle in Carbon Events?</header>
    <body>On Jul 19, 2005, at 3:15 PM, Jean-Fran√ßois Brouillet wrote: When Quartz initializes the context, it places the origin in the lower left corner.  HIView changes that to the top left, and reflects the Y axis so that it matches the QuickDraw coordinate system as a convenience for programmers that are using HIView as a replacement for their old control manager routines.  That way, the graphics you draw, and the positioning of sub-views all occur in the same coordinate system. When you draw Images and Text with Quartz, however, they always draw so that &amp;quot;upward&amp;quot; on the image points in the same direction as the positive Y axis.  That means that if you were to draw images or text with the coordinate system in the QuickDraw orientation, they would draw upside down.  For that reason, HIView provides routines like HIViewDrawImage and HIViewDrawThemeTextBox which temporarily flip the coordinate system, draw whatever they are supposed to draw, and return the coordinate system to its previous orientation. If I understand your question properly, the answer is that some programmers (myself included) will undo the transformation that the HIView system has done for convenience.  By flipping the coordinate system back all your graphics, including text and images, live in the same coordinate system.  I'm comfortable with the right-handed coordinate system anyway so it doesn't bother me. Scott _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: QuickDraw to Quartz ... Point &amp;amp; RgnHandle in Carbon Events?</header>
    <body>On Jul 19, 2005, at 2:15 PM, Jean-Fran√ßois Brouillet wrote: All CG context's have a &amp;quot;current transformation matrix&amp;quot; (CTM) which is always applied to map points between user space and device space. The default CTM in a CGContext uses the default PDF transformation, but an HIView initially sets the CTM to the default QuickDraw transformation.  Therefore a context from an HIView does nothing more or less &amp;quot;on the fly&amp;quot; than any other context, it just uses a different default. Nick _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: QuickDraw to Quartz ... Point &amp;amp; RgnHandle in Carbon Events?</header>
    <body>I'm curious. Since I don't know better (yet! :-) I've got to ask why HIView, in some context, does translate &amp;quot;on the fly&amp;quot; between botLeft and topLeft (and thus needs the notion of height to perform the transformation), so as to map the QD coordinate system to Quartz' when all it could have done was to change that of Quartz to match the QD one, once for all (or at least, for the lifetime of the CGContext in question) ? Or more succinctly: why perform on the fly what could have been done at CGContext creation/initialisation time? -- JFB -- Jean-Fran√ßois Brouillet email@hidden</body>
  </mail>
  <mail>
    <header>Re: Crash converting CVOpenGLTeexture to NSImage (using CIImage)</header>
    <body>+ (NSBitmapImageRep *)imageRepFromTexture: (CVOpenGLTextureRef) theTexture glGetTexLevelParameteriv(textureTarget, 0, GL_TEXTURE_WIDTH, glGetTexLevelParameteriv(textureTarget, 0, GL_TEXTURE_HEIGHT, theImage = [[[NSBitmapImageRep alloc] initWithBitmapDataPlanes: NULL pixelsWide: textureWidth pixelsHigh: textureHeight bitsPerSample: 8 samplesPerPixel: 4 hasAlpha: YES isPlanar: NO colorSpaceName: NSCalibratedRGBColorSpace bitmapFormat: (NSBitmapFormat)0 bytesPerRow: textureWidth * 4 bitsPerPixel: 32] glGetTexImage(textureTarget, 0, GL_RGBA, GL_UNSIGNED_BYTE, On 17 juil. 05, at 07:37, Simon Wilson wrote: Jean-Alexis Montignies</body>
  </mail>
  <mail>
    <header>CGSGetGlobalCursorData -- BIG Cursors (Modified by Jonathan	Gillaspie)</header>
    <body>Hello, I'm the maintainer of OSXvnc and we are currently having a problem with calls to First off, I have no idea if this is the right call to be using to get cursor data.  It seems to be almost completely undocumented and unused, if anyone has better APIs that I should be using please enlighten me. The problem is that calls to this method sometimes return a rather LARGE cursor.  It appears to contains a whole SET of cursors that would be used for say an animated cursor of a blue ball spinning below an arrow. (To see an example of this you can use the OS X version of http://www.smartcvs.com). Obviously when I get a cursor this big I don't want to send the whole thing across via VNC but rather just one frame.  Unfortunately I don't really have any good information about how to know that it's one of these cursors or what section constitutes just one frame of one of these animated cursors. Any pointers to documentation or suggestions would be very welcomed. -- Jonathan Gillaspie http://www.redstonesoftware.com Redstone Software Inc. -- Makers of Platform Independent Automation Software _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: QuickDraw to Quartz ... Point &amp;amp; RgnHandle in Carbon Events?</header>
    <body>CGPoint and HIPoint generally mean two different things.  CGPoint lives in the Core Graphics layer (below the application framework layer) and typically means a point in a right handed coordinate system (origin in the lower left corner, y axis pointing &amp;quot;upward&amp;quot;. HIPoint is part of the Carbon application framework and typically represents a point in a left handed coordinate system (origin upper left, y axis pointing down).  Since an HIPoint is a CGPoint the distinction is purely ideological, but I find it helpful to think of it that way.  YMMV :-) You might also enjoy More than likely, if you are using Cocoa, you will be working with NSPoint, NSRect, etc... This goes back to the earlier point &amp;lt;no pun intended&amp;gt; that CGPoint is a data type that &amp;quot;lives&amp;quot; at a level below the application frameworks.  Cocoa also provides handy classes like NSBezierPath and NSAffineTransform that will allow you to avoid using the Quartz 2D APIs directly if you are so inclined.  (You can still call Quartz directly from Cocoa, but Cocoa provides some additional tools as well). No... not exactly.  HIViews were introduced to be an improvement over the old control manager.  The control manager has some &amp;quot;charming idiosyncracies&amp;quot; that required developers to understand a lot of &amp;quot;Legend and Lore&amp;quot; information to get it to work properly. Apple took the opportunity offered by the Carbon transition to introduce an a greatly improved control management system, HIView, and an improved event system, Carbon Events.  Apple has been recommending that applications adopt HIView, in the place of the Control Manager, for about four or five years now. Retrofitting Quartz into the control manager would have been hard. HIView had the advantage of evolving with Quartz and enjoys a much tighter integration. As a result, if you work through the HIView system, it is much easier to create a QuickDraw-free application. In short, HIView wasn't created to make it easier to adopt Quartz. HIView was developed to make working with controls easier.  The fact that it makes it easier to adopt Quartz is a valuable side benefit. :-) The view and event system in Cocoa comes from the legacy of NeXT which used Display PostScript as its graphics engine. Display PostScript and Quartz 2D share the same underlying graphics model.  Those parts of the view system which rely on the graphics model (such as the representation of a point) didn't have to change. On Cocoa, the graphics system provided by NSBezierPath, NSAffineTransform, NSImage, NSText, etc. is now based on Quartz 2D. Those were the &amp;quot;intermediary helpers&amp;quot; that allowed Apple to change the graphics system out from under Cocoa applications with minimal disruption.  Older Cocoa applications, however, still had to make a transition similar to the QuickDraw/Quartz 2D transition if they were using Display PostScript at a level lower than the Cocoa helper classes. Scott</body>
  </mail>
  <mail>
    <header>Re: QuickDraw to Quartz ... Point &amp;amp; RgnHandle in Carbon Events?</header>
    <body>Makes sense. Gasp. I was expecting CGPoint and I see HIPoint .. Many thanks. OK. I'll start digging that route. Thanks again. I see that I had by-passed that HIView stuff ... OK. I begin to see. But... I'm under the impression that if I was using Cocoa, the only thing I would need to know would be GCPoint and all that HIView business would be mostly irrelevant. Is that right? I'm not trying to judge whether this is good or bad, only to fill gaps in my understanding: A direct transition from QuickDraw to Quartz was just impossible because too many Carbon Managers relied on the QD data types and behavior. So this HIView thing has been introduced for the benefit of Carbon programmers to help bridge the gap with Quartz. On Cocoa, the implementation part of NSWindow, NSView and friends was just updated to refer to Quartz instead without the need for an intermediary helper. Many Thanks. -- Jean-Fran√ßois Brouillet email@hidden</body>
  </mail>
  <mail>
    <header>Re: QuickDraw to Quartz ... Point &amp;amp; RgnHandle in Carbon Events?</header>
    <body>On Jul 17, 2005, at 8:10 AM, Jean-Fran√ßois Brouillet wrote: The function and parameter types won't disappear. They are needed for legacy support of existing clients. There is documentation about replacement functions and parameter types... Very pervasive. No. This exists now: verify_noerr( GetEventParameter(mouseDownEvent, kEventParamMouseLocation, typeHIPoint, NULL, sizeof(HIPoint), NULL, There are floating point structures (HIPoint, HIRect, etc) used for geometry in HIToolbox. They are defined and explained a bit in HIGeometry.h, and are used in Carbon Events. Coordinate conversion (see HIGeometry.h). HIShapes are the equivalent* to RgnHandles. *Full support of HIShape is not complete yet in HIToolbox, but that's where it's headed. date.html Carbon_Event_Manager/index.html Carbon_Event_Manager/Tasks/chapter_3_section_10.html HIShape_Reference/index.html The HIToolbox header files. There is a lot of documentation in the header files. You might also want to subscribe to carbon- email@hidden. /* *  kEventClassMouse / kEventMouseDown ... *  Parameters: * *    --&amp;gt; kEventParamMouseLocation (in, typeHIPoint) *          The mouse location, in global coordinates. ... */ /* *  CreateMouseTrackingRegion()   *** DEPRECATED *** * *  Deprecated: *    Use HIView-based tracking areas instead of MouseTrackingRegions. *    In this case, use HIViewNewTrackingArea(), defined in HIView.h, *    instead of CreateMouseTrackingRegion(). ... */ /* *  HIPoint * *  Discussion: *    HIPoint is a new, floating point-based type to help express *    coordinates in a much richer fashion than the classic QuickDraw *    points. It will, in time, be more heavily used throughout the *    Toolbox. For now, it is replacing our use of typeQDPoint in mouse *    events. This is to better support sub-pixel tablet coordinates. *    If you ask for a mouse location with typeQDPoint, and the point *    is actually stored as typeHIPoint, it will automatically be *    coerced to typeQDPoint for you, so this change should be largely *    transparent to applications. HIPoints are in screen space, i.e. *    the top left of the screen is 0, 0. */ HIToolbox.framework/Headers/HIView.h: extern OSStatus HIViewNewTrackingArea( HIViewRef                inView, HIShapeRef               inShape,       /* can be NULL */ HIViewTrackingAreaID     inID, David. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>QuickDraw to Quartz ... Point &amp;amp; RgnHandle in Carbon Events?</header>
    <body>I just searched the quartz archive (and read all the 22 hits on &amp;quot;deprecated&amp;quot;), found a reference to &amp;quot;Transitioning to Quartz 2D&amp;quot; that was supposed to be listed at Quartz-date.html but isn't (or at least, is not the name of any document: none of them contains the word &amp;quot;Transitioning&amp;quot; ...) I then googled &amp;quot;Transitioning to Quartz 2D&amp;quot; but wasn't exactly pleased with the results ... My question really is: how deprecated is &amp;quot;deprecated&amp;quot; when it comes to QuickDraw? OSStatus CreateMouseTrackingRegion (WindowRef inWindow, RgnHandle inRegion, RgnHandle inClip, MouseTrackingOptions inOptions, MouseTrackingRegionID inID, void* inRefCon, EventTargetRef inTargetToNotify, GetEventParameter (mouseDownEvent, kEventParamMouseLocation, typeQDPoint, RgnHandle ??? Point ??? - how pervasive is Quartz going to be w.r.t all the managers that used to use QuickDraw data types? GetEventParameter(mouseDownEvent, kEventParamMouseLocation, typeCGPoint, As I understand the planned move to &amp;quot;resolution independence&amp;quot; I don't see how we could still get 16 bits integer coordinates for mouse events and expect to keep the accuracy necessary for magnification ... Many thanks. -- JFB</body>
  </mail>
  <mail>
    <header>Re: CIGaussianBlur filter: inputRadius parameter units?</header>
    <body>On Jul 16, 2005, at 4:59 PM, Roland Torres wrote: A gaussian blur performs a convolution using a convolution kernel built with a gaussian function.  You can google this for a lot more information.  The radius is a parameter in the gaussian function, to understand it you would have to study that function.  It's relative to the pixel resolution but it's not proportional to the size of the blur.  Because the size of the convolution kernel increases with the radius, the amount of processing required increases roughly as the square of the radius.  The limit at 100 is arbitrary, but produces sufficient blur that any more is pointless and computationally prohibitive.  Note that this isn't a hard limit on the radius parameter, but a recommended limit on any slider used to represent this value in a user interface. Nick</body>
  </mail>
  <mail>
    <header>Re: Crash converting CVOpenGLTeexture to NSImage (using CIImage)</header>
    <body>I've reported this already as issue #4176762. I've uploaded a copy of the CIVideoDemoGL sample application with the necessary changes required to replicate the issue, although I'm having the same issue in my own application. This issue was originally raised on the mac- opengl list, and we've done a little detective work ourselves to try and isolate the issue ( 2005/Jul/msg00023.html), but with no luck so far.</body>
  </mail>
  <mail>
    <header>Re: Large CoreImage Fragment Programs</header>
    <body>On Jul 13, 2005, at 10:20 AM, Nick Nallick wrote: yes, this crash sounds like a bug, please file a radar with a crash log (and some idea of how to reproduce it) in general it's best to keep kernels as small as possible, e.g. try to do looping by stringing together multiple kernel applications, not by making kernels longer (CI will concatenate them if possible) It entirely depends on the graphics card you have - ATI R300 class can support somewhere around 64 instructions per program, X800 increases that, and NV40 effectively has no limit It's best to avoid the software fallback, but not disastrous No, not currently. The first time you use a kernel on each graphics card it may print some error logging. If you really need to disable it, do freopen (&amp;quot;/dev/null&amp;quot;, &amp;quot;w&amp;quot;, stderr)</body>
  </mail>
  <mail>
    <header>Re: clipping a path</header>
    <body>On Jul 16, 2005, at 7:19 AM, Niels Bogaards wrote: There currently isn't any support in Quartz for performing arithmetic like operations on paths (e.g., intersection, union, subtraction) the way you can on regions.  However clipping is cumulative so you could use that to your advantage.  For example, first clip to the apple, then clip again to a rectangle larger than the apple with a circular hole in it.  This will give you a clip path of an apple with a circle bitten out of it. Nick</body>
  </mail>
  <mail>
    <header>clipping a path</header>
    <body>I want to implement an eraser in my CGPath based line drawing system. I figured the smartest way to do this would be to clip lines and strokes to their own path minus the parts that are erased, so if I were to draw the Apple logo I could have an apple and a circle where I would substract the circle from the apple to 'erase' the bitten out part. However, I don't find a way to remove a path from the clipping, only add more paths. If I would revert back to using RgnHandles (that I seem to need for intersection testing anyway; see previous posts), I could use ClipCGContextToRegion() . But that function's deprecated on Tiger. So,I must be missing something, right? Niels</body>
  </mail>
  <mail>
    <header>Re: NSBitmapImageRep and a NSColorSpace</header>
    <body>You can set an NSBitmapImageRep instance to be interpreted using a different color profile by setting its NSImageColorSyncProfileData property: [theBitmapImageRep setProperty:NSImageColorSyncProfileData -- Troy Stephens Cocoa Frameworks Apple Computer, Inc.</body>
  </mail>
  <mail>
    <header>NSBitmapImageRep and a NSColorSpace</header>
    <body>If I am creating a NSBitmapImageRep and I am initing it with colorSpaceName:NSCustomColorSpace  how do I get the imageRep to make use of a NSColorSpace that I have loaded from an ICC profile? Is it just setting the NSContext and then setting the colorspace for the NSColor?</body>
  </mail>
  <mail>
    <header>Re: But where's OSX Window Manager ?</header>
    <body>On Jul 14, 2005, at 10:13 PM, John Davidorff Pell wrote: No, it doesn't.</body>
  </mail>
  <mail>
    <header>Crash converting CVOpenGLTeexture to NSImage (using CIImage)</header>
    <body>I'm rendering a live video to a QTOpenGLContext, I'd like to take a snapshot image upon use action. I convert the texture to a CIImage using imageWithCVImageBuffer: , then to a NSImage with a NSCIImageRep. When the system try to draws the image, the application crashes in CVPixelBufferTextureBacking::initWithPixelBackingContext . Any idea? Should I pass options or use imageWithTexture:size:flipped:colorSpace: instead?</body>
  </mail>
  <mail>
    <header>Re: But where's OSX Window Manager ?</header>
    <body>Doesn't CoreFoundation support some window drawing? How much? Could this be enough of a C API for Monsieur Brouillet? just thinking out loud, JP On 14 Jul 2005, at 16:56, Eric Schlegel wrote:</body>
  </mail>
  <mail>
    <header>Re: But where's OSX Window Manager ?</header>
    <body>I had a related question at WWDC.  I learned that the NSWindow class has a method to return a WindowRef, which is a Carbon Window.  I understand this functionality became available in Panther. Gen</body>
  </mail>
  <mail>
    <header>Re: CGPath intersection</header>
    <body>Well... I wouldn't put it that way.  I would say that we're bound to QuickDraw Regions for something as complex as arbitrary path intersections. The &amp;quot;problem&amp;quot; with paths is that they have no inherit resolution and intersecting two paths is a lot more complex than intersecting regions. In non-degenerate cases, two individual cubic bezier path _segments_ can intersect in as many as 9 points.  Most paths consist of many segments. Calculating the intersection of a path with another shape (even one as simple as a rectangle) can be very difficult. In your case, it doesn't sound like you need the precision of arbitrary intersection calculations.  There are other techniques you might use.  For example, after passing your paths through a trivial rejection process (culling out those whose bounding boxes don't intersect the marquee) you might create an offscreen bitmap whose size and origin matches the location of your marquee.  You could draw each path you want to intersect with the marquee into that bitmap and see if any of the pixels get colored. If that does not suit you, then you may be able to flatten the bezier curves and intersect the flattened polygons with the marquee. In the mean time, I would recommend asking for a routine to convert a path into an HIShape (with a given resolution more likely than not). Several folks have requested it, the more folks that ask for it, the more likely it is to happen. :-) Scott</body>
  </mail>
  <mail>
    <header>CGPath intersection</header>
    <body>I'm trying to upgrade our marquee selection of graphic objects, to be able to use CGPaths and minimize the use of regions. My objects are CGPaths, and I want to check whether these paths intersects with the marquee. In QuickDraw, you would do this by converting both shapes to rgeions and doing a SectRgn. However, it seems a little bit silly to have to convert a CGPath to a region, just to be able to test intersection. There also does not really a function that can do this, so I would have to keep parallel copies of the region and the CGPath. What looks a bit more promising is the HIShape system, which has the HIShapeCreateIntersection() function that can return the intersection of two shapes. However, I see no way of constructing an HIShape other than from a RegionHandle. Which brings us right back to the old QuickDraw way of doing things. Am I missing the point here, or are we still bound to QuickDraw regions for something as trivial as a marquee? Niels Bogaards</body>
  </mail>
  <mail>
    <header>Re: But where's OSX Window Manager ?</header>
    <body>On Jul 14, 2005, at 4:45 PM, Jean-Fran√ßois Brouillet wrote: A CGContext is a virtual drawing space that draws into a window that is owned by Carbon or Cocoa. Think of a CGContext as a pencil that you use to draw onto a piece of paper that is supplied by a WindowRef or an NSWindow. The CGContext is not the paper itself; it's just the means for drawing onto the paper. The CoreGraphics framework does have _private_ interfaces that are used by both Carbon and Cocoa to create windows, but I emphasize private. They are not API, but SPI, and are not available for use by non-Apple code. These interfaces change regularly from release to release and your application is very likely to break on future versions of Mac OS X if you attempt to use them. There is no public windowing API available for the window server. You must use either Carbon or Cocoa. May I ask why you want to avoid those APIs?</body>
  </mail>
  <mail>
    <header>But where's OSX Window Manager ?</header>
    <body>The question might sound stupid, but that's probably because I'm very confused ... In the old days, we had the Window Manager, and then we had QuickDraw. The Window Manager defined windows in terms of GrafPorts that clearly belonged to QuickDraw. Now, with Quartz, we've got this CGContext thingie, and Carbon's WindowRef and Cocoa's NSWindow ... but what's the connexion? In fact I'm not so much interested in the connexion _per se_ as in using Quartz and whatever allows me to create a Window to display into but not necessarily Carbon's Window Manager ... Quartz Service Refences has this comment, on the CGWindowLevelForKey entry: This function is not recommended for use in applications. (It‚Äôs provided for frameworks that create and manage windows, such as Carbon and Cocoa.) The important bits, for me here is the &amp;quot;such as&amp;quot;. Does that mean that _there is_ some window creation/handling API somewhere that both Cocoa and Carbon do access to create their windows? Given the frequent references, here and there to some mythical &amp;quot;Window Server&amp;quot;, the answer must be ... yes What I want to do is this: - use Quartz - forget QuickDraw - avoid Carbon - avoid Cocoa - use whatever (C level?) API is available to reach the window server. Many Thanks. -- Jean-Fran√ßois Brouillet email@hidden</body>
  </mail>
  <mail>
    <header>Re: Shadings don't transfer to PDF properly</header>
    <body>On Jul 14, 2005, at 12:46 PM, Scott Thompson wrote: I gather, then, that it would not make sense to create a shading function that did something really odd (like return a completely random color for each input value).  What sort of function is used to represent the shading function when it goes to PDF? (piecewise linear?, piecewise cubic bezier?)</body>
  </mail>
  <mail>
    <header>Re: Quartz Image drawing Performance problems</header>
    <body>On Jul 14, 2005, at 1:57 PM, Jim Acquavella wrote: The color space you pass to a CGImage is the color space that the image data comes _from_ not where it is going _to_.  It should be the main display if the particular RGB pixel values you have in your image were created by someone that painted it on your screen and used your monitor to select the colors of the image. In any case, Device RGB is not often the right color space to use when creating a CGImage. (That is unless you have VERY specific color management needs). If the image was created on a Mac, and the data is not otherwise tagged, you should use Generic RGB.  In this case I suspect the data was created on a PC and is probably some variant of sRGB. All this, of course, presumes that you care about the color management of your image.  If it's OK if the color of your image shifts (as may be the case with UI) this is largely a moot point to be making. For ideal performance, your context's fill and stroke color spaces should be the display color space.  This allows Quartz to avoid having to color match the colors you supply.  For strokes and fills, however, I would guess that conversion is done only once each time you supply it to the context (which is a good reason to change GState variables as little as is necessary). I don't know exactly what those routines accomplish.  If your code was based on the suggestion that you use the display's color space when creating the CGImage, however, I was wrong when I suggested doing that.  It would probably make no difference in your code.</body>
  </mail>
  <mail>
    <header>CGLayer Pixel Depth</header>
    <body>I've found a somewhat strange behavior that I'm interested in getting feedback on. I'm caching a scaled image in a CGLayer.  If the first thing I draw the layer into is an alpha-only bitmap context and then I draw the layer into an ARGB bitmap context I get a black and white image in the color bitmap. Does this make any sense or is there likely something I'm doing wrong in my app?  If this is CG's fault is there a workaround? Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: How to get CIImage from data stream?</header>
    <body>I was trying to create the image using imageWithBitmapData:bytesPerRow:size:format:colorSpace: but its a very slow process as I have to convert the RGB pixels to ARGB so that format can be given as kCIFormatARGB8. So I have to rule out this option. You may assume the following: 1. void * src ; which has RGB data only (Raw data) 2. bitsperpixel, bytesPerRow and things like that are known. Thanks Raj It would help to have some more information about what format your data is in.&amp;nbsp;&amp;nbsp;If it's raw image data you might be able to use imageWithBitmapData:bytesPerRow:size:format:colorSpace:.&amp;nbsp;&amp;nbsp;I, however it's more complex than that, you may have to go another route. A CIImage does not contain any pixels by itself, rather it contains a set of instructions for drawing some pixels.&amp;nbsp;&amp;nbsp;You're going to need another object that can hold the pixels on behalf of the CIImage. What you may have to do is read your data into a CGImage, create a CIImage from the CGImage and then use the CIImage in the rest of your processing. To create a CGImage, you can use the CGDataProvider API (through CGDataProviderCreate and a set of callback routines) to create the object that pulls data off your C++ stream. The CGDataProvider would then be used to create a CGImage. The way you do that depends on what kind of data it is.&amp;nbsp;&amp;nbsp;If it's just raw image data you can go through CGImageCreate.&amp;nbsp;&amp;nbsp;If it's image file data (JPEG, TIFF, or something along those lines) you could go through CGImageSource or QuickTime to create a CGImage. Once you have the CGImage, you can create a CIImage from it using &amp;quot;imageWithCGImage:&amp;quot;.&amp;nbsp;&amp;nbsp;What this does is creates a CIImage whose only drawing step is &amp;quot;draw the pixels of this CGImage. From there you can proceed as usual with your Core Image work. In the end you can draw the results onto a bitmap context, create an CGImage from that context, and use Image I/O to write the results in whatever file format suits your fancy. Scott</body>
  </mail>
  <mail>
    <header>Processing TIFF/PNG Layers</header>
    <body>Greetings: Does anyone know where I can see an example of how to process/add layers to a TIFF image? I want to be able to grab an image, separate the layers (CGLayer?) into their separate contexts for the user to work with;  and add a new transparent layer. Specifically, 1)  allowing the user to draw/edit upon a layer above a static layer per button. I believe the same could be done with a PNG image? Regards, Ric, neophyte.</body>
  </mail>
  <mail>
    <header>Re: The focus stack and custom NSGraphicsContexts</header>
    <body>Attachment:</body>
  </mail>
  <mail>
    <header>Re: Quartz</header>
    <body>Why do you want the code for Quartz? And no, you can't get the code. As a major selling point for Mac OS X there isn't much reason for Apple to let anyone else have it... -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Quartz</header>
    <body>is there anywhere I can get Quartzs' code without buying Mac OSX or is it just completely under lock and key? _________________________________________________________________ Be the first to hear what's new at MSN - sign up to our free newsletters!</body>
  </mail>
  <mail>
    <header>QT 7.0 - poor CoreVideo performance in 7W98</header>
    <body>Window resizing performance is about 50-60% of former with QT 6. General/various new latencies in GUI response. MacFOH has its own VBL synchronizer, from pre-10.3, which uses a priority 63 extended thread; it is basically the same as the CVDisplayLink callback, as it calculates latencies and schedules itself to wake up just prior to the VBL.  One thread is created for each display, and managed accordingly. With QT 7 installed, I enabled CoreVideo's CVDisplayLink via a loadable bundle (in lieu of weak linking the QuartzCore framework, which crashes it!), and this bundle adds a category to my SKWDisplay class that uses CV's instead of its native sync threads. I can A/B between the two API's and relaunch MacFOH to test the differences.  CV's performance seems to sync okay, but is less reliable and during window resizes is noticeably more latent.  When a zoom box is clicked in a window titlebar, all display activity stops for about 100 ms or so, then the window resizes and after another smaller latency, realtime display resumes.  This is almost nonexistent when using the SKWDisplay native sync. Window resizing is approx 2x slower, again with more latency.  When MacFOH realtime analyzer window are resized, it is very hard to tell if the frame rate slows down and there is no transition from live-resizing to static speeds, whereas when QT 7 is installed.... it sucks, for lack of a better term. -- Shaun Wexler MacFOH To the optimist, the glass is half full. To the pessimist, the glass is half empty. To the programmer, the glass is twice as big as it needs to be.</body>
  </mail>
  <mail>
    <header>Re: Creating 8-bit indexed image</header>
    <body>Oh duh. I'd read that half a dozen times and every time my brain interpreted it such that the &amp;quot;0 to 255&amp;quot; part meant the image data - the index into the color table (which made sense because it's 8-bit data). Thanks for the kick. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Creating 8-bit indexed image</header>
    <body>Hmm, I'd never heard of such a thing. Every pixmap I see has at least one of the two high bit flags set. Man, now I'm going to have to go through all our code and find change this. Not that we ever make bitmaps large enough that it would max it out. Although in the future we'll be increasing our max doc size, so it could happen then. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>re: Creating 8-bit indexed image</header>
    <body>OUCH! Don't do this! Changes are that this is no longer necessary plus on large pixmaps (&amp;gt; 4K 32-bit pixels wide) rowbytes is a long not a short. Your GetBits method should be using the GetPixMapRowBytes API which &amp;quot;does the right thing&amp;quot; in all cases. -- Enjoy, George Warner, Schizophrenic Optimization Scientist Apple Developer Technical Support (DTS)</body>
  </mail>
  <mail>
    <header>Re: CoreImage?</header>
    <body>On Apr 29, 2005, at 11:50 AM, Nick Nallick wrote: I believe... If you have the retail version you can discuss what you find in developer tools since that isn't under NDA. -Shawn</body>
  </mail>
  <mail>
    <header>CoreImage?</header>
    <body>Since (non-developer) people are starting to get their Tiger pre-orders in the mail and Apple is now selling retail copies (at least in the UK) I'm guessing the NDA has been lifted. Thanks, Nick</body>
  </mail>
  <mail>
    <header>Creating 8-bit indexed image</header>
    <body>I have a situation where I have 8-bit indexed bitmap data in a GWorld and need to create a CGImageRef so I can draw it into a CGContextRef. I can't find the correct CGImageAlphaInfo to pass to it. The ones that actually get the bitmap to draw draw it at about 50% gray instead of black. Here's the gist of my code: colorspace = CGColorSpaceCreateIndexed(rgbSpace, (**table).ctSize - 1, rowBytes &amp;amp;= 0x3fff; // Mask out top 2 reserved bits. image = CGImageCreate(RectWidth(box), RectHeight(box), 8 /*bitsPerComponent*/, bitsPerPixel, rowBytes, colorspace, alphaType, provider, nil, 0, The data in bits is 0xf0f0f0etc. That means the index for those pixels is 204. Item 204 in the color table is the gray it's drawing with. BTW, gw is a GWorld wrapper class. The color table has 255 colors going from If I draw the GWorld directly without having a CGContexRef set up, it draws correctly (black). Any ideas what's going wrong here? _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: The inner workings of Quartz</header>
    <body>On 29 Apr 2005, at 14:57, DANIEL hoggan wrote: Check out libxmi and Cairo which both do this sort of thing for X Windows. Jerry</body>
  </mail>
  <mail>
    <header>Re: The inner workings of Quartz</header>
    <body>On Apr 29, 2005, at 7:57 AM, DANIEL hoggan wrote: You could look at GhostScript.  It contains an open source PDF renderer for Unix. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>The inner workings of Quartz</header>
    <body>I am trying to write a quartz like api, that would do the same jobs for *nix as quartz does for Mac OS X, ie be the graphics engine and the windowing system. _________________________________________________________________ Be the first to hear what's new at MSN - sign up to our free newsletters!</body>
  </mail>
  <mail>
    <header>Re: Emulating PlotIconRefInContext icon transform behavior for	CGImages</header>
    <body>On Apr 22, 2005, at 10:18 AM, David Niemeijer wrote: Correct. It looks like we have a &amp;quot;lighten&amp;quot; transformation for the disabled state and a &amp;quot;darken&amp;quot; transformation for the selected state. Lighten: for each pixel component, add 0xFF, and shift right by 1 bit. This pushes every component towards 0xFFFF. Darken: for each pixel component, shift right by 1 find the component with the largest value, and shift the other two components right by 1 again. This emphasizes the largest component. Don't do this step if all of the components (after the first right shift) are within 0x200 of each other - in this case, we have a gray pixel, and there's nothing to emphasize.</body>
  </mail>
  <mail>
    <header>Re: Current CGContextRef?</header>
    <body>On Apr 28, 2005, at 6:30 AM, Steve Mills wrote: No, Quartz doesn't have a concept of a current CGContextRef. You can have multiple CGContextRefs in existence at any time, and drawing occurs in the context that you pass into Quartz. The Window Manager also does not maintain a &amp;quot;current CGContextRef&amp;quot;, other than implicitly having a CGContextRef that it creates and caches for drawing into a composited window. If you are drawing into a composited window, you must use the CGContextRef that is passed into the Draw method of your HIView. If you are drawing into a non-composited window, you can use QDBegin/ EndCGContext to get a context for drawing in the window. I believe it's OK to nest those calls, although you'll be faster if you can avoid calling them more than once.</body>
  </mail>
  <mail>
    <header>Current CGContextRef?</header>
    <body>We have some semi-portable bitmap class. The Mac version of this code keeps a GWorld. The Draw method can take no platform specific parms, naturally. It draws to the current port. Converting it to keep a CGBitmap can't be done at this point, so I need to draw the GWorld to a CGContextRef using the sample provided by Apple QuartzPrimer/qprimer_main/chapter_1_section_6.html#//apple_ref/doc/uid/ TP30001067-CH201-TPXREF19&amp;gt;. Until then, I need to know in the Draw method if there is currently a CGContext that should be drawn to instead of the current QD port. Is there any way to get this info from Quartz, QD, Window Manager, etc? I didn't see anything obvious in QuickDraw.h or CG*.h. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Inverting?</header>
    <body>On Apr 26, 2005, at 9:02 AM, Steve Mills wrote: fwiw: I've tested &amp;quot;live&amp;quot; path resizing with reasonably complex paths on a G4/400 and it's pretty snappy -- better than you might expect.</body>
  </mail>
  <mail>
    <header>Re: Inverting?</header>
    <body>No, I wasn't offended, just pointing out that we need to remember that not every customer has dual 2G G5s sitting on their desk. I'll look into the overlay window method when it comes time to tackle document selections. Thanks all. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Inverting?</header>
    <body>On 25 Apr 2005, at 20:14, Steve Mills wrote: I'd suggest differentiating by shape or border e.g. round handles. This seems to be the way all the Apple apps are going. What we do in this situation is use an overlay window and just draw the rubberbanding stuff in that. You still don't get the XORing, but the update is efficient. To erase the old path, just clear the overlay. Jerry</body>
  </mail>
  <mail>
    <header>Re: Inverting?</header>
    <body>On Apr 25, 2005, at 11:24 PM, Kai Br√ºning wrote: Note that Quartz using hardware acceleration when possible composites areas affected using window buffers (often with those located in vram) without involving the application. In other words the application doesn't have to redraw anything since the window manager already has the last fully drawn (by the application) window buffer to work with (window are double buffered). This is the reason windows don't get partially redrawn or erased if the application hangs or is busy doing something else (showing the beach ball) while you try to move the windows around or change the stacking order like you see on current versions of Windows (god that annoys me to no end!). So redrawing in a window is different then compositing those to the screen, one is the responsibility of the application and the framework it uses and the other is mostly managed by the window manager and quarts compositor. Of course going direct to OpenGL can be different.</body>
  </mail>
  <mail>
    <header>Re: Inverting?</header>
    <body>[snip] I'm sorry if my words were offending, this wasn't the intend. What I meant to say is that with the Quartz drawing model including alpha and anti-aliasing, the only robust scheme to change anything on the screen is complete redraw of the affected area from ground up. This is what windows in compositing mode basically do. And to support live scrolling and live redraw, normal window redraw needs to be reasonably fast anyway. The alternative, as mentioned by others, is to trade memory for speed and use overlay windows for selection hiliting, too. Depends on your case. Best, Kai -- RagTime GmbH                          Tel: [49] (2103) 9657-0 Neustra√üe 69                          Fax: [49] (2103) 9657-96 D-40721 Hilden Deutschland</body>
  </mail>
  <mail>
    <header>Re: Emulating PlotIconRefInContext icon transform behavior for	CGImages</header>
    <body>David Niemeijer: Since no official answer seems to be forthcoming... I expect that the transform is a 50% blend with black for selection and white for disabling. In other words, shift each non-alpha component of each pixel right by one pixel, then add 0x80 for the disabled case. To match Icon Services, which presumably isn't ColorSync aware, use an intermediate bitmap in the output device's colour space. -- Jens Ayton</body>
  </mail>
  <mail>
    <header>Re: Inverting?</header>
    <body>I agree that inverting the pixels can be a little hard to see when the area being inverted is anything more complex than solid colors or a gradient, especially if the color is 50% gray, then it's impossible to see. And as much as I'd love to develop for only current hardware, that just ain't possible for our customers. This is why it's so important for modern OSs to still offer the same useful drawing methods that earlier APIs (QD) have had since the beginning. I realize that Quartz is used for drawing to other contexts besides the screen and inverting there would be useless. But for screen drawing, it's completely justified and needed. Many of our customers are still using OS9. Sucks being us. Tell our customers to get used to waiting for the window to redraw just so they can see the handles and resize something. Interesting. Luckily, our next release will be 10.3 and above. Before that, it was 9.2. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Inverting?</header>
    <body>Have you considered using an overlay window for your handles and outlines. That gives you a lot of flexibility as well as the possibility of making your handles and outlines partially transparent. I think this is how most native OS X apps handle these things nowadays (but let anyone correct me if I am wrong). You group the overlay window together with your normal window so that the handles are not draw on top of someone else's window or so. david.</body>
  </mail>
  <mail>
    <header>Re: Inverting?</header>
    <body>On Apr 25, 2005, at 3:58 PM, Kai Br√ºning wrote: Yep, this works just fine for us in solving the same problem. We opted to do exactly this with overlay windows to do the same kind of &amp;quot;selection box&amp;quot; that Finder uses. No more marching ants in 2005 ;)</body>
  </mail>
  <mail>
    <header>Re: Inverting?</header>
    <body>Point is that inverting is a kind of hack since the introduction of color. Inverting black to white has some intuitive meaning, but with colors it was never &amp;quot;right&amp;quot;. Therefore it is a good thing that it is gone - and that todays hardware is fast enough for better alternatives. Standard solution for this is to draw something like a black rectangle with a one pixel white frame. This guarantees visibility above any background. Similar solutions are possible for the other cases which you mention. Of course this means redraw for selection changes - get used to it. Here the standard solution is a transparent overlay window. A window class exists for exactly this purpose and works beautifully and very fast. You typically create such a window covering everything you may need to draw over when the mouse interaction starts, draw you rubber band stuff into that window never touching the &amp;quot;real&amp;quot; window and dispose the overlay window when done. Just be aware that overlay windows do not support compositing under Jaguar - in case you still support Jaguar. Best regards, Kai -- RagTime GmbH                          Tel: [49] (2103) 9657-0 Neustra√üe 69                          Fax: [49] (2103) 9657-96 D-40721 Hilden Deutschland</body>
  </mail>
  <mail>
    <header>Re: Inverting?</header>
    <body>When elements are selected in Creator (our page layout app), they have handles. We've always used InvertRect to draw these handles, because that ensures they're always visible no matter what's being the handle. Simply drawing a black square won't work because the entire background could be black. Under certain conditions (a contained element is selected), we also use a &amp;quot;hollow&amp;quot; handle, which is a white background with a 1px black frame, so using that somewhat common method already has a meaning in Creator. There's also the problem that I haven't even looked at yet of drawing an outline when new elements are in the process of being created by dragging out their bounds with a tool. Also, when resizing an existing element, we draw with a pattern for the outline being rubber banded around. Both methods use xor drawing to draw the new path and erase the old path. Live drawing of the actual shape *could* be used instead, but it could be very slow if the shape is complex (100-point starburst with round valleys and peaks), and we'd have to draw the entire spread for every mouse movement. That could be painfully slow in this situation as there could be hundreds and hundreds of text, graphic, or shape elements. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Inverting?</header>
    <body>On Apr 25, 2005, at 1:39 PM, Steve Mills wrote: Not really. Not with the model of draw and forget. What are trying to do with InvertRect? There may be a different approach to the problem you can use.</body>
  </mail>
  <mail>
    <header>Re: Default for CGContext's Miter Limit</header>
    <body>On Apr 22, 2005, at 10:13 AM, Mark Morrill wrote: Wouldn't it be better to save and restore the context GState?</body>
  </mail>
  <mail>
    <header>Inverting?</header>
    <body>Is it possible to invert the rendered context under a path? Something equivalent to QD's InvertRect? _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: QD vs CG pixel coords</header>
    <body>The Quartz origin is in the lower left corner of a pixel  in the default user space. So translating the CTM by 0.5, 0.5 is the right thing to do in your case.</body>
  </mail>
  <mail>
    <header>Re: Emulating PlotIconRefInContext icon transform behavior for  CGImages</header>
    <body>Just to ensure I understand correctly, it accesses and manipulates the individual pixels of the icon bitmap directly, right? If so, it sounds like this is a destructive operation requiring the generation of a copy of the bitmap so that the original is not affected. Correct? Are the exact RGB transforms documented anywhere or could they be provided on this list, so that for pre-Tiger systems we can do the same raw pixel manipulations ensuring that an icon drawn by PlotIconRefInContext would looks exactly the same as a larger CGImage that we would &amp;quot;transform&amp;quot; ourselves? When icons and images are mixed in the same window one would prefer to get the same &amp;quot;looks&amp;quot; when the image need to be disabled or selected. Excellent! david.</body>
  </mail>
  <mail>
    <header>Re: Default for CGContext's Miter Limit</header>
    <body>It's not mentioned in the documentation. I wanted to know what to return the value to after I changed it with CGContextSetMiterLimit.</body>
  </mail>
  <mail>
    <header>Re: Emulating PlotIconRefInContext icon transform behavior for	CGImages</header>
    <body>On Apr 21, 2005, at 6:27 AM, David Niemeijer wrote: IconServices does this by manipulating the color components of each pixel in the icon bitmap. It does not use any private CG facilities to do this, just simple changes to the RGBColor structure corresponding to each pixel, so you could do the same if necessary. In Tiger, we'll have a new API in HIView.h, HICreateTransformedCGImage, which will create a new CGImage from an existing image after applying the Selected or Disabled appearance.</body>
  </mail>
  <mail>
    <header>Re: QD vs CG pixel coords</header>
    <body>That is the way I have done it in all my drawing code, and it is also the way I have seen mentioned elsewhere when people have asked how to fix it.  The only catch with it I've ran into is that if I'm drawing at the edges of some view, then I need to compensate at some places so they don't get drawn off screen. On Apr 22, 2005, at 6:48 AM, Steve Mills wrote:</body>
  </mail>
  <mail>
    <header>QD vs CG pixel coords</header>
    <body>I've read all the &amp;quot;transitioning to Quartz&amp;quot; stuff I've found, but I can't find anything that explains how pixels are rendered in relation to the coordinates of a path. Like in QD if you draw a line that goes from 0,0 to 0,10, the pixel that's down and to the right of the 0,0 coordinate (in graph paper terms) will be filled. In moving our custom controls to HIView and using Quartz to draw them, everything I'm doing ends up looking like a CG line drawn the same way will draw down the actual 0 coordinate, which antialiases it so the pixels on either side of the line are rendered at 50%. This is fine for any path that has more than simple vertical and horizontal lines, but there are times when I'd like the vert/horz parts to draw as solid black, but still let antialiasing smooth out any other angled lines. Is translating the CTM by 0.5,0.5 the Right Thing to Do? I'm sure this has been covered before, but I didn't see anything in the archives. Usually the documentation for a drawing API has some graphics that shows coordinate lines and pixels being filled. I recall this being detailed in the QuickDraw section of IM. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Portrait Mode - Coming in Tiger</header>
    <body>Hi All, We got following reply from Apple: email@hidden Re: Apple Products April 22, 2005 2:20:54 PM GMT+05:30 Please include the line below in follow-up emails for this request. Follow-up:  8532755 Re: Apple Products Hello Amit, Thank you for contacting the Apple Developer Connection regarding information about Apple products. It is Apple's policy not to comment on unannounced products or products under development, whether or not they actually exist. Nor is it our policy to discuss release dates. Product announcements are generally accompanied by a press release, which often gives many of the specifications people are interested in: - Apple Press Info - Press Release Library New product information can generally be found on our Hot News web site: - Apple - Hot News http://www.apple.com/hotnews/ For more information, you may wish to contact: http://hardmac.com/ I hope that the above information is of assistance to you. Best Regards, Hiroyasu Shima Apple Developer Connection -------------------------- Spotlight on Innovation Worldwide Developers Conference 2005 http://developer.apple.com/wwdc Inquiry from Amit Gupta regarding Events Email address: email@hidden Region: Other Asia Pacific/Japan &gt;From &amp;quot;http://www.neowin.net/forum/lofiversion/index.php/t300444.html&amp;quot; I read that Portrait Mode is coming in Tiger. --------------------------- I think there are two things being introduced here. 1. A 3rd party product that allows the Apple Cinema Display to rotate 360 degrees. 2. A software update to Mac OS X (probably being introduced into Tiger) that will rotate your desktop. --------------------------- Is this information authentic and true? Regards Amit Gupta Hi All, Sorry, for this cross post, but I need a quick response.</body>
  </mail>
  <mail>
    <header>Re: Portrait Mode - Coming in Tiger</header>
    <body>On Apr 21, 2005, at 9:36 PM, Amit Gupta wrote: Don't apologize just don't do it. If you are trying to do development the channels are their for you to use. Well you have to wait until the 29th since the NDA is still in effect.</body>
  </mail>
  <mail>
    <header>Portrait Mode - Coming in Tiger</header>
    <body>Hi All, Sorry, for this cross post, but I need a quick response. &gt;From &amp;quot;http://www.neowin.net/forum/lofiversion/index.php/t300444.html&amp;quot; I read that Portrait Mode is coming in Tiger. --------------------------- 1. A 3rd party product that allows the Apple Cinema Display to rotate 360 degrees. 2. A software update to Mac OS X (probably being introduced into Tiger) that will rotate your desktop. --------------------------- Is this information authentic and true? Regards</body>
  </mail>
  <mail>
    <header>CGPathApply not working correctly</header>
    <body>I'm converting a CGPathRef to our internal CPath object using CGPathApply. The CG path is created like so: Yeah, it's just a rectangle, but this is a simple example. Then my conversion routine is this: void CGPath2CPath(CGPathRef cg, CPath&amp;amp; path) void func(void* info, const CGPathElement* element) case kCGPathElementMoveToPoint: case kCGPathElementAddLineToPoint: case kCGPathElementAddQuadCurveToPoint: case kCGPathElementAddCurveToPoint: case kCGPathElementCloseSubpath: The problem I'm having is that every time func is called, element contains a different point, but the type is *always* kCGPathElementMoveToPoint. Am I doing something wrong or is CGPathApply hosed? I'm currently running 10.3.9. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: CGImageCreateWithPNGDataProvider</header>
    <body>On 21 Apr 2005, at 14:08, Nick Nallick wrote: We're mostly using the images for icons on buttons and I can't really say whether it would make any perceptible difference to the user if I changed it (I doubt it). My main worry is that I might be keeping both the PNG and decoded data in memory at the same time. It was my fault. Apparently, I can't count to eight and had missed the last float off. I wondered about calling CGImageCreate twice on the same data provider, once with the decode array and once without. I've not tried this yet, but I have tried calling CGCreateImage using the result of CGImageGetDataProvider on the first image and got a NULL image back, so maybe that won't work. Storing two images is a pain because the images are stored on bevel buttons which only maintain the pointer to the normal image. I'd have to start keeping a global cache or mess around with control properties and that way lies madness. Ideally, what I want is just a way to draw an image darkened or lightened. Jerry</body>
  </mail>
  <mail>
    <header>Emulating PlotIconRefInContext icon transform behavior for CGImages</header>
    <body>I am looking for way to give my images (larger than the 128 max icon size) the same look as icons drawn with PlotIconRefInContext in the kTransformDisabled, kTransformSelectedDisabled, kTransformSelected states. Assuming (perhaps wrongly) that PlotIconRefInContext uses Quartz technologies these days, how would I achieve the same dimming and darkening that icons do. Preferably I would like to be able to have an icon drawn with PlotIconRefInContext and one of my own, larger image coexist in the same window, without the user wondering why they look slightly different when selected, disabled, etc. david.</body>
  </mail>
  <mail>
    <header>Re: Can I change the gamma of CILinearGradient with a colorspace?</header>
    <body>Well, I spoke too soon. This only works with a black to white gradient, and moreover, you need to know which side is black and red/green/blue), the gamma correction affects those, so you end up with a gradient that doesn't match your start/end colors. I thought I'd be clever and apply the gamma correction to a white/ black gradient to get a &amp;quot;correct&amp;quot; ramp and then use CIFalseColor to map it to the real gradient colors. In short, this doesn't really work for the same reasons. I played around with Core Image Fun House for a long time and I couldn't find a combination of filters that would match a Photoshop gradient. I'm ready to give up, but maybe after one last try. I'm told that a CIContext defaults to &amp;quot;a version of the GenericRGB colorspace with the gamma set to 1.0 not 1.8&amp;quot;. As alluded to before, I already looked into this and the options made my head swim. Any pointers are truly appreciated. Thanks all, George</body>
  </mail>
  <mail>
    <header>Re: Can I change the gamma of CILinearGradient with a colorspace?</header>
    <body>This is indeed the way to go. I forgot I'm already doing this. Since CILinearGradient is a generator type filter and doesn't have an &amp;quot;inputImage&amp;quot; parameter, you have to specify it in its own CIImage anyway. So, of course, a gamma adjustment applied to it will only affect the gradient. An inputPower of 2.2 works nicely. I think I'm going to file a doc enhancement request for this. I bet most people are unknowingly using it as is, effectively creating non- linear gradients, even though technically they are linear for a certain colorspace. Thanks all, George</body>
  </mail>
  <mail>
    <header>Can I change the gamma of CILinearGradient with a colorspace?</header>
    <body>I would like to be able to draw a gradient with CILinearGradient that matches what I get from Photoshop. When I draw a CILinearGradient, I'm seeing a mid-point gray of only 32% rather than the 50% PS would give me. More explanation here: The response I got from a bug report is: So it's not a bug, but it leaves me wondering how I can possibly achieve results similar to Photoshop. (is the linear default DeviceRGB?) Hmmm, I'm actually taking a bitmap image, applying several filters, one of which is CILinearGradient (with some transparency so the other stuff shows through). Even if changing the color space can affect the CILinearGradient, it's going to affect my underlying bitmap and other filters as well, isn't it? Maybe I'm barking up the wrong tree here. Should I just render the gradient in a separate CIImage, apply CIGammaAdjust and then composite it with the other image? Since the order of filters can be important, this might not be as elegant, but it probably makes more sense. Thanks! George</body>
  </mail>
  <mail>
    <header>Re: Remote event handling application stop execution on popping of	menu</header>
    <body>Thanks for response. I've explored this problem further. I,m pasting my findings below. I've post this question to Cocoa and webkit list as well because now I think problem is going to other domain.. Probelm is not at all with using CGPostKeyboardEvent and CGPostMouseEvent API as I thought earlier: I've further investigated this issue and came up with strange observations. Our application is combination of flash and Cocoa. Using webkit we're playing the swf file. Our flash application takes drawing data from centeral server. As soon as I click on the menu of my application it seems to stops execution. It doesn't&amp;nbsp; update the its screen with new data from centeral server, and as soon as I dispose the menu it starts taking the data from server and update the screen. This behaviour is not limited to drawing operations, log messages are also not&amp;nbsp;displayed.&amp;nbsp;Same flash application is also responsible to send the event data to my other event simulating application over a socket, so it doesn't send the event data to my application when menus are pops up.&amp;nbsp; Above behaviour has confused me to believe that something is wrong with my event handling application...but it&amp;nbsp;was&amp;nbsp;in&amp;nbsp;actual because&amp;nbsp;of Cocoa/Flash application behaviour. &amp;gt; I did not have much idea about debugging from gdb so I'll start Hmm, I still don't have any ideas about what might be causing this. Pulling down a menu will block the main event loop of your application, but if you're running in a separate thread (or even a separate process), that certainly wouldn't affect those other threads. Unless your secondary threads are using some kind of mutex that is shared with the main thread? It's generally better not to include attachments to the list - it's allowed, but not encourage, and I think there's a small size limit. You could send it directly to me if you want. -eric</body>
  </mail>
  <mail>
    <header>Re: Remote event handling application stop execution on popping of	menu</header>
    <body>On Jun 17, 2006, at 12:21 AM, Gurmit Teotia wrote: Hmm, I still don't have any ideas about what might be causing this. Pulling down a menu will block the main event loop of your application, but if you're running in a separate thread (or even a separate process), that certainly wouldn't affect those other threads. Unless your secondary threads are using some kind of mutex that is shared with the main thread? It's generally better not to include attachments to the list - it's allowed, but not encourage, and I think there's a small size limit. You could send it directly to me if you want.</body>
  </mail>
  <mail>
    <header>Kernels and arrays</header>
    <body>Is it possible to pass an array of elements to kernel which is more then vec4 (4 elements that is) ? Using CIVector you can create an array with more then 4 but you can only take max of vec4 in the parameter. Also is it possible to get a specific pixel from an image. For example if I pass to images (samplers) to the kernel and I want to get a specific (for example 1st) pixel from it and enumrate futher through the whole image applying the value of every pixel in one image to the current pixel in another.</body>
  </mail>
  <mail>
    <header>Re: Remote event handling application stop execution on popping of	menu</header>
    <body>I apologies if this question does not belongs to this list but I think it should be. Actually I'm creating two threads one thread read the event data from sockets and add it to static buffer while other thread take that event data and simulate the events using CGPostKeyboardEvent and CGPostMouseEvent. I've used combination of C and objective-C to implement the solutions. I'm using NSThread class to create the threads. 1.When event handling module was part of main cocoa application: I've created two threads using NSThread, one was receiving the data from sockets adding to static buffer while other one was taking it from buffer and simulating it using above mentioned api. I can attach relevent source code of my application, it is really not big but I fear if someone will look at them because of time constraint also I'm not clear about the attachment policy of list. Can I attach source code in&amp;nbsp;emails, I was not able to find anything on it. Gurmit Hi Gurmit, I think you haven't really given enough information in order to even guess at a solution. (And I'm not sure this question really belongs on quartz-dev anyways. It sounds more like a Cocoa question.) For example, how is your application simulating the events that it receives? How does it receive the events? What is the main thread doing? Have you tried connecting to your application in gdb while a menu is open and debugging it at that time? -eric</body>
  </mail>
  <mail>
    <header>Re: Remote event handling application stop execution on popping of	menu</header>
    <body>I think you haven't really given enough information in order to even guess at a solution. (And I'm not sure this question really belongs on quartz-dev anyways. It sounds more like a Cocoa question.) For example, how is your application simulating the events that it receives? How does it receive the events? What is the main thread doing? Have you tried connecting to your application in gdb while a menu is open and debugging it at that time? On Jun 16, 2006, at 10:27 PM, Gurmit Teotia wrote:</body>
  </mail>
  <mail>
    <header>Re: Remote event handling application stop execution on popping of	menu</header>
    <body>It would be useful if you could describe what APIs your app is using, and how it communicates with the remote system. From the description, it sounds vaguely like you have some code that you expect to run at all times, but that is not running in a modal runloop such as menu servicing. There are so many possible paths that could lead a program to stop executing some of it's code that without information on the structure of your program and the API being used, nothing I could suggest would be anything more than a wild guess. On Jun 16, 2006, at 10:27 PM, Gurmit Teotia wrote:</body>
  </mail>
  <mail>
    <header>Fwd: Remote event handling application stop execution on popping of	menu</header>
    <body>Hi, ---------- Forwarded message ---------- Date: Jun 16, 2006 7:07 PM Subject: Remote event handling application stop execution on popping of menu To: I've developed an application to simulate the events on remote computer.&amp;nbsp;This application is only handling the events. For&amp;nbsp;display purpose&amp;nbsp;we are using third party tools. Only one problem I'm facing in simulating the events is that application stops simulating the events once I click on menu from remote computer (from logs it seems to me that the&amp;nbsp;threads in application have stop execution).&amp;nbsp;This problem only occurs&amp;nbsp;if the menu is also of my application. I can easily click on the menus of&amp;nbsp;other applications. Earlier I'd developed that event&amp;nbsp;handling&amp;nbsp;module as extension part of my main Cocoa application.&amp;nbsp;Later I thought that I should run that module as&amp;nbsp;seperate command line application from my Cocoa application. Even after that problem&amp;nbsp;was not resolved. I've tried to run this command line application from terminal as well but&amp;nbsp;as soon as I&amp;nbsp;click on menu of terminal window it stops generating the events. But&amp;nbsp;as I move&amp;nbsp;away the mouse on local computer and click some where to dispose the menu,&amp;nbsp;threads start execution and all the pending events are simulated&amp;nbsp;properly. To investigate this behaviour I've put NSLog in application. I've observed&amp;nbsp;that logging messages are no more printed when application stops responding to remote event. But as I hinted earlier, after disposing the menu logging messages start printing on window. But here interstesting point is that, from logs messages's timestamp it does not look that application stops execution. There is no time gap in log messages's timestamp</body>
  </mail>
  <mail>
    <header>Re: CIImageAccumulator deadlock</header>
    <body>On Jun 16, 2006, at 10:03 AM, Timothy J. Wood wrote:</body>
  </mail>
  <mail>
    <header>Re: CIImageAccumulator deadlock</header>
    <body>On Jun 16, 2006, at 9:50 AM, Timothy J.Wood wrote:</body>
  </mail>
  <mail>
    <header>-[CIImageAccumulator setImage:dirtyRect:]</header>
    <body>Sort of related to my previous question, but only peripherally, I think. The documentation for this method is clear as mud.  In particular, it is doesn't say whether the input image needs to have the same extent as the accumulator (i.e., we are really replacing ALL the context) or whether the image should just enclose at least the dirty rect (i.e., we are replacing a portion of the existing image's backing store via something like glTexSubImage2D). My accumulator code works for a little while when &amp;quot;setting&amp;quot; the content by just passing an image that has its extent equal to the dirty rect (i.e., passing in a 1x1 image and a 1x1 dirty rect).  It still hangs later when applying my filter.</body>
  </mail>
  <mail>
    <header>CIImageAccumulator deadlock</header>
    <body>I'm having very little luck using a CIImageAccumulator.  I usually get deadlock... here is a flatted backtrace from 'sample': 100 -[CIContext drawImage:inRect:fromRect:] 100 -[CIOpenGLContextImpl render:] 100 -[CIOpenGLContextImpl renderAccel:matrix:bounds:] 100 fe_image_render 100 fe_image_render_ 100 fe_tree_render_image 100 fe_tree_render_image_ 100 fe_tree_render 100 fe_tree_render_div 100 fe_tree_render 100 fe_tree_render_apply 100 fe_tree_create_texture 100 fe_texture_new 100 texture_retain 100 fe_image_texture_ref 100 image_buffer_texture_ref 100 fe_context_texture_load 100 image_accum_provide_buffer 100 image_accum_sync 100 fe_tiled_buffer_foreach 100 image_accum_render 100 fe_tree_render_image 100 fe_tree_render_image_ 100 fe_tree_render 100 fe_tree_render_over 100 fe_tree_render 100 fe_tree_render_unary 100 fe_tree_create_texture 100 fe_texture_new 100 texture_retain 100 fe_image_texture_ref 100 image_buffer_texture_ref 100 fe_context_texture_load 100 image_accum_provide_buffer 100 image_accum_sync 100 fe_tiled_buffer_foreach 100 image_accum_render 100 fe_tree_render_image 100 fe_tree_render_image_ 100 fe_tree_render 100 fe_tree_render_over 100 fe_buffer_begin 100 pthread_cond_wait 100 semaphore_wait_signal_trap 100 semaphore_wait_signal_trap I'm doing what I think is a very simple operation as a test: CIImage *cell = [_cellImage imageByApplyingTransform:CGAffineTransformMakeTranslation(position.x, CIImage *cellOverImage = [cell The filter is a simple custom filter that does nothing interesting yet.  It takes two images as input and is just trying to produce a gradient based on the samplerCoord() for the first image (just as a test if I can make CIImageAccumulator work at all). The accumulator is big (2560x1600 for a fullscreen 30&amp;quot; display), so maybe that is causing the issue via tiling.</body>
  </mail>
  <mail>
    <header>Remote event handling application stop execution on popping of menu</header>
    <body>I've developed an application to simulate the events on remote computer.&amp;nbsp;This application is only handling the events. For&amp;nbsp;display purpose&amp;nbsp;we are using third party tools. Only one problem I'm facing in simulating the events is that application stops simulating the events once I click on menu from remote computer (from logs it seems to me that the&amp;nbsp;threads in application have stop execution).&amp;nbsp;This problem only occurs&amp;nbsp;if the menu is also of my application. I can easily click on the menus of&amp;nbsp;other applications. Earlier I'd developed that event&amp;nbsp;handling&amp;nbsp;module as extension part of my main Cocoa application.&amp;nbsp;Later I thought that I should run that module as&amp;nbsp;seperate command line application from my Cocoa application. Even after that problem&amp;nbsp;was not resolved. I've tried to run this command line application from terminal as well but&amp;nbsp;as soon as I&amp;nbsp;click on menu of terminal window it stops generating the events. But&amp;nbsp;as I move&amp;nbsp;away the mouse on local computer and click some where to dispose the menu,&amp;nbsp;threads start execution and all the pending events are simulated&amp;nbsp;properly. To investigate this behaviour I've put NSLog in application. I've observed&amp;nbsp;that logging messages are no more printed when application stops responding to remote event. But as I hinted earlier, after disposing the menu logging messages start printing on window. But here interstesting point is that, from logs messages's timestamp it does not look that application stops execution. There is no time gap in log messages's timestamp</body>
  </mail>
  <mail>
    <header>Python binding &amp;amp; pointer</header>
    <body>Hi, all. I'm interested in Quartz scripting. Now, I've found some Python APIs take pointer as arguments. How could I pass pointers to those APIs in Python? For example, CGPDFDocument class has a method 'getVersion' which takes 2 pointers to integer as arguments. What should I pass to it in Python? In another example, 'setCallback' method in CGPDFOperatorTable class takes a callback function. Passing a function object raises a TypeError. It must be a pointer. How can I pass a function as pointer? Searching quartz-dev archives give me how to pass pointer with setLineDash, but it doesn't work in my case. Working on SWIG gives me no solution yet. Thanks in advance. Regards, K.Mino</body>
  </mail>
  <mail>
    <header>Re: How to invert clip?</header>
    <body>I've got the desired behavior by first tracing a counter-clockwise rectangle around the entire bounds, then tracing a clockwise polygon around the outline of all the rectangles I wish to clip out (i.e. just trace the outside edges and not the intersections). It's a mundane little algorithm, and I'm trying to find all the weird edge cases, but it works.</body>
  </mail>
  <mail>
    <header>Re: Text space units</header>
    <body>Nick, Every thing seems to work fine now ! As I'm using ATSUI, so I have no problem of font size any more. When the view is flipped (table view), I scale and translate CTM as you told me to do and text is drawn in the right direction. Thanks for your help. Pierre Pierre ______________________________________________________________________ Pierre FAMECHON ADDONA</body>
  </mail>
  <mail>
    <header>Re: Text space units</header>
    <body>On Jun 14, 2006, at 2:18 PM, Pierre FAMECHON wrote: The d element of the matrix is the y-axis scale factor, but things can get complicated quickly if you start doing fancy things with your matrix like implementing rotation and shear transforms.  Therefore it's best not to just pull numbers out of the matrix like this.  One technique you can use, if you must, is to use the matrix to transform a vector and then determine the height of the scaled vector.  For example, one end of the vector can be at the origin (0,0) and the other at the height of your font (0,12).  If you transform these points, you'll get the end points of a line that specifies the location of your text. The problem with CGContextShowTextAtPoint is that you're limited to 8 bit characters.  If you will only ever need to use ASCII characters this is fine, but if you might need to use extended characters at some point you're a lot better off using ATSUI.  There is probably a way to tell an ATSUI layout object which type of coordinate system you're using.  I personally think it's easier to carry around a matrix with the layout object and apply this matrix to the context before I draw the layout.  Therefore I always tell ATSUI to draw at the origin (0,0) and make any translation and other adjustments in my matrix. Matrix math can get confusing pretty fast but you convert a right hand coordinate system (Cocoa) to a left hand coordinate system (ATUSI) as follows: Nick</body>
  </mail>
  <mail>
    <header>Re: Text space units</header>
    <body>Nick and David, thanks for your help. I have inspected text matrix and they are different. So, if a divide the font size by the d value of the matrix, which is 12 for window drawing and 1 for PDF drawing, it seems to be OK, is that right ? If so, why do I see in sample code h/10 and not h/matrix.d ? If not, how the matrix should interact on font size ? To make you understand what I'm trying to do, here is a small description : I've a framework (like everybody here !) and graphical C++ classes inside to simply draw strings on everything : window, PDF and printing. I've two Carbon/Cocoa solutions : Quartz and ATSUI With Quarts, I use CGContextShowTextAtPoint. It's simple and now, it seems to work. But it's limited compared with ATSUI and use of old Carbon font IDs is not simple (may be it is but I have to find how ?). With ATSUI, it seems to work but I have a problem : when my Cocoa view is flipped (TableViews), the text is top-down. What is the best solution to unflip the view or to change text drawing ? Thanks, Pierre ______________________________________________________________________ Pierre FAMECHON ADDONA</body>
  </mail>
  <mail>
    <header>Re: How to get the color table from a CGImage ?</header>
    <body>Re: On Jun 14, 2006, at 6:12 AM, Scott Thompson wrote: Quartz does support indexed color for images but Scott is right that most Quartz images are not indexed color images. You can get the CGColorSpaceRef for that image but there is no API to determine what kind of color space it is. I suggest filing an enhancement request if this kind of information is important to you.</body>
  </mail>
  <mail>
    <header>[Job opportunity] - Cocoa and Quartz</header>
    <body>&lt;span style='font-size: 10.0pt;font-family:Arial;color:black'&gt;Mac Software Developer position open in &lt;st1:place w:st="on"&gt;&lt;font size=2 face=Arial&gt;&amp;nbsp; My client is one of the leaders in the desktop publishing industry&lt;font color=navy&gt;The position is a full-time, permanent position that entails developing new products and technologies to help serve the different needs of a diverse client base.&amp;nbsp; &lt;font color=black&gt;&lt;span style='color:black'&gt; will&lt;span style='color:black'&gt;e from enterprise level web applications to components within existing desktop applications and all points in-between&lt;font color=navy&gt; &lt;span style='font-size: 10.0pt;font-family:Arial;color:navy'&gt; font-family:Arial'&gt;The specific experience required for this position is as follows: &lt;span style='font-size:10.0pt;font-family:Arial'&gt;2 + years development experience in the following development areas: C/C++, Objective-C/Cocoa &lt;span style='font-size:10.0pt;font-family:Arial'&gt;Experience in developing custom controls drawn using Quartz drawing engine &lt;font size=2 face=Arial&gt; &lt;span style='font-size:10.0pt;font-family:Arial'&gt;Excellent object-oriented design skills&lt;span style='font-size:10.0pt;font-family:Arial'&gt; &lt;span style='font-size:10.0pt;font-family:Arial'&gt;Knowledge of the publishing, multimedia, and/or graphics design industries a plus &lt;font size=2 face=Arial&gt; font-family:Arial'&gt; font-family:Arial'&gt;Please send a copy of your resume to &lt;a href="mailto:email@hidden"&gt;email@hidden for immediate consideration.&amp;nbsp; There is a relocation package being offered, and salary is negotiable based on industry! Experience. font-family:Arial'&gt; font-family:Arial'&gt; &lt;span style='font-size:20.0pt;font-family:"Kunstler Script";color:navy;font-weight: bold'&gt;Nicole Maddox &lt;span style='font-size:16.0pt;font-family:Arial;color:navy;font-weight:bold'&gt;NOVO Recruiting &lt;span font-style:italic'&gt;The Art and Science of Recruiting &lt;span style='font-size: 12.0pt'&gt;</body>
  </mail>
  <mail>
    <header>Re: How to invert clip?</header>
    <body>On 14 Jun 2006, at 14:13, Scott Thompson wrote: It would be quite handy to have the extra winding rule for occasions like this. There seem to be very graphics packages which support it, which is a shame as it's trivial to support. I blame the PostScript imaging model myself. I think clipping with a mask is probably the only way to go here.</body>
  </mail>
  <mail>
    <header>Re: How to invert clip?</header>
    <body>On Jun 14, 2006, at 8:04 AM, Nick Nallick wrote: I drew it out on a piece of paper and it makes sense to me too now :-)</body>
  </mail>
  <mail>
    <header>Re: How to get the color table from a CGImage ?</header>
    <body>On Jun 14, 2006, at 5:32 AM, Malik NakaMura wrote: As a general rule, Quartz does not support indexed color models.  I don't know what kind of image you are reading in, but Quartz is probably representing it as a 32 bit ARGB color and the CGImageRef has no color table.</body>
  </mail>
  <mail>
    <header>Re: How to invert clip?</header>
    <body>On Jun 14, 2006, at 1:56 AM, Jerry wrote: Yea, you're right.  I was thinking of it backwards and trying to make the transparent area be the fill region in my mind. Thanks, Nick</body>
  </mail>
  <mail>
    <header>How to get the color table from a CGImage ?</header>
    <body>I have a CGImageRef and I would like to know how can I get the color table ? I tried to use CGImageGetColorSpace(...) but I dont know what can I do with a CGColorSpaceRef...</body>
  </mail>
  <mail>
    <header>Re: How to invert clip?</header>
    <body>On 14 Jun 2006, at 00:04, Nick Nallick wrote: This sounds right to me - it's the &amp;quot;non-zero winding rule&amp;quot;, not the &amp;quot;greater-then-zero winding rule&amp;quot;. Your overlapping rectangles would have winding rules less than zero.</body>
  </mail>
  <mail>
    <header>Re: Text space units</header>
    <body>Re: On Jun 13, 2006, at 10:54 AM, Nick Nallick wrote: The size of text is determined by both the text matrix and the font size. I suspect the issue is that when drawing to your Cocoa view you are seeing the fact that Cocoa has already set the text matrix to something other than the identity matrix whereas when you draw to a PDF context that you create, the text matrix is the identity matrix unless you change it to something else. You can check this by using the function CGContextGetTextMatrix with each of the contexts you are working with and inspect the text matrix that is returned to see if that explains the differences you are seeing. In the book &amp;quot;Programming with Quartz&amp;quot; this issue is discussed and we recommend explicitly setting the text matrix in your drawing code IF you are using the low level Quartz text drawing calls. That way you isolate yourself from any text matrix that may already be in place when your drawing code is called. Hope this helps, David</body>
  </mail>
  <mail>
    <header>Re: How to invert clip?</header>
    <body>I'm sorry about the double post.  I accidentally hit cmd-shift-D while composing the message and Mail sent the incomplete message. This also surprised me.  To nay-say it,  looked at a winding rule case where you have several small overlapping rectangles inside a larger rectangle wound in the opposite direction.  I think there is a bug in the Quartz fill logic in this case.  I would expect the union of the area formed by the inner rectangles to  form a hole in the outer rectangle.  Instead what I get is something closer to an even/ odd algorithm where the area where all of the inner rectangle intersect is included in the fill.  If anybody wants to look at it and prove me wrong I can post an image somewhere.  This should probably be written up as a bug. If you're willing to use a deprecated function, a workaround would be to use ClipCGContextToRegion() to clip your CG context to a QuickDraw region.  This probably only works for display or bitmap contexts, and it could stop working in a future OS release (but IMO that's unlikely anytime soon). Nick</body>
  </mail>
  <mail>
    <header>Re: How to invert clip?</header>
    <body>On Jun 13, 2006, at 4:27 PM, Scott Thompson wrote: This also surprised me.  To nay-say it,  looked at a winding rule case where you have several small overlapping rectangles inside a larger rectangle wound in the opposite direction.  I think there is a bug in the Quartz fill logic in this case.  I would expect the union of the area formed by the inner rectangles to  form a hole in the outer rectangle.  Instead what I get is something closer to an even/ odd algorithm where the area where all of the inner rectangle intersect is included in the fill.</body>
  </mail>
  <mail>
    <header>Re: How to invert clip?</header>
    <body>On Jun 13, 2006, at 5:16 PM, Joseph Kelly wrote: That  surprises me.  If all of your interior rectangles are clockwise and the exterior rectangle is counter clockwise then it should, more or less, treat the interior places as &amp;quot;holes&amp;quot;.  :-( Regions are a pixel-based concept and Quartz is a resolution independent API.  Unfortunately Quartz does not have the geometric capability of creating unions and intersections of arbitrary paths (a &amp;quot;hard&amp;quot; problem to begin with).  Nor does it have a CSG-like primitive that would support what you are trying to do. I might be inclined to use OpenGL in this case instead of Quartz 2D. By in large each element of content could handle it's own drawing and rely on the graphics card to do the compositing. I wouldn't dismiss the technique as inefficient out of hand.  The Mac OS X graphics architecture is very different from QuickDraw's architecture.  Some things that seem inefficient (like image masking) can be much faster, and some which seem like they should be fast (like clipping to a path) can wind up being slow.</body>
  </mail>
  <mail>
    <header>Re: How to invert clip?</header>
    <body>When I do this, I get nearly the same results -- not to mention there are cases when I have many overlapping rectangles and the exact direction does not seem deterministic -- I've still got places where the image shows up where I don't want it to. This looks increasingly non-trivial, and I was hoping Quartz would offer something akin to QuickDraw regions, which are subtractive as well as additive. I am working on an app that lets the user lay out various 2d objects, all of which are allowed to overlap. Some of the objects contain dynamic content like QuickTime movies, and it is difficult and inefficient to draw back to front on every update. Like I said, this was very easy with regions -- I would create a region the size of the object, then grab a list of overlapping objects, subtract their bounds from the region, set the clip, and draw. It looks like that API is 10.4 only -- and the approach sounds like it might be inefficient for what I'm doing. Joe K.</body>
  </mail>
  <mail>
    <header>Re: Text space units</header>
    <body>On Jun 13, 2006, at 10:47 AM, Pierre FAMECHON wrote: Text space is the coordinate system used when drawing all text.  The text matrix is used to transform between the text coordinate space and the user coordinate space.  You can set this via CGContextSetTextMatrix().  This can be useful for doing things like simulating italic text by skewing the coordinate system, but for the most part you can ignore it. It's impossible to say without more information about what you're doing.  Generally the point size you set is what you get when drawing to the screen or to PDF. For various reasons, mostly involving the translation of encoded characters to font gylphs, it's generally better not to use CG directly to draw text.  Unless you have specific needs, you're probably better off using Cocoa (or ATSUI if you prefer) to draw your text. Nick</body>
  </mail>
  <mail>
    <header>Re: MLTE &amp;amp; PDF</header>
    <body>One problem down, and another immediately emerge.  I can record an MLTE object in PDF, but how do you prevent MLTE from filling in an opaque background color ?  According to the documentation the background &amp;quot;can&amp;quot; be an RGB color... (The API let you set with RGB (no alpha) ) but how do you prevent a background altogether. All hints appreciated. Interesting.  I gave it a shot, but I still got nothing.  In despair, I attached the MLTE object to an (irrelevant)  front window  and boing, it worked !.  This is really a nuisance, since generating (PDF) graph objects of all sorts of stuff without window interaction is bread and butter in many of our apps.   Radar, here we go again... On Jun 12, 2006, at 11:10 PM, Nick Nallick wrote: On Jun 12, 2006, at 6:41 PM, email@hidden wrote: I use ATSUI directly rather than through MLTE, but one thing I've found to be important is to remove the CGContext tag from the text object after I'm done drawing.  Apparently ATSUI retains the context which then causes the PDF not to complete.  You may want to try setting that tag to a null value after you draw.</body>
  </mail>
  <mail>
    <header>Re: MLTE &amp;amp; PDF</header>
    <body>Interesting.  I gave it a shot, but I still got nothing.  In despair, I attached the MLTE object to an (irrelevant)  front window  and boing, it worked !.  This is really a nuisance, since generating (PDF) graph objects of all sorts of stuff without window interaction is bread and butter in many of our apps.   Radar, here we go again... On Jun 12, 2006, at 11:10 PM, Nick Nallick wrote: On Jun 12, 2006, at 6:41 PM, email@hidden wrote: I use ATSUI directly rather than through MLTE, but one thing I've found to be important is to remove the CGContext tag from the text object after I'm done drawing.  Apparently ATSUI retains the context which then causes the PDF not to complete.  You may want to try setting that tag to a null value after you draw.</body>
  </mail>
  <mail>
    <header>Re: How to invert clip?</header>
    <body>On Jun 13, 2006, at 11:48 AM, Joseph Kelly wrote: Yes.  Paths have differing fill types (winding rules) and you can define a path with arbitrary holes cut out of it.  What you would usually do to create this kind of mask is start with a rectangular path that encircles the entire area of interest, and then add additional contours inside that rectangle that let the information you want to expose come through.</body>
  </mail>
  <mail>
    <header>Text space units</header>
    <body>Hi, In Apple Quartz documentation for CGContextSetFontSize, it's said that size argument is specified in &amp;quot;text space units&amp;quot;. What are text space units ? How can I get or change text space units relative to a CGContext ? When I display strings in a Window CGContext, size seems to be the desired font size (9, 10, 11...48) divided by 10. Why, I don't know (I found that in sample code). But when the same code is used to generate a PDF, text is very, very small. How can I get the PDF CGContext text space units to calculate the right scale to apply to my font size ? I'm using Cocoa and C++. Thanks, Pierre ______________________________________________________________________ Pierre FAMECHON ADDONA</body>
  </mail>
  <mail>
    <header>How to invert clip?</header>
    <body>I posted this on the Cocoa-dev list w/ no response, and considering it is a little more appropriate here: I would like to set the current graphics context clip area to exclude from drawing the union of an arbitrary number of intersecting rectangles. I've used NSBezierPath to build a path of rectangles, and I would like to &amp;quot;invert&amp;quot; this (in the Photoshop sense of the word) and use it as my clip -- I can't find a method to do this. I've tried changing the winding rule to even-odd, but this fails for sub- intersections, etc. Any ideas? I.e. In the old days I would've accomplished this by creating a region spanning the entire view and then subtracting rectangular chunks from it. Is there anything similar in Quartz?</body>
  </mail>
  <mail>
    <header>Re: MLTE &amp;amp; PDF</header>
    <body>On Jun 12, 2006, at 6:41 PM, email@hidden wrote: I use ATSUI directly rather than through MLTE, but one thing I've found to be important is to remove the CGContext tag from the text object after I'm done drawing.  Apparently ATSUI retains the context which then causes the PDF not to complete.  You may want to try setting that tag to a null value after you draw.</body>
  </mail>
  <mail>
    <header>MLTE &amp;amp; PDF</header>
    <body>Hi, (I posted this initially on the Carbon-dev list, but this is probably the right address ?) I am having a hard time finding documentation and a way of recording MLTE into PDF. Her is some pseudocode of what I have tried so far: // the Rect lr corresponds to the viewrect  0,0,right,bottom //////////////////////// I have checked that I can record other elements in this PDF and all is good.  I have tried with all CTM flippings I can think of, but the MLTE object is not recorded. (The TXNDrawObject does not add a single byte). I have checked the VisibilityTag, and it is on.  The only difference between this and working TXNObjects in windows is that this is not attached to any window . Any help or comments would be much appreciated. Johan email@hidden&amp;lt;/x-tad-bigger&amp;gt; _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: sseCGSConvert</header>
    <body>On Jun 9, 2006, at 12:43 PM, M L wrote: Does &amp;quot;the modern&amp;quot; SSE have the same permute crossbar instructions that AlitVec has?  I seem to recall that the permute crossbar was a big win on AltiVec early on.  If there haven't been any changes to SSE It could be that you are seeing a limitation of SSE relative to the capabilities of AltiVec.</body>
  </mail>
  <mail>
    <header>Re: Performance of a custom Level Meter control, using CGLayers.</header>
    <body>In my experience clipping can be a significant drag on Quartz 2D performance.  Since your clipping rectangles are axis aligned and integral valued, you might try turning off antialiasing for the level meter.  That can speed things up a bit. for each level of the meter and use those as cached representations of the meter to a given level. Or you could use lots of individual images for the each level's segment, and draw those images instead.</body>
  </mail>
  <mail>
    <header>Performance of a custom Level Meter control, using CGLayers.</header>
    <body>This is my first attempt at using CG to draw a control.  It's a sound  level meter, that takes as input values from QT.   The values are fed at 15fps. What I'm seeing is that on a DP 2.5 G5, the level meter is taking about 10% of the CPU time.  This is confirmed using shark. I'd like to know if I'm doing something dumb, or if there's a much faster way to do what I'm already doing. Here's my drawRect function (the control is just an NSView): // Fill the inside the an alternating colour rounded rectangle, including limit colours CGContextRef myContext = [[NSGraphicsContext currentContext] // Draw the main bar - the current volume // Draw the peak volume I'm using two CGLayers, as my previous attempt at doing this (using a CGContextDrawShading) was quite CPU intensive at my shading function, I think primarily because it was getting called a huge number of times.     The boxLayer is a background for the control - with some basic dark/light areas, and a greyish background.   The lmLayer is the level meter itself. It's a pre-rendered shading at 100% volume. I'm using a clip area bound to whatever the current volume level is in order to show the right part of lmLayer. I wanted to do things the way I've done them (rather than just rendering an image, and copying some clipped region of it) so that the control would look nice if it were scaled. Strictly speaking, for this app it's not really a requirement - but I'm still interested in knowing if there's a more performant way to do this. I switched to using a CGLayer because the Quartz docs suggested this was a better idea that doing offscreen drawing into a bitmap. Particularly if you were going to reuse the layer many times (which I am)...   It's certainly loads better than doing the shading function every time (that implementation took between 8-17 CPU time).    Shark is showing that roughly 8-10% of total program time is spent in the drawRect method, and that's split pretty evenly between the three CGContextDrawLayerAtPoint calls (2.7%, 3.8% and 1.7% respectively (these figures were taken from a run were drawRect totaled 8.8%)).  I have saved the shark profile, and I'm happy to send that to anyone that would like to see it. Kind Regards, Neil Clayton I've included the code to setup the layers as well, for reference: CGRect cgRect = [Utils nsRectToCGRect:NSFloorIntegralRect([self [self addRoundedRectToPath:context rect:cgRect ovalWidth:4 // Fill it greay // Draw a dark part along the bottom // Draw some darkness at the top // Drawing a darkish line on the outside [self addRoundedRectToPath:context rect:cgRect ovalWidth:4 CGSize size = CGSizeMake([self bounds].size.width, [self CGContextRef parentContext = [[NSGraphicsContext currentContext] CGRect cgRect = [Utils nsRectToCGRect:NSFloorIntegralRect([self CGShadingRef shading = CGShadingCreateAxial( cspace,                    // CGColorSpaceRef colorspace, cgRect.origin,			   // CGPoint start, CGPointMake(cgRect.origin.x + cgRect.size.width, cgRect.origin.y),   // CGPoint end, function,                  // CGFunctionRef function, false,                     // bool extendStart, false                      // bool extendEnd [self addRoundedRectToPath:context rect:cgRect ovalWidth:4 --- Neil Clayton</body>
  </mail>
  <mail>
    <header>Re: Image Size problems with CICircularWrap</header>
    <body>Attachment:</body>
  </mail>
  <mail>
    <header>Re: Image Size problems with CICircularWrap</header>
    <body>The big difference is that Fun House scales the image down to screen size (which would most likely be smaller than 2560 pixels wide) and then applies the filter. So, if you scale the image down first, it would work for you as well, but none the less, this shouldn't barf at you. So please file a bug as this seems to be a problem with the CICircularWrap filter.</body>
  </mail>
  <mail>
    <header>Save as PDF in print dialog</header>
    <body>Is there any way to know what the user is actually printing to? Did he choose the preview button, maybe he chose to save as pdf, or he is printing to a printer. Can that be done? Thanks AC</body>
  </mail>
  <mail>
    <header>Re: CMProfileRef for CGColorSpace</header>
    <body>The quick answer is no.  There are many ways to create a CGColorSpace, several of which do not reference an ICC profile.  Not to mention, you can even create memory based ColorSync profiles which complicates the situation even more. Also, most of the people on the quartz-dev and colorsync-dev lists work at Apple and so emailing twice on the weekend really won't do you much good until Monday.</body>
  </mail>
  <mail>
    <header>CMProfileRef for CGColorSpace</header>
    <body>Is there any way, documented or not to get a CMProfileRef or even a path to the profile that was used to create a CGColorSpace? Thanks AC</body>
  </mail>
  <mail>
    <header>Re: Masking?</header>
    <body>I don't have any direct experience with this method of masking, but my take on the documentation is that the decode array parameter allows for color range limiting, but that it is not intended to invert the color values, which is what you're trying to use it for. I'm guessing that you need to accomplish this inversion at an earlier stage. Good luck, Brian</body>
  </mail>
  <mail>
    <header>Re: Basic rect drawing questions</header>
    <body>A few people commented on this thread that Quartz drawing with antialiasing turned of produces 'unpredictable' results, which is not true.  It produces significantly different results than it does with antialiasing turned on but it is well defined.  When antialiasing is turned on Quartz computes the percentage of each pixel covered by the shape being drawn.  This value is multiplied by the current alpha and composited on top of the existing image.  When antialiasing is turned off *any* pixel with non-zero coverage is treated as if it has 100% coverage.  This generally causes shapes to appear much wider with antialiasing turned off. This behavior is significantly different from most other drawing systems, though as Scott Thompson mentioned a few weeks ago it is similar to PostScript output devices (which typically have a much resolution, so it is less noticeable).  This has caused a never ending headache during the development of Mathematica 6 while trying to implement a system which draws similar looking graphics regardless of whether antialiasing is turned on or off (aside from the antialiasing of course).  I would recommend staying away from non- antialiased Quartz if you can. Rob Raguet-Schofield (rob ra gA skO fEld)</body>
  </mail>
  <mail>
    <header>Masking?</header>
    <body>I'm having trouble with masking. What I'm trying to do is write a converter that load two PICT images from a resource fork.  One of them is RGB color and the other is a B&amp;amp;W mask.  Then apply the mask and save the result and a png. I load them both into 32-bit CGImages, named rgbImage &amp;amp; bwImage. This works.  I can view the images and save them as pngs.  no masking yet. Then I create an offscreen CGBitmapContextCreate using bitsPerComponent = 8, CGColorSpaceCreateDeviceGray and I then draw the bwImage into that 8-bit CGBitmapContext and get a CGImageRef (greyImage) from that. now i'm ready to apply the mask. the problem is that result is the opposite of what I want.  i.e. the good part is masked out and that remains is shapes border.   So, I am getting the correct images and masking is taking place.   I just need to invert my mask. So, I think CGImageMaskCreate takes a decode parameter that should invert the mask, but that doesn't seem to work for me.  I'm not sure how i should specify it?  none of the following have worked.  I always get the same non-inverted mask. CGImageRef myMask = CGImageMaskCreate( CGImageGetWidth(greyImage), CGImageGetHeight(greyImage), CGImageGetBitsPerComponent(greyImage),   // 8 CGImageGetBitsPerPixel(greyImage),              // 8 CGImageGetBytesPerRow(greyImage), CGImageGetDataProvider(greyImage), decode, false);  // don't anti-alias Jim Wrenholt</body>
  </mail>
  <mail>
    <header>Re: CG in an HIView</header>
    <body>usually I fill first and then stroke. if you want the stroke to be entirely inside, you'll want to inset by half a line width before you stroke.</body>
  </mail>
  <mail>
    <header>Re: Basic rect drawing questions</header>
    <body>On May 10, 2007, at 9:45 PM, Rick Mann wrote: 0,0 is saying you want to start drawing from the bottom left corner of a point square with the path proceeding to the right along the bottom edge of the point square and so on... In other words your path lies exactly centered between adjacent point squares for the whole perimeter of your rectangle. If you stroke this with a line width of 1.0 then 0.5 of that stroke hits the point squares on the inside edge of the path and 0.5 falls in the point squares on the outside edge of the path. When this gets mapped to pixels (assuming a 1:1 point-to-pixel mapping) then you are asking the system to attempt to draw a 1 pixel wide line that is centered between adjacent pixels (along the pixels edges). With antialiasing on this system would partially paint all of the touched pixels based on how much the stroke fills the pixel (50% in this example). With antialiasing off who knows exactly what it will do... it will attempt to honor your request but because of floating point precision, etc. it could easily vary what it does along the stroke axis. It could fill the adjacent pixels 100% giving you a 2 pixel wide line, it could round up filling the pixels above the path, it could round down filling the pixels below the path, etc. Also note over time (with OS releases) what Quartz does with paths when anti-aliasing off how has changed slightly in behavior. Also I am not sure how HIView stuff affects this... If you want a certain area filled and then ringed by a stroke you may want to use two different rectangles. To fill use 0,0 / 3,3 and to stroke 0.5,0.5 / 3,3 (or maybe -0.5,-0.5 / 4,4). Also changing your order of fill and stroke can possible get what you want. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Basic rect drawing questions</header>
    <body>On May 10, 2007, at 10:24 PM, Rick Mann wrote: On a display at a scale of 1.0, CG points fall between pixels (the same as QuickDraw points).  However, unlike QuickDraw, CG lines are centered on the vector rather than hanging below it.  My experience in turning off antialiasing on the screen is that the results are somewhat unpredictable.  CG seems to be optimized for use with antialiasing.  When it's disabled the exact behaviors of edge conditions, such as the one you describe, are undefined.  With antialiasing on I would expect you to see a fairly uniform black outline about half the intensity of the black you specified.  With antialiasing off I suspect the results are difficult to predict. This can make CG difficult to use for some user interface work where all you really want is a line of a particular color a specific number of pixels wide.  I find that a combination of turning off antialiasing, adjusting my coordinates by 0.5 to get my lines into the center of pixels, and tweaking the results gives a good final result.  However in this case it's not as easy to use as QuickDraw (at least for me).</body>
  </mail>
  <mail>
    <header>Re: Basic rect drawing questions</header>
    <body>It will be exactly the same size, and you won't see the stroke if you draw the stroke first, then the fill. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>CGEventCreateKeyboardEvent and modifier keys</header>
    <body>The documentation for CGEventCreateKeyboardEvent() states: &amp;quot;All keystrokes needed to generate a character must be entered, including modifier keys. For example, to produce a 'Z', the SHIFT key must be down, However, I find that this isn't true, at least in the target apps I have tested. All I have to do after creating a keyboard event is to set its modifier flags information field, and CGPostEvent() does the right thing. For example, setting the flags mask to the Command key and sending the keycode for &amp;quot;w&amp;quot; down and up causes the target app's window to close, and setting the mask to the Shift key causes the capitalized &amp;quot;W&amp;quot; to be typed. I've tried this with TextEdit, Microsoft Entourage and (for the Command-w case) the Finder, running Mac OS X 10.4 Tiger. Is the documentation wrong (at least for Tiger and newer)? Or are there cases where sending all the modifier key down and up events is in fact necessary in order to type a modified character? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: CGImageSource from raw bytes</header>
    <body>(bah, premature send) Was recently reminded that I'm not correct on being unable to create a CGImage from a data provider - I was thinking of a bitmap context for some reason when I wrote that. You can create a CGImage that has it's image data supplied from a CGDataProvider, which would allow you to partially load an image as the parts that are needed are requested. Creating a data provider with the CGDataProviderDirectAccessCallbacks should give you the most performant provider (probably use the getBytes callback rather than the getBytePointer callback) for an image that you don't want to fully load into memory. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>CG in an HIView</header>
    <body>To add to my pixel confusion, I'm seeing this behavior. I'm drawing in an HIView, so the y-axis is already inverted. The following code creates a rect at 5,5 with width &amp;amp; height 5, then strokes it and draws it (NCGContext is a Nano C++ Wrapper around CGContextRef): The rectangle ends up with its top-left corner at 5,5 and extends 5 pixels to the right and down. However, the red stroke is along the left and bottom edges! If I remove the changes to s.origin, you see no stroke at all. when stroking a rect in an HIView. -- Rick</body>
  </mail>
  <mail>
    <header>Re: Basic rect drawing questions</header>
    <body>On May 10, 2007, at 21:38 , Shawn Erickson wrote: I read that, but it doesn't say what happens when there's no antialiasing. So, a fill of a rect from 0,0 to 3,3 will be 3 pixels wide and tall. A stroke of the same rect, with antialiasing off, should do what, exactly? Right; I recall that now. -- Rick</body>
  </mail>
  <mail>
    <header>Re: Basic rect drawing questions</header>
    <body>Line strokes are mathematically centered on the path, aka half of of the stroke width is to one side of the path while half is on the other side. Fills extend to the path itself but dont extend outside of the path. In other words in your example (assuming infinitesimal pixel sizes) you will see half of your stroke ringing the fill area. User space coordinates of 0.5 x 0.5 correspond to the center of pixel assuming a 1x scale. -Shawn</body>
  </mail>
  <mail>
    <header>Basic rect drawing questions</header>
    <body>Hi. I'm making a custom view, drawn with CG, and I'm having some &amp;quot;endpoint paranoia,&amp;quot; which I figured shouldn't happen. So I have some fundamental questions about CG mathematical primitives and how they're rendered. Let's consider a rectangle, drawn with NO antialiasing. The rectangle goes from 10, 10 to 20, 20. If I stroke the rect with a 1-unit thick line in black, then fill it in red, will I see both the outline and the fill? If not, why not? More fundamentally, does a CG integer point live at the center of a pixel (assuming 1:1 integer-to-pixel mapping)? I looked at the online docs, and couldn't find a clear explanation of Quartz' treatment. In my app, I have the HIView going from 0,0 to 800, 300 in a window sized 800 x 400. When I stroke it, I get black lines on the left and bottom, and no visible line on the top and right. I had expected a black line all the way around, and I was going to move the view in IB so that the left, top and right edges were outset by 1 pixel. Thanks for the explanation. If this is covered somewhere, please let me know where. Thanks! -- Rick</body>
  </mail>
  <mail>
    <header>CoreGraphics Python bindings - addLines?</header>
    <body>I hope this is the right mailing list for my question. I'm trying to get started with the Python bindings for CoreGraphics. Unfortunately, the documentation in API-SUMMARY is not very clear. I'm trying to use CGContext::addLines in Python. I'm assuming it takes a python list as the argument. The error message I get is: Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in ? File &amp;quot;/System/Library/Frameworks/Python.framework/Versions/2.3/lib/python2.3/plat-mac/CoreGraphics.py&amp;quot;, line 617, in addLines def addLines(*args): return _CoreGraphics.CGContext_addLines(*args) TypeError: Expected a pointer What am I doing wrong? Is this only a wrapper-function for a C-array of GCPoints? thank you, Denis</body>
  </mail>
  <mail>
    <header>syncing audio and video</header>
    <body>Note: The topic of this thread technically crosses the boundary beteen coreaudio-api and quartz-dev, but my response to Jeff (on the coreaudio-api list) is really more of a quartz question.  I responded to him on the coreaudio-api list before I realized this, so pardon the cross-post... Thank you.  That worked.  The conversion looks correct and I can successfully pass the converted CVTimeStamp on to the core video renderer. But for some reason my video and audio still drift apart after a minute or so. I can't figure out how this could be happening.  I mean the time stamp is generated from the running audio, so they should naturally be in sync.  I can see how they might be offset by a couple frames maybe, but not drifting more and more and more out of sync as the movie progresses.  They get WAY out of sync after 3 or 4 minutes (couple of seconds out of sync). Also, if i pause the video and let the audio advance for a while and then un-pause the video again, it starts right where i left off (when i originally hit pause) instead of jumping right to the place where audio is currently at. Here's the code: static CVReturn renderCallback(  CVDisplayLinkRef    displayLink, const CVTimeStamp *  inNow, const CVTimeStamp *  inOutputTime, CVOptionFlags        flagsIn, CVOptionFlags *      flagsOut, void *              displayLinkContext  ) - (UInt64)currentHostTime Thanks. - Chase</body>
  </mail>
  <mail>
    <header>Re: is there a more active list on this subject?</header>
    <body>My question was posted yesterday ( &amp;quot;synchronizing with core audio unit AND displaylink&amp;quot; ). But here is a better way of asking I think: Does anyone on here know how to convert the sample time of a core audio unit into a CVTimeStamp (that i can then use to determine which frame to display in a video being played by core video)? Just trying to sync audio and video.  Nothing more. Thanks. - Chase</body>
  </mail>
  <mail>
    <header>Re: is there a more active list on this subject?</header>
    <body>Folks usually post to quartz-dev when they have a problem.  If you would like our help in exploring some difficulty you have, there are plenty of folks around to help... you only need ask. However, we don't chatter incessantly when there's nothing interesting to discuss.</body>
  </mail>
  <mail>
    <header>is there a more active list on this subject?</header>
    <body>Does anyone know of a more active list on either core audio or quartz? The day is half over and there's been only 8 posts to coreaudio-api and 0 to quartz-dev . I know developers are out there somewhere discussing this stuff... but where? Thanks. - Chase</body>
  </mail>
  <mail>
    <header>Re: (SOMEONE HELP ME PLEASE) synchronizing with core audio unit AND	displaylink</header>
    <body>Please someone look at my original post (included below) and tell me what I need to do.  I'm totally stuck on something that should be fairly trivial. Thanks in advance for taking a look at this and helping me out.  :) - Chase</body>
  </mail>
  <mail>
    <header>synchronizing with core audio unit AND displaylink</header>
    <body>Okay, to use core video and core audio to play a video and an audio stream and keep them synchronized, there are actually two synchronization issues: #1 - video frame draws need to be sync'ed to display refresh (taken care of by the displaylink concept) #2 - video must use the audio stream sample time to determine which frame to draw (to keep the two streams in sync) To keep this simple, I'm using example code we all have access to: &amp;quot;/Developer/Examples/Quartz/Core Video/CIVideoDemo GL&amp;quot; ... to which I have added the necessary code for playing a wav file in core audio while the video is going. I start the audio and the video streams. Obviously, if I don't add any sync code at this point, the audio and video drift out of sync after a couple minutes. So to keep the synchronization with the diplaylink working, I do my a/v sync stuff inside the displaylink callback: static CVReturn renderCallback(   CVDisplayLinkRef     displayLink, const CVTimeStamp *  inNow, const CVTimeStamp *  inOutputTime, CVOptionFlags        flagsIn, CVOptionFlags *      flagsOut, void *               displayLinkContext   ) The default code (sans-audio sync) is shown above. Everything boils down to what I send to the renderTime: method.  Instead of sending inOutputTime, I need to send some other CVTimeStamp * that is calculated from the audio stream's current position. The two data structures invloved are: struct AudioTimeStamp struct CVTimeStamp That's all that I know.  I'll save you the pain of listening to the many things I have tried so far. Instead, could someone please show me the calculation I need to prepare the CVTimeStamp * that I send to renderTime: . Thanks. - Chase</body>
  </mail>
  <mail>
    <header>Re: Double &amp;amp; triple click support when posting synthetic mouse	events</header>
    <body>On May 9, 2007, at 3:05 AM, Bill Cheeseman wrote: You should set both the click count, and post the expected number of clicks.  Some applications, often Carbon ones, implement their own click count logic, as the click count encoded as part of the event is a relatively recent introduction.  Most Cocoa applications use the provided click count, as that has historically always been available in Cocoa programming environments. Mike Paquette</body>
  </mail>
  <mail>
    <header>[SOLVED] Custom Image Unit question</header>
    <body>return [self apply: thresholdKernel, src, inputThreshold, return [self apply: thresholdKernel, src, threshold, While I'm still passing an NSNumber, the NSNumber is now being constructed with a float instead of a double. Peter</body>
  </mail>
  <mail>
    <header>Re: Double &amp;amp; triple click support when posting synthetic mouse events</header>
    <body>Shawn Erickson and I have both found, in independent testing, that setting the click count to a number greater than 1 and sending a single pair of mouse-down/mouse-up events does *not* generate a double-click, triple-click, etc. Instead, we have to send two pairs for a double-click, three pairs for a triple-click, and so on. Shawn reports that, for some target applications, the click count must be set to 1 for the first pair, 2 for the second pair, and so on. I found that the Finder doesn't care: it recognized a double-click when all four events were sent with the click count set to 2. Joel, if you can confirm that setting the click count and sending a single pair of events is supposed to generate a multi-click, I'll file a bug. -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: CIImage to CGImage, revisited</header>
    <body>So the methods are essentially different? Method B. +currentContext to get the current NSGraphicsContext -graphicsPort to get the current CGContextRef +contextWithCGContext:options: to create a CIContext -createCGImage:fromRect: to create a CGImageRef How does the current context actually affect the CG image output? Or am I supposed to make a bitmap context first then pass it to contextWithCGContext:options: ? OK, thanks for the tip. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen</body>
  </mail>
  <mail>
    <header>Re: CGMainDisplay API alternative</header>
    <body>If I'm reading you right, then you need to get all the online displays with CGGetOnlineDisplayList() then ask for the IOKit service port via CGDisplayIOServicePort(). That should get you the displays from IOKit's point of view. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Graphics card with dual heads issue.</header>
    <body>Machine Mac OS X 10.4 Graphics card: NVIDIA 7300 GT (Dual head card) I am developing a monitor calibration application and need your help in the following: I have a graphics card with dual heads (NVIDIA 7300 GT, Quadra FX 4500), i attach monitors to both of the heads. Now i need to read EDID information for the both monitors. Can you please suggest any way to find EDID info of both monitors ? Note: I tried out with the sample provided at by feeding it with primary and secondary display id. The got those id by CGDisplayID but it did not worked for these cards. Thanks and Regards, Rohit</body>
  </mail>
  <mail>
    <header>CGMainDisplay API alternative</header>
    <body>I am developing a monitor calibration application which uses EDID data with i2c communication. It works fine with most of the single head cards but now a days most of the cards are dual head like nVidia GeForce 7300 GT and nVidia Quadro 4500 FX. So  when i try to run my application for these dual head cards by connecting both the monitors then it will show random behavior, i.e. I am not able to read EDID data from the monitors. The same application works fine with one arrangement but if i change the arrangement from system preferences then it shows some random behavoir. In case of dual head cards, there are two physical ports. One is the physical primary and other is the physical secondary. Corresponding to each ports, there must be logical port at OS level. i.e. if i connect two monitors then one will become primary monitor and other secondary. So my query is &amp;quot;Is their any key in IORegistryExplorer for getting logical primary port according to system preference or Is there any API for finding logical primary port&amp;quot;?? I know about CGMainDisplay, but this doesnot works for all cards. Anyone knowing another API for finding primary port ? Or any alternative way? Please suggest. Rohit Dhamija</body>
  </mail>
  <mail>
    <header>Re: Double &amp;amp; triple click support when posting synthetic mouse	events</header>
    <body>On May 7, 2007, at 1:54 PM, Shawn Erickson wrote: Each pair of down-up mouse events should have the same click count, as apps can query the click count for either event and so may depend on either or both to be correct. Joel</body>
  </mail>
  <mail>
    <header>Re: Double &amp;amp; triple click support when posting synthetic mouse	events</header>
    <body>Thanks again... one last stupid question (well hopefully last). The click count is important for both down and up events or just down events, etc.? -Shawn</body>
  </mail>
  <mail>
    <header>Re: Double &amp;amp; triple click support when posting synthetic mouse	events</header>
    <body>On May 7, 2007, at 1:30 PM, Shawn Erickson wrote: You can get the click timing with NXClickTime(), defined in IOKit's event_status_driver.h. Joel</body>
  </mail>
  <mail>
    <header>Re: Double &amp;amp; triple click support when posting synthetic mouse	events</header>
    <body>Joel</body>
  </mail>
  <mail>
    <header>Re: Double &amp;amp; triple click support when posting synthetic mouse	events</header>
    <body>Thanks for confirming that. So what is the &amp;quot;Right Way(TM)&amp;quot; to get the users currently defined click timing so I can determine double, triple, etc. clicks? Well to be more correct I guess my events are a playback of an event stream, possibly from a remote unrelated system. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Double &amp;amp; triple click support when posting synthetic mouse events</header>
    <body>But you do have to post 2, down and up, I assume? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: Double &amp;amp; triple click support when posting synthetic mouse	events</header>
    <body>You'll need to set the click count yourself, using This means that if your events are synthesized, you don't need to post six mouse events in order to get a triple-click. Joel On May 7, 2007, at 11:14 AM, Shawn Erickson wrote:</body>
  </mail>
  <mail>
    <header>Re: Double &amp;amp; triple click support when posting synthetic mouse events</header>
    <body>Here's how I do it, in a Cocoa class method. This code snippet calls into an event taps framework I'm writing, but the strategy should be clear: + (void)clickMouseButton:(unsigned)button atPosition:(NSPoint)position // Sends all of the events needed to click the specified mouse button at the specified position in global display coordinates. The clickCount value should be 1 for a single-click, 2 for a double-click and 3 for a triple-click. case 1: case 2: default: [[[self class] mouseEventWithEventSource:nil type:downType [[[self class] mouseEventWithEventSource:nil type:upType -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Double &amp;amp; triple click support when posting synthetic mouse events</header>
    <body>I am using the following code to post mouse events to the current user session. Things are working great for all operations except double / triple clicking. The even stream appears to contain nothing but down/up events for the button I am exercising but no applications appear to detect double or triple clicks. Do I have to track the down/up button timings and set the click count event field myself or can I convince the system to do that for me or am I doing something that prevents that from taking place? void InputPlaybackMacOSX::_postMouseEvent(CGEventType type, CGPoint location, int32 wheelDeltaAxis1) LOG_DBG(_T(&amp;quot;%s - type:%d loc:%4.0f,%4.0f wheelDeltaAxis1:%d&amp;quot;), EC_FUNCTION, // Our event stream currently only supports 3 buttons (left, right, and middle) so we // always state kCGMouseButtonCenter for &amp;quot;mouseButton&amp;quot; in the following since that mouse button and // the only &amp;quot;other&amp;quot; mouse button we support is the center button. CGEventRef event = CGEventCreateMouseEvent(NULL, type, location, CGEventSetType(event, type); // Apple bug... need to set the type manually // Our event stream only axis 1 (x) for wheel events CGEventSetIntegerValueField(event, ... moving the mouse around ... ... _postMouseEvent - type:5 loc: 771, 522 wheelDeltaAxis1:0 _postMouseEvent - type:5 loc: 770, 521 wheelDeltaAxis1:0 _postMouseEvent - type:5 loc: 770, 519 wheelDeltaAxis1:0 ... now try double click ... _postMouseEvent - type:1 loc: 770, 519 wheelDeltaAxis1:0 _postMouseEvent - type:2 loc: 770, 519 wheelDeltaAxis1:0 _postMouseEvent - type:1 loc: 770, 519 wheelDeltaAxis1:0 _postMouseEvent - type:2 loc: 770, 519 wheelDeltaAxis1:0 -Shawn</body>
  </mail>
  <mail>
    <header>Re: CIImage to CGImage, revisited</header>
    <body>Am 04.05.2007 um 02:25 schrieb Glen Low: Profile it on your target platform - it probably varies depending on machine, OS revision etc. Decoding time is depending on the target buffer type. Setup time is generally rather low (IIRC 0.05 sec on my machine for 6 MPix) I could not decode more than one CGImageRef that way. Following attempts cause odd crashes or (not relasing some resources) in CGContextRefs where drawing works but ImageIO simply does not work.</body>
  </mail>
  <mail>
    <header>Re: How to get a control under mouse position without using	QuickDraw</header>
    <body>I've got a problem when I tried to replace the call to GlobalToLocal function in the code below</body>
  </mail>
  <mail>
    <header>CIImage to float data</header>
    <body>I would like to render a CIImage to a float bitmap (at least before I die of natural causes).  I've had partial success with CGBitmapContextCreate/CGContext/drawImage way but want to try the CGLContext/drawImage/glreadPixels way. [Aside: CGBitmapContextCreate/CGContext/drawImage renders a float bitmap but with &amp;quot;8 bit quantized&amp;quot; values under PowerPC/Rosetta and garbage under i386.] There are a good few examples of how to get an 8bit bitmap of a CIImage using CGLContext/drawImage/glreadPixels [glReadPixels( 0, 0, pixelsWide, pixelsHigh, GL_BGRA, GL_UNSIGNED_INT_8_8_8_8_REV, [rep bitmapData] )] 1) the CGLPixelFormatAttribute needed for CGLChoosePixelFormat()? kCGLPFAColorFloat + ??? glReadPixels( 0, 0, pixelsWide, pixelsHigh, GL_BGRA, ? Rodney Kennedy email@hidden</body>
  </mail>
  <mail>
    <header>Re: How to get a control under mouse position without using QuickDraw</header>
    <body>On 06.7.31 2:48 PM, &amp;quot;Vinay Prabhu&amp;quot; &amp;lt;email@hidden&amp;gt; wrote: Hope this helps you. I'm working on a project porting to Quartz from &amp;nbsp;QuickDraw. I've got a problem when I tried to replace the call to &amp;nbsp;GlobalToLocal function in the code &amp;nbsp;below Several day ago, I posted &amp;nbsp;a question asking about how to get desktop's bounds without using QuickDraw, &amp;nbsp;but still not received an answer yet. If any one could, please give me an &amp;nbsp;answer! Thanks shankar</body>
  </mail>
  <mail>
    <header>RE: How to get a control under mouse position without using	QuickDraw</header>
    <body>&lt;FONT face=Arial color=#0000ff size=2&gt;Regarding getting the desktop bound's, &lt;FONT face=Arial color=#0000ff size=2&gt; I know of a method to obtain the fullscreen bounds using Cocoa. Hope this helps you. &lt;FONT face=Arial color=#0000ff size=2&gt; NSRect &lt;FONT face=Arial color=#0000ff size=2&gt; I don't know how to obtain the screen bounds using Quartz, Carbon... &lt;FONT face=Tahoma size=2&gt;-----Original Message----- quartz-dev-bounces+vinayprabhu=email@hidden [mailto:quartz-dev-bounces+vinayprabhu=email@hidden]On Behalf Of  Monday, July 31, 2006 1:05 PM How to get a control under mouse position without using QuickDraw&lt;FONT face="Verdana, Helvetica, Arial"&gt;Hi everyone!I'm working on a project porting to Quartz from QuickDraw.I've got a problem when I tried to replace the call to GlobalToLocal function in the code belowcontrol = FindControlUnderMouse(m_MousePosition,windowRef,&amp;amp;sPart);The problem is I don't know which function to replace the GlobalToLocal function. Is there a function like that or Is there any finding-control function which accepts mouse position gotten from the field "" in an Several day ago, I posted a question asking about how to get desktop's bounds without using QuickDraw, but still not received an answer yet. If any one could, please give me an answer!</body>
  </mail>
  <mail>
    <header>Re: How to get a control under mouse position without using QuickDraw</header>
    <body>Hi everyone! I've got a problem when I tried to replace the call to GlobalToLocal function in the code below Several day ago, I posted a question asking about how to get desktop's bounds without using QuickDraw, but still not received an answer yet. If any one could, please give me an answer! Thanks</body>
  </mail>
  <mail>
    <header>Re: Getting resolution data out of a PDFPage</header>
    <body>The pixel size of what, the image or the PDF page? PDF pages don't have pixel sizes or DPIs. All their sizes (media box, crop box) are defined in device-independent points. See section 14.11.2 of the PDF Reference: --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: Getting resolution data out of a PDFPage</header>
    <body>Yes, I stuck a 200 -600DPI scanned image (Pixel data) as a LZW TIFF, into a PDF. Right, but I don't want device independent points. I want pixels. Or perhaps, I do but I'm making the PDF wrong. I give the PDF when I make it a 72 DPI (points) media size and a (in my current case) 200 DPI  (pixels) image size. When I get the image out, I only have the 72 DPI size. I can't seem to even find the 200 DPI numbers, even though I DO see them with voyeur. Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: Getting resolution data out of a PDFPage</header>
    <body>So you stuck a big bitmap image into the PDF, or am I misunderstanding you? Is this what you intended to do? PDFPage has a -boundsForBox: method, which I believe just thunks down to CGPDFPageGetBoxRect. Either of those will return you the page size in device-independent points. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Getting resolution data out of a PDFPage</header>
    <body>I have a PDF file which is made up of pages, each of which is an image. I created the file by making NSImages with an NSBitmapRep with the size in pixels, and the NSImage itself has a size in points. I then made a PDFPage, initialized with the NSImage, and saved the PDF file. Looking at the PDF file with Voyeur, I see that the pixel size  of the images and media size of the file are correct. However, when I open the file again, I can't seem to find the pixel sizes anymore. I've tried two strategies: get the PDFRepresentation data from the page, and use it to make a new NSImage via initWithData. This works, however, the image has both size and pixel size in 72dpi. Draw the PDFPage into a new off-screen context, and then get a CGImage from that. The problem is that when I'm making the context, it appears that I need to know the size in pixels already, which defeats part of the purpose... So, how am I supposed to get this information out? Is there a way to get the XObjects out of a PDFPage and inspect them? (OTHER than parsing the PDF myself, or trying to cobble together something from Voyeur.app) Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: Creating a CIContext from the current NSGraphicsContext's	graphicsPort causes console spew.</header>
    <body>It's not telling you that the current context is invalid... just that it's not a bitmap context.  Presumably it could be something like a PDF context.  It may also be that the operating system has access to context types that are not available in general. What is the current NSGraphicsContext at the point you are calling contextWithCGContext:options:?  Are you certain that it's a bitmap context? Scott</body>
  </mail>
  <mail>
    <header>Re: Resolution of a CGImageRef?</header>
    <body>&amp;gt; Then I suppose the question becomes, how do I get that from the CGImageSource? Does it have a kCGImagePropertyTIFFDictionary field (assuming it's a TIFF file)? the documentation makes it look like it doesn't... ---- /* The DPI in the x- and y-dimensions, if known. If present, the value of * these keys is a CFNumberRef. */ ---- If the resolution is known it will be stored in these properties. Scott</body>
  </mail>
  <mail>
    <header>Re: Add CGImage into a PDFDocument? or an NSImage?</header>
    <body>//Save the old context</body>
  </mail>
  <mail>
    <header>Re: Add CGImage into a PDFDocument? or an NSImage?</header>
    <body>There may be a better way to do this -- I haven't done much with PDFs.  There is no error checking in this code and there should be (e.g., CGBitmapContextCreate can return nil). PDFPage *page; //Assuming this is initialized elsewhere //Create a bitmap context CGContextRef context = 	CGBitmapContextCreate(nil, NSWidth(pageBounds),  //These aren't necessarily the correct width/height for this NSHeight(pageBounds), //I haven't read enough on the meaning of the values 8, 4 * NSWidth(pageBounds), CGColorSpaceCreateDeviceRGB(), //Save the old context //Make a new one //Draw the page //Switch back //Create the CGImageRef CGImageRef cgImage = CGBitmapContextCreateImage(context); //You will need to release this once you're done</body>
  </mail>
  <mail>
    <header>Re: Add CGImage into a PDFDocument? or an NSImage?</header>
    <body>Excellent! Is there a better way to get the image OUT of the PDFPage and back into a CGImage than: CGImageSourceRef imageSource = NSDictionary * options = [NSDictionary dictionaryWithObject: (id)kCFBooleanTrue ? About 1/2 of the time I seem to be crashing in the dataRepresentation call for some reason... Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: Add CGImage into a PDFDocument? or an NSImage?</header>
    <body>If you are targeting 10.5+, there's a cleaner way to do all of that.  You're creating a duplicate of the data in memory with your method.</body>
  </mail>
  <mail>
    <header>Re: Add CGImage into a PDFDocument? or an NSImage?</header>
    <body>Currently I'm using this function: NSImage* cgImageToNSImage(CGImageRef image) But the finalize is giving me the error:  tiff data provider: Not a TIFF file, bad magic number 0 (0x0). Is there something obvious I'm doing wrong here? I'm pretty sure that the CGImageRef is a valid image... and it doesn't look like either of the options I COULD be giving the AddImage call would help... (The DestinationCreate call can't actually TAKE any options of course...) Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: Time-related EXIF tags</header>
    <body>Ah! Thank you, that makes sense. I'm struggling with the correctness of modifying the timestamps in this situation: I take a bunch of pictures with my camera, whose close is off by some number of seconds. At the same time, I have a separate GPS recording where I am and when, at what can be accepted as an accurate time. To help me sync the two, I note the time on the GPS at which I take a picture. I use this to calculate an offset from the camera's clock to the GPS's clock, and use that offset across a set of images taken around the same time. Now, this allows me to geotag images correctly, but I'm wondering if I should also update the timestamps (and file modification dates) for the more precise time, and throw away the camera's values for these? -- Rick</body>
  </mail>
  <mail>
    <header>Re: Time-related EXIF tags</header>
    <body>Rick, Generally: DateTimeOriginal : The time the exposure occurred (aka when photons hit something sensitive) DateTimeDigitized : The time the image was converted into digital form. Usually, those two are the same (or very close to each other) for a a digital camera. Where they differ, and this is the situation the two tags were designed for, is when you shoot actual film, and then later scan the film. In that case, the DateTimeDigitized is the date and time the scanning occurred. In that case usually the DateTimeOrigina would have to be manually entered. Sandy</body>
  </mail>
  <mail>
    <header>Re: Time-related EXIF tags</header>
    <body>I agree that it's something like that. I'm more interested to know how to interpret the two times. I'm experimenting with geotagging apps that exist, and one of them updates the Original time stamp, but leaves the Digitized time stamp intact (I think; it seemed to get screwy on multiple writes).</body>
  </mail>
  <mail>
    <header>Re: Time-related EXIF tags</header>
    <body>This doesn&amp;#39;t necessarily seem that strange - when writing to file, a file name typically has to be created and written to whatever the file system is, and then an a/d conversion, and/or codec conversion, would take place (digitization). -George Toledo Unhappy with all the geotagging software out there, I&amp;#39;m working on my own utility. But I&amp;#39;m having trouble finding information on how some of the EXIF tags should be interpreted. When I copy images from my camera&amp;#39;s SD card to my Mac, I get, for a typical image, a file modification time of 15:57:52. But the EXIF tags say: Date Time Digitized: ¬† ¬† ¬† ¬† ¬† ¬†2010:08:08 15:57:53 Date Time Original: ¬† ¬† ¬† ¬† ¬† ¬† 2010:08:08 15:57:53 FWIW, this is a Panasonic Lumix DMC-ZS3. What is the meaning of these two different times? Looking at other images, about half show a file system time one second EARLIER than the Digitized time. Strange. TIA, Rick ¬†_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list ¬† ¬† ¬†() Help/Unsubscribe/Update your Subscription: This email sent to</body>
  </mail>
  <mail>
    <header>Re: Time-related EXIF tags</header>
    <body>I'm sure it's the firmware looking at the camera's internal clock at two different times during the processing and writing of an image file to the SD card. More important is what is the intended interpretation of the two time stamps. -- Rick</body>
  </mail>
  <mail>
    <header>Time-related EXIF tags</header>
    <body>Unhappy with all the geotagging software out there, I'm working on my own utility. But I'm having trouble finding information on how some of the EXIF tags should be interpreted. When I copy images from my camera's SD card to my Mac, I get, for a typical image, a file modification time of 15:57:52. But the EXIF tags say: Date Time Digitized:		2010:08:08 15:57:53 Date Time Original:		2010:08:08 15:57:53 FWIW, this is a Panasonic Lumix DMC-ZS3. What is the meaning of these two different times? Looking at other images, about half show a file system time one second EARLIER than the Digitized time. Strange. TIA, Rick</body>
  </mail>
  <mail>
    <header>Re: CGImageSourceCreateWithURL always returning NULL?</header>
    <body>Shouldn't that URL have 3 slashes after &amp;quot;file:&amp;quot;? -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Add CGImage into a PDFDocument?</header>
    <body>Is there an easy way to add a CGImage into a PDFPage/PDFDocument? Currently, I'm turning it into an NSImage first with: CGImageDestinationRef dest = CGImageDestinationCreateWithData but that seems really sloppy. Also, it seems to crash with a Program received signal:  ‚ÄúEXC_BAD_ACCESS‚Äù. regularly... Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: CGImageSourceCreateWithURL always returning NULL?</header>
    <body>No message in the console log. I'm doing all of this in the debugger with console open, I'd see any messages... and it doesn't appear to be that. I added the type hint: NSDictionary * sourceDict = [NSDictionary dictionaryWithObject: kUTTypeTIFF and : (gdb) po url file://Users/acordex/Desktop/testing/page-020 (gdb) n (gdb) po sourceDict (gdb) po imageSource 0x0 does not appear to point to a valid object. Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: CGImageSourceCreateWithURL always returning NULL?</header>
    <body>Offhand I would guess because your image doesn't have a file extension and you aren't passing a type hint (since your options dictionary is NULL). There may also be a message on your console log. -- David Duncan</body>
  </mail>
  <mail>
    <header>CGImageSourceCreateWithURL always returning NULL?</header>
    <body>I have a TIFF file on my computer at /Users/acordex/Desktop/testing/test3/page-002 I create a CGImageSource with: where url is: (gdb) po url file://Users/acordex/Desktop/testing/test3/page-002 however, imageSource is consistantly 0x0, with no other form of error that I can see. is it possible to get more information about why it fails? Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: Resolution of a CGImageRef?</header>
    <body>As far as I remember, CFShow might come handy in any situations where you want to discover what's in a CF Object and how to access it. Normally, also as far as I remember, if you select your CF Object in the debugger you can have the CFShow executed by choosing : Run &amp;gt; Variables View &amp;gt; Print Description to Console from the menu bar. Raphael</body>
  </mail>
  <mail>
    <header>Re: Resolution of a CGImageRef?</header>
    <body>CFDictionaryRef CGImageSourceCopyProperties ( CGImageSourceRef isrc, CFDictionaryRef options You don't need additional options so pass an empty dictionary there, but the return dictionary should have the properties you are looking for, then use the kCGImagePropertyTIFFDictionary key to get access to the TIFF file dictionary if thats what the image file format is.</body>
  </mail>
  <mail>
    <header>Re: Resolution of a CGImageRef?</header>
    <body>Then I suppose the question becomes, how do I get that from the CGImageSource? Does it have a kCGImagePropertyTIFFDictionary field (assuming it's a TIFF file)? the documentation makes it look like it doesn't... Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: Resolution of a CGImageRef?</header>
    <body>From what I can tell, CGImageRef doesn't store anything like resolution, only properties you are able to access are actual number of pixels for width and height through CGImageGetWidth( ) and CGImageGetHeight( ).</body>
  </mail>
  <mail>
    <header>Resolution of a CGImageRef?</header>
    <body>I'm creating a CGImageRef out of, in one case, a TIFF file, through a CGImageSource, in another case raw bitmap data via a CGDataProvider and in another case, from a PDFPage via an NSImage. I need to know the resolution of the CGImage. Is there an easy way to find this out? in the PDFPage case, I should be able to get the pixelwidth and the pointwidth and do the arithmetic. however, in the other two cases, I don't see a way to get resolution (or pointwidth, or physical width or anything like that) out of the source. Actually, in the Raw bitmap case, I have the resolution separately, but I guess I'm wondering if I need to carry that along separately, or if there's a clever way to just get it out of the CGImageRef... Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Creating a CIContext from the current NSGraphicsContext's	graphicsPort causes console spew.</header>
    <body>The documentation for CIContext's contextWithCGContext:options: says this for the context argument: Yet, when I try and create a context like so: [CIContext contextWithCGContext:[[NSGraphicsContext currentContext] graphicsPort] options:someOpts] I get the following error spewed out in the console: &amp;quot;CGBitmapContextGetBitsPerComponent: invalid context 0x11c03b4c0 Since I'm using a CGContext handed to me &amp;quot;from the system&amp;quot;, I would think the current CGContextRef is a valid argument.  Even though I get the error, drawing seems to work fine.  But the console output happens every time I draw with it. (I can't use [[NSGraphicsContext currentContext] CIContext], because I want the software renderer to be used, and I want to set a custom working color space.) So, is the documentation wrong, or am I doing something wrong?  Or is there another answer? thanks, -gus -- August 'Gus' Mueller Flying Meat Inc.</body>
  </mail>
  <mail>
    <header>Re: IKImageView choppy?</header>
    <body>No, in the previous version, the user viewed a wide variety of sizes, and could page through any number of pages very very fast. In the original version, the images were each a separate TIFF image on disk, so when you went to the next page, it loaded the image from disk, decompressed it, scaled it and put it on screen. This takes 1/10th time time that my current program takes to take an image that's already in memory, and just put it on screen. All of the strategies so far suggested are based on the assumption that the problem is that my image has too much pixel data, and so that makes everything slow. Doing a lot more work (including disk access) with the same number of pixels USED to take 1/10th the time. The problem isn't that I have too much pixel data, so thumbnailing won't help. *I'M* the one who suggested background loading, and since it takes so long to display the image, I'd have to load the first 5 images, and THEN be able to cycle through them. this will make the first 5 pages really slow, and not really work. All of the solutions I've seen here have been predicated on the concept of reducing the resolution of the image and making it smaller, and since that isn't my problem, that won't actually solve it... Unless I'm misunderstanding something, which is always possible. thanks Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: IKImageView choppy?</header>
    <body>&amp;gt; The problem is *NOT* data size. Or if it is, then for some reason, it didn't used to be. I couldn't agree with you more. Originally, you asked why it took so long to load an image.  I believe you have received numerous answers to that question that offer alternative strategies (e.g., thumbnailing, background image loading, etc.) that will address the length-of-time issue. Different strategies have various consequences that you must consider before deciding which path is best for your project.  If you only want to have one image that scales from 100% downward, that is perfectly achievable with the solutions that you was been given (e.g., background loading).  Picking this strategy has cost. You have to decide whether those cost are worth it if, for example, your user only views images at discreet sizes ( 25%, 50%, 75%, 100%), or they spend a considerable time viewing images at 200% or 400% (with some type of loupe tool). I'm sure your use case analysis will be of great help when you pick the correct solution. regards, douglas</body>
  </mail>
  <mail>
    <header>Re: IKImageView choppy?</header>
    <body>Well, since what I have is the length and width in pixels, if I want to scale to 72 DPI for the screen, I need to know the resolution of the original image. NSImages being resolution independent, is part of why I was using the in the first place... Do I really have to do that manually? What's the point of having an IKImageView which claims to zoom for me, etc, if I have to be doing that manually myself? Is this how applications like preview, etc do this? I have some old software that does this manually, using deprecated system calls. When it is working with the full-sized data, 1 image pixel = 1 screen pixel, or 300% zoom, when I page through the image, it zips right along. The problem is *NOT* data size. Or if it is, then for some reason, it didn't used to be. Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: IKImageView choppy?</header>
    <body>Perhaps you should try this workflow: PDFpage  =&amp;gt; NSData =&amp;gt; NSPDFImageRep =&amp;gt; CGImage the -CGImageForProposedRect:context:hints: method in the NSPDFImageRep class is very convenient for getting this done. Image resolution is only relevant within a &amp;quot;context&amp;quot; (e.g., printing or drawing onscreen).  In essence, NSImage is resolution independent and a CGImage is a context-specific representation of the data.  (I must assume that a PDFPage is also resolution independent...  isn't that the whole point of pdfs?) If you want to know the resolution at which an image was captured, you can check the CGImage properties for DPI width and DPI height. If I were you, I would keep several CGImages around and observe the zoomFactor property of IKImageView.  Based on how that property changes, I would then set/reset the associate image as needed. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: IKImageView choppy?</header>
    <body>CGImages are in pixels. If you want resolution you need to consult the metadata of the source file. PDFPages and NSImages give sizes in points. Since PDF is mean to be primarily a vector format, DPI is basically arbitrary. NSImages are whatever DPI the source file claimed to be. You can get the DPI by walking the NSImageReps and comparing their pixel sizes against the host NSImage's size. Each NSImageRep can have a different DPI as each one may be created for different purposes. -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: IKImageView choppy?</header>
    <body>Also, next question, How do I find the resolution of a CGImage, PDFPage, or NSImage? Everything seems to be in pixels, which doesn't tell me what I need to know... Also, also:  since I am only giving the image to the IKImageView once, don't I need to give it at full resolution in case the user wants to zoom in to 1-1 size? or do most people just live with 1-1 being fuzzy? Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: IKImageView choppy?</header>
    <body>Ok, I'll try that. At the moment, I don't seem to be able to make a CGImageRef straight out of a PDFPage... PDFPage's dataRepresentation gives me PDFData. I seem to be able to make a CGImageSource out of the PDFData, but when I try to get the CGImage from the source, I get NULL. Maybe going through the NSImage step is necessary? Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: IKImageView choppy?</header>
    <body>See my comments embedded below: Ultimately, the size of your view doesn't really matter.  What I was trying to get you to do was think about how many pixels you going to display.  IKImageView uses CGImage for display.  CGImage is a bitmap. Let's say your IKImageView is 600 by 800 pixels (total 480,000 pixels).  Cramming the 3,740,000 pixels from the image you are currently generating will take time and processing power, whereas simply generating a thumbnail at 600 by 800 would display quickly with the least amount of overhead. (If you don't understand why this is so, then I suggest you re-read the Quartz 2D programming guide and perhaps the Cocoa Drawing guide as well) I would strongly urge you to reconsider the conclusions that you have reached.  I see a flaw in your premise. Oops... my bad.  I meant for you to check out kCGImageSourceThumbnailMaxPixelSize, which can be used to specify the maximum number of pixels to place your aspect-ratio-maintained image into.  In essence , it lets you control how big your thumbnails are.  Use this image source property with CGImageSourceCreateThumbnailAtIndex() and life is good.</body>
  </mail>
  <mail>
    <header>Re: IKImageView choppy?</header>
    <body>I don't see keys for image width and height. I see: Image Source Option Dictionary Keys Keys that you can include in the options dictionary to create an image source. and I don't *THINK* I want to work with thumbnails, because those are usually just 200 x 200 pixels or so... Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: IKImageView choppy?</header>
    <body>It depends. it's the size of the window... if I don't know that at the time, I can just use 72dpi as an assumption... I'm using the NSImage because... well, I'm doing this in Cocoa so I thought that using NSImage was the right tool. After I had already made my entire infrastructure with NSImages, I found out that you can't draw an NSImage directly into an IKImageView... Also, working with NSImages didn't force me to worry about contexts and dictionaries, as that appears to all live within the NSImage, whereas CGImages are more primitive and more of a pain to work with... Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: IKImageView choppy?</header>
    <body>On Mon, Aug 2, 2010 at 6:53 PM, douglas welton You need to render the PDF page somewhere. That doesn't have to be an NSImage, strictly speaking. You could create a bitmap context of the appropriate size and render the PDF there. Brian, when you say &amp;quot;full-sized,&amp;quot; do you mean you want to display the pages onscreen 1:1 with their physical size, or do you want to display your 200 DPI image at 72 DPI (thus displaying them at 2.7x actual size)? Right now you're storing enough data to do the latter, but if the former is what you want, then create a bitmap context and render the page into it to save some serious memory. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: IKImageView choppy?</header>
    <body>How do I do that? I'm just taking the image that the PDFDocument gives me and throwing it on the screen, at what point (PDFDocument, PDFPage, NSImage, CGImage, or something in between) do I have the ability to re-render the image? And won't that take just as much time as it's currently taking? I can't pre-render anything because in the most important case, I'm getting full-sized bitmaps from a scanner and need to display them in real-time. I WAS able to do this with 15 year old technology, so it's not that the images are too big, because they haven't gotten any bigger. Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: IKImageView choppy?</header>
    <body>Wow, that's huge. Follow Douglas's advice and render your big original into a thumbnail. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: IKImageView choppy?</header>
    <body>I'm keeping whatever NSImage thinks it needs to keep, I'm not sure how to change that. I'm starting out with a 200 DPI 8.5x11 black and white (8 pixels per byte) image. I'm getting it out of a PDF/TIFF. Here's what I'm doing and then ( in a different function, but the same object) And then I use the IKImageView method: setImage:imageProperties: with imgRef and nil. is there a faster way to do this? Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: IKImageView choppy?</header>
    <body>Question:  How big (in pixels) is the image that you are rendering?  Do you need to render the image at that size or can you keep a smaller/lower-resolution image around for the purpose of display?</body>
  </mail>
  <mail>
    <header>javascript called every N frames/seconds</header>
    <body>I was just wondering if its at all possible to create a javascript element that would re-execute itself every second or so? I need a mathematical equation to be processed untill a particular variable is = to 0 [ the javascript is just a exponential fall off equation i need for an app im building ] Cheers guys ! Send instant messages to your online friends</body>
  </mail>
  <mail>
    <header>Drawing a compressed PNG image</header>
    <body>font-family:Arial'&gt;I have some monochrome PNG&amp;#8217;s that are not drawing in Quartz with the correct foreground / background color orientation.&amp;nbsp; i.e. the black pixels are coming out white and the white pixels are coming out black. font-family:Arial'&gt; font-family:Arial'&gt;This is only happening with PNG&amp;#8217;s that have a clutID == 0 in the ImageDescription structure I am getting from Quicktime.&amp;nbsp; When this field is 0, it seems that the ImageDescription idSize &amp;nbsp;is 24 bytes larger to accommodate a ColorTable at the end. font-family:Arial'&gt; font-family:Arial'&gt;I am creating my PNG data provider with a pointer to the image data portion of the atom, which is separate from the ImageDescription I thought that the ImageDescription part of a Quicktime atom was just a redundant extraction of information from the image data. font-family:Arial'&gt; font-family:Arial'&gt;Is this possibly a bug in Quartz handling of PNG&amp;#8217;s, or am I not supplying the right information to Quartz?</body>
  </mail>
  <mail>
    <header>Re: Controlling Transparency of CIContext:drawImage</header>
    <body>Honestly, never played around with the AGL context, but I did compositing of CI images in CGL contexts. Although I still would recommend looking into CI for the compositing as well.</body>
  </mail>
  <mail>
    <header>Re: Controlling Transparency of CIContext:drawImage</header>
    <body>Well, I assume from your question that it _should_ work. That's at least good to know. However, all of my other OpenGL rendering (which is in the same OpenGL context) works with transparency as expected... so I would imagine that the alpha channel is set in the pixelformat of my context. AGL_RGBA AGL_DOUBLEBUFFER AGL_DEPTH_SIZE 32 (Again, this is an agl context and I'm using aglGetCGLContext to get the CGLContext and create the CIContext) Best, Mark -- ===================================================================== Mark Coniglio, Artistic Co-Director | email@hidden Troika Ranch Dance Theater          | =====================================================================</body>
  </mail>
  <mail>
    <header>Re: Core Image responsiveness</header>
    <body>Creating the CGImage will probably move the data from the GPU back to main memory.  A good strategy is use a custom NSView that creates (and caches) a CIContext from within its drawRect: method.  Caching the context is important if you're going to be drawing a lot (animation, for example) because creating it is relatively expensive. Do your rendering in another thread, and render to an output CIImage.  Give that to the view, locking appropriately.  So you'd wind up with something like this in the view: - (void)setImage: (CIImage *)image - (void)drawRect: (NSRect)rect @try if (nil == _drawingContext) CGContextRef cgContext = [[NSGraphicsContext currentContext] _drawingContext = [[CIContext contextWithCGContext: cgContext if (_image) [_drawingContext drawImage: _image atPoint: CGPointZero fromRect: You should probably dump the cached context if the window moves to another screen. On Jun 7, 2007, at 10:48 AM, Richard Knight wrote: Attachment:</body>
  </mail>
  <mail>
    <header>Core Image responsiveness</header>
    <body>I'm starting to look at using Core Image in an application I'm developing, where there would be a main image view which contains an image with several filters applied (pretty similar to Core Image Fun House).  What I'm concerned about is keeping the UI responsive when the filters runs slowly, which Core Image Fun House doesn't manage to do at all.  Was wondering if anyone else has figured out a way to do this? What I was planning on doing was to create a background thread with its own CIContext, then call createCGImage: fromRect: on that to create a CGImageRef which I can render on the main thread.  Does this seem reasonable?  If Core Image is using the GPU, will it result in the image getting copied back to main memory, then back up to the graphics card to be rendered (which doesn't seem like it would be a particularly great thing...)?  Is there may be a more efficient way to handle this?</body>
  </mail>
  <mail>
    <header>Re: Overprinting in Quartz PDF (Was: &amp;quot;Transparent&amp;quot; black	with	Quartz PDF)</header>
    <body>On Jun 7, 2007, at 4:57 AM, Izidor Jerebic wrote: Ha ha!  We used to hear that all the time on the FreeHand team.  My response was always something along the lines of &amp;quot;You can't ship breakfast cereal in an e-mail&amp;quot;. :-)</body>
  </mail>
  <mail>
    <header>Re: Overprinting in Quartz PDF (Was: &amp;quot;Transparent&amp;quot; black with	Quartz PDF)</header>
    <body>On 5.6.2007, at 17:29, Scott Thompson wrote: Well, old stuff would ignore it (as it does ignore most of new features, e.g. CoreImage), but new applications would surely use it if it would be possible. I can't see any problems in allowing an option such as overprint (or OPI or some other print specific feature) to propagate from application to the PDF generator and back This is precisely one of the problems in writing Mac OS X app for print publishing - you cannot extend PDF handling. Application writers have no choice but to parse/generate with their own PDF engine, instead of using Quartz for 95% and adding some specifics. But I can also see that print is not that important anymore in today's digital world...</body>
  </mail>
  <mail>
    <header>Re: drawing rectangle on CGContext whites out rest of screen</header>
    <body>hi all, thanks for the feedback on this i was able to reproduce the problem with the &amp;quot;basic&amp;quot; plugin sample that comes with firefox dev kit so this is 100% a firefox bug, this does not happen in safari or chrome so i filed it with bugzilla i have no idea why this issue is not a widespread problem have a nice long weekend bill hi all, i am drawing an update rectangle on a CGContext delivered to me by FireFox in an NPAPI plugin. when i draw my graphics, or even just a colored rectangle, the rest of the screen outside my rectangle turns white. if i always draw to the entire CGContext area then everything looks fine, no white areas or flickering, etc. i don&amp;#39;t expect anyone to respond to issues about NPAPI or FireFox, however in general what would cause this to happen? seems like there are areas on the CGContext that need to be drawn, but the flush happened before i could draw them? does this ring any bells with anyone? thanks in advance. bill</body>
  </mail>
  <mail>
    <header>Re: iOS: -[NSString drawInRect:...] fills color for one call,	not for another</header>
    <body>[Cut down a bit.] ... I don't think I'm constructing the string properly. The following code shows three ways to get the checkmark string. Using the literal character draws, but in black. The various ways I've tried to produce a Unicode string with the variant selector appended draw (within much larger bounds) as a black checkmark followed by the classic &amp;quot;missing character&amp;quot; rectangle, in red. As suggested, I tried substituting X for the character; that draws in red, as I'd hoped. For review, the set font is [UIFont fontWithName: @&amp;quot;Helvetica-Bold&amp;quot; size: 16]. ‚Äî F --- #define DRAWS_IN_BLACK      1 #define SWC_GARBAGE_STRING  2 #define UNI_GARBAGE_STRING  3 #define IWB_GARBAGE_STRING  5 #define JUST_DRAW_AN_X      4 #define CHECKMARK_CHOICE    IWB_GARBAGE_STRING #if CHECKMARK_CHOICE == DRAWS_IN_BLACK //  debugger shows checkStr == @&amp;quot;‚úî&amp;quot;. #elif CHECKMARK_CHOICE == SWC_GARBAGE_STRING //  debugger shows checkStr == @&amp;quot;‚úî&amp;quot;, plus garbage #elif CHECKMARK_CHOICE == UNI_GARBAGE_STRING //  debugger shows checkStr == @&amp;quot;‚úî&amp;quot; plus garbage. #elif CHECKMARK_CHOICE == IWB_GARBAGE_STRING checkStr = [[NSString alloc] initWithBytes: bytes length: sizeof(bytes) //  debugger shows checkStr == @&amp;quot;‚úî&amp;quot; plus garbage. #elif CHECKMARK_CHOICE == JUST_DRAW_AN_X #endif [checkStr drawInRect: checkmarkRect withFont: [UIFont fontWithName: @&amp;quot;Helvetica&amp;quot; size: 12] lineBreakMode: UILineBreakModeWordWrap //  When there are garbage characters, the check is drawn in black, //  followed by a red &amp;quot;missing character&amp;quot; rectangle.</body>
  </mail>
  <mail>
    <header>Re: iOS: -[NSString drawInRect:...] fills color for one call,	not for another</header>
    <body>Thats my understanding (one of the text guys pointed me there). There's probably also a unicode escape syntax (such that you could do it inline). -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: iOS: -[NSString drawInRect:...] fills color for one call,	not for another</header>
    <body>I think I see. I want U+2714 FEOE. How would I get that? ? ‚Äî F</body>
  </mail>
  <mail>
    <header>drawing rectangle on CGContext whites out rest of screen</header>
    <body>hi all, i am drawing an update rectangle on a CGContext delivered to me by FireFox in an NPAPI plugin. when i draw my graphics, or even just a colored rectangle, the rest of the screen outside my rectangle turns white. if i always draw to the entire CGContext area then everything looks fine, no white areas or flickering, etc. i don&amp;#39;t expect anyone to respond to issues about NPAPI or FireFox, however in general what would cause this to happen? seems like there are areas on the CGContext that need to be drawn, but the flush happened before i could draw them? does this ring any bells with anyone? thanks in advance. bill</body>
  </mail>
  <mail>
    <header>Re: iOS: -[NSString drawInRect:...] fills color for one call,	not for another</header>
    <body>Sanity check: if you replace the checkmark with, say, an &amp;quot;X&amp;quot;, does it still draw in black? H</body>
  </mail>
  <mail>
    <header>Re: iOS: -[NSString drawInRect:...] fills color for one call,	not for another</header>
    <body>May I renew this? ‚Äî F</body>
  </mail>
  <mail>
    <header>Re:  1. What is kCGImagePropertyJFIFDensityUnit (David Hoerl)</header>
    <body>see the following Scroll down to the &amp;quot;JFIF segment format&amp;quot; section to find Density units.</body>
  </mail>
  <mail>
    <header>What is kCGImagePropertyJFIFDensityUnit</header>
    <body>I'm trying to better understand the options to create progressive jpegs, and cannot find any reference to this field other than some usage where its set to &amp;quot;1&amp;quot; (with x and y set to 72). Anyone know? David</body>
  </mail>
  <mail>
    <header>iOS: -[NSString drawInRect:...] fills color for one call,	not for another</header>
    <body>iOS Simulator, Xcode 4.4, SDK and target 5.1. The following code was generated by PaintCode, except for small changes of my own: ==================== //// rankText Drawing /****** CHANGED: Replaced a string literal with a view property. *******/ //// checkmark Drawing /****** CHANGED: Wrapped the drawing in a conditional. *******/ /****** CHANGED: Tried to set the stroke color as well; no change in behavior. *******/ ==================== The rankText string is either ASCII numerals or @&amp;quot;--&amp;quot;. The drawn checkmark string is U+2714 HEAVY CHECK MARK. I want rankText to be Helvetica-Bold 16, white, shadowed. That's how it is drawn. I want the checkmark to be Helvetica 12, red, shadowed. That's not how it is drawn; the checkmark is drawn in black. What am I doing wrong? ‚Äî F</body>
  </mail>
  <mail>
    <header>Re: Playing a QC movie with a custom plugin (patch) in Quicktime	player</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Window Server issues while at login window on Mountain Lion</header>
    <body>Any suggestions? I'm about to open an developer incident over this as it's a big problem for our internal support of Mountain Lion.</body>
  </mail>
  <mail>
    <header>Re: Getting a glyph outline as a path?</header>
    <body>Yep, that's it! Thanks! Sent from my iPhone</body>
  </mail>
  <mail>
    <header>RE: Getting a glyph outline as a path?</header>
    <body>Try this: CTFontCreatePathForGlyph Creates a path for the specified glyph. CGPathRef CTFontCreatePathForGlyph ( ¬† ¬†CTFontRef font, ¬† ¬†CGGlyph glyph, ¬† ¬†const CGAffineTransform *transform Geza ________________________________________ From: quartz-dev-bounces+gfabry=email@hidden [quartz-dev-bounces+gfabry=email@hidden] on behalf of Rick Mann [email@hidden] Sent: Friday, August 03, 2012 10:09 To: Quartz Dev List List Subject: Getting a glyph outline as a path? I thought there was a way to get a glyph's outline as a path. I found some good glyphs, but I'd like to get their shape as a path, and then get the tightest bounding rectangle, and draw with that path. Googling is turning up some NSBezierPath things, but I'm not seeing it for Core Graphics. Thanks! -- Rick</body>
  </mail>
  <mail>
    <header>PocketSVG</header>
    <body>Hello all, For all of you interested in vector graphics on iOS, here's a library that parses an SVG's d attribute into a UIBezierPath, allowing you to use a pre-existing SVG file to create vector-based paths and shapes. The library is still in its infancy, so feedback is welcome. We're also looking for contributors, so if you can help improve the code, get in touch! Cheers Ariel</body>
  </mail>
  <mail>
    <header>Getting a glyph outline as a path?</header>
    <body>I thought there was a way to get a glyph's outline as a path. I found some good glyphs, but I'd like to get their shape as a path, and then get the tightest bounding rectangle, and draw with that path. Googling is turning up some NSBezierPath things, but I'm not seeing it for Core Graphics. Thanks! -- Rick</body>
  </mail>
  <mail>
    <header>Re: Does Instruments work on iPad CGPDF?</header>
    <body>The &amp;quot;copy hit&amp;quot; is not a rendering performance issue at this stage, but one in terms of memory usage (see below). Correct. This is where you were taking the copy hit. 11.25MB savings, congrats! :). Memory warnings are like taxes ‚Äì you are bound to get them and shouldn't be surprised when they arrive. They also don't indicate any wrong doing on your part. -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Does Instruments work on iPad CGPDF?</header>
    <body>‚Ä¶ Doing this ‚Ä¢ Didn't seem to do much for performance; render time is about 0.83 seconds instead of 0.87 (measured by NSLogs at entry/exit). I'm a little surprised, given that &amp;quot;copy hit&amp;quot; suggested a slowdown to me. ‚Ä¢ Didn't change the application-heap statistics (2 MB typical, 10.5 peak). Not expected to, I think. ‚Ä¢ Brought total VM to 26/56 MB dirty/resident. This is down from the 37/68 of my last run without the change. ‚Ä¢ Eliminated the 11.25 MB &amp;quot;Core Animation&amp;quot; allocation region. ‚Ä¢ Gets a level-1 memory warning, where the kCGImageAlphaPremultipliedLast version did _not_. The VM stats represent a big improvement, but the memory warning mystifies me. ‚Äî F</body>
  </mail>
  <mail>
    <header>Re: Does Instruments work on iPad CGPDF?</header>
    <body>Your image is likely taking are likely taking a copy-hit due to your image format. Try using kCGImageAlphaPremultipliedFirst | kCGBitmapByteOrder32Little instead. Yes, CGBitmapContext only supports premultiplied alpha as you've discovered. That would explain the 32MB region indeed :). I could be wrong, but I think it may have to do with the WebThread that is created for every application, but either way it is handled differently on the device. In terms of importance for an iOS application, typically Dirty &amp;gt; Resident &amp;gt; Virtual (dirty being most important). This is because Dirty memory must be kept resident (because there is no page file). Resident is usually more important when code is being evicted, and as an explanation for large VM allocations (that is, you might see a very large address space allocation that has a very small resident value). Hence why I asked about Resident before. The output of the Core Animation instrument from within Instruments itself is only the FPS gauge. The instrument however also provides flags that will color layers on the device itself so that you can do some visual debugging. -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Does Instruments work on iPad CGPDF?</header>
    <body>As you say, I'm using a UIImageView. It's inside a UIScrollView for zooming and panning, which in turn is inside another UIScrollView for paging. (I'm following a WWDC example for this.) I'm creating two images, both ways, once for a 1x image that I use for the paging animation, and once for a 2x image that I use for zooming once the user settles on a page. The 1x is done with UIGraphicsBeginImageContext, and the 2x with CGBitmapContextCreate, because I was toying with rendering it on a background thread. The 1x render takes about 0.25 second (visual estimate); the 2x render 1.5 s. I'd like to do better. Here is the CGBitmapContextCreate call: //  scaledBoundsRect is a little less than the screen rect, doubled. #define BITS_PER_COMPONENT      8 #define COMPONENT_COUNT         4 #define BITS_PER_PIXEL          (BITS_PER_COMPONENT * COMPONENT_COUNT) #define BYTES_PER_PIXEL         ((BITS_PER_PIXEL + 7) / 8) CGContextRef    context = CGBitmapContextCreate(NULL, scaledBoundsRect.size.width, scaledBoundsRect.size.height, BITS_PER_COMPONENT, bytesPerRow, colorSpace, /*  I need alpha, because much of the image is to be transparent. kCGImageAlphaLast is illegal: */ I'm ashamed to admit that my most recent measurements had been on the iPhone Simulator. The Core Animation tool forced me over to running against the device. The tracks are Allocations, VM Tracker, and Core Animation. Allocations peak at 10.2 MB, with 1.39 MB typical. VMTracker shows a stable resident size of 59 MB. No VM_ALLOCATE region. 11.25 MB each for &amp;quot;CG image&amp;quot; and &amp;quot;Core Animation,&amp;quot; dirty/resident/virtual. I'm not sure what I'm looking for in the Core Animation instrument. Scrolling between pages is sometimes jerky (as I render the 1x image), though it peaks at 44 fps. Running the application on the actual iPad eliminates that region. ‚Äî F</body>
  </mail>
  <mail>
    <header>Re: Does Instruments work on iPad CGPDF?</header>
    <body>I would assume that each of these 11.25MB allocations represents one of your PDF images, presumably at a size that is on the order of twice that of the screen. Perhaps the first question is how are you generating those images (are you using UIGraphicsBeginImageContext or CGBitmapContextCreate) and if using the latter what are the format flags you are passing. Then I presume you are just using a UIImageView to display the resulting image. The Core Animation tool may also be of help to ensure that you aren't taking image copies. Most of the other settings are more performance than memory related. Not sure about the VM_ALLOCATE region. What is the resident size of that region (vs the Dirty or Virtual sizes)? Yes, I suspect that &amp;quot;man vmmap&amp;quot; would help explain quite a bit of the terminology, as the VM Tracker instrument is basically the same data. -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Does Instruments work on iPad CGPDF?</header>
    <body>Thanks. Okay, what I'm seeing is a resident heap size that starts at about 3 MB, then stabilizes at 113 MB¬±, which as I understand is barely livable on an iPad. Switching among page images does not grow the heap, which I am guessing means that I am not inflating the caches. In the regions map, I see 11.25 MB resident for each of &amp;quot;CG image&amp;quot; and the first of three &amp;quot;Core Animation&amp;quot; regions, plus 32 MB for &amp;quot;VM_ALLOCATE&amp;quot;. The rest is a couple of MB here, a couple there, and soon you're talking about a lot of RAM. Other than the overall picture (huge heap, but not disastrous), I'm not entirely sure what I can do with this information. Is there a tutorial or explanation? The Instruments User Guide is not helpful at all. Would I profit from &amp;quot;man vmmap&amp;quot;? To Julius Oklamcak, I am grateful for the lead on CATiledLayer. I've never seen an example of CATiledLayer that did everything I needed, but maybe  will help. (For instance, I based my page-pan-and-zoom architecture on the Photoscroller example from WWDC 2010, but the tiling was done in advance.) ‚Äî F</body>
  </mail>
  <mail>
    <header>Re: Does Instruments work on iPad CGPDF?</header>
    <body>Note here that where I say &amp;quot;Instruments&amp;quot; I mean &amp;quot;Allocations&amp;quot;. VM Tracker is just another instrument (and comes with the Allocations template, but does not sample by default). -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Does Instruments work on iPad CGPDF?</header>
    <body>I'll see what I can do for a bug report.  It's been my experience with an iPad app centered around large-ish image files that I can successfully open and process images well into the 8 megapixel range much of the time, but other times I run into a wall with image files 25% of the size.  It may not be fragmentation like I think but rather something in the background (this is still iOS 3.2, so the only background processes should be OS ones) demanding more memory and lowering the amount I'm able to use.  Rebooting the iPad resolves it completely but isn't the best solution.</body>
  </mail>
  <mail>
    <header>Re: Does Instruments work on iPad CGPDF?</header>
    <body>By one measure, it's been up since the last 3.2.x revision (mid-August?), which fits any reasonable definition of &amp;quot;up a while.&amp;quot; By another measure, when one of my earlier versions got killed for memory, the iPad briefly displayed the glossy-apple startup graphic, though it came back to the home screen faster than it would have for a full restart. I'd understand fragmentation being a problem, but I can't help thinking that a well-ordered app should not demand regular reboots. (&amp;quot;Properly written Macintosh programs will run for hours, even days, without crashing.&amp;quot; ‚Äî Apple DTS's sardonic motto, circa 1988.) In any event, given that iOS uses VM (just not swapping) and completely purges application memory on quit, I'd think fragmentation would not be a really big issue. I'm getting non-null CGPDFDocuments and CGPDFPages, and Instruments isn't showing much net growth in the heap at all (a couple of hundred bytes per image, attributed to dyld, though I'm skeptical). I'd expect fragmentation problems to yield different behavior. But that is beside my question. The Instruments Allocations track showed _no_ difference in my memory usage between my version that kept a CGPDFDocument open all the time (which I gather caused cache buildup that eventually ran me out of memory), and the version that aggressively closes CGPDFDocuments (which runs, but with memory warnings). &amp;quot;No difference&amp;quot; can't be right. How can I get accurate and timely measurements on the memory I'm using (which I think Instruments isn't giving me with respect to CGPDF)? Refined question: Is it possible that CGPDF memory spikes occur internally to CG code, and Instruments is somehow shut out of heap monitoring during those periods? In that case, how do I detect and measure the spikes? ‚Äî F</body>
  </mail>
  <mail>
    <header>Re: Does Instruments work on iPad CGPDF?</header>
    <body>Instruments does not see the Quartz image cache or Core Animation backing stores, you will need to use VM Tracker to see these, and they can consume a huge amount of memory (and in fact they probably are since you say your heap size is only 4MB). Memory fragmentation is not a typical issue with a virtual memory system, as the VM map allows you to use physical pages from all over memory as if they were contiguous in the majority of cases that an application would care about. That said, that only discounts the explanation, not the symptoms, and a bug report is always welcome here. -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Does Instruments work on iPad CGPDF?</header>
    <body>Just curious -- how long as your iPad been up?  That is, how long since its last reboot?  I've run into memory warnings because the RAM gets very heavily fragmented after it's been up a while and found that the amount of memory usage that once worked is suddenly triggering memory warnings and even memory kills.</body>
  </mail>
  <mail>
    <header>Does Instruments work on iPad CGPDF?</header>
    <body>I'm not sure whether this is a Core Graphics question or an Instruments question. As it touches on the CG architecture and how to debug against it, I'm trying here first. I am working on a PDF browser for the iPad (iOS 3.2). The app runs, and I've at least masked the worst of the memory and performance bugs, but memory usage is still very dicey ‚Äî level-1 memory warnings pop up with almost every page. What mystifies me is that the Allocations instrument in Instruments is telling me everything is splendid; the typical heap size is less than 4 MB, with a peak around 9 MB as I scroll from page to page. 9 MB should never trigger a memory warning, right? So I'm left to suspect that Instruments isn't telling me everything that's going on. I've followed the advice I've found on minimizing memory footprint, principally that the CGPDFDocument should never be open unless to access dictionaries or to draw _one_ page. That stopped iOS from killing my app. I also draw my pages (into a UIImage) at 1x (which is unusable for zooming into fine print) when scrolling between them, and then substitute a 2x rendition once the user settles on a page. That made paging fast enough to be usable. I've been aggressive enough that the application runs well enough, but the memory warnings worry me. My question for the moment is: I don't think Instruments is giving me a complete picture of my memory usage with CGPDF. How do I get one? ‚Äî F</body>
  </mail>
  <mail>
    <header>Re: Using CG from Thread</header>
    <body>So long as only one thread at a time is using any given CG object. Sent from my iPhone Federico As long as you aren't trying to manipulate the same Quartz data type from multiple threads at the same time, you should be fine. Keep in mind however that if you didn't create the context you are targeting, then you cannot assume that the creator of that context will not manipulate it from a thread other than the one you are using. For example, it is invalid to use the current context from a -drawRect: call from anywhere but within -drawRect: because you have no guarantee that the context will remain valid outside of -drawRect:, or that it won't be manipulated by the framework if it does. -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Using CG from Thread</header>
    <body>Hi and thanks for the fast reply! Federico As long as you aren&amp;#39;t trying to manipulate the same Quartz data type from multiple threads at the same time, you should be fine. Keep in mind however that if you didn&amp;#39;t create the context you are targeting, then you cannot assume that the creator of that context will not manipulate it from a thread other than the one you are using. For example, it is invalid to use the current context from a -drawRect: call from anywhere but within -drawRect: because you have no guarantee that the context will remain valid outside of -drawRect:, or that it won&amp;#39;t be manipulated by the framework if it does. -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Using CG from Thread</header>
    <body>As long as you aren't trying to manipulate the same Quartz data type from multiple threads at the same time, you should be fine. Keep in mind however that if you didn't create the context you are targeting, then you cannot assume that the creator of that context will not manipulate it from a thread other than the one you are using. For example, it is invalid to use the current context from a -drawRect: call from anywhere but within -drawRect: because you have no guarantee that the context will remain valid outside of -drawRect:, or that it won't be manipulated by the framework if it does. -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Using CG from Thread</header>
    <body>What graphics context are you using when you call these routines? &amp;nbsp;Are you trying to draw to the screen? &amp;nbsp;If so then it is not OK to be doing that. &amp;nbsp;All drawing to the user interface should be done from the main thread.</body>
  </mail>
  <mail>
    <header>Re: Raster operations equivalent to GDI/Windows on MAC</header>
    <body>Ah, OK.  Those aren't blend modes, they're binary raster ops.  Quartz doesn't really support those (they don't actually make much sense other than for black and white bitmaps), though some of them can be emulated easily enough (e.g. the masking functions can be replaced with image masks). My guess is that you're going to have to do the raster operations yourself into a buffer.  If you're clever, you might be able to use Core Image to accelerate this (though remember that Core Image, like Quartz, regards colours as continuous values between 0 and 1 inclusive). Kind regards, Alastair. p.s. Please make sure to reply to the list as well as to posts from the list. --</body>
  </mail>
  <mail>
    <header>Re: Raster operations equivalent to GDI/Windows on MAC</header>
    <body>I think you should probably have a read of the Quartz 2D Programming Guide, which, if you are familiar with GDI will probably answer most of your questions: To address your specific queries: 1. When you talk about raster operations and blending modes, you'd have to be more specific about exactly what you're using on the Windows side.  Certainly the AlphaBlend() API and the GDI+ SetCompositingMode() API look like they only support the source-over blending mode, which is one of the ones Quartz supports.  It's possible that you're talking about some other API, in which case the answer may or may not be different. 2. There is no direct equivalent to pens and brushes in Quartz; rather, the stroke and fill colours (or patterns), the line dash pattern and the line ending and join settings are attributes of the graphics context (the equivalent of a DC). Kind regards, Alastair. --</body>
  </mail>
  <mail>
    <header>Raster operations equivalent to GDI/Windows on MAC</header>
    <body>hi Does anyone know how raster operations(equivalent to GDI/windows) can be ported to Mac OS X using Quartz?¬† Are the Blending Modes equivalent?¬† Also, as regards pens and brushes used in GDI/windows, are these possible in Quartz 2d? If yes can you pls post some links to sample codes or links to related information. Thanks for any reply</body>
  </mail>
  <mail>
    <header>Re: Should I beware of using CGEventPostToPSN?</header>
    <body>That's correct, but most of those issues were fixed in 10.5. There was another issue fixed in 10.5.5 that occurred when using a unicode-based keyboard layout. So as far as I know, using CGEventKeyboardSetUnicodeString should work in 10.5.5 and later, for both Carbon and Cocoa apps. I just tested Matt's sample app with Simple Text (which is a Carbon app, still WaitNextEvent-based, so about as old-school as you can get) and it worked properly there, and in TextEdit as well. If the app is not using the standard event system input, then it might not see keyboard input provided by CGEventKeyboardSetUnicodeString. For example, since Reaktor is a music application, I could imagine that perhaps it is just constantly scanning the state of the hardware key matrix, and reacting to key presses that way, rather than receiving keyboard input as events. If that were the case, then any input you provided in the event stream would be ignored. However, since I don't have Reaktor here (and it doesn't look like NI provides a demo download), I can't prove or disprove that theory. Matt, I received the bug you filed about remote control of menu items (8413311), but I'm having trouble reproducing it with the sources for Freeman on git. I modified the sources to target TextEdit instead of Reaktor, but pressing cmd-option-i just brings up a dialog with an edit field, and doesn't cause the TextEdit contextual menu to open. It would be most helpful if you could prepare a test app that specifically targets TextEdit and reproduces the problem and send that to me. -eric</body>
  </mail>
  <mail>
    <header>Re: Re: CGPDF navigation (links, outline)</header>
    <body>Good point about scaling! It eliminates need of full page scan on every click. Here&amp;#39;s how I approached the problem. It scales better than the linear method, assuming the document has its pages in a reasonably balanced tree. static NSInteger OTPageNumberFromPageDictionary(CGPDFDictionaryRef target) ¬† ¬†// there&amp;#39;s nothing in the page dictionary to identify page number ¬† ¬†// so we have to work it out by counting our elder siblings ¬† ¬†// and the descendents of elder siblings of each ancestor ¬† ¬†while (CGPDFDictionaryGetDictionary(target, &amp;quot;Parent&amp;quot;, &amp;amp;parent)) ¬† ¬† ¬† ¬†if (CGPDFDictionaryGetArray(parent, &amp;quot;Kids&amp;quot;, &amp;amp;kids)) ¬† ¬† ¬† ¬† ¬† ¬†for (kidNum = 0; kidNum &amp;lt; numKids; ++kidNum) ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†if (CGPDFArrayGetDictionary(kids, kidNum, &amp;amp;kid)) ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†if (kid == target) ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†if (CGPDFDictionaryGetInteger(kid, &amp;quot;Count&amp;quot;, &amp;amp;count)) ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†else Best wishes, Hamish &amp;gt; ¬†_______________________________________________</body>
  </mail>
  <mail>
    <header>Re: Re: CGPDF navigation (links, outline)</header>
    <body>Here's how I approached the problem. It scales better than the linear method, assuming the document has its pages in a reasonably balanced tree. static NSInteger OTPageNumberFromPageDictionary(CGPDFDictionaryRef target) // there's nothing in the page dictionary to identify page number // so we have to work it out by counting our elder siblings // and the descendents of elder siblings of each ancestor while (CGPDFDictionaryGetDictionary(target, &amp;quot;Parent&amp;quot;, &amp;amp;parent)) if (CGPDFDictionaryGetArray(parent, &amp;quot;Kids&amp;quot;, &amp;amp;kids)) for (kidNum = 0; kidNum &amp;lt; numKids; ++kidNum) if (CGPDFArrayGetDictionary(kids, kidNum, &amp;amp;kid)) if (kid == target) if (CGPDFDictionaryGetInteger(kid, &amp;quot;Count&amp;quot;, &amp;amp;count)) else Best wishes, Hamish</body>
  </mail>
  <mail>
    <header>Re: Should I beware of using CGEventPostToPSN?</header>
    <body>I don't know why it didn't occur to me to test this before. I switched the test app I posted a link to yesterday to point at TextEdit instead and it works. So it looks like the issue is that the application, Reaktor, is the sticking point. Is there something an application needs to do before unicode key events work? Or something it could be doing that could block them? Or maybe some subtlety about how you create them than different applications can respond to? I definitely feel like I am right out on the end of the branch here. I've attached my test application (notes in CGTestAppDelegate.m) in case anyone has time &amp;amp; inclination to take a look. If you can help me figure this out it would be much appreciated. Thanks, Matt -- Matt Mower :: Attachment:</body>
  </mail>
  <mail>
    <header>Re: Should I beware of using CGEventPostToPSN?</header>
    <body>Hi James. This is bad news, but thank you for sounding the alarm. I'm automating Native Instruments Reaktor which is a pretty old app and mainly Carbon although I think the latest version has made some steps towards Cocoa. Otool certainly says it links to Cocoa. I'm on 10.6.4 and my app is (presently) 10.6 only. I'd like to backport it to 10.5 but now I'm thinking that might be a major PITA. I'm not sure how to determine if the current keyboard layout is 'uchr' or even if that's a good or bad thing. Presumably I can't make any wide assumptions about other peoples keyboard layouts. It's definitely beginning to sound like CGEventKeyboardSetUnicodeString isn't worth the hassle. But then I'm faced with virtual key codes which are almost as impractical it seems. Even if I map out the keyboard layout I am faced with other people having different layouts and I've no idea what the situation is if anyone tries to insert an √©. I guess I am looking at the problems, lack of available documentation &amp;amp; examples, and wondering if this API is meant for general use. Am I doing something wildly out of scope here*? Matt. * This application remote operates a complex, deeply nested, context menu within a second application, sending a right click to open the menu and then navigating the menu structure to select an item. -- Matt Mower ::</body>
  </mail>
  <mail>
    <header>Re: Flip coordinates system of an NSImageView?</header>
    <body>Hi Mathieu, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; You can also use the rotateByAngle method of NSView to flip your view. Sample Code : -JanakiRam. On 7/27/07, It&amp;#39;s going to look something like: [flipTransform scaleXBy: 1.0 yBy: -1.0] This was typed into Mail so it may be off a bit.</body>
  </mail>
  <mail>
    <header>Re: Flip coordinates system of an NSImageView?</header>
    <body>On Jul 26, 2007, at 3:50 PM, Mathieu Thouvenin wrote: It's going to look something like: [flipTransform scaleXBy: 1.0 yBy: -1.0] This was typed into Mail so it may be off a bit.</body>
  </mail>
  <mail>
    <header>Re: Flip coordinates system of an NSImageView?</header>
    <body>apply an NSAffineTransform with appropriate negative scale values before you draw. On Jul 26, 2007, at 1:50 PM, Mathieu Thouvenin wrote:</body>
  </mail>
  <mail>
    <header>Re: image resizing options?</header>
    <body>You could jump straight to vImage Although I expect this is the same code library that sits underneath Quartz 2D so there may not be any benefit to using it. You could also perform an affine transform by texture mapping the image onto an OpenGL quad and sending that through the OpenGL pipeline.  This has the advantage that most of the scaling/resizing will be handled by the graphics card.  If you're just sending your transformed image to the screen (not performing other processing on it) then this might be the speediest way, but the quality could depend on the graphics card. Scott</body>
  </mail>
  <mail>
    <header>image resizing options?</header>
    <body>hey! i'm playing around with images in my program, and looking for ways to scale and resize images.  the two methods i've found so far appear to be using affine transforms (fast, but icky-poo quality), and CILancoszScaleTransform (slow, but great quality). i was wondering, however, if there are other options?  i'm assuming that Cocoa's NSAffineTransform is the same as the quartz affine transform APIs.  do i have any other options i can investigate and play with for image scaling/resziing? thanks, marc.</body>
  </mail>
  <mail>
    <header>What is DigitalColor Meter measuring?</header>
    <body>Hi all, does anybody have a clue what DigitalColor Meter is actually measuring? Changing the monitor profiles give's me different values for some apps, on other's they stay the same. Mark</body>
  </mail>
  <mail>
    <header>Flip coordinates system of an NSImageView?</header>
    <body>Hi! I'm looking for a way to flip the coordinates system of my NSImageView (so that the coordinates start in the upper-left instead of the bottom-left). Thanks a lot for your help! Mathieu</body>
  </mail>
  <mail>
    <header>Performance problem in odd use of CGPatterns</header>
    <body>I've been tracking some performance problems in the Qt library and traced it to how we're making use of patterns. The code to reproduce is here: Its CoreGraphics only, and the code doesn't do anything reasonable except illustrate the problem: What we do is to create a pattern tile that is 200x200 in size which can scale up and down. (In the real scenario it follows the transformation of the context). As the scale factor grows, using this pattern becomes very slow. The example above I scale it by 40x40 in case it takes a few seconds to fill a 10x10 pixel rectangle. The reason for this, as far as I can tell, is because CoreGraphics allocates the full 8000x8000 pixel tile to fill this tiny rectangle. If I increase the scale to a factor of 100 I get malloc failures... In a perfect world, since the area to fill with the tile is so small compared to the actual tile, one would use the callback function directly to fill the tile and this only do calculations for the 100 pixels in the rectangle instead of using the huge temporary tile pixmap. best regards, Gunnar</body>
  </mail>
  <mail>
    <header>Re: Photoshop Plugin architecture for Quartz</header>
    <body>Am 23.07.2007 um 20:09 schrieb Mark: AFAIK It is not possible to write legally an host application with the recent SDKs any more. The last SDK that allowed it was IIRC the (pre carbon, pre-universal) Photoshop 4 SDK. Regards, Tom_E</body>
  </mail>
  <mail>
    <header>Re: Preview.app not colorsync enabled ???</header>
    <body>In a message dated 7/23/07 1:37:18 PM,  writes: Is it possible that Preview app does not actually use the embeded ICC¬† profiles of jpeg files? My images look very different in Preview vs Photoshop CS 3. How can this be?</body>
  </mail>
  <mail>
    <header>Photoshop Plugin architecture for Quartz</header>
    <body>I'd like to add Photoshop plugin support for my app. Is anybody aware of a framework that quickly enables that for Carbon, Cocoa or CoreVideo? Cheers Mark</body>
  </mail>
  <mail>
    <header>Preview.app not colorsync enabled ???</header>
    <body>Is it possible that Preview app does not actually use the embeded ICC profiles of jpeg files? My images look very different in Preview vs Photoshop CS 3. How can this be?</body>
  </mail>
  <mail>
    <header>Re: Quickest Drawing Method</header>
    <body>My current situation is that I have a custom NSWindow with a single custom view in it. I do my drawing in the view's drawRect function. The drawing consists of: - stretching a couple of images to size and drawing them - applying a mask image (I think using a NSBezierPath to make the mask image would be quicker than using a black and transparent PNG file). This all happens when I call setFrame for the custom window. Is OpenGL still viable for an odd-shaped image (not rectangular; with transparencies)? Or by using Quartz and CG* functions is that the same as OpenGL? Yeah I checked it out. There's a window for each icon plus more! Crazy. Right now I have a &amp;quot;scheduledTimerWithTimeInterval:0.1&amp;quot; that calls a function that just updates the custom window's size; this is quite draining on the resources as well as jumpy. What can I do to get this resize function called a lot, but without killing the system? Maybe a run loop callback?</body>
  </mail>
  <mail>
    <header>Quickest Drawing Method</header>
    <body>I'm writing an app which will constantly write an image to the screen. I would like to know the quickest way to draw an image to the screen (the image needs to be composed first). Currently I have a borderless window the size of the screen with clear background, and I set the window background to an image. I would like it to be an animation, like the Dock resizing; how is the Dock drawn without a window, and how can I accomplish this? Cheers Matt</body>
  </mail>
  <mail>
    <header>NSImage Compilation</header>
    <body>Hi, I'm new at Cocoa and Quartz. I am developing an app which draws an image on the screen, given a NSRect. This image has a background, some items composite over the top, and then a mask applied (for unusual shape). Currently update resizes and composes the image onto a global NSImage, applies it to the background of a NSWindow, and moves the window to the NSRect. This is very slow, and the image size jumps by approximately 10 pixels, instead of a smooth transition. Is there a better way to do this with Quartz? The window is not selectable with mouse, so I thought of having it the size of the screen and just drawing in the NSRect appropriately. I want the transitions to look as smooth as the Dock resizing, and I assume that means adding it to the run loop. Cheers, spiderlama</body>
  </mail>
  <mail>
    <header>Printing a PDF using PDFView from a non Cocoa Application ( Mozilla	Extension ).</header>
    <body>Hi ALL, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; I&amp;#39;m developing a Firefox Extension/XPCOM which prints the PDF using C++ , Objective-C. &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; I&amp;#39;ve written a Objective-C class which creates a PDFView , PDFDocument. &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; To print PDF , i&amp;#39;m using PDFView&amp;#39;s printWithInfo:autoRotate method. This method is opening a print dialog as sheet( i guess its expecting&amp;nbsp; a Cocoa application/window ) and the dialog UI is not displaying properly. &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; But if i create a Cocoa application and call the same code for printing its working fine. &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Can&amp;#39;t we use the Cocoa framework classes without a Cocoa Application/Window. &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; OR &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; is there any method for PDFView like print in NSView which opens a print dialog ( without looking for a window ) &amp;amp; prints without any UI Problem. &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; One more question is there any restriction like we should call PrintDialog/UI related activities in Main thread ?. &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Please help me to solve this. Thanks in Advacnce. -JanakiRam.</body>
  </mail>
  <mail>
    <header>Printing a PDF using PDFView from a non Cocoa Application ( Mozilla	Extension ).</header>
    <body>I'm developing a Firefox Extension/XPCOM which prints the PDF using C++ , Objective-C. I've written a Objective-C class which creates a PDFView , PDFDocument. To print PDF , i'm using PDFView's printWithInfo:autoRotate method. This method is opening a print dialog as sheet( i guess its expecting  a Cocoa application/window ) and the dialog UI is not displaying properly. But if i create a Cocoa application and call the same code for printing its working fine. Can't we use the Cocoa framework classes without a Cocoa Application/Window. is there any method for PDFView like print in NSView which opens a print dialog ( without looking for a window ) &amp;amp; prints without any UI Problem. One more question is there any restriction like we should call PrintDialog/UI related activities in Main thread ?.</body>
  </mail>
  <mail>
    <header>Re: Reflection in Perspective</header>
    <body>So is there one CIPerspectiveTransform for the main image and another CIPerspectiveTransform for the reflected image? &amp;nbsp;CIPerspectiveTransform distorts the image to make it fit inside the 4 corners you specify. &amp;nbsp;It looks like the distortion is coming out differently in the reflected image, which makes perfect sense. I think you could get what you want by only doing one CIPerspectiveTransform on an image where the top half is the main image and the bottom half is the faded reflection. Greetings, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; I have to admit I am pretty green when it comes to using Quartz, and my programming skills are mediocre. So, please bare with me here. &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; I am using the Core Image Filter &amp;#39;CIPerspectiveTransform&amp;#39; to give an image perspective, this works fine and well. But when I try to give the perspective image a reflection using similar methods, the reflective image&amp;#39;s ratios do not match source image&amp;#39;s ratio. I have attached an image in hopes that you can see what I am talking about. &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; I have looked through the docs, hoping to find a solution, but the docs are lacking something to be desired. &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Am I heading totally in the wrong direction? Any hints or pointers would be appreciated. Cheers, Brian</body>
  </mail>
  <mail>
    <header>Re: CIImage to an image file?</header>
    <body>This will work, but there may be better ways to do it. This is adapted from other code, so there may be minor syntax errors, and you may need to fiddle with stuff for your particular needs. I'm pretty certain I grabbed it from online somewhere (Dan Wood's blog possibly). Creating the graphics context is expensive, so cache it and the bitmap, and reallocate them if the size changes (or cache one for each size you'll be using). // You can get this from CIImage extents, but sometimes they're a bit off. NSBitmapImageRep *bitmap = [[NSBitmapImageRep alloc] initWithBitmapDataPlanes:NULL pixelsHigh:ourRect.size.height bitsPerSample:8 samplesPerPixel:4 colorSpaceName:NSDeviceRGBColorSpace bytesPerRow:0 NSGraphicsContext  graphicsContext = [NSGraphicsContext // Save the previous graphics context and state, and make our bitmap context current. // first, clear the context - you might not need to do this if you're not caching the context. // Get a CIContext from the NSGraphicsContext, and use it to draw the CIImage into the NSBitmapImageRep. [[mNSContext CIContext] drawImage:theCIImage inRect:CGRectMake(0.0f, 0.0f, ourRect.size.width, ourRect.size.height) // Restore the previous graphics context and state. NSImage *image = [[[NSImage alloc] initWithSize:NSMakeSize(ourRect.size.width, ourRect.size.height)]</body>
  </mail>
  <mail>
    <header>Re: CoreImage rendering intent - no word about it on the docs ?</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>CIImage to an image file?</header>
    <body>Hello everybody! I have a problem on a small app I'm working on, and I haven't been able to resolve it after hours on google and this mailing list. I have a CIImage and I want to save it as an image file (JPEG format for example). Right now I have the NSCIImageRep of my CIImage by doing this: Then, I add this representation to an NSImage (theImageIWantToSave) and then I'm doing: [[theImageIWantToSave TIFFRepresentation¬∞ But it doesn't work. Is there a better method to have an image file based on my CIImage? Thanks a lot for your help! Regards, Mathieu</body>
  </mail>
  <mail>
    <header>Re: CoreImage rendering intent - no word about it on the docs ?</header>
    <body>Almost. If you find some clipping, you won't be able to say if it is absolute or relative because they share the same white point. Maybe too much. Normally you're just looking for clipped values or not. Primary color ramps and visual checks should be enough considering the problem... but that's less academic :) You're welcome ! Raphael</body>
  </mail>
  <mail>
    <header>Re: CoreImage rendering intent - no word about it on the docs ?</header>
    <body>though I've learned a lot I didn't know about CoreImage's inner workings through this discussion (many thanks to you both), my original question remains unanswered. So I'm thinking about how to best setup a test that shows how CoreImage is mapping gamut. As I am pretty new to all this color space messiness, I ask you once more to help me a bit so I setup my test correctly. So I have a TIFF file in Adobe RGB that simulates the ColorChecker (downloaded from babel color). It does only have one color that falls outside the sRGB gamut so I'm thinking about modifying it a bit so I have more colors outside sRGB. I then plan to generate copies of the file mapped to sRGB through different rendering intents. I would do that using PhotoShop (which uses Adobe's CMM), Quartz (which uses Apple's CMM) and lastly CoreImage. I would then write down the L*a*b values of all images for comparison in a Excel sheet for comparison. Does this sound reasonable? Would this be enough to qualify CoreImage's rendering intent? Do you have any recommendations for the input image's colors - is what I described above good for the test? My fault - I've been reading too much about L*a*b lately and mixed all up. Thanks the clarification! Mark</body>
  </mail>
  <mail>
    <header>Re: Odd old messages reposted</header>
    <body>Plus, one of them had a half-meg attachment. I just sent a note to the list mom asking why big attachments are being allowed through. I thought that was against the rules. -- Bill Cheeseman - email@hidden Quechee Software, Quechee, Vermont, USA www.quecheesoftware.com PreFab Software - www.prefabsoftware.com</body>
  </mail>
  <mail>
    <header>Odd old messages reposted</header>
    <body>On Jul 17, 2007, at 1:02 PM, Steve Mills wrote: I do see a large group of older messages.</body>
  </mail>
  <mail>
    <header>Re: Problem with CGShading</header>
    <body>Did anybody else just see a hunk of old messages that were either resent or just now arrived? The date sent ranges from June back to February. Maybe not everybody noticed this because I have the Date Sent column visible in Mail, and I don't think that's the default. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Problem with CGShading</header>
    <body>It's an inaccuracy with the way your setting up your transformation matrix. I would either remove the matrix code entirely (and adjust your shading points), or do the matrix math &amp;quot;the long way&amp;quot; rather than filling out the matrix directly. -- David Duncan Apple DTS Quartz and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Re: CoreImage rendering intent - no word about it on the docs ?</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Reflection in Perspective</header>
    <body>I have to admit I am pretty green when it comes to using Quartz, and my programming skills are mediocre. So, please bare with me here. I am using the Core Image Filter 'CIPerspectiveTransform' to give an image perspective, this works fine and well. But when I try to give the perspective image a reflection using similar methods, the reflective image's ratios do not match source image's ratio. I have attached an image in hopes that you can see what I am talking about. I have looked through the docs, hoping to find a solution, but the docs are lacking something to be desired. Am I heading totally in the wrong direction? Any hints or pointers would be appreciated.</body>
  </mail>
  <mail>
    <header>Re: Seeing Alpha after CIFilters</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Another CIImage Bitmap problem</header>
    <body>Hi, like so many others, I'm trying to¬†build¬†a CIFilter that will do some¬†computation (like¬†histogram)¬†that cannot be done in the kernel. So I started with the CIDemoImageUnit and in the outputImage method, I try to get at the bitmap to do the computation.2) when I try to move their creation into the CIFilter init method, and I store them into instance variable into the CIFilter. Yet, they seem to disappear when the outputImage is called.¬†</body>
  </mail>
  <mail>
    <header>QCRenderer with CVImageBuffer input crash</header>
    <body>I&amp;#39;m using the CVDisplayLink to call my rendering routine, and I&amp;#39;m using QCRenderer&amp;#39;s to draw to an on screen OpenGL context. One of my QCRenderer&amp;#39;s has a published input of type Image which I populate with a CVPixelBufferRef each time through the rendering routine. &amp;nbsp;The CVPixelBufferRef is obtained from QTVisualContextCopyImageForTime() after I make sure there is a frame available and the frame isn&amp;#39;t NULL. I was having crashes moments after the video starts playing. &amp;nbsp;I&amp;#39;d get a couple frames drawn to the screen and hear the sound from the QTMovie, but then (after 1 or 2 seconds) the debugger pops up with the following trace [1]: For shits and giggles I surrounded the rendering calls with CVPixelBufferLockBaseAddress() and CVPixelBufferUnlockBaseAddress(). Now it takes a while (10 to 20 seconds) for a crash to come, and instead, the trace is as follows [2]: Does anyone know what&amp;#39;s going? &amp;nbsp;Here&amp;#39;s my rendering routine [3], which is called from the display link callback. Thanks for any help. ######################### [1] ######################### #0 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x940fb3ae in CVImageBacking::provideImageTexture #1 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x940f2777 in CVImageBufferProvideImageTexture #2 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x93eef826 in -[CICVImageProviderDelegate provideImageTexture:bounds:userInfo:] #3 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x93ee14c2 in provide_texture #4 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x93eb9bcf in fe_gl_texture_load #5 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x93ee13c6 in fe_context_texture_load #6 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x93ee11c7 in image_buffer_texture_ref #7 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x93f602d9 in texture_retain #8 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x93ee0da9 in fe_texture_new #9 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x93ee09f2 in fe_tree_create_texture #10 &amp;nbsp; &amp;nbsp; 0x93ee015c in fe_tree_render_apply #11 &amp;nbsp; &amp;nbsp; 0x93edea43 in fe_tree_render #12 &amp;nbsp; &amp;nbsp; 0x93eddcee in fe_tree_render_image #13 &amp;nbsp; &amp;nbsp; 0x93edd89e in fe_image_render_ #14 &amp;nbsp; &amp;nbsp; 0x93edd7ac in fe_image_render #15 &amp;nbsp; &amp;nbsp; 0x93edd6e7 in -[CIOpenGLContextImpl renderAccel:matrix:bounds:] #16 &amp;nbsp; &amp;nbsp; 0x93edc3c0 in -[CIOpenGLContextImpl render:] #17 &amp;nbsp; &amp;nbsp; 0x93eef597 in -[CIContext drawImage:inRect:fromRect:] #18 &amp;nbsp; &amp;nbsp; 0x97b1c53e in -[QCGLCIImage(Override) _uploadTexture:] #19 &amp;nbsp; &amp;nbsp; 0x97b1aa0e in -[QCGLImage textureName] #20 &amp;nbsp; &amp;nbsp; 0x97b1b978 in -[QCGLImage(Private) _setTextureOnContext:unit:useTransformationMatrix:] #21 &amp;nbsp; &amp;nbsp; 0x97b1b91e in -[QCGLPort_Image set:unit:useTransformationMatrix:] #22 &amp;nbsp; &amp;nbsp; 0x97b64cb6 in -[QCBillboard execute:time:arguments:] #23 &amp;nbsp; &amp;nbsp; 0x97b15c1c in -[QCPatch(Private) _execute:arguments:] #24 &amp;nbsp; &amp;nbsp; 0x97b16499 in -[QCPatch(Execution) executeSubpatches:arguments:] #25 &amp;nbsp; &amp;nbsp; 0x97b15c1c in -[QCPatch(Private) _execute:arguments:] #26 &amp;nbsp; &amp;nbsp; 0x97b155f4 in -[QCPatch(Runtime) render:arguments:] #27 &amp;nbsp; &amp;nbsp; 0x000558cf in -[AppController _renderForTime:] at AppController.m:511 #28 &amp;nbsp; &amp;nbsp; 0x0005337f in _displayLinkCallBack at AppController.m:35 #29 &amp;nbsp; &amp;nbsp; 0x941013a0 in CVDisplayLink::performIO #30 &amp;nbsp; &amp;nbsp; 0x941019f2 in CVDisplayLink::runIOThread #31 &amp;nbsp; &amp;nbsp; 0x900245c7 in _pthread_body ######################### [2] ########################## #0 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x940fb120 in CVImageBacking::deleteImageTexture #1 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x93eef8c3 in delete_texture #2 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x93f5bbcf in fe_gl_texture_finalize #3 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x93ed1543 in fe_release #4 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x93ee384c in fe_texture_release #5 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x93ee04dc in fe_tree_render_apply #6 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x93edea43 in fe_tree_render #7 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x93eddcee in fe_tree_render_image #8 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x93edd89e in fe_image_render_ #9 &amp;nbsp; &amp;nbsp; &amp;nbsp;0x93edd7ac in fe_image_render #10 &amp;nbsp; &amp;nbsp; 0x93edd6e7 in -[CIOpenGLContextImpl renderAccel:matrix:bounds:] #11 &amp;nbsp; &amp;nbsp; 0x93edc3c0 in -[CIOpenGLContextImpl render:] #12 &amp;nbsp; &amp;nbsp; 0x93eef597 in -[CIContext drawImage:inRect:fromRect:] #13 &amp;nbsp; &amp;nbsp; 0x97b1c53e in -[QCGLCIImage(Override) _uploadTexture:] #14 &amp;nbsp; &amp;nbsp; 0x97b1aa0e in -[QCGLImage textureName] #15 &amp;nbsp; &amp;nbsp; 0x97b1b978 in -[QCGLImage(Private) _setTextureOnContext:unit:useTransformationMatrix:] #16 &amp;nbsp; &amp;nbsp; 0x97b1b91e in -[QCGLPort_Image set:unit:useTransformationMatrix:] #17 &amp;nbsp; &amp;nbsp; 0x97b64ce5 in -[QCBillboard execute:time:arguments:] #18 &amp;nbsp; &amp;nbsp; 0x97b15c1c in -[QCPatch(Private) _execute:arguments:] #19 &amp;nbsp; &amp;nbsp; 0x97b16499 in -[QCPatch(Execution) executeSubpatches:arguments:] #20 &amp;nbsp; &amp;nbsp; 0x97b15c1c in -[QCPatch(Private) _execute:arguments:] #21 &amp;nbsp; &amp;nbsp; 0x97b155f4 in -[QCPatch(Runtime) render:arguments:] #22 &amp;nbsp; &amp;nbsp; 0x000558ba in -[AppController _renderForTime:] at AppController.m:511 #23 &amp;nbsp; &amp;nbsp; 0x0005334f in _displayLinkCallBack at AppController.m:35 #24 &amp;nbsp; &amp;nbsp; 0x941013a0 in CVDisplayLink::performIO #25 &amp;nbsp; &amp;nbsp; 0x941019f2 in CVDisplayLink::runIOThread #26 &amp;nbsp; &amp;nbsp; 0x900245c7 in _pthread_body ####################### [3] ############################### - (void) _renderForTime:(const CVTimeStamp*) time &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// check for new shadow source frame &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// get a &amp;quot;frame&amp;quot; (image buffer) from the Visual Context, indexed by the provided time &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;OSStatus status = QTVisualContextCopyImageForTime(_sourceContext, &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// the above call may produce a null frame so check for this first &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;// if we have a frame, then draw it &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;NSTimeInterval &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;videoTime, &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;//Compute the video time &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;if(time-&amp;gt;flags &amp;amp; kCVTimeStampVideoTimeValid) &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;videoTime = (NSTimeInterval)time-&amp;gt;videoTim e / &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;else &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;videoTime = 0; //Not sure what the best thing to do is &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;//Compute the local time as the difference between the current video time and the video time at which the first frame was rendered &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;else &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;//Render content and then shadows with QCRenderer&amp;#39;s</body>
  </mail>
  <mail>
    <header>Problematic Code in Flashkit</header>
    <body>This is latest actionscript file.  The code is very messy since I had to whip this up so quickly and I'm no Flash guru.  The performance culprit is most definitely the way we're loading images into the scrolling animation at the very bottom of the file.  As far I can tell images in Flash are not handled like HTML, there's no image caching involved, so it actually creates tons of image objects. This is a bandwidth hog and also slows down the movie.  Any tips for optimizing many small thumbnails? David Attachment:</body>
  </mail>
  <mail>
    <header>Problem with CGShading</header>
    <body>Hi, I have attached a piece of code and the graphical result that it produces. Everything seems to be working as expected except for a 1 pixel vertical line half the height of the rectangle that's being drawn on the left edge of the rectangle. I have the impression that even if I set the shading starting and ending point to be a prefectly horizontal line the shading ends up being not perfectly horizontal. As a matter of fact it seems to be 1 pixel sheared towards the bottom. Please let me know where I am screwing up. Thank you. - Luigi ------------------------------------------------------------ THIS E-MAIL MESSAGE IS FOR THE SOLE USE OF THE INTENDED RECIPIENT AND MAY CONTAIN CONFIDENTIAL AND/OR PRIVILEGED INFORMATION.  ANY UNAUTHORIZED REVIEW, USE, DISCLOSURE OR DISTRIBUTION IS PROHIBITED.  IF YOU ARE NOT THE INTENDED RECIPIENT, CONTACT THE SENDER BY E-MAIL AT email@hidden AND DESTROY ALL COPIES OF THE ORIGINAL MESSAGE. WITHOUT PREJUDICE UCC1-207. ------------------------------------------------------------ ____________________________________________________________________________________ Moody friends. Drama queens. Your life? Nope! - their life, your story. Play Sims Stories at Yahoo! Games. Attachment:</body>
  </mail>
  <mail>
    <header>Re: Extracting a pixel's color from a  CIImage</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Memory Leak with CIContext?? (Still Broken)</header>
    <body>Re: Memory Leak with CIContext?? (Still Broken) On Dec 19, 2006, at 14:49, Mark Coniglio</body>
  </mail>
  <mail>
    <header>Re: CoreImage rendering intent - no word about it on the docs ?</header>
    <body>I'm very sorry I don't agree with that. I've made a test with a linear gradient in Photoshop, in working color space sRGB. Linear gradient here means linear in the sense of sRGB, not linear to light intensity. I've made a compare function in a kernel from 0.5, and the outputs give that CoreImage will first gamma decompensate the input image to map it to a a color space linear to light intensity. Common RGB working spaces like sRGB and aRGB are not linear transforms of XYZ, it is just a part of the encoding defined in their relative specs. The encoding factor and precision problem is determined by what the eye would see as a loss of precision (obviously). The gamma factor is also a mean to represent something that is somehow &amp;quot;more linear&amp;quot; to the eye.</body>
  </mail>
  <mail>
    <header>Re: CoreImage rendering intent - no word about it on the docs ?</header>
    <body>Well no, because as you've stated just before, the image will be converted to the equivalent space linear to light intensity before you arrive in a CI kernel. Therefore a 50% gray of generic gray space will give on r, g, b around 0.28 in the kernel. but a 68% gray would give 0.5 on r, g, b. A picture is always easier to figure things out : On entry an image which is a linear gradient of values with respect to aRGB (*not* linear to light intensity) the kernel will clip the values upper to 0.5. The clip does not occur at the middle of the image, but around 70%, so at least in this example (which is the default case described in the CoreImage documentation), the values are first linearized with respect to light intensity. But yet, this still does not reply to your original question, please test and let me know.</body>
  </mail>
  <mail>
    <header>Re: CoreImage rendering intent - no word about it on the docs ?</header>
    <body>...</body>
  </mail>
  <mail>
    <header>Re: CoreImage rendering intent - no word about it on the docs ?</header>
    <body>Hi Brendan, Raphael</body>
  </mail>
  <mail>
    <header>Re: CoreImage rendering intent - no word about it on the docs ?</header>
    <body>On Jul 16, 2007, at 3:36 AM, Dinge Raphael wrote: I'm not sure exactly how you're generating your image, but the discussion of gamma is beside the point.  CoreImage deals only with light linear (not gamma corrected) values and so it doesn't matter what the gamma of the RGB working space is.  Hence the RGB values you deal with inside a CoreImage filter are just a linear transformation from the XYZ values.</body>
  </mail>
  <mail>
    <header>Re: CoreImage rendering intent - no word about it on the docs ?</header>
    <body>Just a quick note on this. XYZ is a space that is linear with respect to light intensity, as opposed to aRGB and sRGB which are gamma compasensated, i.e. there is a mapping from linear energy to values using a gamma function. I've made some experiment on this, linearizing Adobe RGB and sRGB (that is the equivalent spaces, with same white point, black point and color matrix, but with no gamma curve), to found that the ICC engine from Apple does not map well a linear space to a gamma compensated space. It includes a very rough linear approximation in the 0 - 20 range in 8 bit color components. You can see it there : So first you'll have to check that every thing is ok with XYZ, since Apple CMM is not likely to do explicitely the input -&amp;gt; XYZ -&amp;gt; output transformation when mapping from 2 already gamma compensated spaces. About your problem, the best answer might be to run an unit test. You simply put an image with ramps on the components color, and see what it gives.</body>
  </mail>
  <mail>
    <header>vnc - screen capturing</header>
    <body>Hi, I created a program which captures the screen image.&amp;nbsp; I tried by both by using OpenGL and Quartz and my program, works fine unless I switch the user via fast user switching or switch to the login screen.&amp;nbsp; The problem is that if, say, I use Quartz by making use of , the function returns NULL instead of the address of the display buffer. Somehow VNC manages to have the screen captured even in such cases.&amp;nbsp; Does somebody know how they accomplish this?&amp;nbsp; Is it by making use of virtual framebuffers ? And if so, how please? Thanks, Josianne</body>
  </mail>
  <mail>
    <header>Re: Same image but different looks</header>
    <body>Switching off color management is not advised(!). I'm not even sure you could on a Mac. Best suggestion is to make sure that either all the images you use are tagged, or all your images are not tagged. That will at least achieve consistency on an application by application basis, although it won't between applications.</body>
  </mail>
  <mail>
    <header>Re: Same image but different looks</header>
    <body>Ok, thanks for clearing that up. In my application I've the same problem: the two images look different. How should I fix this? Should I turn color management off? How would I do that? My application is written using Carbon and optimized for portability between Windows/Mac/Linux and some others. Thus I'm storing all image data as raw 32-bit ARGB pixel arrays (32-bit per pixel). Because my application is multi-platform and might use lots of BMPs without these sRGB color profiles I'm wondering what I can do so that the look under Mac OS is identical to the look under Windows &amp;amp; Linux. Thanks Andreas --</body>
  </mail>
  <mail>
    <header>Re: Same image but different looks</header>
    <body>Sandy</body>
  </mail>
  <mail>
    <header>Same image but different looks</header>
    <body>Hi, I've some trouble with the colors of images under Mac OS X. I've used Windows to save an image two times: First, as a PNG picture, and then as a BMP picture. Under Mac OS X, only the PNG picture has the correct colors, the BMP picture is a little too bright. What's really confusing is that under Windows both pictures look exactly the same... as they contain exactly the same RGB pixel data this is how it should be. So why is it that when viewed with Preview under Mac OS X the BMP picture suddenly appears brighter than it should be? You can easily try it for yourself: This is how it should look: And this is how the same image looks as a BMP (too bright!): x2.png and x1.bmp have exactly the same pixel data but to my great confusion only x2.png looks correctly under Mac OS X... can someone explain this behaviour to me? Thanks! Andreas --</body>
  </mail>
  <mail>
    <header>Problem drawing over parent view with transparent background</header>
    <body>I have an app which is a floating window which is an analog of a mechanical clock face.  All the drawing is done using CGContext functions. The window has the content view set to my subclass of NSView as the clock face, which has subviews for each of hour hand, minute hand and second hand.  That is, hour hand is a subview of clock face, minute hand is a subview of hour hand, and second hand is a subview of minute hand. My problem is that, if the bottom view has a transparent colour set by the user, then when the hands move on, a gap appears in the clock face where the second hand was. The screen shot was made five seconds after resizing the window.  The resize causes the clock to reset the time and to redisplay. To update the hands, a timer callback simply tells one of the views to setNeedsDisplay:YES every second. If I call on any of the views to display singly or as a group the problem stays the same.  At first I was simply calling the second hand to update which causes all the other views to redraw, so I changed that to call the bottom view, the clock face.  But that didn't fix the problem. The clock face background colour drawing takes place at the beginning of it's -drawRect:, which it must do so that I can draw the pip marks and numerals on top of it. I've tried calling on the various types of redisplay such as -display, -displayIfNeeded, -displayIfNeededIgnoringOpacity, but have had no joy yet.</body>
  </mail>
  <mail>
    <header>Re: filling a path that has sub paths</header>
    <body>If they're in the same direction, and you use an even-odd fill, then they still don't draw as independent paths, as illustrated. Anyway, if you have to think about the direction and the fill rule, then the statement about &amp;quot;fills each subpath independently&amp;quot; seems like an oversimplification, to be charitable. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: filling a path that has sub paths</header>
    <body>When the second path is expressed in the opposite direction to the first path, it will cut holes in the first path, as illustrated. But, when the subpaths are in the same direction, they will draw like independent paths.&amp;nbsp; You would not notice much when opacity is 100%, but at 50% the areas where the subpaths overlap would be 100%.</body>
  </mail>
  <mail>
    <header>Re: How to set the video input programmatically?</header>
    <body>Make your QTVisualContext with QTOpenGLTextureContextCreate and pass in the same CGLContext you created with your QCRenderer. This will assure that your CVBuffers out of the QTCaptureVideoPreviewOutput are 'shareable' with your QCRenderer / GL context for speed. Make a QTCaptureVideoPreviewOutput and attach it to a running session, and set its visualContext to the above created QTOpenGLTextureContext. You can now get frames out of your QTOpenGLTextureContext via QTVisualContextCopyImageForTime, and pass those to QC via input keys. Hope that helps!</body>
  </mail>
  <mail>
    <header>Frustrating layer-backed geometry issue</header>
    <body>I need to make a view that can zoom in on its subviews. This works well on the Cocoa side, I just change the view's bounds to have a different size than its frame. But as soon as I turn on layer backing, the sublayer drawing breaks. If the parent view or a particular subview has layer backing, it is no longer drawn zoomed. I've made a sample project to show what I'm talking about: If you try it as downloaded, you will get a large red box and an enlarged button in the middle of the view. The check button works when you click it. Then uncomment the [self setWantsLayer:YES] line in -[TestView awakeFromNib]. The subviews are no longer drawn at double size, even though the check box is still receiving events in the bounds where it would be drawing without layer backing. The same wrong sized drawing effect can be triggered on an individual subview by setting it alone to be layer backed. I can't get my head wrapped around what could be going on here. Is this a bug, or is it something I should expect based on how Core Animation layer-backing is supposed to work? How do I get this working as intended? thanks, -natevw</body>
  </mail>
  <mail>
    <header>filling a path that has subpaths</header>
    <body>I must be misunderstanding something, because the Quartz 2D Programming Guide seems to contradict itself.  The first sentence under &amp;quot;Filling a Path&amp;quot; is &amp;quot;When you fill the current path, Quartz fills each subpath independently.&amp;quot;.  A few paragraphs later, it illustrates the difference between the even-odd rule and the winding number rule using a path consisting of two concentric circles.  If each subpath were really filled independently, then the inner circle would always be filled, yet it isn't in 3 out of 4 cases.  So, what does that sentence mean?</body>
  </mail>
  <mail>
    <header>Re: How to set the video input programmatically?</header>
    <body>On 21/08/2009, at 4:09 AM, vade wrote: Hi Vade, Thanks for the reply, Can you possibly be a little more descriptive of this process in creating and sharing this 'visual context'. Anything that could possibly save me another 8 weeks of googling every possible combination of your words and potentially coming up with results that lead me around in circles would be most appreciated...  :) Im not exactly asking you to spell it out programatically for me, I kind of am actually, but a few more hints would be really really helpful.</body>
  </mail>
  <mail>
    <header>Drawing PDF while printing</header>
    <body>I encountered a crash when printing a document, imaged with quartz. The print crash occurs in PMSessionEndPage, which made me suspect that the pdf data was being deleted too soon. When printing, I noticed that the DataReleaseProc was being called as soon as I called CGPDFDocumentRelease. I thought that the Draw would retain the document, and my release would just decrement the retain count, and the final release would occur when the page ended. When I skip the call to CGPDFDocumentRelease, the print crash is avoided. This code works fine with the same PDF in OS10.4 on a PPC.&amp;nbsp; Observed crashes in 10.5.6, 10.5.7, 10.5.8, on Intel. static void DataReleaseProc (void* infoPtr, const void* UNUSED(data), size_t UNUSED(size)) &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; If (infoPtr) &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; delete void DrawPDF(void* pdfDataPtr, Uint32 dataSize, Uint32 pageNumber) &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; char* &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; if &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; memcpy(pData, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; CGDataProviderRef &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; if &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; document &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; if &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; size_t &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; if &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; CGPDFPageRef &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; CGContextDrawPDFPage(fQuartzContext, &lt;span &lt;span Crash callstack</body>
  </mail>
  <mail>
    <header>Re: How to set the video input programmatically?</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: How to set the video input programmatically?</header>
    <body>// its doable.</body>
  </mail>
  <mail>
    <header>Re: Why is so much extra memory used after you set a layer's 	contents?</header>
    <body>David, I've done some further experimentation and discovered that the bitmap context is not to blame; the problem appears to be with CALayer. The following is the output of a simple test app that simply shows the free memory (and the delta from the previous free memory) from within -viewDidLoad of the view controller created by the &amp;quot;view-based application&amp;quot; template: start 22089728 (22089728) iteration -10 22089728 (0) iteration -9 22110208 (20480) iteration -8 22110208 (0) iteration -7 22110208 (0) iteration -6 22110208 (0) iteration -5 22110208 (0) iteration -4 22110208 (0) iteration -3 22110208 (0) iteration -2 22110208 (0) iteration -1 22110208 (0) I guess the 20480 bytes is something to do with the app's startup cost and is reclaimed after a couple of iterations of the run loop. The code is as follows: @implementation LayerMemoryTestViewController - (void)dumpFree:(NSString *)title if (host_statistics(host_port, HOST_VM_INFO, (host_info_t)&amp;amp;vm_stat, &amp;amp;host_size) != KERN_SUCCESS) fprintf(stderr, [[NSString stringWithFormat:@&amp;quot;%@ %d (%d)\n&amp;quot;, - (void)dumpFreeForRunLoopIterations:(NSNumber *)iterations if (remainingIterations &amp;gt; 0) [self dumpFree:[NSString stringWithFormat:@&amp;quot;iteration -%d&amp;quot;, [self performSelector:@selector(dumpFreeForRunLoopIterations:) withObject:[NSNumber numberWithInteger:remainingIterations - 1] - (void)viewDidLoad If we create a CGImage from a PNG data provider and then release it straight away (using a 1024*1024*4 image saved uncompressed to a PNG file of 4202193 bytes): - (void)viewDidLoad CFURLRef url = (CFURLRef)[NSURL fileURLWithPath:[[NSBundle CGImageRef image = CGImageCreateWithPNGDataProvider(imageDataProvider, NULL, false, We pretty much break even (I've trimmed all but one of the trailing zero-delta lines, for brevity): start 16818176 (16818176) iteration -10 16928768 (110592) iteration -9 16949248 (20480) iteration -8 16949248 (0) ... Sometimes, instead of reclaiming around 100000 bytes, we lose about as many; I guess the file operations introduce a little variance. If we remove the CGImageRelease(), we leak as much memory as one would expect: start 17457152 (17457152) iteration -10 13189120 (-4268032) iteration -9 13209600 (20480) iteration -8 13209600 (0) ... If we create a layer and set its contents to the image, but allow the layer to be autoreleased: - (void)viewDidLoad CFURLRef url = (CFURLRef)[NSURL fileURLWithPath:[[NSBundle CGImageRef image = CGImageCreateWithPNGDataProvider(imageDataProvider, NULL, false, layer.frame = CGRectMake(0, 0, CGImageGetWidth(image), Memory is reclaimed, as you would expect: start 16711680 (16711680) iteration -10 13565952 (-3145728) iteration -9 16842752 (3276800) iteration -8 16842752 (0) ... HOWEVER! If we add the layer to our view's layer, so that it is displayed: (just adding &amp;quot;[self.view.layer addSublayer:layer];&amp;quot; after setting layer.contents) start 17764352 (17764352) iteration -10 12947456 (-4816896) iteration -9 8806400 (-4141056) iteration -8 8806400 (0) ... We clearly incur the memory cost of another copy of the image. We can verify this by using an image of different size, e.g. a 512*512*4 image of size 1050865 bytes: start 19132416 (19132416) iteration -10 17948672 (-1183744) iteration -9 16953344 (-995328) iteration -8 16953344 (0) ... Is it possible to avoid this extra copy? Thanks, Hamish</body>
  </mail>
  <mail>
    <header>Re: Why is so much extra memory used after you set a layer's 	contents?</header>
    <body>I should mention that I used +imageWithContentsOfFile: rather than +imageNamed: when using the CGImage directly as the contents of the layer, i.e. applied the technique from part 2 to part 1 to better control the variables. And I meant &amp;quot;but only once the layer is displayed&amp;quot; rather than &amp;quot;but only after the layer is displayed&amp;quot;! H</body>
  </mail>
  <mail>
    <header>Re: Why is so much extra memory used after you set a layer's 	contents?</header>
    <body>Hi David, Thanks for your reply. If I match the dimensions of the PNG file to the rect (i.e. make sure the contents end up the same size as they would if I drew to a bitmap context), the &amp;quot;after&amp;quot; delta is negligible, but the &amp;quot;post-after&amp;quot; delta is the same order of magnitude as I'd expect for the memory required (e.g., 1024*1024*4 bytes). That seems pretty reasonable -- the PNG isn't rendered to the layer's backing store straight away, but only after the layer is displayed. So this would seem to suggest the bitmap context is to blame for the extra memory required? Should I be doing something in addition to CGContextRelease? Similar for the &amp;quot;after&amp;quot; delta, but the &amp;quot;post-after&amp;quot; delta is reduced. Again, seems reasonable because imageNamed: caches the PNG data, right? Thanks, Hamish</body>
  </mail>
  <mail>
    <header>Re: Why is so much extra memory used after you set a layer's	contents?</header>
    <body>Is the memory usage similar if you either 1) Use the CGImage directly as the contents of the layer (rather than drawing it to a bitmap context first) or 2) Use +imageWithContentsOfFile: instead of +imageNamed:? -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Core Video -  getting square pixels out of a QTVisualContext</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Why is so much extra memory used after you set a layer's contents?</header>
    <body>Hi, I wrote some code to test available memory (using mach_host's host_statistics; code at end of email) before and after creating a bitmap from a bitmap context and assigning it to a layer's contents, and then again after one iteration of the run loop. Output typically looks like the following (e.g. a 1024*1024*4 image): 2009-08-17 23:55:47.294 LayerMemoryTest[1379:207] before 10276864 (10276864) 2009-08-17 23:55:48.148 LayerMemoryTest[1379:207] after 5242880 (-5033984) 2009-08-17 23:55:48.328 LayerMemoryTest[1379:207] post-after 1687552 (-3555328) The spin of the run loop causes extra memory to be allocated, which seems to increase with the size of the bitmap but not be proportional to it; for example, a 512*512*4 image produces something like: 2009-08-18 00:03:36.047 LayerMemoryTest[1472:207] before 4800512 (4800512) 2009-08-18 00:03:36.251 LayerMemoryTest[1472:207] after 2842624 (-1957888) 2009-08-18 00:03:36.331 LayerMemoryTest[1472:207] post-after 1867776 (-974848) Whereas as 768*768*4 image produces something like: 2009-08-18 00:08:14.119 LayerMemoryTest[1521:207] before 7925760 (7925760) 2009-08-18 00:08:14.723 LayerMemoryTest[1521:207] after 4980736 (-2945024) 2009-08-18 00:08:14.806 LayerMemoryTest[1521:207] post-after 2387968 (-2592768) Why is this extra memory used, and is it unavoidable? Thanks, Hamish @implementation LayerMemoryTestViewController - (void)dumpFree:(NSString *)title if (host_statistics(host_port, HOST_VM_INFO, (host_info_t)&amp;amp;vm_stat, &amp;amp;host_size) != KERN_SUCCESS) - (void)viewDidLoad CGContextRef context = CGBitmapContextCreate(NULL, rect.size.width, rect.size.height, 8, 4 * rect.size.width, colorSpace, [self performSelector:@selector(dumpFree:) @end</body>
  </mail>
  <mail>
    <header>Re: How to set the video input programmatically?</header>
    <body>Unfortunately this is currently the case with the Video Input patch in QC, the video input patch does not let you programatically get the available inputs and set them (only via the settings UI). Your best and simplest bet is to use a QTCaptureSession, publish an input key for your QC patch in lieu of the Video Input patch, and pass in a CVOpenGLBufferRef for the image value. It seems like a lot of work but its doable. for details.</body>
  </mail>
  <mail>
    <header>How to set the video input programmatically?</header>
    <body>My app, Conjure, uses a number of Quartz Compositions. I am using video capture input for one of them, but I noticed there is no (that I can find) way to set the video source without using Quartz Composer. Is that the case? Is there a way to set the video input from a QCView? Thanks! -Chilton</body>
  </mail>
  <mail>
    <header>Core Video -  getting square pixels out of a QTVisualContext</header>
    <body>Hello I am attempting to write an alternate movie player plugin for Quartz Composer with some additional features etc. Quartz Composer assumes square pixels will be provided either as a texture or in a pixel buffer (at least, this is true as far as Ive read in the docs, ive seen no indication otherwise). I am implementing an QCPluginOutputImageProvider within my plugin, and need to provide square pixels from a QTVisualContext. It seems like only the QTMovieApertureModeEncodedPixels mode provides me with a square pixel CVImageBuffer that I can confidently make a square pixel texture out of. I have attempted two paths so far, one using CVOpenGLTextureRefs from a QTOpenGLTextureContext, and another using a QTPixelBufferContext with CVPixelBuffer refs, both using QTVisualContextCopyImageForTime. If I use aperture modes other than encoded pixels, my output image is only drawn correctly if it is sent to a consumer (destination geometry) patch. If I send it to an intermediate effect like a Core Image Kernel, it will output an image at the proper aspect ratio that I desire, but the content of the image will be squished to one side with a black area filling the rest. The image content (the squished content) lines up perfectly with the raw encoded pixels aspect ratio. For example, if I load an HDV file that is 1440x1080 raw (4x3), 1920x1080 (16x9) production aperture, when I output from my plugin to a billboard I see the proper full image at 16x9. if I render to the default custom core image effect in Quartz Composer and then to the billboard, I see a 4x3 image on the right hand side with a black area on the left that in fills up the rest of the 16x9 image. Is there a way to request textures from a QTOpenGLTextureContext to be square pixels, 1:1 aspect ratio regardless of the QTMovies current aperture mode? I know I can render to an intermediate FBO or something equivalent, but this is an additional render stage and can be quite slow. Id love to request and get square pixels right from Core Video. Is this possible? I hope ive made my issue clear, and I think this is more suited to the Quartz list than the Quartz Composer/Quicktime list since its a bit Core Video centric. Forgive me if ive missed something obvious in the docs somewhere, and thanks!</body>
  </mail>
  <mail>
    <header>Fwd: Anyone had problems with multiple CATiledLayers crashing with 	threads</header>
    <body>I&amp;#39;ve got a application that is making extensive use of CATiledLayer-derived layer-backed-views and have run into a problem recently when updating the contents of these. The scenario is that I have a main display window with a CATiledLayer layer-backed-view in it which has an observer set to a color well in an auxiliary window. ¬†In the auxiliary window, I have another CATiledLayer backed-view (it&amp;#39;s a subclass of the view used in the main window) that also observes the same color well. Under 10.5.7, clicking in the color panel (brought up by clicking on the well) will cause an immediate and effective change of both views that are observing the color. ¬† I&amp;#39;ve put logging in to see how the views are drawing, and they&amp;#39;re each being drawn to their own context (which is the same from invocation to invocation) and each in its own thread. Under 10.5.7, clicking and vigorously dragging in the color panel above updates both the views quickly during the drag, but will eventually (sometimes after a second, sometimes after 10-20 seconds of dragging) crash one of the two threads (usually the main window, as it takes longer to finish drawing) in a CGContext graphics call. ¬† The stack traces always include ripc_ calls under the CGContextDrawPath (for example) call. It&amp;#39;s pretty clear from the backtraces and running the code with CFZombies turned on that by the time the CGContext graphics call is made, some of the elements of the CGContext have been freed. ¬† However, I&amp;#39;ve spent about two and a half days trying to track this down so far, to no avail. ¬† Often the problem involved getting the stroke color, so I made all references to the colors create and explicit copy (leaking like a sieve), but the same errors occurred and if I execute the internal CGContext routine CGContextGetStrokeColor, it was invalid during the crash, even though it had just been set to a copy that was still valid (confirmed both through &amp;#39;po&amp;#39; and by an additional CFRetain in the code). I&amp;#39;m at a bit of a loss, and the problem doesn&amp;#39;t happen under 10.6 (although updating is much, much slower under 10.6, which is also curious). ¬† One thought we&amp;#39;ve had here is that it might be some additional locking under 10.6 that is saving us, but we can&amp;#39;t even figure out how to emulate that without having to add a specific coordination layer between the two similar views. Any thoughts or problems of this type seen before on quick-changing CATiledLayer backed views? Other things I&amp;#39;ve looked at: - the observers are all being invoked on the main thread - Color doesn&amp;#39;t seem to be the only important property (sometimes it&amp;#39;ll crash on linemode or some similar method) - We are setting color using the CGContextSetXXColor( cs, rgba[] ) calls, although we&amp;#39;ve also tried using the SetXXXColorFromColor calls as well (complete with making copies every time and verifying the CGColor validity before the calls) and it doesn&amp;#39;t make a difference - Hiding the auxiliary view causes the problem to disappear - We&amp;#39;ve confirmed that we&amp;#39;re using threadsafe color setters and getters (using @synchronize(self) in the block around reading and writing) Thanks, -Gaige</body>
  </mail>
  <mail>
    <header>Re: problem with 16bits data</header>
    <body>it works with&amp;nbsp;kCGBitmapByteOrder16Little.</body>
  </mail>
  <mail>
    <header>Re: How to get &amp;quot;pixel zoom&amp;quot; / nearest neighbor interpolation	in	CoreImage?</header>
    <body>you can use the CIPixellate filter with scale=1.0 to create a little '1x1 square' for each sampling point in the working coordinate space, then append an affine transform to scale this to whichever zoom level you'd like. Hope this helps, - Ralph</body>
  </mail>
  <mail>
    <header>Re: How to get &amp;quot;pixel zoom&amp;quot; / nearest neighbor interpolation in	CoreImage?</header>
    <body>Well, here a guess, or at least the place I'd be suspicious of first - the drawImage inRect: fromRect:. I'm not sure that using inRect fromRect with fromRect the the same size as the original image will result in your kernel seeing a request for interpolation; it may just be seeing 1:1 pixel mapping. All my CI image code just uses drawImage fromRect: with fromRect set to the final interpolated size I want.</body>
  </mail>
  <mail>
    <header>How to get &amp;quot;pixel zoom&amp;quot; / nearest neighbor interpolation in	CoreImage?</header>
    <body>I'm trying to get a CI image to be &amp;quot;pixel zoomed&amp;quot;, similar to what image editors do when you zoom in on the image (nearest neighbor interpolation, or kCGInterpolationNone in CG land) . Reading the docs, it seems like I'd want to create a CIFilter which sets up the sampler with the filter mode set to kCISamplerFilterNearest.  However, I can't seem to get what I want. @implementation TSNNFilter return sample(src, samplerCoord(src));\n\ CISampler *sampler = [CISampler samplerWithImage:inputImage CIImage *out = [self apply:TSNNFilterKernel arguments:[NSArray arrayWithObject:sampler] And here is how I use it in my NSView subclass: [ctx drawImage:[f valueForKey:@&amp;quot;outputImage&amp;quot;] inRect:r fromRect: Is what I'm doing correct? (probably not, since I seem to be getting the default interpolation).  And if not, what do I need to do? August 'Gus' Mueller Flying Meat Inc.</body>
  </mail>
  <mail>
    <header>Re: simple CG question about brushes</header>
    <body>-drawRect: does not accumulate drawing, it starts from a fresh canvas every time (and no, you can't change this at the UIView level). You need to maintain your list of drawning positions and redraw all of them every time. PS: This would be much more efficient if you loaded your bush outside of -drawRect: and reused it. You can use UIImage to load the image and get a CGImageRef from that if you need to. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>simple CG question about brushes</header>
    <body>Hello, all ... I'm trying to draw where the user moves their finger, leaving a trail ... but the brush i'm drawing with (a PNG with a transparent background) is erasing parts of the trail drawn before, making for a very ugly fingerpaint program :-(  How can I stop this from happening?  My drawRect follows ... any help is appreciated! - (void)drawRect:(CGRect)rect NSString* imageFileName = [[[NSBundle mainBundle] resourcePath] CGDataProviderRef provider = CGImageRef image = CGImageCreateWithPNGDataProvider(provider, Regards, Falling You - exploring the beauty of voice and sound</body>
  </mail>
  <mail>
    <header>Re: How can I modify a context's clip rect independently of its 	transform?</header>
    <body>And therein lay my problem; I was creating the context with a smaller bitmap than I thought I was. Sorry to have wasted your bandwidth! Best wishes, Hamish</body>
  </mail>
  <mail>
    <header>Re: How can I modify a context's clip rect independently of its 	transform?</header>
    <body>Please excuse me a self-reply to re-work my question, as I don't think it was very clearly stated. Hopefully this will improve comprehensibility. With the context passed to a CATiledLayer's delegate's drawLayer:inContext:, each time the CTM's scale is doubled, the clip rect's width and height are halved. For instance, these are the clip rects / CTMs passed to drawLayer:inContext: for a CATiledLayer of tileSize (512, 512) when zooming into the top left hand corner of a UIScrollView: // initial four tiles, no zoom (scale of 2 to allow for zoom up to zoomScale of 2) // once zoomScale of 2 is reached, redraw with scale of 4 to allow for zoom up to 8 // etc. However, if I arrange for the same CTM to be passed for a bitmap context of size (512, 512) I want to render into, the clip rect's height and width reduce in inverse proportion to the scale: // works fine with scale of 2 I don't want to use a larger bitmap, I simply want a smaller area to be rendered at full detail into the same size of bitmap. How can I achieve this? Thanks, Hamish</body>
  </mail>
  <mail>
    <header>How can I modify a context's clip rect independently of its 	transform?</header>
    <body>Hi, I want to generate bitmap tiles from a layer by repeatedly calling drawLayer:inContext: with the context's clip rect and transform determining what's drawn, similar to the way CATiledLayer operates. A typical example of CATiledLayer's calls into drawLayer:inContext: might yield the following context clip rect and CTM: If I start with a blank canvas - bitmap context with default clip rect and no transform - I would see something like the following: If I translate the CTM to get the clip rect where I want it, I get: But if I scale the CTM to get the transform I want, I get: How can I modify the clip rect independently of the transform, or otherwise arrive at the combination I'm looking for? Thanks, Hamish</body>
  </mail>
  <mail>
    <header>Re: drawing with a pattern?</header>
    <body>On Aug 31, 2007, at 12:57 PM, Bill Dudney wrote: I'm not on the Quartz team but I haven't had any problems using patterns as the stroke color. Nick</body>
  </mail>
  <mail>
    <header>RE: drawing with a pattern?</header>
    <body>So i've done a bit more digging and I'm wondering if this is just a bad idea. I read something in the archives (back from 2004 so prob out dated) about how its really hard to get 'good results' with pattern based stroking of lines. Could someone from the quartz team comment? Am I nuts for wanting to use pattern based drawing to stroke a line? Thanks in advance. -bd</body>
  </mail>
  <mail>
    <header>Re: Executable vs nonexecutable CIFilter</header>
    <body>Unless your GPU is too slow, or the filter chain you build is too complex for it, then as long as you entire filter chain exists as filters that don't require an external executable, then it should run on the GPU. Ralph already confirmed that all built-in CI filters are non- executable, so in most cases they should be running on the GPU. If you don't want those that do require an executable to be part of your filter chain, don't include them. Finally, there is no such thing as a GPU-only filter, it's determined at run time if the particular filter chain that you created will run on the GPU or the CPU. If your user has a graphics card with programmable hardware, then CI Filter chains will usually run on the GPU unless you exceed some GPU limit (I believe the only exception is the GeForce 5200). See Technical Q&amp;amp;A QA1416 Specifiying if the CPU or the GPU should be used for rendering. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Basic image processing</header>
    <body>On Aug 31, 2007, at 5:07 AM, Michael Kluev wrote: Crop you can do with a clipping path or by creating a sub-image of a CGImage. Scaling you can do by drawing into a CGBitmapContext and creating an image from the result. Adjusting the color components is probably best done with Core Image or QuickTime. (or in Leopard with the ImageKit).  I would guess that the QuickTime mechanism is quite likely to be tied to QuickDraw and a legacy architecture.  QuickTime does not have an ingrained color management strategy (Core Image does), and QuickTime seems less likely to make use of GPU resources.  For it's part, Core Image is Tiger and later so if you have to support older systems... QuickTime is the only way to go. Scott</body>
  </mail>
  <mail>
    <header>Basic image processing</header>
    <body>I need to do some basic image processing on images in memory (typically RGB 32 bits): Crop, scale proportionally and non proportionally, Rotate by +-90 and 180 degrees, flip vertically and horizontally, Adjust brightness, contrast, lightness, hue, saturation. I can do this with CoreImage filters (namely CICrop, CIAffineTransform, CIColorControls, CIHueAdjust). I also can do this with QT filters (namely kBrightnessContrastImageFilterType, kHSLColorBalanceImageFilterType and while I didn't find specific built-in filter to do geometric transforms I can do this passing a needed transform matrix to DecompressSequence). Given that Apple is good at providing duplicating technologies and reinventing wheels, what are my other options? Can I do the task by other means (e.g. directly with Core Graphics)? I don't need advanced processing, just basics listed above. Mike</body>
  </mail>
  <mail>
    <header>CoreImage Filters vs QT Filters</header>
    <body>I've tried both CoreImage filters and QT Filters/Effects and the two are damned too similar. Looks like Apple reinvented the wheel and provided us with duplicated technology. With no clear guideline what to use when. Or is there such a guideline? What are the key benefits of using CoreImage filters than QT filters? What are the key benefits of using QT filters than CoreImage filters? I'm looking at the list like: - CoreImage filters is newer technology - QuickTime filters is cross platform technology - CoreImage filters can only work with RGB format - QuickTime filters use C API - CoreImage filters are GPU accelerated while QT filters are .... Mike</body>
  </mail>
  <mail>
    <header>Re: Executable vs nonexecutable CIFilter</header>
    <body>On Thu, 23 Aug 2007 22:09:49 -0800 Correct me if this is wrong: there is no way to disable CPU execution path of built-in renderers, there is no way to disable those CPU-only built-in renderers completely, and there is no way to identify GPU-only filters. Mike</body>
  </mail>
  <mail>
    <header>Re: CI bug on simple flip filter?</header>
    <body>You can also just ask the image for its extent and depending on if you need more then width and height convert the rect or the size into a CIVector.</body>
  </mail>
  <mail>
    <header>Re: CI bug on simple flip filter?</header>
    <body>Still, would you be so kind to explain me why vertical flipping works fine when flipping both, but fails (without the abs(...)) for the vertical only flipping. I'm trying to understand what is going on behind the scenes. Frank, you mean passing the sampler two variables with the width and height of the image. Would it be save to pass these variables from the sampler extend, something like: - (CIImage *)outputImage Thanks Mark On 30.08.2007, at 02:06, Frank Doepke wrote:</body>
  </mail>
  <mail>
    <header>Re: CI bug on simple flip filter?</header>
    <body>In general I would advise to pass in the size of the image rather than relying on sampler size. There are cases where samplerSize might not return what you expect the image size to be.</body>
  </mail>
  <mail>
    <header>Re: What does CATransaction really do?</header>
    <body>On Sep 11, 2009, at 11:32 PM, Glen Low wrote: No, transactions can't fail or abort. They're not isolated across threads ‚Äî if multiple threads modify layers at roughly the same time, each thread sees all the uncommitted changes and the order in which the changes appear on the screen is undefined. (Although we try not to flush changes from other threads when pushing changes to the render thread, because we only have one set of data structures representing the layer tree that all threads are writing into, that's often not possible.) Transactions are only atomic in the sense that if a single thread is making changes you won't see intermediate states appear on the screen.</body>
  </mail>
  <mail>
    <header>Crash in QCRenderer's setValue:forInputKey:</header>
    <body>We've got a report of the following crash which we've yet been unable to reproduce. The value is an NSImage produced from -[NSWorkspace iconForFileType:] or +[NSImage imageNamed:@&amp;quot;NSApplicationIcon&amp;quot;] We think it may be a Snow Leopard bug as it appears that blocks are being used. Any hints as to what's happening? Thanks, Kevin</body>
  </mail>
  <mail>
    <header>Re: CIFilter Questions</header>
    <body>You can examine the object and get all the keys for it. I don't recall how but you have all the info you need to create a UI for each filter. Look at how FunHouse does it. If you build a filter stack, or tree, then you need to re-build it every time any of the inputs is changed since the output image that you would want to link to the input of the next filter would now be a different object. CoreImage will start with the destination and apply the filters needed to produce that pixel. This is why ROI and DOD are important so that ONLY the needed filters are applied and only the needed pixels are processed. NO. You can only use (expr) ? true result : false result. The image extents are in a global coord space. One could be a 10x10 rect origin 100,100, and the other could be a 1024x0124 rect, origin -24,-24. What pixels are use depends on what you render in your window to the screen. If you only draw a 100x100 rect origin 0,0, then the smaller image would not even be touched. They are an optimization. DOD you probably don't need to touch. ROI you ONLY need to touch if you work on pixels from the inputs that are not the current pixel, that is if you sample(destcoord() + vec2(1,1) then your ROI is the passed in dest rect + 1 pixel on each edge. When the layer changes size, you will probably need to apply the filter again using the new size. You will also need to tell the layer to re-display.</body>
  </mail>
  <mail>
    <header>Re: CIFilter Questions (Gordon Apple)</header>
    <body>Hi Gordon, I'm a CIFilter newbie as well and I've recently encountered some of these issues. I'll do my best to answer and perhaps more experienced folks can chip in where need be... Yep, you set the value for the input image. Next you just call the equivalent to get the output image. Usually the key names for these are standard, e.g. I think the filters are applied as you call them in your code. However internally this might be different. There's a section on this in the Core Image Programming Guide. See also the Image Unit Tutorial. You can only use the ternary operator (?) and work-arounds using multiplication etc. I've seen some posts on this with more detail if you have a look on Google. -- you use samplerCoord and sampleTransform for this. See the Core Image Kernel Language Reference. If your doing the filter in Quartz Composer I think they need to be equal. Exactly. Sorry I haven't played round with CALayer. Maybe someone else could help with this? A couple of general pointers. It's worth reading and re-reading again all the intro sections of the manuals mentioned above. They do cover a lot of this stuff. Similarly, there's an amazing amount of sample code out there. Have a search on your XCode documentation and you'll see it. I've found sample code for just about everything I need to do. Sometimes it is hard to find though. The  site is also excellent. Have you got into Quartz Composer yet? It's an excellent way to play round with filters before getting into code. You can then either convert the kernel to your Image Unit, or call your QC composition from the code direct. A great way to tweak things as you debug. Hope this helps! Cheers, Max 2009/9/12  &amp;lt;email@hidden&amp;gt;:</body>
  </mail>
  <mail>
    <header>Weird issue creating and placing CALayer sublcass</header>
    <body>tests to manipulate this layer after creating it in this way. The position animates, but the layer always keeps the chopped off corner.</body>
  </mail>
  <mail>
    <header>CIFilter Questions</header>
    <body>I've been fighting the custom filter battle for over a week now, and having only limited success.  I still have not been able to get a custom filter unit to load.  Having given up on that approach, I've gone to the embedded filter approach.  I have managed to get the custom CIChromaFilter to work with a QTCaptureView.  Now, I'm trying to get it to work with a QTCaptureLayer.  However, I have a lot of questions.  The available documentation and sample code are much appreciated, but they have some gaping holes. 1.  Although output from a filter seems to be adequately and obviously defined, input is not.  The &amp;quot;inputImage&amp;quot; key seems to be the required key for inputting a CIImage to a filter.  It this what a filter stack automatically assumes for each filter?  (Note that I'm referring to an array of filters, such as defined for a CALayer.  If you are just implementing filters in your own code, I suppose you could use any key for input.) 2.  When such a filter stack is applied, is each filter executed independently, using output from the previous filter, or does the sampling process and kernel processing reach all the way back through the stack for each output pixel? 3.  Are conditional statements (if, else) allowed in the kernel?  The language definition seems to indicate so, but the filter docs seem to indicate otherwise. 4.  When combining two images, do the samplers automatically map the CIImage extents to the same output space, or do I have to scale input images myself ahead of time to be the same size, or via the sampler transform option? 5.  Frankly, I'm still confused about DoD and RoI, even after reading the docs.  Seems to be mainly an optimization technique to limit required processing in the kernel? 6.  Going slightly afield into the Cocoa realm, applying any (standard) filter to a CALayer is somewhat problematic in that the filter effect does not scale to the layer size (bounds).  I'm not sure if this is due to the image size that the filter is processing, or due to the lack of scaling of linear filter parameters to match image size.  Any thoughts?</body>
  </mail>
  <mail>
    <header>Re: What does CATransaction really do?</header>
    <body>OK, thanks for clearing that up. At the risk of beating a dead horse, my idea of transactions comes from database theory, and some of the Xcode documentation seem to indicate this. Documentation: &amp;quot;CATransaction is the Core Animation mechanism for batching multiple layer-tree operations into atomic updates to the a.	Can a transaction in flight fail or be aborted? If so, will the layer state revert to what it was before (assuming the animation delegates do the right thing)? b.	Can a transaction be explicitly aborted? i.e. something like a [CATransaction rollback]? a.	What happens if a transaction is in flight, and I start another transaction? Will the second transaction happen sequentially after the first transaction, or will the first transaction abort and then the second transaction apply? Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>More issues with QCCompositionLayer in Snow Leopard</header>
    <body>First off, I recently wrote about timing issues here with QCCompositionLayer that was solved. The solution was posted to the list for SL, but I want to add that in order to get the timing issues to work on both SL and Leopard, you must override both drawInCGLContext:pixelFormat:forLayerTime:displayTime: and canDrawInCGLContext:pixelFormat:forLayerTime:displayTime:. Now on to my new question. We've noticed that a qtz file that we're rendering in a custom NSView via QCRenderer is no longer rendering identically in Snow Leopard. It works fine in Quartz Composer though. We've also switched over some of our use of this custom view to a custom QCCompositionLayer for improved animation support. So to investigate this issue I created a test project for the qtz and set up a stock QCCompositionLayer and a QCView side by side. The QCView ends up rendering it as expected, but the layer fails miserably. Here is the only code I am using (layerView and qcView are IBOutlets): QCCompositionLayer *qcLayer = [QCCompositionLayer qcLayer.autoresizingMask = kCALayerWidthSizable | 1. The layer doesn't render our image at all, yet the view does. 2. The layer doesn't render a radial gradient at all, yet the view does. 3. The layer doesn't render our test data (from within the qtz) at all IF the view is rendering as well. If the QCView is turned off, it does. I can't post a screenshot since the product is unreleased, but it's obvious that the layer just doesn't work while the view works fine. So in conclusion, QCRenderer in a custom view and a stock QCCompositionLayer both are not rendering correctly, yet QCView is. Is there something I should be doing differently? Or file a bug? I should add we haven't had any other problems with our other qtz files. However we do use a custom QCPlugIn (doesn't render, just handles data) for this qtz file. Thanks. Kevin</body>
  </mail>
  <mail>
    <header>Re: pixel quantization formulas</header>
    <body>Then you almost certainly want error diffusion dithering...</body>
  </mail>
  <mail>
    <header>Re: pixel quantization formulas</header>
    <body>Of course... I completely agree. However, my initial question remains. What would be better than rounding? Would the 2nd method provide even more precision? For my field of use rounding is barely sufficient. I wouldn't mind getting a more precise method even at the expense of a slower algorithm. - Luigi</body>
  </mail>
  <mail>
    <header>Re: pixel quantization formulas</header>
    <body>Have you actually benchmarked the difference ? (A lot of stuff is I/O bound now, given that processors have outrun their memory systems). Whether the error is minor is entirely up to the field of use. Some classes of users would consider rounding as insufficient too, expecting that it will be dithered into 8 bits instead (that's what something like Photoshop does by default). Graeme Gill.</body>
  </mail>
  <mail>
    <header>Re: pixel quantization formulas</header>
    <body>1/255 is a pretty minor error. Is there a reason I should slow down my code to avoid it? On Sep 11, 2009, at 12:14 AM, Graeme Gill wrote:</body>
  </mail>
  <mail>
    <header>Re: pixel quantization formulas</header>
    <body>The problem with this is that casting to int rounds down, so the maximum quantization error is 1, with an average of 0.5. on the other hand rounds to nearest, so it has a maximum quantization error of 0.5 and an average of 0.25, so it's twice as good! Graeme Gill.</body>
  </mail>
  <mail>
    <header>Re: pixel quantization formulas</header>
    <body>1.0 maps to 255, and 0 maps to 0. Is there some reason you're making it more complicated? On Sep 10, 2009, at 10:03 PM, Luigi Castelli wrote:</body>
  </mail>
  <mail>
    <header>pixel quantization formulas</header>
    <body>Hi, when quantizing pixel values from double precision to byte (0-255) is there a specific algorithm to use ? After going through many subtly different ways to perform this task I ended up with two. I am wondering if one is to be preferred upon the other, or if there's a standardized way to perform this conversion given that it's a very common one in computer graphics. 1. 2. Any comments, remarks, suggestions? Thanks. - Luigi</body>
  </mail>
  <mail>
    <header>Re: Core Animation layer drawing using NSTimer</header>
    <body>The CAOpenGLLayer already has a facility for continuos updates. Set the asynchronous flag to YES and if necessary the - canDrawInCGLContext: method. For notes on this, see the CALayerEssentials sample. I can't recall if you will be throttled directly, but if you call - setNeedsDisplay 1000/s then your certainly drawing far more than you need to. Again, I would recommend the builtin asynch support. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Core Animation layer drawing using NSTimer</header>
    <body>I'm using Core Animation for OpenGL rendering. Within my drawInCGLContext method I can determine if there's more drawing to be done. If there is then I presently fire an NSTimer object to call setNeedsDisplay 50 times per second. The timer can also be removed within drawInCGLContext if there is no more drawing to be done. My question is: does the invocation of setNeedsDisplay on a layer cause my layer's drawInCGLContext to be called in unison with any other animation that's happening? Putting it another way, if I invoke setNeedsDisplay a 1000 times then will Core Animation throttle the calls to drawInCGLContext? Kind regards, Christopher</body>
  </mail>
  <mail>
    <header>serializing animations (was: What does CATransaction really do?)</header>
    <body>I was reading about CATransactions and then thought this might help solve my problem. This is what I wan't to do: I have 3 animations which all have their own duration. say for example: anim1 -&amp;gt; duration 5s anim2 -&amp;gt; 3s anim3 -&amp;gt; 10s Now I want to serialize them as follows: At time 0s : launch anim1 + be able to force passing immediately to anim2 with pushing key 'S' At time 5s: anim1 finishes (that's ok). wait... At time 15s: launch anim2 At time 18s: anim2 finishes (ok). wait... At time 25s: launch anim3 quit when finished timing must be very precise, so I wanted to use a NSTimer to control the launch of the anims (i.e. queuing then anim launch directives). is this the good approach ? or should I think about something else ? -- Erik</body>
  </mail>
  <mail>
    <header>Re: What does CATransaction really do?</header>
    <body>On Sep 8, 2009, at 8:55 AM, Glen Low wrote: The main thread generally sets up a transaction for you.  Generally explicitly.  However, transactions do have properties (see &amp;quot;valueForKey:&amp;quot; and &amp;quot;setValue:forKey:&amp;quot; in CATransaction.h) and those properties are defined within a transaction scope and are used to establish default values for animations used in that scope.  For example: [CATransaction setAnimationDuration: 0.1] While in the scope of the transaction block, animations that are added to a layer will have a default duration of 0.1 seconds.  For a simple example, it might be just as easy to set the duration of the individual animations, but if you're calling a library (like the system APIs) which is adds animations on your behalf (animations that you have no control over otherwise), this can help you affect their values. Transactions can also be used to disable implicit animations (so, for example, if you don't want the default operating system animations applied to your layers but you do want your own, explicit animations to run).  Suppose you want to move a view without a lovely slide animation... one way to do that is to change the frame inside of a transaction block with the Actions disabled. Amusingly the iPhone headers define a &amp;quot;completionBlock&amp;quot; as one of the properties that can be set on a transaction.  Since blocks aren't supported on the iPhone (at least not publicly?) perhaps this is an area for future enhancement :-) Having said all of that, creating explicit transactions is not something that I have found the need to do very often.  More often than not, I've found that an animation group, or an API which (I assume) creates transactions on my behalf (say UIViews &amp;quot;beginAnimations:context:&amp;quot; method) are something I use more often.</body>
  </mail>
  <mail>
    <header>Re: Adding single animations to a layer vs. adding an	animation	group</header>
    <body>[... snip ...] In terms of the way they look when they run, there shouldn't be a difference, but there are other reasons for putting animations into a group. For example, If you add your animations to a group, you can add and remove the group as a whole without having to keep track of the individual animations. You can establish a delegate for the group to (for example) and be notified when all the animations complete rather than having to track the individual animations. You could give the group a timing function that is different than the individual animations and manipulate them all at once using that. Scott</body>
  </mail>
  <mail>
    <header>Re: CIFilter/CIKernel RGB Values</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CIFilter Load Problem</header>
    <body>I just tried it in SL.  Same deal.  I also tried custom filters from sample code and they would not load either.  What gives?</body>
  </mail>
  <mail>
    <header>Re: Adding single animations to a layer vs. adding an animation	group</header>
    <body>But if I add the animations singly in a block of code with identical durations, then when the run loop resumes, surely they will all start and stop together?</body>
  </mail>
  <mail>
    <header>Re: Adding single animations to a layer vs. adding an animation	group</header>
    <body>AFAIK, the animation group will make sure the animations start and stop together, whereas individual animation do not. On Sep 8, 2009, at 5:02 PM, Glen Low wrote:</body>
  </mail>
  <mail>
    <header>Adding single animations to a layer vs. adding an animation group</header>
    <body>Is there any functional difference between adding several single CAAnimation objects to a layer, each with its own key of course, vs. adding them to an animation group and adding the group to the layer? Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>What does CATransaction really do?</header>
    <body>Besides enabling/disabling actions and animation duration in a transactional way, what does bracketing code with [CATransaction begin] and [CATransaction end] really do? AFAICT, all changes, animations that are set up in code are then run simultaneously when the run loop resumes, and I can't detect any visible differences between code bracketed with [CATransaction begin] and [CATransaction end] in iPhone programming. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Re: CIFilter Load Problem</header>
    <body>Sorry.  I should have added that this is 10.5.8.  Maybe I should try it under SL and see what happens. -- Gordon Apple Ed4U Little Rock, AR</body>
  </mail>
  <mail>
    <header>Re: CIFilter Load Problem</header>
    <body>Is that in snow leopard ? I noticed my custom filters in my Chocoflop app do not load anymore either since updating to SL rendering my whole app useless.</body>
  </mail>
  <mail>
    <header>CIFilter Load Problem</header>
    <body>I've created a custom CIFilter from the CIChromaKey example.  The plugin is placed in Library/Graphics/Image Units.  My app delegate calls [CIPlugIn loadAllPlugIns].  In my code, I call [CIFilter filterWithName:filterName] but it doesn't load.  I also checked the filters list immediately after loadAllPlugIns and it wasn't there. Running ImageUnitAnalyzer results in everything passing, except the last step, where I get: VALIDATING IMAGE UNIT CIFILTERS - PASS: The filter RTPChromaKeyFilter was created successfully Testing filter RTPChromaKey Filter swap constant CFString 'IOGLBundleName' swap constant CFString 'IOGLBundleName' ERROR: The filter RTPChromaKey Filter threw an exception Verification of /Library/Graphics/Image Units/RTPChromaKey.plugin failed with code: -1 ** FAIL The filter was created using the template CIDemoImageUnit.  The kernel code is straight out of the CIChromaKey (QC) example. I get a similar result when I actually use the CIDemoImageUnit, unmodified. Any ideas or suggestions?</body>
  </mail>
  <mail>
    <header>Anti-aliasing in Snow Leopard (10.6) PDFView</header>
    <body>It used to be that PDFView anti-aliased nicely. Now it doesn't, at least not for some files. Text and scans rendered in both the PDKKitViewer and PDFLinker2 samples are horrible in 10.6. I understand that shouldAntiAlias defaults to true, but even setting it explicitly does not seem to help. The display is the same as in Preview when PDF smoothing is turned off in Preview preferences. Am I missing something obvious? Thanks in anticipation Duncan McGregor</body>
  </mail>
  <mail>
    <header>Re: kernel destCoord() values slightly off</header>
    <body>Hi Frank, here's my bitmap render: [curContext render:image toBitmap:outputPixels rowBytes:width * bytesPerPixel bounds:extent format:kCIFormatRGBAf bytesPerPixel is set at 16. And some results.... index 0 A 0.499939 R 479.500122 G 0.000000 B 1.000000 index 4 A 1.500427 R 479.500000 G 0.000000 B 1.000000 The filter code is... Could the issue be with my CIContext? I'm just creating a context like this... Cheers, Max 2009/9/4 Frank Doepke &amp;lt;email@hidden&amp;gt;:</body>
  </mail>
  <mail>
    <header>Re: CGDataProviderCopyData() lock up under 10.4</header>
    <body>Well, actually it's working also under 10.4 but it seems that there are some problems with TIFFs. It works perfectly with PNG, GIF, JPEG but when used with TIFF images it crashes... I have to correct myself here: The crash is not only with image source objects created from a CFDataRef source but also with normal image source objects if the image to be loaded is a TIFF. There are no problems with PNG, GIF, JPEG but TIFF creates a lockup. Well, I guess I've to live with it. If the docs say that it's only available in 10.5, then the 10.4 version should be considered unstable :) Andreas --</body>
  </mail>
  <mail>
    <header>Re: CGDataProviderCopyData() lock up under 10.4</header>
    <body>CGDataProviderCopyData( provider Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: kernel destCoord() values slightly off</header>
    <body>brilliant! You guys are all great! Any thoughts on the slight imprecision? I can just round it of course, but I'm curious why it's happening. My newbie type guess would be that it's something to do with how the kernel handles the infinite work coordinate space when it has to collapse that to be a float value. 2009/9/4 Frank Doepke &amp;lt;email@hidden&amp;gt;:</body>
  </mail>
  <mail>
    <header>Re: kernel destCoord() values slightly off</header>
    <body>CI samples in the middle of the pixel which is 0.5. 1.0 would interpolate between the first and second pixel.</body>
  </mail>
  <mail>
    <header>Re: kernel destCoord() values slightly off</header>
    <body>I would guess that the middle of a pixel IS at 0.5 etc... The left edge is at 0.0 and the right edge is at 1.0. On 2009-09-04, at 8:36 AM, email@hidden wrote:</body>
  </mail>
  <mail>
    <header>Re: kernel destCoord() values slightly off</header>
    <body>As I recall it, the kernel regards the coordinate of the pixel at row 0, column 0 to be 0.5,0.5, etc On Sep 4, 2009, at 5:36 PM, email@hidden wrote:</body>
  </mail>
  <mail>
    <header>kernel destCoord() values slightly off</header>
    <body>Hi Folks, here's a weird one for you. I have a kernel returning destCoord() x and y values. For x values, I'd expect... 0, 1, 2, 3, .... 10 What comes back is... 0.499939, 1.500427, 2.499878, 3.501953, .... 10.500549 Now, I know this is not a precision issue as far as the output of the kernel is concerned. If I hard code a return value of 0.000001, I'll see exactly that back in Objective-C land. Similarly, the problem is not because my numbers are outside a -1.0 and +3.0 range. Values that are within that range, e.g. 0.499939 are also wrong. So, I think this is internal to the kernel. I'd guess that it's float types just can't hold the precision to represent whatever values are in the coordinate space. But the almost 0.5 wrong for every value, that'd got me stumped. Any ideas? Cheers, Max</body>
  </mail>
  <mail>
    <header>Analysing and editing image Carbon (10.4)</header>
    <body>I am starting work on an application for image analysis and editing. Using Quartz makes it really easy to read images in and display them in a scroll view without knowing much about them. But, it appears all the CGImage operations are 'write only'. Is there a way to look at a CGImage pixel-by-pixel, and perhaps change individual pixels? If not, is there a simple way of getting the image out of a CGImage in a format-independent way so that I can manipulate it directly? Thanks Andrew</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics and Python</header>
    <body>That depends on how you have your server set up.  You could have an FCGI server running in a user session that the web app can talk to, for example. --Amanda -- &amp;quot;Portability is generally the result of advance planning rather than trench warfare involving #ifdef&amp;quot; -- Henry Spencer (1992)</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics and Python</header>
    <body>for the record, I really wish those of us wanting to use CG + Python had a</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics and Python</header>
    <body>That doesn't sound like a good idea.  You can't connect to the window server from whatever namespace your app lives in. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: CGContextDrawImage, alpha premultiplication, and dark halos.</header>
    <body>I'm probably just confused as to where I'm seeing these dark pixels come from.  No matter how many times I read about premultiplication, I always seem to get something wrong conceptually :) So here's what I'm doing (and the whole source file is located here: // set the color to teal CGContextSetRGBFillColor(backgroundContext, 0.049112, 0.569168, // set the color to teal. CGContextSetRGBFillColor(circleContext, 0.049112, 0.569168, 0.572023, // draw 20 times, simulating a brush being dabbed over and over: CGContextDrawImage(backgroundContext, CGRectMake(0, 0, 10, 10), After drawing that image 20 times, on a background made from the same color, I get this: Which is not at all what I expected.  What's the cause of the dark pixels on the edges, and how do I keep that from happening? -- August 'Gus' Mueller Flying Meat Inc.</body>
  </mail>
  <mail>
    <header>CGDataProviderCopyData() lock up under 10.4</header>
    <body>Hi, I noticed that CGDataProviderCopyData() runs into a deadlock when it is used on an image source that is created from a CFDataRef (e.g. from pasteboard data). The deadlock only happens under 10.4 (PowerPC). Under Leopard (Intel) it seems to work fine. The code goes something like this: CFDataRef theData = ... // this call does never return... CPU usage goes to 100% and the app is dead Is there any way to work around this? I don't know if this happens with all image formats but with TIFFs the code above results in a deadlock. Tks Andreas --</body>
  </mail>
  <mail>
    <header>Re: Getting data out of an image.</header>
    <body>Sorry I should have been clearer. I did mean PBO rather than Pixel Buffer. See 2009/9/2 James Walker &amp;lt;email@hidden&amp;gt;:</body>
  </mail>
  <mail>
    <header>Thresholding Image</header>
    <body>Hello, I want to convert grayscale or RGB image to Binary Image. Please provide sample code. Thanks in advance. Regards, Fawad Shafi</body>
  </mail>
  <mail>
    <header>Re: Quartz2D drawing in AGL context</header>
    <body>This is not exactly the case, but the exact details are under NDA. Suffice to say if you have access to the WWDC Videos, then you should watch the session on Performance Tuning Quartz 2D for more details. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Quartz2D drawing in AGL context</header>
    <body>On Sep 7, 2007, at 5:47 PM, Thijs Koerselman wrote: I must confess that I am not particularly knowlegeable of OpenGL though I am trying to learn as much as I can.  Still, to answer your question I might suggest you look at this article: It explains how to use Apple's OpenGL extensions to optimize the transfer of texture information to the GPU. However, if you need to do screen-sized transfers to the GPU at high performance intervals, you might want to look into using Core Video. I do not know much about Core Video other than the fact that it uses fancy techniques to shuffle images to the GPU at regular intervals, but it might help if you are doing full-screen animations. Quartz 2D Extreme (the drawing library , not to be confused with the Quartz Extreme enabled window server) was planned for Leopard. however , but after pursuing it for some time, Apple discovered that it did not have the beneficial effects that it was expected to have. I don't recall the exact details, but in the final analysis, Quartz 2D extreme did not (significantly) improve the performance of most applications.  As a result, I gather that investigating the technology lead to improvements in technologies like Core Image and Core Video (and perhaps Core Animation) but the OpenGL Quartz 2D Extreme engine will not proceed beyond the prototype stage found in Tiger for the forseeable future.</body>
  </mail>
  <mail>
    <header>Re: drawing with a pattern?</header>
    <body>- translation to 0, 0 for the pattern, which on first draw would be 0, 0 for the rectangle I'm filling (my views bounds in the drawSelf: method)- pattern matrix is identity so no changes as part of step 3 (above from 'how patterns work') - clip to 0,0,16,16 - call CirclePatternDrawingFunction which places a circle centered at 8,8 of radius 5 and closes the path and fills it - repeat until the rectangle is filled</body>
  </mail>
  <mail>
    <header>Re: bitmaps with CMYK and Alpha,</header>
    <body>I don't think it will require two renderers, just two bitmap contexts.  One bitmap context you set up as CMYK and one you set up as alpha only.  You will have to draw the context twice (once into each context) but the same rendering code should work for both, I think.</body>
  </mail>
  <mail>
    <header>Re: bitmaps with CMYK and Alpha,</header>
    <body>I think you misunderstood a bit - I need to create some CMYKA bitmaps, not CMYK. A CMYK CGBitmpContext won't work here This would probably work, too but since it requires two renders (one for for the mask) and some fooling around. I might as well just do the two renders and hand stuff the results into an 40bpp NSBitmapImageRep since the rest of the code is presently set up to use NSImages.</body>
  </mail>
  <mail>
    <header>Re: CI bug on simple flip filter?</header>
    <body>I know it's been a week, but I am also very curious as to why the flipVertical kernel fails without the abs(), but the flipHorizontal does not. I can contribute that the abs() is not a general solution... if your flip filter is &amp;quot;chained&amp;quot; with other filters, several of the built-in filters will expand the extent of the image. That is, if you pass a (coord.x, abs(size.y - coord.y))) with that output image as an input image to your filter would sample some strange y coords. In order to overcome problems with what seemed like a similarly simple and straightforward filter, I had to pass a &amp;quot;my own Region Of Interest&amp;quot; parameter to my filter when chained with other filters using pretty much the same mechanism in outputImage that you have below. But I've been thinking about reasons why your flipVertical kernel would fail, and none of the theories I've come up with make sense given that your other two flip___ filters work. There is definitely something interesting going on behind the scenes. Curiously, Brian</body>
  </mail>
  <mail>
    <header>Re: the HIShapeRef parameter in kEventControlDraw event is	overwritten after HIViewCreateOffscreenImage is called,	is this an apple bug?</header>
    <body>On Sep 6, 2007, at 6:52 PM, yun yiqun wrote: I agree, it's a bug (and I replied as such to someone else inside Apple this morning; have you been working with Apple Developer Tech Support about this?). Unfortunately, there's really nothing you can do about this right now. It's best not to use HIViewCreateOffscreenImage from within a kEventControlDraw handler. Please do file a bug about this, if you haven't already.</body>
  </mail>
  <mail>
    <header>the HIShapeRef parameter in kEventControlDraw event is overwritten	after HIViewCreateOffscreenImage is called, is this an apple bug?</header>
    <body>We are switching QuickDraw to Quartz, and we use HIShape to replace RegionHandle, and i find such a problem: The kEventParamShape/typeHIShapeRef in kEventControlDraw event will be overwritten after calling HIViewCreateOffscreenImage(hiview2,...). The HIShape size is changed to the size of hiview2. But the region parameter's bounds is not changed, it is still correct. Let me give some code sniping to describe it more clearly. In a custom HIView draw event:(Which is very similar to HITestView example) CCustomView::Draw( HIShapeRef                      inShape, RgnHandle                       inLimitRgn, CGContextRef            inContext, CGrafPtr                        inPort ) .... //Save current shape's bounds HIShapeGetBounds(inShape, &amp;amp;r1); // 1. GetRegionBounds(inLimitRgn, &amp;amp;rect1); // 2. .... HIViewCreateOffscreenImage(hiview2, &amp;amp;hirect, &amp;amp;outImage); //3. //Get the shape's size again HIShapeGetBounds(inShape, &amp;amp;r2);// 4. GetRegionBounds(inLimitRgn, &amp;amp;rect2);//5. if(!CGRectEqualToRect(r1, r2)) printf(&amp;quot;\n Not eqaul\n&amp;quot;); //6. rect1.right != rect2.right &amp;amp;&amp;amp; rect1.bottom != rect2.bottom) ... printf(&amp;quot;\n Region bounds are eqaul\n&amp;quot;)//7. Comments: 1.2.Save current shape bounds and region bounds to r1 and rect1 respectively. 3.Call HIViewCreateOffscreenImage 4.After calling HIViewCreateOffscreenImage, get shape bounds to r2. 5.After calling HIViewCreateOffscreenImage, get region bounds to rect2. 6.This is true and it will print this line! 7.Compare rect1 with rect2, they are equal . I think it is a bug. The shape should not been changed so. what do you think?</body>
  </mail>
  <mail>
    <header>Re: Basic image processing</header>
    <body>Hmm, see below. Yes, this one works. Slower than rgb but that's understandable. BTW, there is CIImage imageWithCVImageBuffer:options:. Why this latter variant? Makes me wonder what'll happen if I pass different formats in CV pixel buffer and in the options parameter... I am not quite sure what are you guys suggesting here. CIImage imageWithBitmap clearly states that it accepts only RGB formats. Do you suggest to pass something else as a format and pray it will work?! Or do you suggest to lie and pass RGB as a format for yuv data and, again, pray it will work correctly?! Why do you think any of these could work at all? Mike</body>
  </mail>
  <mail>
    <header>Re: Quartz-dev Digest, Vol 4, Issue 209</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: bitmaps with CMYK and Alpha</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>TIFFs from Preview in floating point format?</header>
    <body>Our app can read and write TIFFs. A customer tried to place a TIFF created by Preview and it failed. What I found is that Preview- created TIFFs contain the SampleFormat tag, and it's set to 3, kSampleFormatIEEFloat. We don't handle that format, only unsigned int data. Since I'm not the engineer that wrote our TIFF reading code, I was curious to see why we don't handle it, so instead of allowing the exception to be thrown, I let the code proceed as normal. Strangely enough, the image appeared and looked totally normal, just like it did when viewed in Preview or any other app. This leads me to believe that the tag value generated by Preview is lying. Can anyone confirm whether or not the tag value is wrong? BTW, I haven't tested all Preview TIFFs. The steps the user took (and that I took to verify) was to take a screenshot, open that png with Preview, and Save As it to a TIFF. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Basic image processing</header>
    <body>to get that into Core Image. If you have just a data pointer, wrapping in in a CVImageBufferRef would do the trick... - Ralph</body>
  </mail>
  <mail>
    <header>Re: Basic image processing</header>
    <body>Errm, cant you just use [CIImage imageWithBitmap] and specify a YUV colourspace for the last argument. Granted the constants for the format: parameter are all name *RGB*, but I think they're mainly used for the pixel size (32, 64 or 128 bits).</body>
  </mail>
  <mail>
    <header>Re: Quartz2D drawing in AGL context</header>
    <body>In my experience, CGGLContextCreate has not really ever worked properly.&amp;nbsp;&amp;nbsp;My understanding is that the routine will be deprecated, or</body>
  </mail>
  <mail>
    <header>Re: Quartz2D drawing in AGL context</header>
    <body>On Sep 5, 2007, at 8:52 AM, Thijs Koerselman wrote: In my experience, CGGLContextCreate has not really ever worked properly.  My understanding is that the routine will be deprecated, or at the very least strongly discouraged, in Leopard. One mechanism would be to use a CGBitmap context.  When you do that you are drawing into memory that you own.  You could select a convenient pixel format and use the CGBitmapContext to draw your Quartz 2D content, then create an OpenGL texture from the same memory.</body>
  </mail>
  <mail>
    <header>Quartz2D drawing in AGL context</header>
    <body>Hi all, I&amp;#39;m trying to use Quartz 2D / CG for a 3rd party plugin inside an application that is using AGL to draw to its window context. I&amp;#39;m trying to &amp;quot;overlay&amp;quot; the AGL context with Quartz 2D drawings. I managed to create a CG context from the existing AGL one using the following functions: The problem is I&amp;#39;m getting a lot of, what looks like, graphic state conflicts when trying to draw to the same window. I&amp;#39;m already have CGContextSaveGState and CGContextRestoreGState surrounding my CG code, but that doesn&amp;#39;t solve it. Depth, blending, lighting, and antialiasing for example all seem to get seriously messed up. I don&amp;#39;t know how to deal with this issue, and I&amp;#39;m wondering if its even possible to render to the same context without creating conflicts. As an alternative I&amp;#39;d like to isolate the Quartz drawings in an offscreen context, and render that context to a (shared) OpenGL texture. Drawing the texture on a quad in the AGL window context. I don&amp;#39;t need depth testing between the 2 contexts because my drawings are a 2D overlay of the 3D opengl rendering in the rest of the scene, so a textured quad with the rasterized Quartz drawing will suffice in this case. How can I possibly do this? Cheers, Thijs</body>
  </mail>
  <mail>
    <header>Re: Basic image processing</header>
    <body>On Tue, 4 Sep 2007 08:06:37 -0500 Hmm, that's interesting. I only see these formats in CIImage.h: As I understand it I can't use, say, imageWithBitmapData to create CIImage based on, say, YUV data. Maybe I can do this with imageWithCGImage, I haven't tried. Mike</body>
  </mail>
  <mail>
    <header>Re: Strange CGContextDrawShading with axial shader crash</header>
    <body>kCGInterpolationLow works just fine and there doesn't seem to be very noticeable quality decrease. Regards, Aidas</body>
  </mail>
  <mail>
    <header>Re: Strange CGContextDrawShading with axial shader crash</header>
    <body>Are you calling CGContextSetInterpolationQuality() anywhere with kCGInterpolationHigh? If so, this is a known issue on 10.4 that should be fixed in a future release of Mac OS X. Otherwise, we'll need to see the code leading up to this crash to understand what's going on. -- David Duncan Apple DTS Quartz and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Strange CGContextDrawShading with axial shader crash</header>
    <body>I'm having some trouble with CGContextDrawShading. It crashes every time I try to draw vertical or horizontal (i.e. the x or y coordinate of start and end points are equal) axial shader (created with CGShadingCreateAxial). Everything works fine none of the coordinate points are equal. Any ideas what could be wrong? Any workarounds? msg00006.html ) reported similar problem, but it seems nobody had any workarounds back then. #0  0x90471a1d in getNextAxialShadingScanline () #1  0x90471f46 in getBytesAxialShadingDataProvider () #2  0x90364749 in CGAccessSessionGetChunks () #3  0x9037cd17 in img_decode_read () #4  0x9037c883 in img_colormatch_read () #5  0x9037b516 in img_alphamerge_read () #6  0x90339f18 in img_data_lock () #7  0x9033872d in CGSImageDataLockWithReference () #8  0x94300318 in ripc_AcquireImage () #9  0x942fe07a in ripc_DrawImage () #10 0x90336de1 in CGContextDrawImage () #11 0x904710c7 in drawAxialShading () #12 0x9046fd3e in CGContextDrawShading () Thanks, Aidas</body>
  </mail>
  <mail>
    <header>Re: Executable vs nonexecutable CIFilter</header>
    <body>On Sep 4, 2007, at 3:14 AM, Mike Kluev wrote: The fact that a CIKernel might ever run on the CPU instead of the GPU is an implementation detail which has absolutely no bearing on the current topic at hand.  If you're packaged filter contains only a .kernel file, an appropriate Info.plist and absolutely no compiled binary then it's considered nonexecutable since, when run, it cannot do anything except run the .kernel code.  In contrast, there may be other calculations which you need to make outside of your kernel routine which then have to be expressed in ObjC and compiled.  Since there's no way to restrict arbitrary ObjC code from doing other nasty things, this type of filter is distinguished from the other type. Brendan Younger</body>
  </mail>
  <mail>
    <header>Re: Basic image processing</header>
    <body>On Sep 4, 2007, at 5:07 AM, Mike Kluev wrote: The filters themselves work in the RGB working space, that is true, but it seems that you should be able to front-load the filter stream with an image in any color space you like.  That image will be transformed into the working color space (an RGB space), operated on by the filters, and then may be drawn to any other color space.</body>
  </mail>
  <mail>
    <header>Re: Strange Behaviour</header>
    <body>On 4 Sep, 2007, at 9:02 am, Mike Kluev wrote: Mike, you‚Äôre a genius. documentation/DeveloperTools/Conceptual/LowLevelABI/index.html&amp;gt; and even put a comment in the code to remind me to align the stack, when I looked more closely I found I wasn‚Äôt actually doing the alignment. Doh! Simon _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Basic image processing</header>
    <body>What do you mean? Isn't CoreImage (Filters) limited to RGB data? Mike</body>
  </mail>
  <mail>
    <header>Re: Executable vs nonexecutable CIFilter</header>
    <body>Then, I don't quite follow the following fragment from the doc: #Security is the primary motivation for distinguishing CPU executable #and CPU nonexecutable filters. Nonexecutable filters consist only of #code that runs on the GPU. In contrast, an executable filter has #binary code that runs on the CPU. Code that runs on the GPU cannot #pose as a virus, Trojan horse, or other security threat, whereas #code that runs on the CPU can. If there is no no such thing as a GPU-only filter then why did they bother putting the above fragment in the documentation at all... Mike</body>
  </mail>
  <mail>
    <header>Re: Strange Behaviour</header>
    <body>I did it recently without any assembler glue of my own :) I heard this could be caused by improper stack alignment. How do you allocate stack? Make sure it is aligned to at least 16 bytes. How big is your stack? Mike</body>
  </mail>
  <mail>
    <header>Re: drawing with a pattern?</header>
    <body>On Sep 2, 2007, at 2:34 PM, Bill Dudney wrote: I can't say why, but I've found it' useful to set the pattern's matrix to the CTM of the context I'm drawing into.  You might want to try that. Nick</body>
  </mail>
  <mail>
    <header>Re: drawing with a pattern?</header>
    <body>Hi Scott, Thanks for the reply. I've tried both identity transform and a translation transform. I can get the pattern to draw properly with a translation but  it does not make sense to me that i should need that. I've not scaled the pattern as its already the size I want, at least for this simple example that I'm working on. Basically I'm trying to get my head wrapped around drawing with patterns, and its not working so well just yet :) Thanks again, -bd-</body>
  </mail>
  <mail>
    <header>Strange Behaviour</header>
    <body>The background I have a Cocoa application running on a PowerMac G4, OS 10.4.10, that does some drawing using CoreGraphics; nothing fancy, just filling rectangles and drawing images. It uses ATSU to draw text. The twist Although single-threaded, the application implements its own cooperative (ie not pre-emptively scheduled) multi-tasking. Each task within this system has its own stack and set of processor registers, switched with a little PPC assembler glue. The problem When called directly from the main thread, my sample drawing function does what I expect. However, when called from within one of the application‚Äôs tasks, drawing is corrupted. See &amp;lt;http:// homepage.mac.com/simonbell/problem.pdf&amp;gt; for an example. Basically, this example calls: CGContextFillRect()   to erase the background to black CGContextDrawImage()  to draw a PNG image ATSUDrawText()        to draw lines of text Incidentally, although always of the same general appearance there is a random factor to the corruption - different colours, placement of text - each time executed. The question What am I doing wrong (aside, obviously, from trying to replicate what OS X already does excellently)? My multi-tasking system works fine in all other respects; calls to CoreAudio functions from it sound okay. What could cause this kind of corruption? The CG Refs don‚Äôt seem to have been trampled on when I examine them. I‚Äôm assuming it has something to do with my switching processor registers and stack, but they look okay to me too. How could CG be affected by that anyway? Thanks for any light you hardened quartz types can shed on this. I‚Äôll admit mine is an insane project, but this is driving me even further round the twist. Simon _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: drawing with a pattern?</header>
    <body>On Sep 2, 2007, at 3:34 PM, Bill Dudney wrote: Are you using a non-identity scaling transform?  I've noticed that, when scaling up, the patterns shift vertically by a factor that is related to the scale factor.</body>
  </mail>
  <mail>
    <header>Re: drawing with a pattern?</header>
    <body>Thanks Nick, So any insight to the original question? There must be something fundamental that I don't get. My pattern will not draw as expected. I've built the example code and it does not draw as expected. Essentially I can not get the pattern to draw as expected. It is always translated by what appears to be half the width or height of the pattern cell. On another random note; Purgatory Design. I skied Purgatory as a kid every weekend. Viva La Lower Hades :) Anyway thanks again for any help you can offer. TTFN, -bd-</body>
  </mail>
  <mail>
    <header>Re: CIAreaAverage returning black on small images</header>
    <body>Are you also making sure to set the inputExtent of the CIAreaAverage to whatever the h/w of the input image is? If you don't do that, you will likely return black values. -gt</body>
  </mail>
  <mail>
    <header>CIAreaAverage returning black on small images</header>
    <body>Dear List I'm trying to calculate the average color of a CIImage (acquired by QuickLook). The following code is returning black color R:0 G:0 B:0 for all images sized smaller than 16 pixels width or height. Does anyone has an idea what i'm doing wrong? -(NSColor *)calcAverageColorForImage:(CGImageRef) image // is it a bug in CIImage/Filter? when using the filter on a small image, the resulting color is 0 0 0 Best regards and happy holidays mahal</body>
  </mail>
  <mail>
    <header>Re: QCRenderer and HiDPI display pixelation</header>
    <body>Can you share some code, so that we can get an idea of what APIs you are using? Otherwise, its all a shot in the dark.</body>
  </mail>
  <mail>
    <header>Problems with PresentationLayer being cached?</header>
    <body>Hey all.. So we are running into some interesting issues with what appears to be the caching of the presentation layer, and we are wondering if other people are seeing this, and what workarounds we can do. We have a couple of our own properties that we are implicitly animating. We are doing this by declaring them as properties and synthesizing them dynamically: @interface GOOLayer : CALayer @end @implementation GOOLayer In most cases this works fine, and we see the appropriate values being logged. In the first case, we get proper implicit animation: Output: 2012-12-19 10:10:41.440 CoreAnimation[94149:c07] Display GooValue: 1804289408.000000 2012-12-19 10:10:41.502 CoreAnimation[94149:c07] Display GooValue: 1804289408.000000 ... 2012-12-19 10:10:41.739 CoreAnimation[94149:c07] Display GooValue: 898766464.000000 2012-12-19 10:10:41.757 CoreAnimation[94149:c07] Display GooValue: 846930880.000000 First time through: 2012-12-19 10:11:16.208 CoreAnimation[94149:c07] newValue = 1714636928.000000 2012-12-19 10:11:16.209 CoreAnimation[94149:c07] Display GooValue: 1714636928.000000 Second time through: 2012-12-19 10:11:26.258 CoreAnimation[94149:c07] newValue = 1957747840.000000 2012-12-19 10:11:26.259 CoreAnimation[94149:c07] Display GooValue: 1957747840.000000 It&amp;#39;s the third case in which we just call [CALayer presentationLayer] before we do the transaction that is interesting: First time through: 2012-12-19 10:12:12.505 CoreAnimation[94149:c07] presoLayer.gooValue = 1957747840.000000 newValue = 424238336.000000 2012-12-19 10:12:12.505 CoreAnimation[94149:c07] Display GooValue: 1957747840.000000 Second time through: 2012-12-19 10:12:13.905 CoreAnimation[94149:c07] presoLayer.gooValue = 424238336.000000 newValue = 719885376.000000 2012-12-19 10:12:13.906 CoreAnimation[94149:c07] Display GooValue: 424238336.000000 Not sure if this is iOS specific, but it&amp;#39;s where I have done all my testing.</body>
  </mail>
  <mail>
    <header>Re: Moving CAAnimation forward and backward in time?</header>
    <body>I think you have most of it in place, but in order to preserve the animations like this you need to set removeOnCompletion=NO so that the animations aren't removed. You may also want to change their fill mode so that they extend forwards, backwards, or in all directions on the timeline. -- David Duncan</body>
  </mail>
  <mail>
    <header>Moving CAAnimation forward and backward in time?</header>
    <body>I'd like to move a running CAAnimation forward and backward through time, ideally from it's layer without access to the original animations (which may not be possible.) Basically, I have a running keyframe animation, that may be playing at 1.0 speed, but at some point I'd like to move it to a previous time, or a later time based on user input, ideally without readding a bunch of animations. I've tried using a combination of different experiments on the layer's beginTime and timeOffset, but haven't had any result that is near the right behavior. I've seen the guides on pausing/starting, and I thought this might work similarly, but I must be missing something basic. Anyone have any pointers? The starting point I'm working with is trying to alter a CALayer's time to move it back to it's initial keyframe time at creation (basically, whereever I am in the animation, back up the layers time to where it started), but again, I've got nothing working so far. Thanks, Colin</body>
  </mail>
  <mail>
    <header>Re: Setting Colorspace in PDFContext how?</header>
    <body>As far as I've seen it happens to all PDF!</body>
  </mail>
  <mail>
    <header>Re: Setting Colorspace in PDFContext how?</header>
    <body>I also tried this: Thanks,</body>
  </mail>
  <mail>
    <header>Setting Colorspace in PDFContext how?</header>
    <body>Hi folks, Thanks,</body>
  </mail>
  <mail>
    <header>Reproducing Quartz Filters programmatically (Re: PDF CMYK black to RGB	black)</header>
    <body>I have noticed that 2.b (i.e. swapping the embedded profile in a PDF's object, instead of manipulating it) and 3 (i.e. changing a PDF's object's rendering intent) can be accomplished from within ColorSync Utility by creating a Quartz Filter and applying it (if doing so indiscriminately to all CMYK objects is acceptable), so since I still have no clue about 2.a yet (i.e. altering the original profile to accomplish appropriate CMYK black to RGB black), what I am experimenting with is [creating a command-line quartz-based PDF rasterizer that accepts an optional .qfilter file][1] However, besides the fact that right-out swapping the profile is anything but ideal since it introduces changes to all colors instead of only affecting the luminance of the darkest areas (which is why I 2a in the old one), .qfilter files seem to be binary blobs and thus lessen the flexibility of the tool. So, how do I create a Quartz Filter on the fly, programmatically, that does those 2 things from 2 input arguments: what ICC profile and rendering intent to overwrite the PDF with? The only documentation I found regarding Quartz Filters is Apple's [Technical Note TN2035: ColorSync on Mac OS X][2] from 2005 where it is stated: which is not very encouraging. [1]: Hint taken from [2]: ‚Äî Jorge</body>
  </mail>
  <mail>
    <header>Re: Resulting CIFilter image size</header>
    <body>I think there may be a terminology disconnect here. When you apply a CIGaussianBlur to an image, the result is a CIImage. But you can't do per-pixel processing on a CIImage; you have to render the CIImage into a bitmap or texture first. So, just render it into a bitmap or texture that is the same size as your original image, and you'll be fine. Unless I misunderstand the issue.</body>
  </mail>
  <mail>
    <header>Re: Resulting CIFilter image size</header>
    <body>On 10/18/07, I think there may be a terminology disconnect here. When you apply a CIGaussianBlur to an image, the result is a CIImage. But you can&amp;#39;t do per-pixel processing on a CIImage; you have to render the CIImage into a bitmap or texture first. So, just render it into a bitmap or texture that is the same size as your original image, and you&amp;#39;ll be fine. Unless I misunderstand the issue.</body>
  </mail>
  <mail>
    <header>Resulting CIFilter image size</header>
    <body>When I apply a CIGaussianBlur to an image the resulting image is larger than the original. Is there any way to ensure that the result of a CIFilter is the same size as the input image?&amp;nbsp; I ask because I need to perform per-pixel processing upon _both_ the original and the &amp;quot;filtered&amp;quot; image.&amp;nbsp; This is a little difficult when they are two different sizes. :) Thanks, CxT</body>
  </mail>
  <mail>
    <header>Re: Multiple-monitors, Multiple-Video cards</header>
    <body>This is correct.  That much at least makes it easier.  It is not very likely that I will ever render to more than 2 video cards (knock on wood) but if I go for 2 I might as well go for more, the trick will be figuring out which display is on which card.  As Allan mentioned there is a new app shipping with Leopard that I might be able to glean some ideas from so I think I will put this part of the project on hold for a week.  If by chance there is a new feature in Leopard that lets me do what I want, I am okay saying that people need to run Leopard to get multiple video card support.  It is not terribly often that I will run into that scenario. Thanks all for the input, until Leopard!</body>
  </mail>
  <mail>
    <header>Re: Multiple-monitors, Multiple-Video cards</header>
    <body>The multi display is done by a application called Quartz Composer Visualizer, but maybe it's a Leopard feature, and we cannot talk about this? I am not sure, a friend of mine demo'd it to me.</body>
  </mail>
  <mail>
    <header>Re: Multiple-monitors, Multiple-Video cards</header>
    <body>On Oct 17, 2007, at 17:07 , Daniel Hazelbaker wrote: i struggled with a similair setup a whila ago. in the end i took a completeky other approach, but i guess in this case you have to copy seperate the pixelbuffers/ textures to each gl_context/card. probably have a look at the QuatrtzComposerPerformer example. there, a movie is first rendered into an offscreeen context (QTVisualContextRef), and from that, a CVOpenGLTextureRef is created. i guess you can jut repeat this copy for each of your contexts.. and i have the impression that contexts that live on the two outports of the same graphics card, can actually share contexts, which would reduce the amount of copies.. but i'm noty sure about that. goodluck arri</body>
  </mail>
  <mail>
    <header>Quartz, CIImage and vImage</header>
    <body>I wondering if there is a recommended way of setting up floating point bitmaps for use in vImage and core image. vImage  seems to require ARGB (ARGBFFFF) whereas core image only seems to support RGBA (kCIFormatRGBAf). Is there something I am missing or am I going to have to swizzle the pixels each time I move between apis? Henry</body>
  </mail>
  <mail>
    <header>Quartz color problem</header>
    <body>I'm having some trouble getting colors to display right. While even setting my RGB to 0,0,0,1 I still get blue and any other color i try is way off - not even close. I tried setting color with CMYK and that gives the same result. I've also tried setting the color space manually and it always is incorrect. Here is my code. Am i missing something basic here? // color GREEN #define FILL_COLOR 0, 1, 0, 1 int Open_Display() if (err == kCGErrorSuccess) if (ctx != NULL) void Close_Display() if(display != NULL) void DrawQuartzString(float x, float y, char *text, char *font, float size, float red, float green, float blue, float alpha) void DrawRectangle(float x, float y, float w, float h, float borderwidth, float red, float green, float blue, float alpha) void DrawRectangleFill(float x, float y, float w, float h, float red, float green, float blue, float alpha) int main (int argc, const char * argv[]) if(Open_Display())</body>
  </mail>
  <mail>
    <header>Re: CIImage &amp;lt;-&amp;gt; bitmap data &amp;lt;-&amp;gt; NSImage</header>
    <body>On 18/10/2007, at 3:27 AM, Simon Raisin wrote: There seem to be two types of code floating around. One WRAPS the CImage in an NSImage - (NSImage *)toNSImageFromRect:(CGRect)r NSImage* image = [[[NSImage alloc] initWithSize:NSMakeSize This doesn't do anything. There are no bits associated with the NSImage. The CIImage  gets re-rendered every time you draw the image. - (NSBitmapImageRep *)RGBABitmapImageRepWithCImage:(CIImage *) ciImage NSBitmapImageRep* rep = [[NSBitmapImageRep alloc] initWithBitmapDataPlanes:nil pixelsWide:width pixelsHigh:rows bitsPerSample:8 samplesPerPixel:4 hasAlpha:YES isPlanar:NO colorSpaceName:NSCalibratedRGBColorSpace bitmapFormat:0 CGColorSpaceRef colorSpace = CGColorSpaceCreateWithName CGContextRef context = CGBitmapContextCreate( [rep bitmapData], CIContext* ciContext = [CIContext contextWithCGContext:context [ciContext drawImage:ciImage atPoint:CGPointZero fromRect: This creates a bitmap from the CIImage by drawing the CIImage. You can do almost anything with the NSBitmapRep including grabbing the raw bytes by calling bitmap. You should read the docs for each of these functions to understand what they are doing. Especially the format stuff. This has been very frustrating for me too. It seems to be a Quartz right of passage.</body>
  </mail>
  <mail>
    <header>CIImage &amp;lt;-&amp;gt; bitmap data &amp;lt;-&amp;gt; NSImage</header>
    <body>I have a test application that creates a CIImage instance from an on-disk image file (jpg, tiff, bmp, etc.). I need to apply several core image filters then obtain the raw bitmap data for that image so that I can apply a few hand-created &amp;quot;filters&amp;quot;.&amp;nbsp; I _may_ need to convert the resulting data into an NSImage.&amp;nbsp; Actually I may have to move between an arbitrary number of a closed set of representations. I&amp;#39;ve found several code snippets than render a CIImage instance into an offscreen bitmap, but I&amp;#39;ve seen non-deterministic results (possibly a problem with the code I grabbed). This doesn&amp;#39;t seem, to me anyway, that what I am trying to do is that uncommon, yet moving image representations between Core Image, Cocoa (NSImage), and raw data doesn&amp;#39;t seem very straightforward - which leads me to believe that I am missing something.&amp;nbsp; (And yes, I realize that a CIImage is just a &amp;quot;recipe&amp;quot;; there is no underlying image representation until it is &amp;quot;rendered&amp;quot;). Any pointers would be appreciated, CxT</body>
  </mail>
  <mail>
    <header>CWFillLookupTexture usage</header>
    <body>Hi all, another question I had asked on the Quartz list but that might be better placed here: Has anyone used or is there any sample showing how to use CWFillLookupTexture? Also, this function has a parameter to describe the format of the resulting texture. Looking at the headers I can see only one valid format: cmTextureRGBtoRGBX8 - isn't 8 bit precision too little for high quality gamut mapping transforms? Mark</body>
  </mail>
  <mail>
    <header>Re: Multiple-monitors, Multiple-Video cards</header>
    <body>The multi display is done by a application called Quartz Composer Visualizer, but maybe it&amp;#39;s a Leopard feature, and we cannot talk about this? I am not sure, a friend of mine demo&amp;#39;d it to me. Regards, Sven Hoffmann How were you able to accomplish this?&amp;nbsp;&amp;nbsp;When I try entering full- screen mode in Quartz Composer it only displays on the monitor the window most resides on, the others are left alone. Daniel</body>
  </mail>
  <mail>
    <header>Re: Multiple-monitors, Multiple-Video cards</header>
    <body>How were you able to accomplish this?  When I try entering full- screen mode in Quartz Composer it only displays on the monitor the window most resides on, the others are left alone.</body>
  </mail>
  <mail>
    <header>Re: Multiple-monitors, Multiple-Video cards</header>
    <body>I am not sure this will help you, but Qartz Composer knows to draw a composition to any number of hooked up displays. Maybe you can realize your project in Quartz composer and use its capability to draw to multiple screens. Regards, Sven Hoffmann</body>
  </mail>
  <mail>
    <header>Multiple-monitors, Multiple-Video cards</header>
    <body>Does anybody know of a way to use CoreVideo with multiple-monitors on multiple-video cards?  I have 4 monitors hooked up on 2 video cards and it seems to render to only 1 video card at a time.  The other will draw any &amp;quot;cpu&amp;quot; based images (i.e. text drawn to a texture) but any video gets lost.  I assume this is because it is being rendered to the VRAM of only 1 video card and not the other.  I see a few possible options, but have not got far enough along in my development of them to see which ones would work: 1) Find some magic way to tell openGL/CV to use both video cards for rendering. 2) Somehow keep track of which CGDisplay is on which video card and to make it all happy. 3) Force CV to use the apple software renderer and hope the cpu is fast enough. Has anybody ever played with this kind of setup?  Essentially I am wanting to render a single Quicktime video (via CV and Quartz) to 4 monitors / 2 video cards. Daniel Hazelbaker</body>
  </mail>
  <mail>
    <header>CWFillLookupTexture usage</header>
    <body>has anyone used or is there any sample showing how to use CWFillLookupTexture? Also, this function has a parameter to describe the format of the resulting texture. Looking at the headers I can see only one valid format: cmTextureRGBtoRGBX8 - isn't 8 bit precision too little for high quality gamut mapping? Thanks for any pointers</body>
  </mail>
  <mail>
    <header>Horizontal flip using CIAffineTransform</header>
    <body>I&amp;#39;d like to perform a horizontal flip on an existing CIImage. I thought that simply scaling by -1.0 along the x-axis would do it, but the resulting image is not &amp;quot;flipped&amp;quot;. What am I missing? Thanks, CxT</body>
  </mail>
  <mail>
    <header>Re: Color Management in Carbon - possible bug with NPickColor</header>
    <body>After spending some more time with this, I believe I understand better what might be going on and believe there is a bug in the Carbon implementation of NPickColor. As near as I can determine, gColorInfo.theColor.profile is simply ignored (which would be the bug) by the NPickColor dialog in Carbon. The dialog assumes the values in gColorInfo.theColor.color.rgb represent a color in the 'Generic RGB' color space. Furthermore, the color values that comes out will be a color in the 'Generic RGB' space as well. I've also posted an updated project and application based on this and can now explain the results I am getting... The basic idea of the application is to test how different colors can look if one says they belong to the wrong colorspace. Have I missed something?</body>
  </mail>
  <mail>
    <header>Color Management in Carbon</header>
    <body>(I posted this to the ColorSync mailing list already, but thought it might be more appropriate here) I have some questions concerning NPickColor. ) has 'Generic RGB' selected and I enter the the RGB values ( 204, 97, 143 ), after I press OK, gColorInfo.theColor.color.rgb contains values which correspond to ( 204, 97, 143 ). Case #2: If the Color Profile menu has 'sRGB Profile' selected and I enter the the RGB values ( 204, 97, 143 ), , after I press OK, gColorInfo.theColor.color.rgb contains values which correspond to ( 192, 72, 126 ). For Case #1, I would expect ( 204, 97, 143 ) to be a value in the generic color space, because 'Generic RGB' was selected in the Color Profile menu and, when the RGB value comes out, to be converted into the equivalent RGB value in the sRGB color space - this assumes, of course, that the generic color space is different from the sRGB color space. For Case #2, I would have expected the numbers to not change because I wanted a value in the sRGB color space and that is what I thought I was entering because 'sRGB Profile' was selected in the Color Profile menu and it is the profile gColorInfo.theColor.profile was set to. Can someone explain this behavior? After I press 'OK' on the NPickColor dialog, does gColorInfo.theColor.profile contain the sRGB color profile and is the RGB value in gColorInfo.theColor.color.rgb a color contained within that space? To help understand this better, I wrote a simple test application which can be found at: Assuming there aren't any bugs in the code (might suggest taking a look at the implementation and usage of ConvertColor in main.cp), I am having a hard time explaining the results based upon what I enter in a NPickColor dialog. In the tests I have been running, I have only been entering the ( 204, 97, 143 ) RGB value in the NPickColor dialog. In one case, I am using CGContextSetRGBFillColor...according to the Quartz 2D Programming Guide, this creates a color in the &amp;quot;Generic RGB color space in Mac OS X v10.4 and later&amp;quot; and I am running 10.4.x. thank you.</body>
  </mail>
  <mail>
    <header>Floating point images</header>
    <body>I want to do some floating point manipulation of images. I have a few questions... If I create a 32 bit floating point bitmap context and then draw an 8 bit image into it, will quartz convert the image to float? Henry</body>
  </mail>
  <mail>
    <header>Re: CoreVideo bug + how to create alphaless RGB image with CoreImage</header>
    <body>On Mon, 01 Oct 2007 17:30:11 +0400</body>
  </mail>
  <mail>
    <header>Re: Getting embedded ICC Profile data</header>
    <body>NSImage all by itself won't correctly handle images that contain custom color spaces. If an image has custom color space, will lose the color space info, while // Use CoreImage to interpret the image. This retains color space correctly. NSImage *image = [[NSImage alloc] initWithSize: NSMakeSize NSCIImageRep *ciImageRep = [NSCIImageRep imageRepWithCIImage: will retain it.  (at least on 10.4.9 and 10.4.10) Cheers, Brian</body>
  </mail>
  <mail>
    <header>Re: Average luminance of an image</header>
    <body>Sorry this was Georgiev not Gorgiev. I found the article I was talking about :</body>
  </mail>
  <mail>
    <header>Equivalent of PS highpass filter?</header>
    <body>Hi, Is there any way to achieve similar results to those of Photoshop&amp;#39;s highpass, minimum, maximum, or custom filter (under the Filter-&amp;gt;Other menu item) using existing CoreImage filters? Thank you, CxT</body>
  </mail>
  <mail>
    <header>Re: Average luminance of an image</header>
    <body>Of course this is not true, but this is of the same kind (of error, or approximation or anything else, somehow) Y is defined with no much meaning to the eye relative to its mapping. However L is defined so that the average of two values a and b should be seen to the eye &amp;quot;as the middle&amp;quot;. Talking of average brightness in an image is somehow hard, because the eye make some local adaptation on a global scene. So the problem is hard anyway. For example you may expose right an object in a scene and surround it by black. The more black in surface you put arround the object, the more the average will be darker, yet the object has still a good exposure. Sure. As far as I remember, the Retinex theory and fiber bundle work from todor gorgiev is about that at least in part. As far as I remember, I read an article where there was a direct application for HDR and human vision, so you may probably derive this work to find a good basis for the notion of perceptual luminance of a scene. Raphael</body>
  </mail>
  <mail>
    <header>Re: Average luminance of an image</header>
    <body>:) Then probably working in Y is a good pick : it has the same good properties as RGB linear. Exposure is multiplicative and won't &amp;quot;damage contrast&amp;quot; (in the light sense) as far as the transform aera is &amp;quot;big enough&amp;quot;.</body>
  </mail>
  <mail>
    <header>Re: Average luminance of an image</header>
    <body>Not much, since you can first average all the RGB values in your linear space, then apply the linear transformation to find Y and then the L value, which you would have to compute on one Y. If you average all the L values, the average is of the same kind than averaging in the non linear RGB space (somehow) The simple formula here comes from the row of the transformation matrix from RGB to XYZ from the NTSC color space. The value you obtain from the linearized RGB value using this formula is the Y value. This Y value is luminosity, but is not &amp;quot;linear to eye&amp;quot;. The simple formula changes for all input space.</body>
  </mail>
  <mail>
    <header>Re: Getting embedded ICC Profile data</header>
    <body>Le 5 oct. 07 √† 21:38, Aaron Alpher a √©crit : Is there a way to get embedded ICC profile data from an image using a Core Graphics (CGXXX) or Cocoa (NSXXX) call?</body>
  </mail>
  <mail>
    <header>Re: Average luminance of an image</header>
    <body>Thanks for all of the suggestions. I&amp;#39;ve already converted my image into a grayscale (luminance only) floating point representation (which is what my application requires). The end goal is to remove/lessen harsh or uneven lighting in a given image.&amp;nbsp; Thus, I am experimenting with methods of reducing the &amp;quot;hot&amp;quot; areas without damaging the contrast in surrounding areas. Not asking for solutions... just thinking out loud... :) Thanks again, CxT</body>
  </mail>
  <mail>
    <header>Re: Getting embedded ICC Profile data</header>
    <body>To the best of my knowledge, no. I submitted a bug about that two utility can extract profile, and could be used as a workaround.</body>
  </mail>
  <mail>
    <header>Re: Average luminance of an image</header>
    <body>The problem of average luminance is something I am trying to get a grip on relating to some images I have. With a linearized R-G-B image, would the most accurate value be the average of all &amp;quot;L&amp;quot; values (using L*a*b*) for each pixel (a huge computing task)? Would an average &amp;quot;L&amp;quot; differ much from the value obtained by obtaining the average R-G-B values then using the simple luminance formula [29*red + 58*green + 11*blue], which is obviously a lot simpler to compute. PS: I tripped across another color space called &amp;quot;HCL&amp;quot; that also provides an &amp;quot;L&amp;quot; value that is perceptually accurate, and does not involve conversion to XYZ space.</body>
  </mail>
  <mail>
    <header>Re: Getting embedded ICC Profile data</header>
    <body>You know, most of the old ColorSync APIs aren't really necessary anymore.  I've found that pretty much anything I want to do can be accomplished by rendering a CGImage to a CGBitmapContext with the appropriate destination profile or by using CGImageCreateCopyWithColorSpace.  The advantage of these is that you needn't worry about the source color space.  Is there a particular need to use ColorWorld's?</body>
  </mail>
  <mail>
    <header>Re: Getting embedded ICC Profile data</header>
    <body>Most notably, I don't believe that QuickTime handles EXIF, RAW and some floating point formats. Having said that, we are quickly straying into areas of the system (QuickTime graphics importers) where I have very little direct experience. wrote:</body>
  </mail>
  <mail>
    <header>Re: Getting embedded ICC Profile data</header>
    <body>On Oct 5, 2007, at 3:13 PM, Aaron Alpher wrote: The functionality of CGImage and CGImageSource is all wrapped up in the NSImage class.</body>
  </mail>
  <mail>
    <header>Re: Getting embedded ICC Profile data</header>
    <body>This is what I am currently doing. This gives me the name of the embedded profile (if present) - ex. &amp;quot;AdobeRGB1998.icc&amp;quot;. But this doesn't give me the profile data so I can use it to setup a ColorWorld for color conversion. Is there nothing in Cocoa that will handle this? If so how do I convert a CGImage or CGImageSource into a NSXXX? Is there a better way to go here?</body>
  </mail>
  <mail>
    <header>Re: Getting embedded ICC Profile data</header>
    <body>Nope. 10.4 and newer. What are the Image I/O calls to be made?</body>
  </mail>
  <mail>
    <header>Re: Getting embedded ICC Profile data</header>
    <body>On Oct 5, 2007, at 2:38 PM, Aaron Alpher wrote: I believe that Image I/O can extract the ICC profile from an image. Do you need code for pre-Tiger systems?</body>
  </mail>
  <mail>
    <header>Getting embedded ICC Profile data</header>
    <body>Is there a way to get embedded ICC profile data from an image using a Core Graphics (CGXXX) or Cocoa (NSXXX) call? Currently I parse images myself and load the embedded profile, however I want to move to a more system-level approach. My way requires specific knowledge of the type of file (TIFF, JPEG, PDF, etc.) as each file format uses a different method (and location) to embed the ICC Profile. I'd like to use a system call so I no longer need to bother with the specifics of each file type, and gain support for the plurality of file formats which CoreGraphics supports today.</body>
  </mail>
  <mail>
    <header>RE: MS Mincho, MS Gothic fonts in PDF, displaying on Mac</header>
    <body>It's from a 3rd library called PDFTron.  I will let them know about the stickiness of Preview.  They might be able to tweak it for the next version. -----Original Message----- From: Baz [] Sent: Friday, October 05, 2007 1:52 PM To: John Kerr Cc: email@hidden Subject: Re: MS Mincho, MS Gothic fonts in PDF, displaying on Mac fonts? my app Just recently in the cairo project a similar bug was fixed where embedded CFF fonts wouldn't display in Preview but they worked in Acrobat, xpdf, etc. Cairo was generating these embeddings, and we found that the pieces of the embedded font had to be written in a particular order for Preview to work; other viewers weren't so picky. I've seen other cases where font embeddings don't work in preview too, eg this: ... which appears to be an independent bug, don't know the cause. If you have control of the generator on windows, you could maybe take a look at the cairo bugfix for inspiration, or hope that the Preview bug gets fixed. Out of curiosity - what generated your file? Cheers, Baz PS: if anyone from apple is curious about the Preview bug triggered by cairo, the commit that fixed it is here: a5a162d4d1d9ac83413233 and our bug report with sample files attached: We cracked this when we found an example of a similar embedding that works:</body>
  </mail>
  <mail>
    <header>Re: MS Mincho, MS Gothic fonts in PDF, displaying on Mac</header>
    <body>Just recently in the cairo project a similar bug was fixed where embedded CFF fonts wouldn't display in Preview but they worked in Acrobat, xpdf, etc. Cairo was generating these embeddings, and we found that the pieces of the embedded font had to be written in a particular order for Preview to work; other viewers weren't so picky. I've seen other cases where font embeddings don't work in preview too, eg this: ... which appears to be an independent bug, don't know the cause. If you have control of the generator on windows, you could maybe take a look at the cairo bugfix for inspiration, or hope that the Preview bug gets fixed. Out of curiosity - what generated your file? Cheers, Baz PS: if anyone from apple is curious about the Preview bug triggered by cairo, the commit that fixed it is here: and our bug report with sample files attached: We cracked this when we found an example of a similar embedding that works:</body>
  </mail>
  <mail>
    <header>Re: Average luminance of an image</header>
    <body>what about affine-tranform the image to 1x1 pixel and take the values from that pixel? i'm not at all an expert, i'm just curious if that might actually give any relevant results.. (and speed things up a bit) . a</body>
  </mail>
  <mail>
    <header>MS Mincho, MS Gothic fonts in PDF, displaying on Mac</header>
    <body>font-family:Arial'&gt;Has anyone noticed problems displaying PDF&amp;#8217;s, on the Mac, that were generated on Windows and contain embedded MS Mincho or MS Gothic fonts? font-family:Arial'&gt; font-family:Arial'&gt;I am seeing scrambled text for Mincho and missing text for Gothic, in my app and the Preview app. font-family:Arial'&gt; font-family:Arial'&gt;Is there some characteristic about these fonts that&amp;#8217;s special?</body>
  </mail>
  <mail>
    <header>Re: Average luminance of an image</header>
    <body>It depends on the source profile of the image. You'll have to take the adaptation matrix from your RGB color space to the XYZ one. In your case and using a CIFilter, and as far as I remember, the RGB space of the CIFilter comes already with the components linearized (not gamma compensated). Then you just have to make a linear combinaison of the RGB with the coefficients of the matrix that leads to the Y component. (for this you can find the matrix dependent on your source profile with the Bradford chromatic adaptation). you can transform it to L* by simply raising it to power 1/3 (not completely true near the origin Lab speaking, but the approximation is good enough). For that, I guess there is a CIFilter for gamma that will run in hardware. If you want a value that does not suffer too much from aliasing, you'll have to iterate on each pixel anyway. If you only need an approximation, averaging the green channel should be good enough, but you'll have to linearize (degamma) the components before that to have a &amp;quot;correct average&amp;quot;.</body>
  </mail>
  <mail>
    <header>Average luminance of an image</header>
    <body>Hi, Does anyone know of a CI filter or a vImage_ function that I can use to quickly calculate the overall &amp;quot;brightness&amp;quot; or &amp;quot;light level&amp;quot; of a given image? I am currently iterating over the raw pixel data myself and this works but I have a feeling there is an easier way. TIA, CxT</body>
  </mail>
  <mail>
    <header>Re: xml to quartz</header>
    <body>If I found the right hit in a google search for &amp;quot;ganglia&amp;quot; &amp;lt;http:// ganglia.sourceforge.net/&amp;gt;, the xml file is just a data file describing some values. I would assume your instructor wants you to plot these values in a window in some sort of line graph or bar graph, such as seen here &amp;lt;&amp;gt;? If so, drawing it is easily accomplished with Quartz. Check the documentation for CGContextRef and you'll see routines for drawing lines and rectangles. As for parsing the xml, well, xml is just a markup language (eXtensible Markup Language), but the actual keywords and values used within each file are completely up to the creator. You'll learn more if you figure out how to parse it on your own. Check the Ganglia site above and see if they have a schema file for their xml format. Schema files describe what keywords are available and how their data values are read. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: xml to quartz</header>
    <body>I have a school project, which is to create a grid system using ganglia, then to extract the xml file output for the quartz to render it/ represent the data in some form. How can this be done? Thx Noob qn: can quartz read/parse xml file directly? If not, then how do I pass the xml into quartz? Thx</body>
  </mail>
  <mail>
    <header>Crash in CGPSConverterConvert</header>
    <body>From the crashlog, it seems that the converter tries to return some error message, but somehow fails and crashes: Host Name:      CN487-MacBook-2084 Date/Time:      2007-11-07 15:16:42.359 +0100 OS Version:     10.4.10 (Build 8R2218) Report Version: 4 Command: CDFinder Path:    /Applications/CDFinder 5.0.1/CDFinder.app/Contents/MacOS/CDFinder Parent:  WindowServer [58] PID:    558 Thread: 3 Exception:  EXC_BAD_ACCESS (0x0001) Codes:      KERN_PROTECTION_FAILURE (0x0002) at 0x00000000 Thread 3 Crashed: 0   libnserverlite.dylib           	0x96b16168 CantHappenCB + 3 1   libnserverlite.dylib           	0x96b121c0 FinishExportAbort + 108 2   libnserverlite.dylib           	0x96b15fe4 YieldCB + 36 3   libnserverlite.dylib           	0x96b81f4c PSRunJobInternal + 337 4   libnserverlite.dylib           	0x96b82453 PSRunJob + 273 5   libnserverlite.dylib           	0x96b164a1 PSRIPRunAProcJob + 152 6   libnserverlite.dylib           	0x96b0cbdd NormalizerServerRunJob + 832 7   ...normalizer.privateframework 	0x969fd820 doNormalizeWithMutex + 4413 8   ...normalizer.privateframework 	0x969fd970 normalize + 215 9   libCGNormalizer.A.dylib        	0x9a3c3fc7 normalize_from_data + 112 10  com.apple.CoreGraphics         	0x904e5202 sync_convert_to_url + 177 11  com.apple.CoreGraphics         	0x904e4f21 wrap_consumer + 256 12  com.apple.CoreGraphics         	0x904e52bb sync_convert + 42 13  com.apple.CoreGraphics         	0x904e3ab3 CGPSConverterConvert + 39 14  CDFinder                       	0x00069ed4 0x1000 + 429780 I have tried to find out what is going on, but since that crash happened at a customer site, and the customer has a confidentiality problem with giving me the EPS files that causes the crash, I am a bit stuck. Is there anything I could do wrong what can cause this problem? Anything I could improve? Must I provide some error callback? I only provide a progress callback in CGPSConverterCallbacks, all others are set to NULL... Norbert M. Doerner CEO, West-Forest-Systems In der Trift 13 56459 Langenhahn, Germany Fax: +49 (2663) 91 70 126 ----------------------------------------------------- -----------------------------------------------------</body>
  </mail>
  <mail>
    <header>Re: Quick Look API's</header>
    <body>On Nov 19, 2007, at 6:29 AM, Scott Thompson wrote: I filed a bug report on this a few months ago.  In the meantime, I'm using qlmanage -p /path/to/file with NSTask for this.  It's not very flexible, but it's easy and it works pretty well. -- adam</body>
  </mail>
  <mail>
    <header>Re: CGContextDrawShading performance problem on 10.5</header>
    <body>On Nov 19, 2007, at 6:27 AM, Scott Thompson wrote: Since there was never a performance issue on 10.4 (AFAIK), I never looked at optimizing it :).  That sounds like a reasonable approach, though, so it might be time to learn about CGLayer.  I could also try using NSGradient on 10.5, but then things get messy. thanks, adam</body>
  </mail>
  <mail>
    <header>Re: Quick Look API's</header>
    <body>Search the following release note for HUD.</body>
  </mail>
  <mail>
    <header>Re: CGContextDrawShading performance problem on 10.5</header>
    <body>On Nov 19, 2007, at 1:20 AM, Adam R. Maxwell wrote: Have you given any thought to caching the gradient into a simple bitmap or CGLayer and using that repeatedly instead of asking Core Image to regenerate the image every time? Scott</body>
  </mail>
  <mail>
    <header>Re: CGContextShowTextAtPoint() Help</header>
    <body>On Nov 18, 2007, at 12:11 PM, Shree wrote: Where to begin... First of all, you really should not try to use the Core Graphics Context APIs to draw text. The drawing routines in Core Graphics are there to support higher level text systems (like ATSUI, Cocoa Text, and CoreText) and don't really have the right functionality for general use. If you have very specific drawing needs (i.e. you are creating your own layout engine) then you might use these text routines, but in general, you should avoid them. Since your application is being written to 10.4, CoreText will not be available to you. The most direct replacement I can think of for DrawString is probably HIThemeDrawTextBox or something along those lines. If your needs are more complex than that, you will probably have to look into using ATSUI directly. Now, as to why your code is not printing text. Because the CGContext routines are very primitive, there are a lot of details you will likely have to take care of. First of all, as others have pointed out, you will need a valid CGContext. In a Carbon application, that context will likely come out of a kEventControlDraw Carbon Event. Once you have that context, you will need to set the context's font and font size. You may also have to set up the fill color (although it may default to black).</body>
  </mail>
  <mail>
    <header>CI intermediary renders are 16 float PBuffers - can this be changed?</header>
    <body>I'm using Core Image with an OpenGL context and have traced what is done behind the scenes using the OpenGL Profiler. For filter chains that require multiple passes CI is using PBuffers for the &amp;quot;ping-ponging&amp;quot;. According to the OpenGL Profiler, the PBuffers are created as 16 bit float on Leopard: CGLCreatePBuffer(768, 576, GL_TEXTURE_RECTANGLE_EXT, Is there any way to change this CI behavior - how can I get 32 bit floats, or 16 bit ints? Cheers Mark</body>
  </mail>
  <mail>
    <header>CGContextDrawShading performance problem on 10.5</header>
    <body>I have a user who discovered that scrolling speed of our app has degraded significantly after installing 10.5.  This is on recent hardware (MacBook Pro &amp;amp; Mac Pro).  It works fine on my systems, but they're both 3 year old PPCs.  I'm planning to file a bug report, but wondered if anyone has seen a similar problem. We're using a CILinearGradient filter to draw a CIImage as a gradient fill for an NSView background.  Here's a sample that leads me to believe the gradient is what's causing problems (and the user reports that hiding that gradient view speeds up drawing again). Sampling process 8804 for 3 seconds with 1 millisecond of run time between samples Sampling completed, processing symbols... Call graph: 1620 Thread_2503 1620 start 1620 _start 1620 main 1620 NSApplicationMain 1620 -[OAApplication run] 1620 -[NSApplication run] 1492 -[NSApplication nextEventMatchingMask:untilDate:inMode:dequeue:] 1490 _DPSNextEvent 1488 BlockUntilNextEventMatchingListInMode 1488 ReceiveNextEventCommon 1488 RunCurrentEventLoopInMode 1487 CFRunLoopRunInMode 1487 CFRunLoopRunSpecific 1206 __CFRunLoopDoObservers 1058 _handleWindowNeedsDisplay 1057 -[NSWindow displayIfNeeded] 1057 -[NSView displayIfNeeded] 1050 -[NSView _displayRectIgnoringOpacity:isVisibleRect:rectIsVisibleRectForView:] 1033 -[NSThemeFrame _recursiveDisplayRectIfNeededIgnoringOpacity:isVisibleRect:rectIsVisibleRectForView:topView :] 1031 -[NSView _recursiveDisplayRectIfNeededIgnoringOpacity:isVisibleRect:rectIsVisibleRectForView:topView :] 1003 -[NSView _recursiveDisplayRectIfNeededIgnoringOpacity:isVisibleRect:rectIsVisibleRectForView:topView :] 987 -[NSView _recursiveDisplayRectIfNeededIgnoringOpacity:isVisibleRect:rectIsVisibleRectForView:topView :] 972 -[NSView _recursiveDisplayRectIfNeededIgnoringOpacity:isVisibleRect:rectIsVisibleRectForView:topView :] 938 -[NSView _recursiveDisplayRectIfNeededIgnoringOpacity:isVisibleRect:rectIsVisibleRectForView:topView :] 903 -[NSView _recursiveDisplayRectIfNeededIgnoringOpacity:isVisibleRect:rectIsVisibleRectForView:topView :] 772 -[NSView _recursiveDisplayAllDirtyWithLockFocus:visRect:] 762 - [NSView _drawRect:clip:] 761 - [BDSKStatusBar drawRect:] 750 - [NSBezierPath(BDSKGradientExtensions) fillPathVerticallyWithStartColor:endColor:] 750 - [NSBezierPath(BDSKGradientExtensions) fillPathVertically:withStartColor:endColor:] 747 -[CIContext drawImage:atPoint:fromRect:] 747 -[CIContext drawImage:inRect:fromRect:] 747 -[CICGContextImpl render:] 746 CGContextDrawShading 746 ripc_DrawShading 623 ripc_RenderImage 623 ripl_BltImage 321 ripd_Mark 321 argb32_image 318 _blt_image_initialize 317 ripc_CoreImageAccess 317 cgxcoreimage_instance_wait 314 glReadPixels_Exec 309 gldAllocVertexBuffer 299 gldAllocVertexBuffer 257 IOConnectCallMethod 257 io_connect_method 256 mach_msg 256 mach_msg_trap 256 mach_msg_trap 1 io_connect_method 20 __memcpy 20 __memcpy 20 gldUpdateDispatch 20 io_connect_map_memory 19 mach_msg 19 mach_msg_trap 19 mach_msg_trap 1 io_connect_map_memory 1 gldAllocVertexBuffer 1 gldAllocVertexBuffer 1 gldAllocVertexBuffer 6 glgProcessPixelsWithProcessor 6 glgCopyRowsWithMemCopy(GLGOperation const*, unsigned long, GLDPixelMode const*) 6 __memcpy 6 __memcpy 3 valloc 3 malloc_zone_valloc 3 large_and_huge_malloc 3 mmap 3 mmap 1 free</body>
  </mail>
  <mail>
    <header>Re: Quick Look API's</header>
    <body>Search the following release note for HUD.</body>
  </mail>
  <mail>
    <header>Re: Quick Look API's</header>
    <body>On Nov 18, 2007, at 1:21 PM, Joe Ranieri wrote: Search the following release note for HUD. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Quick Look API's</header>
    <body>There is no public API to do this - I'd recommend filing a request with radar. Joe Ranieri Lead Magician, Alacatia Labs, Inc.</body>
  </mail>
  <mail>
    <header>Re: CGContextShowTextAtPoint() Help</header>
    <body>Two rather obvious mistakes here: 1) The space before the n in strlen should cause a syntax error for the compiler. 2) The variable context is uninitalized. You need to pass in a valid CGContextRef. The answer depends on the type of application you have. For a Carbon GUI app, you would register a callback for a Other possible problems: 3) You probably did not set the Font (using CGContextSelectFont). Without setting the font Quartz has no way to know which glyphs to draw and how to interpret the encoding of the bytes you passed in. 4) Why are you passing strlen(str) - 1 instead of strlen(str)? strlen gives you the length of the C-string not counting the zero termination. So unless you really want drop the last character (&amp;quot;t&amp;quot;) you don't need to subtract anything. Aside from the above problems your approach is probably wrong. It is probably not a good idea to port your code 1:1 from Mac OS 9 to Mac OS X because some things are handled differently on Mac OS X. You don't give enough contect though to give good advice here. -- Mike Fischer     Softwareentwicklung, EDV-Beratung Schulung, Vertrieb Address: Bundesstrasse 9, D-20146 Hamburg, Germany Phone: +49 (0)40/45038886, Fax: +49 (0)40/45038887</body>
  </mail>
  <mail>
    <header>Quick Look API's</header>
    <body>Sorry if this is off-topic. I can't seem to locate the Apple docs and API's regarding the new look (black) windows in Leopard. Any idea's which header files I should look in? -- Thomas C.</body>
  </mail>
  <mail>
    <header>Re: Quartz Debug window list is empty</header>
    <body>It's the one that came with the Leopard GM.  It says it is 3.0. That works.  Thanks. I just tried another machine, a Mac mini, and Quartz Debug's window list works fine there.  The machine it's having problems on is an original Core Duo MacBook Pro 15 that seems healthy otherwise.  I ran the mini's copy of Quartz Debug on the MacBook Pro and it didn't work either.  Both machines are running 10.5.1.  The mini is running Server, the MacBook Pro is running Client. I'll file a bug and submit a system profile.  Troubleshooting suggestions are welcome.</body>
  </mail>
  <mail>
    <header>Re: Core Image: Average an image</header>
    <body>that is correct; the CIAreaAverage filter will compute the average color over a rectangular area on the GPU. That filter is new for Leopard; if you need that functionality on Tiger, it just so happens that the 'GPU Gems 3' book has the source for a Core Image filter which essentially implements that operation... - Ralph</body>
  </mail>
  <mail>
    <header>Re: Quartz Debug window list is empty</header>
    <body>First, are you sure your running the correct version of Quartz Debug? Another check you can make on this is to try running the Son of Grab index.html&amp;gt; to see if you get a window list there. -- David Duncan Apple DTS Animation and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Re: Core Image: Average an image</header>
    <body>On 16 Nov 2007, at 14:50, Peter Laurens wrote: Algorithm sounds fine so far. I think the bit you're missing is CIAreaAverage. It's a filter on 10.5.</body>
  </mail>
  <mail>
    <header>Core Image: Average an image</header>
    <body>I am wanting to find the 'average' colour of an image, and need to do this very fast (for batch operations) so I have turned to Core Image. The reason is because I want a simple way to tell how similar two image are to each other, and have a single metric (in the form of a single float) to represent that. 1. Use Core Image filter to turn ImageA monochrome (CIColorMonochrome). 2. Use Core Image filter to turn ImageB monochrome (CIColorMonochrome). 3. Use Core Image filter to give me a difference image (CIDifferenceBlendMode). 4. Now I have an monochrome image (but I assume not in representation though, i.e. it's still a full 32bit image I believe) where darker areas are more similar, and lighter areas denote larger differences between the two images. 5. &amp;gt;&amp;gt;&amp;gt; Here's where I would like to use a filter to get me the Average grayscale colour of my CIImage difference image. Instead I use a costly process where I transform the CIImage into an NSBitmapImageRep and then literally loop through all pixels, asking each for its color through [[imageRep colorAtX:x y:y] getHue:NULL saturation:NULL brightness:&amp;amp;a alpha:NULL];. Just taking the brightness value. Then I average that figure (by dividing the total by the number of pixels). This is a bit of a pain, clunky and slow, surely Core Image can come to my assistance?! - Peter</body>
  </mail>
  <mail>
    <header>Quartz Debug window list is empty</header>
    <body>I just tried to look at the window list in Quartz Debug and no windows are listed.  Rebooting didn't help and there's nothing informative in any logs, not even windowserver.log.  Toggling Show settings didn't help, nor did Refresh.</body>
  </mail>
  <mail>
    <header>Re: CGRectIntersection in Leopard</header>
    <body>As an aside, depending on your usage pattern, it may be more appropriate to use CGFLOAT_MAX, which is DBL_MAX under 64-bit. (However, if you're making this value persistent, it might be better to stick to a value that can work under both 32 and 64-bit, and FLT_MAX may be reasonable there.) Ali</body>
  </mail>
  <mail>
    <header>Re: CGRectIntersection in Leopard</header>
    <body>On Nov 15, 2007, at 11:50 AM, Derek Clegg wrote: FWIW, here's a way to get a rectangle with infinite values that should be valid but is now considered null. CGRect r = CGContextConvertRectToDeviceSpace(context, Nick</body>
  </mail>
  <mail>
    <header>Re: CGRectIntersection in Leopard</header>
    <body>On Nov 15, 2007, at 11:50 AM, Derek Clegg wrote: Thanks for the explanation.  FWIW, I'm actually using FLT_MAX already but if I do math with the rectangle values I can end up with inf (e.g., 2*FLT_MAX = inf).  That's when my code breaks in Leopard.  Just off the top of my head (but I'm probably wrong), it doesn't seem like you'd need to test for a null rectangle within an intersection function since the infinite values would tend to take care of that automatically. Nick</body>
  </mail>
  <mail>
    <header>Re: CGRectIntersection in Leopard</header>
    <body>CGRectIntersection has changed the way it works with infinite values in Leopard. For example consider intersection with a rectangle infinitely wide where 0 &amp;lt; y &amp;lt; 1: Prior to Leopard the intersection produces the expected results, in Leopard the intersection produces CGRectNull. Is this by design or a bug?</body>
  </mail>
  <mail>
    <header>CGRectIntersection in Leopard</header>
    <body>CGRectIntersection has changed the way it works with infinite values in Leopard. For example consider intersection with a rectangle infinitely wide where 0 &amp;lt; y &amp;lt; 1: Prior to Leopard the intersection produces the expected results, in Leopard the intersection produces CGRectNull. Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: Spaces API?</header>
    <body>You can get some information about the current Spaces configuration by reading properties from com.apple.dock.plist and com.apple.symbolichotkeys.plist.  The former will tell you how many rows and columns of spaces there are, and any applications that are configured to a specific space.  The latter can tell you which hotkeys activate spaces.  However, I'm not sure how or if it's possible to know which window is in which space, or what space is the current one. On Wed, 14 Nov 2007 08:12:30 -0500, Thomas Lazar &amp;lt;email@hidden&amp;gt; wrote:</body>
  </mail>
  <mail>
    <header>CGShading Bad Behavior in Leopard</header>
    <body>I've fixed this in my app but perhaps I can save someone else a few hours of puzzlement. Leopard has introduced some unfortunate behavior in shadings. To recap a bit, to draw a shading you: The problem is that, with Leopard, the results of running the interpolation function are cached: The function is only run the first time you draw the shading. The results are then cached somewhere and the cached results are used in subsequent drawing operations. In Tiger and before, the interpolation function is run every time you draw. (This was verified by putting a print statement in the interpolation function.) Very nice and efficient I suppose, except that among the things you can pass in when creating the CGFunctionRef is a blind pointer to data that is passed to your interpolation function when it is called. There is a requirement that this storage continue to exist, but no requirement (nor should there be) that the contents of this storage be immutable. For any application beyond a toy application or a static UI gradient, say one in which a user might be editing the gradient, you would *expect* this data to change between executions of CGContextDrawShading. To make matters worse, releasing the CGShadingRef and getting another with the same GFunctionRef doesn't invalidate the cache. To make matters much worse, releasing the CGShadingRef, releasing the GFunctionRef, and the getting a new GFunctionRef with the same interpolation function doesn't invalidate the cache (!).  If you have a general interpolation function that works off the data you are supposed to have a separate copy of the function with a different name for each set of data ??? * ask for a new CGFunctionRef with the same interpolation function without releasing the old one This is *really* bad behavior. As a matter of personal taste, I'd prefer an &amp;quot;opt in&amp;quot; arrangement for something like this, but at a minimum, if you're going to do this you have to provide a way of invalidating the cache:  CGFunctionRefInvalidateCache( CGFunctionRef ref) or something. Robert Clair</body>
  </mail>
  <mail>
    <header>Spaces API?</header>
    <body>-----BEGIN PGP SIGNED MESSAGE----- Hash: SHA1 Hi. I don't know if it's the right place to ask this, but is there a API for accessing information of how spaces is configured? Like how much spaces there are or how are they arranged or perhaps what windows are in the different spaces. I ask because there is one feature in spaces that is missing for me. A non full-screen pager, that's floating on the desktop and is easy accessible and can change spaces with just one click. If i can find the time i'd very much like to develop one. Regards Thomas - -- HUMAN BEINGS MAKE LIFE SO INTERESTING. DO YOU KNOW THAT IN A UNIVERSE SO FULL OF WONDERS, THEY'VE MANAGED TO INVENT BOREDOM? QUITE ASTONISHING. [Death] homepage: public key: -----BEGIN PGP SIGNATURE----- Version: GnuPG v1.4.7 (MingW32) Comment: Using GnuPG with Mozilla - iD8DBQFHOvQ+BN1/A7AADvwRAn8EAJsHCaJQq29ZlR3TUZrgF0srgpl2CgCcCQ00 v9UOORkT1FBc+c2e4HYDHEs= =+3Nx -----END PGP SIGNATURE-----</body>
  </mail>
  <mail>
    <header>Kernel crashes in core image filter code</header>
    <body>So a client who has upgraded to Leopard is having his video system crash I think during some core image filtering.   Does anyone know how I should start looking at debugging this kind of thing?  I am getting a machine set up to run leopard in the next day or so.  It's a MacBook Pro if that helps anyone give me any pointers. Nov 13 18:00:34 Origin kernel[0]: NVChannel(GL): Graphics channel exception!  status = 0xffff info32 = 0xd = GR: SW Notify Error Nov 13 18:00:34 Origin kernel[0]: 0000000c Nov 13 18:00:34 Origin kernel[0]: 00200000 00005097 0000047c 00000000 Nov 13 18:00:34 Origin kernel[0]: 00000486 00001b0c 1000f010 00000003 Nov 13 18:00:34 Origin kernel[0]: 00000000 00000000 01100011 Nov 13 18:00:34 Origin kernel[0]: 0000000c Nov 13 18:00:34 Origin kernel[0]: 00200000 00005097 0000047c 00000000 Nov 13 18:00:34 Origin kernel[0]: 00000486 00001b0c 1000f010 00000003 Nov 13 18:00:34 Origin kernel[0]: 00000000 00000000 01100011 Nov 13 18:00:34 Origin kernel[0]: NVChannel(GL): Graphics channel exception!  status = 0xffff info32 = 0x3 = Fifo: Unknown Method Error Nov 13 18:00:34 Origin kernel[0]: 0000000b Nov 13 18:00:34 Origin kernel[0]: NVChannel(GL): Graphics channel exception!  status = 0xffff info32 = 0x3 = Fifo: Unknown Method Error Nov 13 18:00:34 Origin kernel[0]: 0000000b Nov 13 18:00:34 Origin kernel[0]: NVChannel(GL): Graphics channel exception!  status = 0xffff info32 = 0x3 = Fifo: Unknown Method Error Nov 13 18:00:34 Origin kernel[0]: 0000000b Nov 13 18:00:34 Origin kernel[0]: NVChannel(GL): Graphics channel exception!  status = 0xffff info32 = 0x3 = Fifo: Unknown Method Error Nov 13 18:00:34 Origin kernel[0]: 0000000b Nov 13 18:00:34 Origin kernel[0]: NVChannel(GL): Graphics channel exception!  status = 0xffff info32 = 0x3 = Fifo: Unknown Method Error Nov 13 18:00:34 Origin kernel[0]: 0000000b</body>
  </mail>
  <mail>
    <header>NSAlphaNonpremultipliedBitmapFormat no longer OK for Quartz	contexts?</header>
    <body>Maybe someone can answer if this is a Leopard feature or a bug: the following code generates a runtime error: kCGImageAlphaLast; 416 bytes/row. // *** NSBitmapImageRep* bitmap = [[NSBitmapImageRep alloc] initWithBitmapDataPlanes: NULL pixelsWide: bmSize.width pixelsHigh: bmSize.height bitsPerSample: 8 samplesPerPixel: 4 hasAlpha: YES isPlanar: NO colorSpaceName: NSDeviceRGBColorSpace bitmapFormat: NSAlphaNonpremultipliedBitmapFormat bytesPerRow: 0 NSGraphicsContext* context = [NSGraphicsContext // *** Replacing &amp;quot;bitmapFormat: NSAlphaNonpremultipliedBitmapFormat&amp;quot; with &amp;quot;bitmapFormat: 0&amp;quot; works, so that is definitely the source of the error. Using NSAlphaNonpremultipliedBitmapFormat works in Tiger, so I was quite surprised it didn't in Leopard, is this a bug, or are non-premultiplied bitmap contexts no longer supported for Quartz drawing? Cheers, - D.G.</body>
  </mail>
  <mail>
    <header>Re: Problem with keyboard events, event taps, and Leopard</header>
    <body>wrote: That's a good utility for seeing this bug happen.  I've been able to figure out that at least the problem I posted about earlier happens only if Opera is running.  More specifically, if any of the following circumstances are true: 1. Opera is frontmost 2. Opera has no open windows 3. All of Opera's windows are minimized I don' think any other application than Opera causes this behavior.  It's likely that this bug isn't directly the fault of the Opera developers, since it seems like it stems from an issue in Quartz event services.  Then again, Opera has always interacted with the rest of Mac OS X a bit weird, so there might be a way to work around the issue on their end.  I've brought it to the attention of the Opera developers, but is this something that should be submitted to the Apple bug tracker? - Brian</body>
  </mail>
  <mail>
    <header>Re: CALayer color properties need to be released?</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CoreImage Un-Explained Border</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Problem with keyboard events, event taps, and Leopard</header>
    <body>Here's a problem I found... In Leopard, it is supposed to be possible to assign a Unicode string to a keyboard event with CGEventKeyboardSetUnicodeString, and not worry about the key code.  This works except when there's a Carbon application and a Unicode keyboard layout.  (Bug filed.) -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: CoreImage Un-Explained Border</header>
    <body>On 12 Nov 2007, at 20:29, Mike Miller wrote: The only way I've ever seen to set that key (which is what I was referring to in my other mail) is if you write your own filter, as you set it when you instance the CISampler. Maybe the best way would be to write a pass-through filter, that didn't do anything, but did this right for your case. As far as I'm concerned this is an API bug (which I should probably report), as I think it needs to be up one level, accessible from the CIFilter. I can only guess that QC uses an API that isn't documented to do the same thing.</body>
  </mail>
  <mail>
    <header>Re: Problem with keyboard events, event taps, and Leopard</header>
    <body>My event recording application that uses event taps also appears to be broken under Leopard. Keyboard events seem to record correctly but put out garbage when played back. I also see other weird event flattening/ unflattening errors that I can't explain. Will need to track this down some more but there are definitely problems with Leopard and event taps right now. Same sources work fine on Tiger. Doug Hill Oracle Corp.</body>
  </mail>
  <mail>
    <header>Re: Creating Oil Paint/Water Brushes</header>
    <body>Andy Finnell wrote a water color sample app for a presentation a while back, using OpenCL: Though I can't find his original post on it, his website is here: -gus</body>
  </mail>
  <mail>
    <header>Re: Creating Oil Paint/Water Brushes</header>
    <body>So, you're just looking for the ability to draw with a pattern? In Cocoa, use +[NSColor colorWithPatternImage:] along with one of the -[NSColor set‚Ä¶] methods.  Then, you can use NSBezierPath to fill or stroke with that pattern. Using CoreGraphics directly, you would use CGPattern.  See the Quartz 2D Programming Guide: Patterns &amp;lt;&amp;gt;. Regards, Ken</body>
  </mail>
  <mail>
    <header>Creating Oil Paint/Water Brushes</header>
    <body>DISCLAIMER ========== This e-mail may contain privileged and confidential information which is the property of Persistent Systems Ltd. It is intended only for the use of the individual or entity to which it is addressed. If you are not the intended recipient, you are not authorized to read, retain, copy, print, distribute or use this message. If you have received this communication in error, please notify the sender and delete all copies of this message. Persistent Systems Ltd. does not accept any liability for virus infected mails.</body>
  </mail>
  <mail>
    <header>Summation of rows and columns,	similar to CIRowAverage and CIColumnAverage</header>
    <body>Dear list, I'm  running code on the CPU that sums columns of a grey scale NSImage to find the total intensity along the line (I'm also doing the same with the rows). I would like to move this process to the GPU. This is conceptually similar to the CIFilter CIColumnAverage. However, because summing can easily overflow the 255 bit per pixel I'm wondering how and if I can implement this in Quartz? Alternatively what would you recommend if Quartz is not a good fit? Best wishes, Dan</body>
  </mail>
  <mail>
    <header>Weak performance of CGEventPost under GPU load</header>
    <body>Hi! We've stumbled upon a performance problem with Quartz Events, more specifically CGEventPost: during heavy GPU load CGEventPost can block. We've created a small benchmark application to demonstrate the issue: This application is just a loop that creates, post and releases events. You can see the results below. The first run is on an idle system. The second run is with FurMark (GPU stress test) with the dials cranked up as much as possible. ‚Äî Inner is how long the inner loop takes, basically just creating, posting and releasing an event with Quartz Events. ‚Äî Outer is how long our program is waiting to be woken up (a sleep). Should be close to the time we sleep, but if the system is under pressure it could be delayed. ‚Äî Post is how long the event post takes. ----------------------------------------------------------------------- 18:58:01.683 EventPerformance[4946:707] Measurements: (outer should be close to 10) 18:58:01.684 EventPerformance[4946:707] inner (ms): 0.04, outer (ms): 11.02, CGEventPost (ms): 0.03 18:58:01.684 EventPerformance[4946:707] inner (ms): 0.04, outer (ms): 11.02, CGEventPost (ms): 0.03 18:58:01.685 EventPerformance[4946:707] inner (ms): 0.07, outer (ms): 10.26, CGEventPost (ms): 0.03 18:58:01.685 EventPerformance[4946:707] inner (ms): 0.06, outer (ms): 10.85, CGEventPost (ms): 0.05 18:58:01.686 EventPerformance[4946:707] inner (ms): 0.07, outer (ms): 10.41, CGEventPost (ms): 0.04 18:58: 01.686 EventPerformance[4946:707] inner (ms): 0.04, outer (ms): 10.39, CGEventPost (ms): 0.03 18:58:01.686 EventPerformance[4946:707] inner (ms): 0.05, outer (ms): 11.02, CGEventPost (ms): 0.03 18:58:01.687 EventPerformance[4946:707] inner (ms): 0.03, outer (ms): 10.67, CGEventPost (ms): 0.03 18:58:01.687 EventPerformance[4946:707] inner (ms): 0.08, outer (ms): 10.09, CGEventPost (ms): 0.05 18:58:01.688 EventPerformance[4946:707] Averages: (outer should be close to 10) 18:58:01.688 EventPerformance[4946:707] avg inner (ms): 0.05, avg outer (ms): 10.64, avg post (ms): 0.03 ----------------------------------------------------------------------- Here we can see that posting the event takes about 0.03 ms on average. Also the thread seems to be woken up around 0.5ms too late. No spikes in CGEventPost. ----------------------------------------------------------------------- 19:02:02.150 EventPerformance[5241:707] Measurements: (outer should be close to 10) 19:02:02.151 EventPerformance[5241:707] inner (ms): 0.03, outer (ms): 10.23, CGEventPost (ms): 0.02 19:02:02.151 EventPerformance[5241:707] inner (ms): 0.02, outer (ms): 10.54, CGEventPost (ms): 0.02 19:02:02.151 EventPerformance[5241:707] inner (ms): 0.02, outer (ms): 11.01, CGEventPost (ms): 0.01 19:02:02.152 EventPerformance[5241:707] inner (ms): 0.02, outer (ms): 10.74, CGEventPost (ms): 0.01 19:02:02.152 EventPerformance[5241:707] inner (ms): 0.02, outer (ms): 10.20, CGEventPost (ms): 0.01 19:02:02.152 EventPerformance[5241:707] inner (ms): 10.35, outer (ms): 11.01, CGEventPost (ms): 10.35 19:02:02.152 EventPerformance[5241:707] inner (ms): 0.03, outer (ms): 10.02, CGEventPost (ms): 0.02 19:02:02.153 EventPerformance[5241:707] inner (ms): 58.90, outer (ms): 10.11, CGEventPost (ms): 58.90 19:02:02.153 EventPerformance[5241:707] inner (ms): 0.03, outer (ms): 10.12, CGEventPost (ms): 0.02 19:02:02.153 EventPerformance[5241:707] Averages: (outer should be close to 10) 19:02:02.371 EventPerformance[5241:707] avg inner (ms): 7.71, avg outer (ms): 10.44, avg post (ms): 7.71 ----------------------------------------------------------------------- When the system is under heavy GPU load, posting an event can take (spikes) milliseconds instead of microseconds. Under extreme GPU stress (&amp;lt; 1 FPS), this value can take seconds. CGEventPost _sometimes_  seems to be waiting for the GPU to finish some work before returning. Our thread is still scheduled normally with no noticeable delay/spikes (outer). Any ideas are appreciated. Thanks! Eugene / Dae</body>
  </mail>
  <mail>
    <header>Re: CARenderer to NSOpenGLContext  not working</header>
    <body>Why not use the AVVideoCompositionCoreAnimationTool instead? -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>CARenderer to NSOpenGLContext  not working</header>
    <body>Hi, I want to render a Core Animation Layer Animation with CARenderer offscreen into a bitmap and from there into AVFoundation CVPixelBufferRefs to make a movie. The problem is that the bitmap shows grey only or shows an image of the background window. Does anybody have some experience wit CARenderer to NSOpenGLContext and could show me what I am doing wrong ? Or has a working example‚Ä¶ This is the code I am using so far : -(void)saveSnapshotToFileTEST2:(NSString*)filePath NSOpenGLPFAAllowOfflineRenderers, kCGLPFADepthSize, (CGLPixelFormatAttribute)24, (CGLPixelFormatAttribute)0 // fbo // Texture // Test // Handle errors [glcontext makeCurrentContext]; // ? // Render // output //glPopAttrib();  // ? NSBitmapImageRep* bitmap = [[NSBitmapImageRep alloc] initWithBitmapDataPlanes:NULL pixelsWide:imageWidth pixelsHigh:imageHeight bitsPerSample:8 samplesPerPixel:4 hasAlpha:YES isPlanar:NO colorSpaceName:NSDeviceRGBColorSpace bitmapFormat:NSAlphaFirstBitmapFormat bytesPerRow:imageWidth * 4 // Read pixel data glReadPixels(0, 0, imageWidth, imageHeight, GL_BGRA, GL_UNSIGNED_BYTE,</body>
  </mail>
  <mail>
    <header>CIImage with texture</header>
    <body>Hello, I have a problem with CIImage created from OpenGL texture. I am creating CIImage with texture using imageWithTexture method. After that I am applying various filters to generated CIImage, and use filters output image further. Is there is some way to know when no CIImages use my generated texture, so I can safely delete texture? Rolan</body>
  </mail>
  <mail>
    <header>Re: Problems with PresentationLayer being cached?</header>
    <body>Hey all.. So we are running into some interesting issues with what appears to be the caching of the presentation layer, and we are wondering if other people are seeing this, and what workarounds we can do. We have a couple of our own properties that we are implicitly animating. We are doing this by declaring them as properties and synthesizing them dynamically: @interface GOOLayer : CALayer @end @implementation GOOLayer In most cases this works fine, and we see the appropriate values being logged. In the first case, we get proper implicit animation: Output: 2012-12-19 10:10:41.440 CoreAnimation[94149:c07] Display GooValue: 1804289408.000000 2012-12-19 10:10:41.502 CoreAnimation[94149:c07] Display GooValue: 1804289408.000000 ... 2012-12-19 10:10:41.739 CoreAnimation[94149:c07] Display GooValue: 898766464.000000 2012-12-19 10:10:41.757 CoreAnimation[94149:c07] Display GooValue: 846930880.000000 First time through: 2012-12-19 10:11:16.208 CoreAnimation[94149:c07] newValue = 1714636928.000000 2012-12-19 10:11:16.209 CoreAnimation[94149:c07] Display GooValue: 1714636928.000000 Second time through: 2012-12-19 10:11:26.258 CoreAnimation[94149:c07] newValue = 1957747840.000000 2012-12-19 10:11:26.259 CoreAnimation[94149:c07] Display GooValue: 1957747840.000000 It&amp;#39;s the third case in which we just call [CALayer presentationLayer] before we do the transaction that is interesting: First time through: 2012-12-19 10:12:12.505 CoreAnimation[94149:c07] presoLayer.gooValue = 1957747840.000000 newValue = 424238336.000000 2012-12-19 10:12:12.505 CoreAnimation[94149:c07] Display GooValue: 1957747840.000000 Second time through: 2012-12-19 10:12:13.905 CoreAnimation[94149:c07] presoLayer.gooValue = 424238336.000000 newValue = 719885376.000000 2012-12-19 10:12:13.906 CoreAnimation[94149:c07] Display GooValue: 424238336.000000 Not sure if this is iOS specific, but it&amp;#39;s where I have done all my testing.</body>
  </mail>
  <mail>
    <header>Forcing evaluation of CIFilters</header>
    <body>I'm having an issue with CoreImage.  First I generate a base image running a handful of image files through a few filters.  Then I use this base image as input to a few more filters to make a few individual CIImages for other uses.  The problem I'm seeing is the base image is being evaluated for each of these secondary filter operations, rather than once the first time and cached as I would hope CI would do.  So my question is, does a way exist to force evaluation of the base image into a singular CIImage before passing it off to the secondary filters?  None of the input values of the base image's filters are getting changed after I start the secondary steps.</body>
  </mail>
  <mail>
    <header>Re: python+quartz in a web app...</header>
    <body>have you taken a look at the python+cocoa bindings?  you can try using the Cocoa Text system with your CGContext to get simpler text layout.</body>
  </mail>
  <mail>
    <header>Re: CGEventCreateKeyboardEvent and CGEventPostToPSN</header>
    <body>I've seen some very strange behavior using CGEventPostToPSN, although in the case of posting simple key events it seems to work fine. Try this (which works like a champ and types five Zs for me): if( err != noErr ) for( NSInteger x=0; x&amp;lt;repeatCount; x++ ) I wonder if that by posting the modifier key event separate of the key event its supposed to modify means it can fall victim to the OS resetting the global modifier state. Someone with more CGEvent kung-fu would need to confirm that. But it seems to work as expected if you post the key event with the modifier attached. -- Jim</body>
  </mail>
  <mail>
    <header>Information on CWColorWorldGetProperty() CWColorWorldSetProperty()</header>
    <body>Is there any documentation (header file info or otherwise) on these calls? CWColorWorldGetProperty() CWColorWorldSetProperty() They are defined in CMMComponent.h as 10.5 and later but no other documentation is provided. I'd like to know what are the available keys and expected values? Aaron Alpher ColorBurst Systems</body>
  </mail>
  <mail>
    <header>Re: problems during CGContextDrawPDFPage</header>
    <body>Wild stab in the dark: Are you doing this on the main thread? If not, do you still get these errors when you do it on the main thread? H</body>
  </mail>
  <mail>
    <header>problems during CGContextDrawPDFPage</header>
    <body>Forgive me for sending this a second time, but I sent it on New Year's Eve, and I was hoping that perhaps I could get more attention if I selected a better day :) I really am hoping for a workaround...even if that workaround is to detect that I had a problem.&amp;nbsp; I have a fallback solution if I can detect that the PDF did not write correctly. ..... I have a program that reads a PDF and generates another PDF from the original.&amp;nbsp; Most of the time, everything works fine.&amp;nbsp; I have some documents that do not work properly and result is a garbled PDF.&amp;nbsp; I am not sure what is going on specifically, although this example appears related to fonts? I pulled out the relevant code that reproduces my problem.&amp;nbsp; I can reproduce this with a government form, fw9.pdf.&amp;nbsp; My platform is the iPhone. What should I do in this situation?&amp;nbsp;&amp;nbsp; Is there a way I can workaround it and still generate a readable PDF?&amp;nbsp; What about detecting potential problems so I can notify the user? I get this error in my console log during the CGContextDrawPDFPage Thank you! -alexander == LOG FILE SNIPPET == Wed Dec 30 09:27:47 Macintosh-9.local PDFTester[834] &amp;lt;Error&amp;gt;: CGFont/Freetype: The function `get_subset_format' is currently unimplemented. Wed Dec 30 09:27:47 Macintosh-9.local PDFTester[834] &amp;lt;Error&amp;gt;: can't get CIDs for glyphs for `HelveticaNeue-Roman'. Wed Dec 30 09:27:47 Macintosh-9.local PDFTester[834] &amp;lt;Error&amp;gt;: can't get CIDs for glyphs for `HelveticaNeue-Roman'. == CODE EXAMPLE == &amp;nbsp;&amp;nbsp;&amp;nbsp; // query for the number of pages &amp;nbsp;&amp;nbsp;&amp;nbsp; CGFloat width = &amp;nbsp;&amp;nbsp; // determine width height omitted (not relevant) &amp;nbsp;&amp;nbsp;&amp;nbsp; // create a url from the file path &amp;nbsp;&amp;nbsp;&amp;nbsp; // create a dictionary &amp;nbsp;&amp;nbsp;&amp;nbsp; // create a context for drawing &amp;nbsp;&amp;nbsp;&amp;nbsp; CGContextRef gc = CGPDFContextCreateWithURL ((CFURLRef)outurl, &amp;amp;outRect, myDictionary); // 5 // THIS IS WHERE I GET ERRORS IN LOG Thank you, -alexander</body>
  </mail>
  <mail>
    <header>Core Image, Core Video &amp;amp; Garbage Collection: fixed in 10.6?</header>
    <body>The release notes from 10.5 mention that using Core Image to process CV frames under garbage collection produces a memory leak: It appears that this is still the case under 10.6 (memory usage of a prototype app using CV, Core Image and garbage collection grows during movie playback) but I can't find any update (for 10.6) in the documentation. Is this still a known issue? Does anyone know if it has been fixed in 10.6(.2)? Thanks, Barry</body>
  </mail>
  <mail>
    <header>Re: using quartz from carbon</header>
    <body>Yes, you can simply use QDBeginCGContext/QDEndCGContext to wrap your Quartz drawing. -eric</body>
  </mail>
  <mail>
    <header>CALayer masksToBounds with shadow</header>
    <body>Hi all, I have run into an issue with Core Animation. It seems like a bug in CA to me. Before I file the bug, perhaps somebody can correct me if I'm wrong. I have a CALayer with rounded corners and a shadow. The CALayer has an image (background pattern) as contents. The image needs to be scaled to fit any size CALayer (kCAGravityResize), which is why I can not &amp;quot;include&amp;quot; the rounded corners in the image. In order to get rounded corners also on the contents image, I set masksToBounds = YES on the CALayer. After setting masksToBounds to YES the shadow is no longer visible (masked, it seems). Is this behavior expected? According to the compositing order illustrated in the &amp;quot;Layer Style Properties&amp;quot; section of Apple's Core Animation guide I expected masksToBounds to take effect before the shadow was added. Was this assumption wrong or should I file a bug? Many thanks, Greg _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>using quartz from carbon</header>
    <body>hi all, unfortunately because of firefox i need to use quartz along with windowref windows from carbon, we can't move to cocoa yet reading the docs, it seems like i have to use the legacy hiview to do this with interface builder windows i don't want to adopt hiview but i do need to move to quartz is there a way to just draw into a windowref window using quartz, kinda like we did with quickdraw? in other words i am trying to replace grafports with quartz in a large code base with a minimum of disruption thanks, bill appleton</body>
  </mail>
  <mail>
    <header>Re: can't stop a bitmap from being interpolated</header>
    <body>I&amp;#39;m trying to blit a B&amp;amp;W bitmap onto the screen (or printer) without anti-aliasing. ¬†This is on 10.6.x only, the same code is fine on older systems. I&amp;#39;ve boiled the problem down to a sample project. ¬†Build it, run, choose menu &amp;quot;file &amp;gt; print&amp;quot;, click &amp;lt;preview&amp;gt;, and when Preview.app opens, zoom in a few times. ¬†You&amp;#39;ll see the following ¬† ¬† ¬† ¬† I&amp;#39;ve also tried using CGContextCreateImageMask() and CGContextSetInterpolationQuality( &amp;#39;none&amp;#39; ), but without sucess. I&amp;#39;ve seen the SL Release Note, in particular where it talks of NSImage/CGImage changes. ¬† I&amp;#39;ve seen hints on the web saying that I may be need to draw on the half-pixel (that&amp;#39;s a very bad summary, I know) so the sample code draws the 2nd attempt 10.5 pixels to the right of the first. Can someone please cast me a pearl? Here&amp;#39;s the meat of the code, again, in case the list server doesn&amp;#39;t forward attachments: static void DrawImage(CGContextRef cg, float x, float y) ¬† ¬†for (int i=0;i&amp;lt;8;++i) - (void) print:(id) sender Project is here: ¬† ¬† ¬† ¬†</body>
  </mail>
  <mail>
    <header>can't stop a bitmap from being interpolated</header>
    <body>I'm trying to blit a B&amp;amp;W bitmap onto the screen (or printer) without anti-aliasing.  This is on 10.6.x only, the same code is fine on older systems. I've boiled the problem down to a sample project.  Build it, run, choose menu &amp;quot;file &amp;gt; print&amp;quot;, click &amp;lt;preview&amp;gt;, and when Preview.app opens, zoom in a few times.  You'll see the following I've also tried using CGContextCreateImageMask() and CGContextSetInterpolationQuality( 'none' ), but without sucess. I've seen the SL Release Note, in particular where it talks of NSImage/CGImage changes.   I've seen hints on the web saying that I may be need to draw on the half-pixel (that's a very bad summary, I know) so the sample code draws the 2nd attempt 10.5 pixels to the right of the first. Can someone please cast me a pearl? Here's the meat of the code, again, in case the list server doesn't forward attachments: static void DrawImage(CGContextRef cg, float x, float y) for (int i=0;i&amp;lt;8;++i) - (void) print:(id) sender Project is here:</body>
  </mail>
  <mail>
    <header>Re: mem leak in img_data_lock?</header>
    <body>That simple. Any ideas. I feel like if I could find out why img_data_lock is holding the memory (and find a way to release it), I could code it in such a way that it would work?</body>
  </mail>
  <mail>
    <header>mem leak in img_data_lock?</header>
    <body>f you display the UIView below, img_data_lock is holding on to a large chunck of memory. Additional chunks of memory are allocated for each page drawn. This memory does not appear to be released back into the wild. - ( CGAffineTransform m = That simple. Any ideas. I feel like if I could find out why img_data_lock is holding the memory (and find a way to release it), I could code it in such a way that it would work?</body>
  </mail>
  <mail>
    <header>Posting modifier keys with CGEventPost - earlier bug</header>
    <body>event1 event2 event3 event4</body>
  </mail>
  <mail>
    <header>Re: Relationship between CGBitmapContext size and -[CIContext	render:toBitmap:...]</header>
    <body>2. Does it have any performance implications (is it going to create many rendering tiles, or not), or is it better to create the CGBitmapContext closer to the actual buffer size? -- Raphael Sebbe Creaceed ‚Äî Creative iPhone &amp;amp; Mac apps ‚Ä¢‚Ä¢‚Ä¢ Twitter: ‚Ä¢‚Ä¢‚Ä¢ Web: Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CIImage to 8 bit in one easy step?</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: vImage vs. Core Image for large filter chains</header>
    <body>Rebonjour Dennis, Oh, and I almost forgot, their is a bug in 10.6.2 relating to how we loaded tiles on the CPU path (fe-context-cl-cpu). That definitely *will* make a difference when doing CI work on the CPU, especially for larger images that require lots of tiling. Then again, if you're going down the GPU path this is of no consequence to you ... And I had a typo in my text ... CPU-&amp;gt;CPU should of course have been CPU-&amp;gt;GPU. A+, Alex.</body>
  </mail>
  <mail>
    <header>Re: vImage vs. Core Image for large filter chains</header>
    <body>Hi Dennis, Generally speaking (which is to say basically &amp;quot;always&amp;quot;) you're best to let CI try and optimize things for you. Once you have given us a render graph we go through it and perform many optimizations. In your case you will likely end up with many passes (which is to say we won't be able to evaluate everything in a single pass or &amp;quot;inline&amp;quot;) and in very rare instances we don't chose the absolute best method for deciding where to &amp;quot;break up&amp;quot; your render graph and might end up processing more data than needed but that's rarely the case (I've only seen it prop up once in reality). You are correct in your statement about the CPU-&amp;gt;CPU upload occurring only once. Unless you are doing some sort of histogram work this is indeed the case. But ... if you're using vImage then perhaps you are doing histogram work and that code path in CI could use a little TLC (on my todo list). The size of your input images shouldn't matter. Internally we perform tiling. True, if you ROI is such that in order to compute a single pixel you need to see the data for the entire image then our tiling scheme will fall apart and things will be slow. Are you re-using the same CI context in between each render call? Note your first render may be slower than the others because it is at this point when we do all the work relating to compiling your programs optimally (which takes time) for the target device and warming up our caches. Subsequent renders should perform better. Keep your CIContext's around!! A+, Alex. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Graphics chip suddenly outputting black</header>
    <body>Sorry for this slightly off-topic post, but it occurred to me that some on this list might know what's going on. I have a 2.5 GHz 15&amp;quot; MacBook Pro (aluminum) with: Chipset Model:	GeForce 8600M GT Type:	GPU Bus:	PCIe PCIe Lane Width:	x16 VRAM (Total):	512 MB Vendor:	NVIDIA (0x10de) Device ID:	0x0407 Revision ID:	0x00a1 ROM Revision:	3212 Displays: Color LCD: Resolution:	1440 x 900 Pixel Depth:	32-Bit Color (ARGB8888) Mirror:	Off Online:	Yes Built-In:	Yes In the last week, it has begun doing this: occasionally, the screen will go black. This lasts anywhere from a second to several minutes. You can see that the backlight is still on, and the system thinks there's still a display there. Restarting does NOT fix the problem, but a power cycle did. Sleeping the system and re-awakening it also did not fix the problem. As I understand it, this is the chipset that had flaws due to packaging of the ICs. But this note doesn't list my specific machine: Does anyone know if this is what it is I'm seeing? On a side note, I've seen another display-related problem. When disconnecting the power adapter, which typically immediately causes the screen to be dimmed a little (LED backlight), on very rare occasions, every other LED along the bottom will turn off, so that I get a kind of stage lighting effect where the spots along the front edge of the stage are too far apart. Any info would be much appreciated. This will be hard to reproduce on demand at the Genius Bar, so hopefully they'll know what it is by the description alone and offer to fix it. Thanks! -- Rick</body>
  </mail>
  <mail>
    <header>Garbage when starting immediately in full-screen.</header>
    <body>Hello, We're seeing an issue where when our application is started going immediately in full-screen, the previous contents of the video memory (at least that's what it looks like) are displayed garbled on the screen until we draw something. Unfortunately our app is a mixture of PyObjC and Cocoa so it's tricky to create a small app to reproduce this. A peculiarity is that we initialise our window and view in applicationDidFinishLaunching: and all drawing is done programmatically using CALayers. Is there something obvious we are missing? Thanks, Orestis _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CIShadedMaterial Filter</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: vImage vs. Core Image for large filter chains</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: vImage vs. Core Image for large filter chains</header>
    <body>Hi Andrew, Denis, (going to get to your previous email in a sec but let me attack this one first) First off, on SnowLeopard you shouldn't be seeing CoreImage use the GPU to do CL work. And ... just for the record, ARGB888 should work just fine. Can you provide me with a more detailed sample? You can run your app by setting the environment variable CI_PRINT_TREE=1 to see which renderer is being used. &amp;quot;fe-context-gpu&amp;quot; is pretty self explanatory and &amp;quot;fe-context-cl-cpu&amp;quot; is your other option. A+, Alex.</body>
  </mail>
  <mail>
    <header>Re: Core Animation thread safety (very slow to set CALayer contents	from background thread)</header>
    <body>Thanks for your persistent detective work in this, mahal! The run loop explanation makes sense, and the [CATransaction flush] solution is great. So in terms of my original code, here's the options: // WRONG: returns quickly, but takes a while to show up (likely only when another thread triggers a flush) // WORKAROUND: the image shows up quickly (usually returns fairly quickly) // SOLUTION: works quickly, may avoid potential image decode on main thread thanks, -natevw</body>
  </mail>
  <mail>
    <header>Re: Core Animation thread safety (very slow to set CALayer contents from background thread)</header>
    <body>it's not a bug but a behaviour of NSOperationQueue. the CATransaction Class Reference describes in the Overview the behaviour of implicit transactions toghether with the runloop. regards, mahal the actual response from engineering (if i may post it here): Am Dienstag 22 Dezember 2009 um 07:25PM schrieb &amp;quot;martin halter&amp;quot; &amp;lt;email@hidden&amp;gt;:</body>
  </mail>
  <mail>
    <header>Re: Jpeg decoding errors in Console, how to quiet warnings?</header>
    <body>That has a high dependence on where the NULL value is returned. If it is returned in a function that can reliably recover from the situation, then you can avoid the crash. If it returns from a function whose only recovery mechanism is to allocate more memory, then it is highly likely that you cannot avoid the crash. Overall however, if your that close to running out of address space, then your application will face issues at some point unless it is fairly trivial. If you want to consume as much physical memory as possible, then query the machine's physical memory and see if you can allocate that much memory, rather than trying to test the limits of your address space via malloc(). Even then, you will often find situations where a 32-bit process cannot use that much memory. At most malloc() can return to you about a 1.5GB block depending on memory layout, so if use has more memory than that installed (and every new Mac ships with at least 2GB last I looked) then it is unlikely you can allocate a single block to use all available physical memory. This all goes away in 64-bit, but then of course you shouldn't be trying to use the probing method you describe either. There are Mach calls you can make to determine how much VM space is available to your application without having to make these types of probes. I would recommend that instead of what you are currently doing, although I'm not terribly familiar with the calls. You might try the darwin-dev list. Of course, this isn't really physical memory either, so you will still need to query how much physical memory the machine has if you want to create a cache that uses as much of it as possible. The overhead of malloc() is of absolute importance and we've been making improvements to this as we can. If you feel that malloc() is being unnecessarily wasteful, then please file a bug report describing your issues. In general, these message are left in in order to facilitate diagnosing what is going wrong. It is unexpected that a developer (let alone a user) would want to turn them off or would really care that they are being logged. Feel free to file a bug if you think any particular message is spammy or useless, or if you want a method to disable them. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: vImage vs. Core Image for large filter chains</header>
    <body>Raphael Hi all, I'm working on a multi-scale image processing application based on the Laplacian pyramid transform. &amp;nbsp;In short, high-frequency content is separated at each detail level of the image, and added back after processing to restore the original. This process involves multiple steps of deconstruction and reconstruction, each consisting of several (separated) convolutions, decimations and additions. &amp;nbsp;The total amount of these can quickly add up to ~100. Moreover, to guarantee precision over such a long filter chain, I need to both store and compute 32-bit components. In order to execute all of this in a reasonably performant way, there are two options: vImage or Core Image, since both are vectorized. &amp;nbsp;I've tried both. * the vImage implementation uses the performance advice given in the docs: planar formats, tiling, aligned data, 1D kernels. &amp;nbsp;Although it is pretty fast (Core 2 Duo 2.6Ghz), I would expect the CI implementation to outperform it (obviously, the GPU has more FPUs). * the Core Image implementation uses separate kernels for convolution, addition, decimation. &amp;nbsp;In the entire algorithm, a complex CI-recipe is constructed and only one image is ever rendered: the final one, triggering a long filter chain. &amp;nbsp;In this chain, some intermediate images are used several times by different subsequent filters (in other words, the chain is rather an arbitrary acyclic graph). While testing, the CI implementation (GeForce 8600M GT) is slower than the vImage one (C2D 2.6), especially on larger images. &amp;nbsp;I would expect it to be the other way around, since the math is probably faster on the GPU and in theory only one CPU-&amp;gt;GPU upload and download should occur. The only reason I could come up with is that the CI cache is continuously being trashed, forcing re-evaluation of previously computed intermediate images. &amp;nbsp;The CI docs are rather opaque on the internals, other than to say "CI is smart about optimizing stuff, so you shouldn't worry about this". Am I trying to use CI for something it is not designed for? &amp;nbsp;Is the internal CI texture cache policy not adequate for a chain of this size? &amp;nbsp;In that case, would it be advisable to force evaluation (render) of intermediate results? &amp;nbsp;Or would I then lose any advantage over CPU-based vImage? Thanks, Dennis &amp;nbsp;_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list &amp;nbsp; &amp;nbsp; &amp;nbsp;() Help/Unsubscribe/Update your Subscription: This email sent to</body>
  </mail>
  <mail>
    <header>Re: vImage vs. Core Image for large filter chains</header>
    <body>Hi Andrew, Thanks for the tip.  However, I'm looking to execute things faster than (vectorized) CPU speed, so I don't think this is an option (although it might be more predictable in terms of running time). For the record, I tried to use the software renderer, but in 10.6 its implementation is based on OpenCL -- which defaults again to GPU on my hardware :)  This complicates things, because OpenCL has trouble interfacing with reasonably standard ARGB8888 CGContexts. regards, Dennis</body>
  </mail>
  <mail>
    <header>Re: Jpeg decoding errors in Console, how to quiet warnings?</header>
    <body>Sounds like a pain then. Well, no, things are only going to crash if you ignore the return value. This particular code is a cache, and hence will use as much physical memory as it can. It knowns when it's run out of virtual memory space when malloc returns NULL, at which point it stops trying to malloc any more (or rather this is a probe to try and figure out when it is going to hit the virtual address space limit, and thereafter it tries to stop some margin short of using all the virtual address space.) It's messy because there doesn't seem to be any clean way of figuring this out, particularly when other libraries use unknown amounts of virtual memory space, and when malloc itself chews up a large amount of the virtual memory space with its own book keeping (ie. the available virtual memory space depends on the size of the chunks allocated. You can tell that all this was designed when it seemed like no-one would every be able to have as much physical memory as virtual memory.) A clean way of turning off these sorts of warning messages when they are expected would be welcomed. Graeme Gill.</body>
  </mail>
  <mail>
    <header>Re: Jpeg decoding errors in Console, how to quiet warnings?</header>
    <body>This won't work on 10.5 or later (Quartz uses a private ASL connection). If malloc() is returning NULL, then your application has run out of memory address and is probably going to crash soon, doesn't matter how much memory the installed machine has. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: Jpeg decoding errors in Console, how to quiet warnings?</header>
    <body>You could try redirecting stderr. dup2(old_stderr, fileno(stderr));   /* Restore stderr */ (I have to do this to suppress warnings from OS X whenever malloc returns NULL. This happens a lot in normal operation for people with 4G of memory on 32 bit systems). Graeme Gill.</body>
  </mail>
  <mail>
    <header>Re: Jpeg decoding errors in Console, how to quiet warnings?</header>
    <body>FWIW, I have not been able to quiet these errors when working on either the iPhone Satromizer or the Mac version... I think they're being emitted from Apple's custom jpeglib. I've tried switching to my own build of jpeglib, which helps with the Mac version, but it's not fast enough on the iPhone -- I wish I knew what build settings or optimizations Apple used. - ben</body>
  </mail>
  <mail>
    <header>Jpeg decoding errors in Console, how to quiet warnings?</header>
    <body>I am purposefully degrading and corrupting in memory jpegs to get 'bad' jpeg decoding for an aesthetic look (should you be curious what I mean, check out : I am using the CGImage API and have everything working as I would like to, however, there is one aspect I am unsure how to handle, quieting the console output of the jpeg decoder complaining: Quartz Composer[16792]	Corrupt JPEG data: bad Huffman code	1/2/10 2:04:59 PM Quartz Composer[16792]	Corrupt JPEG data: premature end of data segment	1/2/10 2:04:59 PM etc etc. I do my jpeg corruption at 60fps, and would like to quiet this log output, but have not seen any clear indication of how to do this. Is it possible? CGImageDestinationAddImage, CGImageDestinationFinalize, CGImageSourceCreateWithData, CGImageSourceCreateImageAtIndex, CGContextDrawImage. This last one is responsible, as far as I can tell, for the logging. Thank you in advance, im guessing Im either glossing over something, or its not possible..</body>
  </mail>
  <mail>
    <header>Re: problems during CGContextDrawPDFPage</header>
    <body>I don't see anything out of the ordinary in your code, so the best I can recommend is that you file a bug report. Sorry for the early send... -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: problems during CGContextDrawPDFPage</header>
    <body>I have a program that reads a PDF and generates another PDF from the original.&amp;nbsp; Most of the time, everything works fine.&amp;nbsp; I have some documents that do not work properly and result is a garbled PDF.&amp;nbsp; I am not sure what is going on specifically, although this example appears related to fonts? I pulled out the relevant code that reproduces my problem.&amp;nbsp; I can reproduce this with a government form, fw9.pdf.&amp;nbsp; My platform is the iPhone. What should I do in this situation?&amp;nbsp;&amp;nbsp; Is there a way I can workaround it and still generate a readable PDF?&amp;nbsp; What about detecting potential problems so I can notify the user? I get this error in my console log during the CGContextDrawPDFPage</body>
  </mail>
  <mail>
    <header>Re: -[NSBitmapImageRep initWithFocusedViewRect:] crashing by calling	CFRelease(NULL) (SOLVED)</header>
    <body>True.  My question was how to compensate for that. I finally realized how HiDPI is creating more pixels.  The initial CTM of an NSImage whose coordinates fall on a HiPDI device has a scaling of 2.0.  That is, m11 and m22 are 2.0, not 1.0. My (inherited) deskew code didn't create its affine transform matrix by building up the individual transforms (translate, rotate, scale, translate back, offset by parameter from OCR engine), it calculated an NSAffineTransformStruct directly.  The calculation used some small angle approximations that no doubt help performance, but didn't aid comprehension.  The use of that struct assumed the CTM was the identity matrix, and it multiplied the calc'd matrix to it. The calc used to create the struct could have been modified to compensate for the initial scaling that the original CTM had, but I found it easier to apply a more general solution: 1) apply the legacy NSAffineTransform via 'set', not 'concat'. 2) setting the CTM to the identity matrix before calling [NSBitmapImageRep initWithFocusedViewRect:].</body>
  </mail>
  <mail>
    <header>CGPSConverter failure</header>
    <body>Hi, We seem to be running across a bug in CGPSConverterConvert. What happens is that after a number of calls to convert EPS files into PDF files, CGPSConverterConvert returns failure and callbacks to read and write bytes are no longer called. The number of calls before failure seems to be related in some way to maxfiles (as in &amp;quot;launchctl limit maxfiles&amp;quot;). Under Snow Leopard and Lion, with maxfiles at its default setting of 256, CGPSConverterConvert fails after 254 (Snow Leopard) or 252 (Lion) iterations. Under Mountain Lion, with the same default maxfiles setting, CGPSConverterConvert fails after 2558 iterations (suggesting that the actual limit has been increased to 2560). [The limits are different if you run from within Xcode - but I think that Xcode is changing the default limits for debugging purposes.] You can see the same problem in Apple's Preview.app (tested under Snow Leopard). 1. Set maxfiles to a low figure - e.g. &amp;quot;launchctl limit maxfiles 40&amp;quot; (I haven't tried lower than 40) 2. Double-click on an EPS file to open it in Preview 3. Close the file and reopen it repeatedly (without quitting Preview) 4. After a while (somewhere close to 40 iterations if maxfiles is 40) Preview stops opening the EPS file, but doesn't report any errors. Jeremy</body>
  </mail>
  <mail>
    <header>Re: -[NSBitmapImageRep initWithFocusedViewRect:] crashing by calling	CFRelease(NULL)</header>
    <body>Why are you assuming 72 DPI here? If you consult the destination image, what is its actual DPI? On HiDPI configurations, NSBitmapImageRep will create higher-denisty bitmaps by default. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: CGGradient</header>
    <body>If CGGradient interpolates in linear color space, and coverts back and forth between the specified color space, that does make it somewhat useful if for no other reason than to reduce lines of code. If not, that makes it buggy IMO. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: Displaying output without a billboard?</header>
    <body>If you want to make an existing composition do that you can make the composition an image filter via the menu in QC for protocol conformance. Compositions can also be made to belong to multiple protocol types simultaneously. An educated guess is that if a composition has protocol metadata and certain input or output names exists, the QC app just does stuff with the info. It's certainly possible to do similar in your own apps. Protocols are one of the cooler things in the QC framework because you can be assured about the execution mode and other things.</body>
  </mail>
  <mail>
    <header>Displaying output without a billboard?</header>
    <body>How does the image filter template display the output image without using a billboard? I see that it publishes the output of an image splitter, but I tried that and it didn't work. -- Rick</body>
  </mail>
  <mail>
    <header>Re: -[NSBitmapImageRep initWithFocusedViewRect:] crashing by calling	CFRelease(NULL)</header>
    <body>Derek</body>
  </mail>
  <mail>
    <header>How to highlight image in quartz?</header>
    <body>I want to draw an image with &amp;quot;highlight&amp;quot; effect, here is what I do now: background with brush kThemeBrushPrimaryHighlightColor 2. Set alpha to 0.25 by calling CGContextSetAlpha() 3. Call HIViewDrawCGImage() to draw the image But the effect does not look like the &amp;quot;highlight&amp;quot; effect in other applications such as Apple Mail.  What should I do?  Use blend mode? Thanks! chenxp</body>
  </mail>
  <mail>
    <header>Re: CoreImage generates an OpenGL error / Leopard</header>
    <body>Most likely the reason is that a texture gets created and released on a different context and a texture id mixup happens in the client code. Best way to track this down is that you create something like your own texture id namespace by adding an offset of lets say 10000 to your own texture ids. Frank Doepke Senior Engineer Core Image Apple Computer, Inc. M/S 302-3PG 2 Infinite Loop Cupertino, CA 95014 phone: 408-974-1266 email: email@hidden</body>
  </mail>
  <mail>
    <header>CoreImage generates an OpenGL error / Leopard</header>
    <body>I'm using CoreImage api to apply effects on OpenGL textures (from quicktime movies, static images, camera, etc) as a extra plugin for a real-time video mixing software. On 10.4, it has been working fine and it's used by thousands users for years now. While debugging using &amp;quot;OpenGL Profiler&amp;quot;, I discovered that CoreImage processing generates an OpenGL error when using some filters on 10.5: Error: GL_INVALID_OPERATION Context: 0x00ae6000 kCGLCPGPUFragmentProcessing:  GL_TRUE kCGLCPGPUVertexProcessing:  GL_TRUE kCGLCPCurrentRendererID:  9732 (0x00002604) Function call stack: ... / when I call: /[ciContext_ drawImage:output_image atPoint:CGPointZero fromRect:CGRectMake(0, 0, size_.x, size_.y)];/ Looking deeper into the OpenGL callstack makes me think the error is due to the fact that CoreImage tries to delete a texture (ID 8) it didn't generate properly, just after this texture ID 8 was just deleted (and created before that) in another part of my program (this texture has absolutely no deal with CoreImage plugin). Did somebody experience problems with CoreImage on 10.5 ? Pierre # &amp;quot;Blur&amp;quot; filters: # disc blur # gaussian blur # median # motion blur # Noise reduction # &amp;quot;Stylize&amp;quot; filters: # Spot color # Shaded material # Line overlay # hexagonal pixellate # height field from mask # Gloom # Edge Work # Edges # Crystallize # Comic effect # Bloom</body>
  </mail>
  <mail>
    <header>IKImageBrowserView: non-documented properties for setting font	attributes</header>
    <body>Hello all, according to the IKImageBrowserView (one of the new Image Kit classes introduced in Leopard) reference it is possible to set specific view options using setValue:forKey method. It is pretty clear for the background color, etc., however text elements take a dictionary as a value: NSDictionary NSDictionary NSDictionary I suppose the content of the dictionary is again some key-value pairs. But what are they exactly? Could someone help me? Best Regards,</body>
  </mail>
  <mail>
    <header>Re: Best source for a CIImage?</header>
    <body>I should have clarified.  I can put the sources into any format I like, my goal is to make the evaluation (the CIContext draw:...) as fast as possible.  The speed of the setup is not really important.  So I was wondering what CIImage source required the least amount of work to be done for the final evaluation into the CIContext.  For instance, a CIImage from a URL causes loading and decoding to be done in the final evaluation which adds a lot of time to it.  Is a CGLayer better, or a CGImage/CGBitmapContext in ARGBf, ect? On Jan 7, 2008, at 12:20 PM, Daniel Beatty wrote:</body>
  </mail>
  <mail>
    <header>Re: OpenGLScreenCapture flipBufferContents issue</header>
    <body>As it stands, FrameReader's flipBufferContents flips the entire buffer ( e.g. 2560 x 1600 x 4 in my case) even if you update only 1 pixel. The most efficient thing to do is always a trade off between ... bunch of things + (code complexity *  other factors) ^ (intended goal). On Jan 7, 2008, at 4:17 PM, Martin Redington wrote:</body>
  </mail>
  <mail>
    <header>Re: OpenGLScreenCapture flipBufferContents issue</header>
    <body>Hi Michel, yep - that seems to be exactly what's going on. I think in practice this won't matter, because I'll only be grabbing the changes anyway. Out of interest, if I did want to clear the pixelbuffer before calling glCopyTexSubImage2D, should I just bzero it, or is there any more efficient way? cheers, m. --</body>
  </mail>
  <mail>
    <header>Re: OpenGLScreenCapture flipBufferContents issue</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Fwd: Best source for a CIImage?</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Event taps and secondary function key</header>
    <body>On Jan 6, 2008, at 5:52 AM, Bill Cheeseman wrote: The kCGEventFlagMaskSecondaryFn flag is not a modifier key flag, but instead indicates that the key event associated with the flag originated with a key considered to be part of the function key set.   The flag is set on both a key down and a key up event, so if the last event seen is a key-up with the flag set, you'll see this flag set in the polling interface for the flags. On some of the laptop keyboards, I understand that the 'function key' generates an event itself that includes this flag bit.</body>
  </mail>
  <mail>
    <header>Best source for a CIImage?</header>
    <body>What's the best source to give for a CIImage, as in to make the drawing the shortest path possible?  I'm currently using a CGLayer created from the destination CIContext, drawing in that and making the CIImage from it.  But does CIImage do better from a CGImage or other source to minimize conversion/decoding?</body>
  </mail>
  <mail>
    <header>Trouble getting CALayers to be transparent</header>
    <body>I have an existing/in progress hybrid Cocoa OpenGL appliction. Before Leopard, I was showing large amounts of scrollable text over an OpenGL texture by overlaying a transparent window (making it a child window) and adding a subview - an NSScrollView with a transparent background, into the child window which displayed my text. When Leopard came along, I thought it would be nice to animate the opacity resulting in a gentle fade in for each 'page' of text. But every experiment I have tried results in the text displayed on a white opaque background. Even without the CAAnimation. I am sure that I am just missing some fundamental simple point. I hope someone can help. I have tried: 1. just telling the existing 'NSScrollview' in the transparent Child window  - setWantsLayer:YES Result - animates fading in text with white Opaque background. 2. Subclassing CALayer and drawing the text in a drawInContext:(CGContextRef)context Using Coretext. Result - animates fading in Text with white Opaque background. 2a. telling the new view that contains the subclassed CALayer - 2b. in my drawInContext:(CGContextRef)context: I try to fill the rectangle of the view with a transparent CGColor: etc. until or CGColorSpaceRef space = 2c. tried adding a compositing filter to the layer compositing 'srcOver' Result: animates fading in Text with white Opaque background. I have tried creating a coloured background, and the colour draws sort of translucently over the text and the white opaque background. Can anyone help? Is it a CoreText bug with CALayers or have I missed something really fundamental? Vickie</body>
  </mail>
  <mail>
    <header>Re: Event taps and coalescing of events</header>
    <body>On Jan 6, 2008, at 6:01 AM, Bill Cheeseman wrote: Probably what's happening is that the Carbon Event Manager normally turns off Quartz-level coalescing of mouse events at app startup (and remember, AppKit event handing is based on the Carbon Event Manager, so this applies to Cocoa apps too), and performs coalescing at the client application level. This is required to allow applications that need the uncoalesced events to work properly.</body>
  </mail>
  <mail>
    <header>Re: Event taps and secondary function key</header>
    <body>The same thing happens when I press any of the &amp;quot;navigation&amp;quot; keys in the center clusters on my desktop keyboard (e.g., &amp;quot;help&amp;quot; key, &amp;quot;page up&amp;quot; key, arrow keys), as well as the &amp;quot;clear&amp;quot; key on the numeric keypad (but not any of the other keys on the numeric keypad). This is an original Apple wireless keyboard. -- Bill Cheeseman - email@hidden Quechee Software, Quechee, Vermont, USA www.quecheesoftware.com PreFab Software - www.prefabsoftware.com</body>
  </mail>
  <mail>
    <header>Event taps and coalescing of events</header>
    <body>I use this Quartz Event Taps code to detect whether mouse and tablet pointer events are noncoalesced: CGEventSourceFlagsState(sourceStateID) &amp;amp; kCGEventFlagMaskNonCoalesced But to my surprise it normally returns true on all of my computers. Various Apple developer documents state that mouse and tablet pointer events are normally coalesced, to avoid bogging down the machine, but admonish that tablet drivers and certain other code should turn off coalescing to get finer-grained information about cursor movement. Is this documentation obsolete? Are mouse and tablet pointer events normally non-coalesced these days? Or are they non-coalesced on faster machines but still normally coalesced on slower machines? -- Bill Cheeseman - email@hidden Quechee Software, Quechee, Vermont, USA www.quecheesoftware.com PreFab Software - www.prefabsoftware.com</body>
  </mail>
  <mail>
    <header>Event taps and secondary function key</header>
    <body>I use this Quartz Event Taps code to detect whether the secondary function key on a laptop (the key labeled &amp;quot;fn&amp;quot;) is currently being held down: CGEventSourceFlagsState(&amp;lt;sourceStateID&amp;gt;) &amp;amp; kCGEventFlagMaskSecondaryFn It works as expected when I hold down the &amp;quot;fn&amp;quot; key on my laptop. However, pressing any top-row function key also causes this code to return true, even when I am not holding down the &amp;quot;fn&amp;quot; key. Even more puzzling, pressing a top-row function key on my desktop Mac also causes this code to return true, even though the keyboard attached to the desktop machine doesn't have a &amp;quot;fn&amp;quot; key. Why does pressing a top-row function key always cause this code to think a &amp;quot;fn&amp;quot; key is being held down? I'm guessing it's because the system always &amp;quot;pretends&amp;quot; that a &amp;quot;fn&amp;quot; key is down when pressing a top-row function key, in order to get the behavior assigned to that key -- but I'm puzzled as to why I get the same result no matter how I have set the laptop's Keyboard system preference with respect key. And while I'm on this topic, why does this code continue to report that the &amp;quot;fn&amp;quot; key is being held down even after I let it up -- until the next user input event is generated? The &amp;quot;fn&amp;quot; key on my laptop does not behave as if the &amp;quot;fn&amp;quot; key is still down after I've let it up. -- Bill Cheeseman - email@hidden Quechee Software, Quechee, Vermont, USA www.quecheesoftware.com PreFab Software - www.prefabsoftware.com</body>
  </mail>
  <mail>
    <header>OpenGLScreenCapture flipBufferContents issue</header>
    <body>I'm playing with the Apple OpenGLScreenCapture example. I've added a ScreenRefreshCallback, to tell me which areas of the screen have been redrawn, and in the FrameReader, I'm merging the dirty rects, and trying to grab only the updated area, by calling glCopyTexSubImage2D(GL_TEXTURE_RECTANGLE_ARB, 0, (int)(dirtyRect.origin.x), (int)(dirtyRect.origin.y), (int)(dirtyRect.origin.x), (int)(dirtyRect.origin.y), instead of Those are the only changes I've made to the example code, so right now, my eventual output is a weird movie, where each frame only contains the updates for that frame. That's fine - I plan to do some compositing of the output, instead of passing it straight to compression and export, later on. The problem I'm getting is as follows. In the FrameReader, there's a flipBufferContents method, which rearranges the pixels to match QuickTime, as opposed to OpenGL co-ordinates. When I grab the full-screen this works fine. If I comment out the method, everything appears upside down and back to front, as expected. If I leave the method commented out, and start grabbing only the dirty rects, everything appears upside down and back to front, as expected. When I re-enable flipBufferContents, still grabbing only the dirty rects, everything starts out ok - I get my funny updates only movie, with everything oriented correctly, but after a few seconds, some of the updates start to come in incorrectly oriented, as though flipBufferContents had never been called on them. I can get the same behaviour with the example code, just by changing to I'm guessing that the original pixelbuffer data is ok (as when unflipped, it correctly draws the wrong ways round). So it seems like flipBufferContents might be broken, except that it works fine when I'm grabbing the entire image. Does anyone have a clue as to what might be going on here? Incidentally, I'm on Tiger 10.4.10, on an MBP, with a ATY,RadeonX1600 cheers, Martin --</body>
  </mail>
  <mail>
    <header>Re: Trouble switching to CGEvents</header>
    <body>Internally, addGlobalMonitorForEventsMatchingMask:handler: uses GetEventMonitorTarget, so it's not surprising that you'd revert to the same level of performance that you had when using GetEventMonitorTarget directly. -eric</body>
  </mail>
  <mail>
    <header>Re: Trouble switching to CGEvents</header>
    <body>After struggling with this for a while a friend suggested that since I'm not modifying events, I should drop CG event taps and instead use NSEvent's newish (10.6+) addGlobalMonitorForEventsMatchingMask:handler:. I did that and my performance problems instantly disappeared. I don't know what it's doing internally that's different than an event tap, but the difference is dramatic and unmistakeable.  I'm doing exactly the same thing in my handler block as I was in the event tap's callback and performance is massively improved. -- Tom Harrington email@hidden AIM: atomicbird1</body>
  </mail>
  <mail>
    <header>Re: Capturing cursor pixmap using carbon / C++</header>
    <body>I'm confused why you can't just use NSCursor. Is there some reason you're avoiding Cocoa? --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Capturing cursor pixmap using carbon / C++</header>
    <body>I'm trying to get the current system cursor pixmap data using only Carbon / C++. &amp;quot;QDGetCursorData&amp;quot; looks like it does what I want, but has been deprecated, and has no official documentation that I can find. &amp;quot;IOFBCreateSharedCursor&amp;quot; looks promising, but seems to be at the bottom of a long tangle of IOKit APi calls; I'm new to programming on a mac, so i was hoping for a tutorial / example code / pointer in the right direction. I'm not sure whether this is the right place to post such a query, but I'm hoping someone here can point me in the right direction. Kind Regards, Thomi Richards</body>
  </mail>
  <mail>
    <header>Re: Making buttons in a CGContext</header>
    <body>ok, great, can you point me at the documentation for HITheme? When I search the apple dev site I get several references suggesting that one should use the HITheme API, but no actual links to what the API actually IS... Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: Making buttons in a CGContext</header>
    <body>Ah, Except that HIToolbox appears to only work with 32b systems.  I'm trying to  work with 64b Mozilla. Does this mean that I can't use Quartz at all and In need to go to cocoa? Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: Making buttons in a CGContext</header>
    <body>Ok, thanks. Hopefully mozilla gives me something outside of the CGContext that I can work with, because that sound unnecessarily painful... Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: Making buttons in a CGContext</header>
    <body>You have to implement your own scrollbars, handle the events and scroll your content. Treat it as if you have a drawing context and have to implement everything from scratch, because that is exactly what you have. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Making buttons in a CGContext</header>
    <body>Ok, but then how does one do scrollbars? I'm putting an image in a context, and if the image is bigger than the clipping rect... No I need to animate those myself? Or, is there a way from within the context to get access to something around the context that can get scrollbars? thanks Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: Making buttons in a CGContext</header>
    <body>My problem is that I need to get control buttons into the context as well, and I'm not sure how to do it. Strategies I've considered and not yet eliminated: 1) Create two images (pngs probably), one of the button unpressed, and one of it pressed. Capture mouse clicks, and keep track of where the button is. If the mouse clicks in the region I know is the button, replace the button image with the pressed image, if it releases, replace it back and do my thing.</body>
  </mail>
  <mail>
    <header>Making buttons in a CGContext</header>
    <body>I'm writing a Mozilla Plugin, where I am given a CGContext. I need to draw an image (TIFF, JPEG, something like that) into the window. I can get the image into a  CGImage, and then draw that into the context, so that part isn't my problem. My problem is that I need to get control buttons into the context as well, and I'm not sure how to do it. Strategies I've considered and not yet eliminated: 1) Create two images (pngs probably), one of the button unpressed, and one of it pressed. Capture mouse clicks, and keep track of where the button is. If the mouse clicks in the region I know is the button, replace the button image with the pressed image, if it releases, replace it back and do my thing. 2) Turn to Cocoa. Put everything into some subclass of NSView, design a bunch of stuff in Interface Builder (probably using a IKImageView for the image) and never actually display the NSView. I would somehow have to turn the NSView into a bitmap or something to get it into the CGContext, and draw that. Which of these is considered the standard solution to this? Or is there some third option that I haven't considered? Thanks. Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: Trouble switching to CGEvents</header>
    <body>Thanks, Wade, that helps. I tried this in Instruments-- Time Profile instrument, running sample times, targeting all processes.  When WindowServer is bogging down on my behalf, it accounted for about 50% of CPU time during the sample. The top entries shown by Instruments all appear to be involved in delivering CGEvent notifications.  The exact stack traces vary but CoreGraphics' postEventTapData() appears in the 8 of the top 10 entries in the results.  My own code spends more time in AXUIElementCopyAttributeValue than anywhere else, but even that accounts for only 10% of my process's CPU time. I had thought that the slowdown might have to do with looking up values on events once they were posted-- i.e. when I call CGEventGetLocation() and CGEventGetDoubleValueField() to get mouse location and delta.  Instruments doesn't show any sign that I'm spending a lot of time there. I'm getting the distinct impression that CGEvents are just taking a lot longer to filter down to my code for some reason. I would have expected performance comparable to or better than Carbon events, so I wonder if I'm somehow misusing the CGEvent system and causing this problem.  Are CGEvents actually harder for the system to deliver?  And if not, is there some other explanation for WindowServer's problems here? -- Tom Harrington email@hidden AIM: atomicbird1</body>
  </mail>
  <mail>
    <header>Re: Trouble switching to CGEvents</header>
    <body>The #1 entry in a Shark "heavy" listing is ml_set_interrupts_enabled(), and following down the call tree doesn't show any function I've ever called explicitly. &amp;nbsp;My own code shows up</body>
  </mail>
  <mail>
    <header>Re: CAShapeLayer lineDashPattern crash</header>
    <body>Hi, the lineDashPattern property is not animatable (the header files are the canonical source of what is and what isn't animatable, each property has a comment describing what it does and whether it can be animated or not). Having said that, trying to animate it shouldn't crash, so please file a bug report. Thanks, John</body>
  </mail>
  <mail>
    <header>Re: Trouble switching to CGEvents</header>
    <body>On Sat, Feb 20, 2010 at 2:21 AM, Nathan Vander Wilt The event tap is active and receiving events.  When I get mouse-move events I look up the position and deltas of the events, and then use accessibility calls to manipulate the window under the mouse. The #1 entry in a Shark &amp;quot;heavy&amp;quot; listing is ml_set_interrupts_enabled(), and following down the call tree doesn't show any function I've ever called explicitly.  My own code shows up farther down with no significant bottlenecks. While running in Shark, Activity Monitor shows my code using 3-4% of CPU, while WindowServer cranks up to over 90%. I also tried Instruments, which shows a severe bottleneck at start_wqthread() and working its way on down to kevent().  I don't do any explicit threading in my code, it's all on the main thread. It seems that I'm pushing an awful lot of work onto the window server, though aside from switching to a CGEventTap I'm doing the same thing I've always done.  Is that just how event taps work? -- Tom Harrington email@hidden AIM: atomicbird1</body>
  </mail>
  <mail>
    <header>CAShapeLayer lineDashPattern crash</header>
    <body>Hi all, Does anyone have an idea why this crashes? (EXC_BAD_ACCESS) myLayer is a CAShapeLayer. Every property seems to work (e.g. lineWidth, strokeColor), but animating lineDashPattern crashes. Setting the property directly works, but I need a CABasicAnimation because I want to group the animation in a CAAnimationGroup. Thanks, Steven</body>
  </mail>
  <mail>
    <header>Re: Trouble switching to CGEvents</header>
    <body>(Sorry about the initial off-list reply. I will apparently never learn...) Without having much of an idea what you're all doing in your event tap, or even if it is active/passive... what does Shark say is taking the most time? regards, -natevw _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Trouble switching to CGEvents</header>
    <body>Maybe if I simplify a little... My code used to use Carbon to monitor mouse movement events, but I want to switch to a CGEventTap. I replaced my event-monitoring code with CGEvent-based code.  All other code stayed the same except for changing the Point type to CGPoint. The resulting code works but has severe performance issues. I would have expected, given the changes I made, that the code would work at least as well as it used to, but instead it's much worse. Does anyone have any thoughts on what I could do with a CGEventTap to at least get close to my old performance with Carbon events? Or a different mailing list I should use to ask about Quartz events? -- Tom Harrington email@hidden AIM: atomicbird1</body>
  </mail>
  <mail>
    <header>Re: Quartz2D paths from vector?</header>
    <body>Hi Scott -¬† Yes, that Heart shape example is from your book - Quartz 2D Graphics for Mac OS X Developers. The book is excellent - and really helped me - many thanks. I think I&amp;#39;ve got a number of options now for getting the path curve points out of a drawing package, so now it&amp;#39;s off to try it out for real. Paul Is anyone aware of a tool that will do that?¬† Or is there any other way to import a curve path in to Quartz, without having to define it manually in code? I need something that will allow me to draw freehand, and then extract the path info from that in a format which I can use to create the NSBezierPaths for functions like:</body>
  </mail>
  <mail>
    <header>Core Text crasher under 64 bit, 10.5 only</header>
    <body>(Posting to Quartz dev too because ct-dev seems pretty new and its archive link gives me a 404.) I have been porting an application -- that needs to produce PDF at runtime -- from ATSUI to Core Text. It is using Core Text to draw into a CGPDFContext. I find that under Leopard (but not Snow Leopard) that running a 64-bit build crashes when CGContextRelease is called after drawing a CJK glyph (below U+5FCD, the Japanese &amp;quot;shinobi&amp;quot; character, which seemed appropriate) into the context with Core Text, specifying a font (Lucide Grande here) that does not contain the glyph. GDB gets real confused and I don't get a backtrace, but Crash Reporter records a location of _eOFAStreamPSDownload + 685. Happens with Hiragana too (U +3040-U+309F), and some other East Asian blocks. Code seems fine with other glyphs, even if they aren't in the selected font. For example, I just tried U+FB50 (from Arabic Presentation Forms- A) which is certainly not supported by Lucida Grande, and there was no crash. I'm pretty confident Apple is not discriminating against Chinese characters in a show of solidarity with Google, so I'd like to know what's going on here. My application *could* be distributed 32- bit only (it's the server part of an Input Method/Input Method UI Server pair) but that's not a very future-proof approach. I'm pretty sure I am doing something wrong, but -- well, you know the state of the CT docs. They tell you the whats but not the whys. As an aside, Apple's docs' claim that the deprecated parts of ATSUI are not supported under 64-bit appears to be untrue, even on Snow Leopard. But I'm not counting on this continuing to be the case, hence the rush to get this working with Core Text. I Googled _eOFAStreamPSDownload and got one hit -- discussion of a Java program that was crashing there in 64-bit only. But nothing in depth. Below are the relevant parts (and many irrelevant) of my test application, which can be downloaded in its full glory at Many thanks for your kind attention, Brian &amp;quot;Moses&amp;quot; Hall www.blugs.com @implementation AppController -(void)awakeFromNib [NSApp getSystemVersionMajor:&amp;amp;major minor:&amp;amp;minor bugFix:&amp;amp;bugfix]; // from a category on NSApplication, snipped CFURLRef url = CFURLCreateWithFileSystemPath(kCFAllocatorDefault, OSStatus err = local_CoreTR(ctx, imgRect, (CFStringRef)str, NSLog(@&amp;quot;%s&amp;quot;, (minor&amp;gt;5)?&amp;quot;You're running Snow Leopard; this won't @end static OSStatus local_CoreTR(CGContextRef ctx, CGRect r, CFStringRef string, CFStringRef fontName, CGFloat fontSize, CGFloat baseline) CTFontDescriptorRef fdesc = CTFontRef font = CTFontCreateWithFontDescriptor(fdesc, fontSize, CFMutableDictionaryRef attrs = CFDictionaryCreateMutable(kCFAllocatorDefault, 1L, CFAttributedStringRef attrString = if (glyphCount) if (baseline &amp;lt; 0.0f) baseline = ((r.size.height - if (fontSupported) CGFloat x = r.origin.x + (r.size.width/2.0f) -</body>
  </mail>
  <mail>
    <header>Re: Mouse moved events &amp;amp; CGWarpMouseCursorPosition</header>
    <body>Since nobody else has replied, I'll take a vague and clueless stab at it.  IIRC, when you post an event there is a period of time following, during which user-generated events get suppressed, in order to avoid the confusion of events coming in from both the user and from software and getting interleaved unexpectedly.  The length of this suppression period is configurable.  Perhaps this same suppression happens when you warp the cursor?  Makes sense to me that it would. So you could call the appropriate API (which I do not recall, but should be easily findable) to change the suppression period length to zero.  My guess is that now the period is causing suppression of user- generated events for long enough that none get through before the next call to warp the cursor, which starts the suppression timer from zero again, so you never see any mouse movement. Ben Haller Stick Software</body>
  </mail>
  <mail>
    <header>Re: Quartz2D paths from vector?</header>
    <body>Is anyone aware of a tool that will do that?&amp;nbsp; Or is there any other way to import a curve path in to Quartz, without having to define it manually in code? I need something that will allow me to draw freehand, and then extract the path info from that in a format which I can use to create the NSBezierPaths for functions like:</body>
  </mail>
  <mail>
    <header>Re: incorrect image size and resolution when using cgimage</header>
    <body>Oh, right under my very nose! Al</body>
  </mail>
  <mail>
    <header>Re: Quartz2D paths from vector?</header>
    <body>. Can export the definition of a path as source code.</body>
  </mail>
  <mail>
    <header>Re: Quartz2D paths from vector?</header>
    <body>It maps rather nicely - have a look at  . It's all the niggly details that are the issue of course, but your use case might be simpler. For one-offs, you could also consider this:  . Can export the definition of a path as source code.</body>
  </mail>
  <mail>
    <header>Re: Quartz2D paths from vector?</header>
    <body>Hi Alastair, Orestis,</body>
  </mail>
  <mail>
    <header>Re: Quartz2D paths from vector?</header>
    <body>Yes, for finished artwork PDF is better. SVG is nice for standalone elements. Why do things manually when you can call a function to do it for you? At the very least you could spit out a piece of code that constructs the cgpath at compile time. About human-readability, while simple lines are easy to parse by hand, any more complex shapes and you end up with this: I wouldn't wish to any human try to decipher this. It's a &amp;quot;thought bubble&amp;quot; BTW. FWIW our use case is to use SVG and replace various elements with content from a database. Hence the need to display as much as possible natively. Cheers, Orestis</body>
  </mail>
  <mail>
    <header>Re: Quartz2D paths from vector?</header>
    <body>I'm not sure Paul meant that he was going to try to read the SVG file into his program with code (indeed, there's no point if you're doing to do that... you might as well just stick your artwork in a PDF and use that). The SVG &amp;lt;path&amp;gt; element *is* fairly straightforward to read (as a human being), and so if his goal was---as I assumed from what he wrote---to draw a complex shape in Illustrator and then obtain a suitable description of it from which he can construct a CGPath or NSBezierPath in code, I don't think it'd be that hard. Kind regards, Alastair. --</body>
  </mail>
  <mail>
    <header>Re: Quartz2D paths from vector?</header>
    <body>While this is possible, keep in mind that it's not trivial. We have implemented this in PyObjC, aiming to open source it sometime. I can send you the code if you want, though.</body>
  </mail>
  <mail>
    <header>Re: Quartz2D paths from vector?</header>
    <body>If you save it as a .SVG file, it should be easy enough to extract the path data you're after. Kind regards, Alastair. --</body>
  </mail>
  <mail>
    <header>Is there an efficient way to draw the Caret in Quartz?</header>
    <body>When caret is to show, we can draw the vertical line directly. But when caret is to hide, we don't have an efficient way to draw the background. If we just invalidate the rect of the HIView where the caret lies, the HIView will draw the background. But sometimes the drawing of the HIView may be very complex. So the drawing of the background may consume a lot of time although the drawing is clipped to caret's size. I wonder if there is an efficient way to handle this, like using blending modes or something else.</body>
  </mail>
  <mail>
    <header>Bypassing ColorSync while printing</header>
    <body>I'm having some difficulty interpreting the documentation on Quartz' handling of color spaces, and was hoping someone might be able to steer me toward the proper sequence of function calls to accomplish what I thought was going to be a fairly simple task. Essentially, what I want to do is take an untagged TIFF and pass the color values its pixels describe *directly* to a printer with *no* transformations on ColorSync's part. The CGColorSpace docs make some mention of this in the line: &amp;quot;If you want to bypass color matching, use the color space of the Now if the TIFF were tagged with a profile, then I'd imagine I could use that for both the source and the destination, but since it is not, what exactly should the space of the destination context be in order to get a null transform here? I don't want any matching behavior at all, just a raw representation of how the printer interprets the device dependent numbers it's been fed from the TIFF source.</body>
  </mail>
  <mail>
    <header>Re: bilinear interpolation in core graphics</header>
    <body>On Feb 26, 2008, at 8:21 AM, chris wade wrote: CGContextSetInterpolationQuality is a hint, and only a hint, to the context about making a speed/quality tradeoff when drawing the image. The actual interpolation mechanism applied to the drawing is up to the context itself and there is no direct way to select a particular interpolation mechanism.</body>
  </mail>
  <mail>
    <header>bilinear interpolation in core graphics</header>
    <body>I'm trying to use bilinear interpolation on my Image to show non pixelated results, i am currently using CGContextSetInterpolationQuality but am finding that it mearly blurs each pixel, you can still see each pixel, I wondered if anyone knows if it is possible to use CGContextSetInterpolationQuality to get bilinear interpolation with Core Graphics. Chris</body>
  </mail>
  <mail>
    <header>Re: Creating PDF from NSView (fast)</header>
    <body>Some time ago, I wrote this: I don't know whether it still works with Leopard, but you could try something similar. Alternatively, you could generate the necessary PDF data yourself; the PDF specification is here: --</body>
  </mail>
  <mail>
    <header>Fast image drawing: Is decode_meshed_mx8bpc_x_3s a bad sign?</header>
    <body>I have an app that relies on doing a lot of drawing of a new image. Specifically, it does drawing into a CG bitmap context in one thread or process, then passes it off to another thread or process for drawing. Right now, I'm working on the cross-process case, so the data is passed in a raw format. I'm trying to optimize this; we're not getting great performance out of it right now.  Ignore synchronization and transport issues, as the former are working just fine and the latter have been heavily optimized by others already.  I'm a little surprised by how much time Shark says we're spending in drawing the image. CGContextDrawImage CGContextDelegateDrawImage ripc_DrawImage ripc_AcquireImage CGSImageDataLockWithReference img_data_lock img_decode_read decode_meshed_mx8bpc_x_3s CGColorTransformConvertNeedsCMS malloc_zone_malloc CGImageGetMaskingColors CGAccessSessionRelease ripc_RenderImage ripc_ReleaseImage rpc_DrawImage takes up almost all of the image drawing time.  Of its cycles, about 60% are in ripc_AquireImage and 40% are in ripc_RenderImage.  I'm particularly interested in the time spent in aquire, and especially decode_meshed_mx8bpc_x_3s.  Is this bad?  Is there a way to reduce or eliminate this decoding step?  The image data is being created as 8 bits per component, 32 bits per pixel, kCGImageAlphaNoneSkipFirst, and kCGBitmapByteOrder32Host.  (This is, of course, the same information we use for our bitmap context.) I create the image using CGImageCreate with a data provider created using CGDataProviderCreateWithData. So, are there any obvious gotchas here?  Is the decode stuff above significant?  It's conceivable that the decode time actually comes from other image rendering calls, I should mention, but I think that I focused it properly.  Mostly I want to know what the decoding actually *is*.  In theory, the image should be in a good format and shouldn't require decoding......right?</body>
  </mail>
  <mail>
    <header>Creating PDF from NSView (fast)</header>
    <body>Hi, I am trying to draw an NSView into a PDF. I know of -dataWithPDF... and the various other methods, but unluckily they're really slow. My goal is to create a multi-page PDF, each page contains one JPEG image, at an approx. resolution of 500 x 300 pixels. NSGraphicsContext *context = [NSGraphicsContext The NSLog yields an NSBitmapGraphicsContext, but I assume that's okay. However, my generated PDF file is broken, preview cannot read it. I cannot find any obvious errors (wrong NSRects, etc.), everything SHOULD work as expected. What could be the problem? Again: My basic goal sounds fairly simple: Draw an NSView into a PDF using Core Graphics (instead of Cocoa-routines). Besides: If somebody knows for a fact that I wouldn't gain any speed (compared to -dataWithPDFInsideRect:), please tell me, as that would be the only reason for me to go through all this trouble. Regards, Michael PS: And yes, y'all guessed correctly, the whole purpose here is to create a QuickLook preview PDF inside my document package :) ---------------------------------------------------------------- This message was sent using IMP, the Internet Messaging Program.</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContext, is it really that slow?</header>
    <body>Paul and Community,</body>
  </mail>
  <mail>
    <header>Re: CILanczosScaleTransform sometimes producing empty output</header>
    <body>I have narrowed down this issue to being hardware specific. My Macbook Pro with a Radeon X1600 does not exhibit this problem. But my Mac Pro with a GeForce 8800 does exhibit this problem. Are there any known issues with the GeForce 8800 drivers, or any potential issues I could have run into? I'm going to test to see if it could be due to some sort of race condition, but everything seems to be working fine.</body>
  </mail>
  <mail>
    <header>Re: CILanczosScaleTransform sometimes producing empty output</header>
    <body>No dice. Manually allocating the memory didn't fix it. I'm actually uncovering more test cases as I do this. I haven't figure out anything in common between them yet. I uncovered another test case where I can reproduce the issue at a scale of two. The only other thing is that at some points in my program I perform the scale in a second thread. The test case I am posting is happening on the main thread, but could a previous scale on a secondary thread screw anything up?</body>
  </mail>
  <mail>
    <header>Re: Real-time rendering to a 2-D image in a window?</header>
    <body>If your tile is 256x256 or 4000x8 doesn't really matter all that much :). Tile it in a way that makes sense. If that is thin horizontal strips or chunky squares or whatever, the point is to batch a large number of pixels together to draw at once. Very much so. The closer you get to real time, the more overhead there is (you may not have noticed it previously, but poking a pixel to the framebuffer per update is considerably higher overhead than batching any larger number of pixels). I realize your worried about the amount of time it could take to generate an update if you batch them, but if your only concern is ensuring that the calculation is progressing correctly, then unless I'm missing something seeing 1 pixel at a time vs seeing 1 line at a time shouldn't be a big difference. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: CILanczosScaleTransform sometimes producing empty output</header>
    <body>Perhaps try making the bitmap's buffer yourself.  I've had similar issues letting the bitmap handle the memory itself and not returning a valid CGImage. On Feb 22, 2008, at 11:58 PM, Colin Cornaby wrote:</body>
  </mail>
  <mail>
    <header>CILanczosScaleTransform sometimes producing empty output</header>
    <body>I have the following code that takes a CGImageRef, and tries to scale it to a larger or smaller size based on the bounds of the view doing the scaling. The following is the code I use: CIFilter *scaleFilter = [CIFilter [scaleFilter setValue:[NSNumber numberWithFloat:scale] [scaleFilter setValue:[NSNumber numberWithFloat:aspectRatio] CGContextRef finalImageContext = CGBitmapContextCreate(NULL, [finalImage extent].size.width,[finalImage extent].size.height, 8,4*ceil([finalImage extent].size.width), colorSpace, CIContext *finalCIImageContext = [CIContext finalCgImage = [finalCIImageContext createCGImage:finalImage fromRect: I can reproducibly cause this code to output a blank image with a scale of .5 and an aspectRatio of 1. The bizarre thing is I can only reproduce this behavior after scaling a specific of series of images. I discovered the issue after scaling two specific images, and then trying to scale any 512x512 image. Any 512x512 image scaled after the first two images produces blank output. I can then run the exact same image through a CILanczosScaleTransform with the same scale and the same aspectRatio a second time, and it will work. This leads me to believe it's some sort of memory issue. However, I'm having a hard time pinning down if the issue is when I create the final CGImageRef, or if the issue is within the CIFilter itself. I have verified the initial CGImage I am pushing in is good. The bad output is always of the correct dimensions, but entirely empty. Thanks, Colin</body>
  </mail>
  <mail>
    <header>Re: Real-time rendering to a 2-D image in a window?</header>
    <body>On Feb 22, 2008, at 10:09 AM, Nick Turner wrote: Well if your image is made of a several paths and points you can keep the current path in a CGPathRef and track the colors and whatnot that was applied to the paths.  Only maintain the user's view but always be keeping track of the paths and colors used to produce the view.  Then at save time you make the large bitmap, scale the paths to what they should be and apply colors and save it to file. You can use scaling, translation, and clipping on the view's context so the drawing commands issued to the view are looking at the right section.  Even if some of the drawing commands are outside the user's viewable area they will be ignored for being outside the clipping area.  The scale and location of the area will be translated by the context.  On the other hand the same commands are being fully rendered to the entire bitmap. Something to remember, when you apply a clip rect to the context, save the graphics state first.  You can't expand the clipping area, the only way to get it back is to restore the state back to when it was full sized. I'm not sure on the nature of the data you're using to make the images, so these suggestions may not work for you.</body>
  </mail>
  <mail>
    <header>Re: Real-time rendering to a 2-D image in a window?</header>
    <body>Would rendering to 2 contexts at the same time do what you want? One would be the context for the display, while the other would be the model universe in bitmap format. All the rest of the code would be the same for both. ____________________________________________________________________________________ Never miss a thing.  Make Yahoo your home page.</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContext, is it really that slow?</header>
    <body>Please keep in mind I still new to the Apple dev community, but I am not a new developer. The reason I started off using Instruments is because I thought it was the only analysis tool available. The definition of the ObjectAlloc instrument is as follows: &amp;quot;Analyzes the memory life-cycle of process' allocated blocks.&amp;quot;  Frequent allocation and deallocation of objects in memory is a costly procedure. If there is a significant difference between the amount of allocations that the two different versions of the code are making, wouldn't you agree that can be considered a performance issue? Also wouldn't analyzing these increases help determine possible performance bottlenecks? However, I found Shark and I have started to play around with it a little bit. It is a little less intuitive then some of the other developer applications. I still need to poke around and understand what it is I am actually looking at after it does its analysis. As far as color spaces go, my CIImages, CIContext, and CGBitmapContext are now all being explicitly told to use the generic RGB color space. This causes no change in the performance of the application. It is still possible I am miss understanding what everyone is trying to convey about color spaces. On a side note, I did get some interesting results last night. Originally I was drawing a CIImage, which is created from a CVPixelBuffer, to the CIContext, which is linked to a CGBitmapContext, and generating a CGImage based on the CGBitmapContext. I modified my processImage function to instead return a CGImage generated from the CIImage using the CIContext. The processor usage only dropped by 10-15%, but it was enough to drastically reduce the visual lag. I am not sure what the difference is between these two approaches and why the yield different performance results. I am also not sure if I can access the pixel data. I think it should be theoretically possible through two different avenues that I need to investigate. Another side bar, I do believe this topic is one that is frequently discussed on many of the lists, modifying an image -&amp;gt; getting the pixel data of the modified image. I have spent a decent amount of time combing the developer's site for solutions. From what I can tell there are essentially 2 possibilities: CGBitmapContext or using OpenGL with some form of read back from the GPU. Unfortunately I have not seen anything about how to actually accomplish this task. There are numerous posts in the lists with suggestion on how to got about doing it, but nothing formally showing the best approach using either option. Is there any way to ask Apple for an addition of such a guide to the developer's site? I could be simply over looking some document, example code or list post, and I apologize in advance if that is the case. I agree that this task can be accomplished in many ways using the tools provided by Apple, but I wouldn't mind seeing an approach done by the people who made the tools in the first place. Cheers, Carmen On Feb 22, 2008, at 5:35 AM, Paul Sargent wrote:</body>
  </mail>
  <mail>
    <header>Re: Real-time rendering to a 2-D image in a window?</header>
    <body>You seem to be suggesting that I should draw the rendered image in pieces.  Sadly, that is not possible because the points that make up the image are generated across the entire image during the entire render process.  If I were drawing a ray-traced image then this chunkified rendering might be possible, but not with this app. Again, what I need is a way to simultaneously do a progressive, high- resolution render in a huge bitmap and at the same time inspect the render-in-progress by looking at it (partially or in a zoomed-down whole) in a custom NSView.  And I need to do this with minimal overhead for the NSView updates, yet I want to update as close to real- time as possible.</body>
  </mail>
  <mail>
    <header>Re: Real-time rendering to a 2-D image in a window?</header>
    <body>I'm not getting what you are suggesting here.  I can't render just to the window because the window's view could be just a portion of the image, and in any event even if the window's view shows the whole image it will be a scaled down version of it.  There has to always be an intact copy somewhere of the whole 4000X4000 bitmap, into which the continuous rendering is happening. This above totally makes sense to me.  I am already anticipating that the big bitmap will need its own rendering thread.  The question is, what's the best way to reflect, in a custom NSView, the ongoing progress of the rendering, in nearly real time, without seriously impacting the amount of CPU that is available for that rendering?  How can I keep a real-time zoomable, pannable NSView &amp;quot;peek&amp;quot; of the big bitmap, without spending most of my CPU time actively updating that NSView's CGContext? I am thinking at this point that it's probably coming down to limiting the frequency of the updates to maybe once or twice per second, to fine tune the tradeoff between (1) cycles spent scanning the big bitmap and converting it into the &amp;quot;peek&amp;quot; to be displayed in the NSView, and (2) the cycles spent actively rendering to the big bitmap. Or maybe, to make it even more efficient, don't update at all if there's been no mouse movement in the last few seconds, then update when the user kicks the machine with a mouse move. I want to render as fast as possible, but I also want to be able to see what's happening in there!</body>
  </mail>
  <mail>
    <header>Re: Real-time rendering to a 2-D image in a window?</header>
    <body>First off, I would recommend a radically different approach from what your used to. Mostly because framebuffer writes are *not* the fastest way to go, but also because they don't play nicely with the window server. Approaches that try to draw to a window outside of the event system have similar problems, although you likely won't realize these problems until you try to implement more complex windows. There was a sample from WWDC that did something similar to what you want to do (that was mostly designed to show off NSOperation) but I can't find it at the moment. The basic idea is that you compute your fractal in chunks on a background thread, then as you complete each chunk you fire off a message to the main thread telling it what part of the image data is complete. You would typically do this in tiles of say 256x256, but any fairly large tile would be fine (i.e. don't try to update pixel by pixel). When the main thread gets around to drawing, it checks up on the chunks that need to be drawn and one by one creates a CGImageRef for each chunk of data and draws that image (no bitmap context necessary). You will likely want to retain the images per tile for redraws. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContext, is it really that slow?</header>
    <body>On 22 Feb 2008, at 02:30, Carmen C. Cerino Jr. wrote: That really doesn't tell you much, and why you're using ObjectAlloc for a performance issue I don't know. I have to side with Martin here. Run Shark on it and measure what is taking the time. At the very least you'll find out which call in your code is taking the time, but you'll probably get a clue as to what process in the Apple code is taking the time. The problem is probably colour space or format conversion, but until you get that profile you won't know. Just put your code in a loop, and use the default 30 second time profile in shark. Once it's done select Heavy view (bottom right of the window) and expand the top item to see what routines are calling that top hit. Paul</body>
  </mail>
  <mail>
    <header>Re: Real-time rendering to a 2-D image in a window?</header>
    <body>What would be more optimal if possible is maintain the user's view and rendering just to the window, and keeping track of the state information.  When they want to save the image generate the large bitmap then using the current state info. If you must maintain a view and a large bitmap simultaneously, you might want to maintain the bitmap on a lower priority secondary thread, so the user's view is more responsive. The latter may take a decent amount of coding setting up distributed objects communication or using an even queue. On Feb 22, 2008, at 12:53 AM, Nick Turner wrote:</body>
  </mail>
  <mail>
    <header>Re: Real-time rendering to a 2-D image in a window?</header>
    <body>On Feb 21, 2008, at 6:39 PM, mikevann wrote: I don't think the above is what  want, since the huge, 4000+ pixel image will eventually have to be saved off to a file, and the window's graphics context does not seem like the right place to build such a thing.  No, I definitely need a real, full size bitmap to render into. Still, this is quite interesting since it seems to come closest to the old-fashioned Classic approach of drawing directly into the window's view using QuickDraw.  For an app where one needs to draw quickly into a view, say by scribbling with the mouse pointer, without any great need to save the results, this could be a good quick and dirty solution. So we can create the image without copying the bitmap, as long as we don't change the bitmap.  Then put the latest version of the image into the NSView in the window, release the image, and until we do this again the window will still update from its own backing store? Doesn't the NSView need to have a valid image to draw from at all times, or is it enough just to draw it once from an image, then release the image? And if I create an image from a bitmap using CGBitmapContextCreateImage, the bitmap is really and truly still mine, so that I can release the CGImage and the CGContext will still be valid? About how long would it take to create an image from a 4000X4000 RGBX bitmap, then update the image into a 500X500 (say) NSView inside of a window?  Wouldn't this still be a fairly expensive operation, that one might not want to do more often than once or twice per second? What about this idea: instead of trying to draw from the big bitmap into the NSView, use the first trick described above to draw a clipped, scaled version of the rendering directly into the NSView's CGContext, while also drawing the same pixels (without clipping or scaling) into the big bitmap.  If the user resizes, zooms, or pans the NSView, we can do the whole image making thing, but for moment-to- moment rendering, just render twice, once into the NSView and once into our big CGContext. Seems to me it would be faster to draw the pixels in both places than to be constantly, repeatedly creating images and copying them into the NSView's context, just to achieve nearly real-time updates. Could this be how Photoshop accomplishes its real-time painting magic, by drawing in both contexts at once? As I'm sure you can tell, this design is still fluid.  Any more suggestions would be appreciated! Maybe someone out there has already created what I need: a scalable, pan-able Cocoa view class, where you can tell it how big to make the source bitmap, and tell it which NSView it draws into, and it has wrappers for the important drawing functions, and it knows when it needs to update its view.  When the rendering is done, you can tell it to save the bitmap into a file.  That would be so nice.  Anybody got one of those lying around?  :-)</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContext, is it really that slow?</header>
    <body>I will check into the color space suggestion, but I think my CGContext's color space is set to the generic RBG colorspace. I am avoiding the GPU as of now because I need to work with the raw data after the effects have been applied. I am not sure if the read back from the GPU will be slower then what I have now.</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContext, is it really that slow?</header>
    <body>The CIImage should get the CGColorSpace from the CVPixelBuffer (I don't know which colorspace it'll use when none is specified). As you are passing no CIContext options, it'll use the defaults: linear Generic RGB as working space and Generic RGB as output space. At last, there's the CGContext colorspace - I'm not sure if CI is smart enough to see it's using a different colorspace than the CIContext output-colorspace and do one more conversion or if it will just ignore this. Anyway, there's a lot being done in the background (at least 2 gamma LUTs and maybe some more matrix calculations for every pixel). You could probably optimize this a bit - but if you intend to use CI to add some filters you might want to use the CI on the GPU.</body>
  </mail>
  <mail>
    <header>Re: Real-time rendering to a 2-D image in a window?</header>
    <body>CGContextRef ctx = [[window graphicsContext] graphicsPort];  //Mac OS X 10.4 and later CGContextRef ctx = [[NSGraphicsContext graphicsContextWithWindow:window] graphicsPort];  //Mac OS X 10.0 and later If your window is double buffered you'll need to call CGContextFlush to push the image to the on screen buffer (or NSWindow's flushWindow). If you need to use a bitmap, then you can draw the bitmap to the window using CGBitmapContextCreateImage and then CGContextDrawImage. Generally if you don't modify the bitmap until after using and releasing the CGImage, you won't be generating an actually copy of the bitmap for the CGImage; it will just point directly to the bitmap data.  Modifying the bitmap while still retaining the CGImage will cause the bitmap to be copied so the CGImage contains the bitmap's state at the time of the CGImage's creation. You could also use NSBitmapImageRep in a similar way.  Set the window's NSGraphicsContext as current, call draw for the rep. On Feb 21, 2008, at 5:47 PM, Nick Turner wrote:</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContext, is it really that slow?</header>
    <body>If I remove the call to my processImage function and just draw the pixelbuffer, my CPU usage drops by half. The issue has to be somewhere in that function.  I used Instruments to compare my code with and without the processImage function, and the ObjectAlloc instrument shows a huge difference between the two.</body>
  </mail>
  <mail>
    <header>Real-time rendering to a 2-D image in a window?</header>
    <body>I've been lurking on this list for a few weeks now, hoping someone is trying to do what I am trying to do, and I'm finally at a point where I'm ready to tear my hair out, so here goes... I'm pretty new to Quartz, having come from an old-fashioned Quickdraw background.  What I want to do seems simple enough but I just can't seem to figure out how to do it.  I've looked at OpenGL, but that seems way too high-end and complicated for what I need. I have an old Classic app that renders highly complex shapes (similar to fractals) into a bitmapped RGB space.  I want to rewrite this for Cocoa under Leopard.  In the old Classic app, I grab the whole screen and just draw right into video memory, which is as fast as it gets. Of course, the largest image I can render is the size of the screen, so that's a problem.  In the Cocoa version the image has to be drawn into a very large bitmap, let's say 4000 pixels wide, and i also need to be able to see the incomplete image as it is being rendered, by zooming and panning around in a window view. The rendering is done pixel by pixel, with hundreds or even thousands of pixels being drawn per second.  Since the pixel outputs are calculated as part of an extended process that builds the image slowly over time, I can't just render the pixels as part of a drawrect call - I actually need to render continuously and have the image-in-progress viewable in a window on the screen.  These images can take hours to render. The challenge is that, since these images can take a long time to build, I want the window updates to be done as blindingly fast as possible.  I suppose update rates of once per second would be (grudgingly) acceptable, but ideally I would like each pixel to show up immediately in the window, as it is added to the bitmap. Looking in Quartz docs, I see that I can create a bitmapped graphics context and render into it, but to get that picture into a window I have to stop drawing, make a copy of the entire bitmap, make that copy into an immutable image, composite the image into the window, and then I can go back to my drawing into the saved copy of the original bitmap.  Next time I want to update the window image I have to throw away the old image, and redo the whole sequence I just described. Gaaaah! I just don't want to spend all the CPU to make a whole damn copy of what could easily be a 4000 pixel wide bitmap, several times per second.  I want to spend those cycles rendering, not slavishly copying pixels! I sure miss the days when I could just grab the whole darn screen and draw directly into the hardware image buffer. I am an avid Photoshop user, and it just bugs me so much that in Photoshop I can scribble huge volumes of freshly rendered pixels right into the damn window and they show up in ultra-fast real time and also go into the backing bitmap of which the window image is a copy.  How the heck do they do that?  Are they sidestepping Quartz entirely? I haven't done much actual coding for this app in the Cocoa/Leopard mileu because I still don't know the best way to approach the whole issue of update speed into a Quartz window from a bitmap-in-progress.</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContext, is it really that slow?</header>
    <body>Don't make assumptions. Measure your code's performance (NSLog will do in a pinch), and optimise based on that. I wouldn't bother throwing away what you've got until you can characterise the problem - otherwise you might find you've just got the same (or a different) problem with the alternative API ... On Thu, Feb 21, 2008 at 8:29 PM, Carmen C. Cerino Jr. --</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContext, is it really that slow?</header>
    <body>I altered the code so the CIContext is only being created once, but unfortunately it does not help much. I am just assuming this process is an intensive one. Maybe I should switch to OpenGL? This code is also an Obj-c and C++ hybrid. I am not sure if that will cause performance issues or not. On Feb 21, 2008, at 2:12 PM, Mark wrote:</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContext, is it really that slow?</header>
    <body>Theres no need to create/release the CIContext for every frame, just create and retain it once in &amp;quot;main&amp;quot;, and use it in processFrame.</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContext, is it really that slow?</header>
    <body>I've got a vague recollection that might be a relatively expensive operation. You might try commenting everything else out  and seeing if it still hits you hard when you're just running that. If so, I think you can probably just set it up beforehand ... --</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContext, is it really that slow?</header>
    <body>Then you should sample manually the time taken by the process, starting from very wide, to finer, and you will see what function takes more time. Camera : some raw image that need demosaiking ? This could be one reason that would explain the 3s delay.</body>
  </mail>
  <mail>
    <header>CGBitmapContext, is it really that slow?</header>
    <body>Howdy, These are initialized in main: pMungData-&amp;gt;cgContext = CGBitmapContextCreate ( pMungData-&amp;gt;image, (size_t)pMungData-&amp;gt;width, (size_t)pMungData-&amp;gt;height, (size_t)8, (size_t)pMungData-&amp;gt;width * 4, CGColorSpaceCreateDeviceRGB(), In my decompression session's callback function which is attached to a Sequence Grabber: processImage(pixelBuffer, pMungData-&amp;gt;cgContext, pMungData-&amp;gt;width, The processImage function is defined as follows: void processImage(CVPixelBufferRef pb, CGContextRef cgContext, int CIContext *ciContext = [CIContext contextWithCGContext:cgContext //Apply filters here. This will happen later when I figure out how to get this working properly. [ciContext drawImage:image atPoint: CGPointZero // use integer coordinates to avoid interpolation In general there is around a 3 second delay from what the camera should be seeing and what is being drawn on the screen. This code also hits my CPU and memory pretty hard. I have a feeling I am just doing something wrong. Thanks, Carmen</body>
  </mail>
  <mail>
    <header>Re: Render CIImage and Back</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>RE: Render CIImage and Back</header>
    <body>&lt;FONT face=Verdana color=#0000ff size=2&gt;Mark, &lt;FONT face=Verdana color=#0000ff size=2&gt; &lt;FONT face=Verdana color=#0000ff size=2&gt;Thank you very much for your help. I'll investigate in this directions. &lt;FONT face=Verdana color=#0000ff size=2&gt; &lt;FONT face=Verdana color=#0000ff size=2&gt;And what about CIContext:render:toBitmap:rowBytes:boubds:format:colorSpce ? &lt;FONT face=Verdana color=#0000ff size=2&gt; Mark [mailto:email@hidden] SCHMITT Jean-Claude RD-MAPS-ISS email@hidden Re: Render CIImage and Back You'll need to code some OpenGL, there's no Apple specific API for glReadPixels is the first stop, next would be PBOs (Pixel Buffer Objects) to make the readbacks faster (asynchronous). If you are not displaying the contents you might also want to look at FBOs (Frame Buffer Objects) to render to a texture (instead of a GL view). The key to get fast readbacks is choosing the "correct" pixel formats (different for PPC and Intel !). Look at the Apple documentation about OpenGL and also on the GL lists - there are many discussions on this there. On 19.02.2008, at 15:30, &amp;lt;&lt;A wrote: &lt;SPAN class=000052414-19022008&gt;Hi, &lt;SPAN class=000052414-19022008&gt; This problem has probably been discuused in the past, but I can not found any clear and complete answer: &lt;SPAN class=000052414-19022008&gt; To increase performances in a real time video application I would like to use the GPU (instead of the vImage librairy)&amp;nbsp;to make some picture transformations. I use CIImage to filter through a CIFilter and send to the GPU via a [NSOpenGLContext drawImage: atPoint: fromRect]. The problem is now to recover my transformed picture from the GPU. Any suggestion for Tiger and/or Leopard ? &lt;FONT face=Verdana size=2&gt;Do not post admin requests to the list. They will be ignored.Quartz-dev mailing list &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;(&lt;A href="mailto:email@hidden"&gt;email@hiddenHelp/Unsubscribe/Update your Subscription:&lt;A email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGWindowListCreateImage Colorspace</header>
    <body>(kCGColorSpaceICCBased; kCGColorSpaceModelRGB; CG Display RGB Color Space) is what I get.</body>
  </mail>
  <mail>
    <header>Re: CGWindowListCreateImage Colorspace</header>
    <body>D'oh! kCGColorSpaceModelRGB; CG Display RGB Color Space) is what I get. Where does this fall in the wonderful world of colorspaces? Why does CG have it's own special color space for displays, and not use the generic RGB? More importantly to me, what makes it different on different machines, and do I have any control over it? Also, when calling CGImageCreateCopyWithColorSpace, does that just replace the color profile, or does it actually map the image from the old profile to the new profile? Cheers, Dave</body>
  </mail>
  <mail>
    <header>Re: CGWindowListCreateImage Colorspace</header>
    <body>On Feb 20, 2008, at 7:53 PM, Dave MacLachlan wrote: The color space for the returned image is built using CGColorSpaceCreateWithName(kCGColorSpaceDisplayRGB). Mike Paquette email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGWindowListCreateImage Colorspace</header>
    <body>Checkout CGImage's CGImageGetColorSpace to find out the colorspace of the returned image.  Also you can use CGImageCreateCopyWithColorSpace to make a new image using the colorspace you want. On Feb 20, 2008, at 8:53 PM, Dave MacLachlan wrote:</body>
  </mail>
  <mail>
    <header>CGWindowListCreateImage Colorspace</header>
    <body>When I call CGWindowListCreateImage, what colorspace is the image of my window being created in? Details: I have some UI unit tests that compare windows. I start off my unittest by changing the monitor profile to the generic RGB profile. I then create a window, populate it with some UI elements, and call CGWindowListCreateImage on it. I then draw the CGImageRef I get back into a NSBitmapImageRep with a NSCalibratedRGBColorSpace, and compare that NSBitmapImageRep against a master image (that I saved off earlier, potentially on a different machine, using the same steps) that has been loaded into an NSBitmapImageRep with a NSCalibratedRGBColorSpace. This appears to work fine when I get the masters using my HP flat screen monitor and then run the tests on my MacBook monitor (the Macbook and the HP screens have considerably different display characteristics), however when I run it on the headless build machines, my image has small pixel difference that appear to have been caused by different color spaces. Since I am using the generic profile everywhere, I would expect the images to be identical. This does work for all my other unittests that don't use CGWindowListCreateImage, so it appears to be something interesting about that call. I used to use an OpenGL solution, but it appears I can no longer get a NSOpenGLPixelFormat for a headless machine. NSOpenGLPFAFullScreen, NSOpenGLPFASingleRenderer, NSOpenGLPFANoRecovery, NSOpenGLPFAScreenMask, CGDisplayIDToOpenGLDisplayMask((CGDirectDisplayID)[theCGID intValue]), 0  // end of list terminator worked just fine under Tiger, but appears to have broken recently. I'm not sure if it was Leopard, 10.5.2, the graphics update or QuickTime update that broke it. This is why I was attempting to switch over to CGWindowListCreateImage. Cheers, Dave</body>
  </mail>
  <mail>
    <header>Leopard Inverts CMYK jpeg pixel values - feature or bug ?</header>
    <body>I write out jpegs from NSBitmapImageReps: NSData* jpegData =   [bitmapRep representationUsingType: NSJPEGFileType (The propertyDict has the profile.) This works fine on Tiger, but produces a black mess on Leopard if it is a CMYK image. I tried rewriting things to use ImageIO instead. Same result. On a whim I tried inverting the pixel values: get the bytes from the image rep and doing Well, that makes it work on Leopard. I gather this has something to do the allegations floating around the web and the comments in libjpeg that Photoshop writes the pixel values inverted. Certainly PS seems to be able to distinguish between the two and do the correct thing in either case. It renders the original versions from Leopard correctly. Preview doesn't. So now we have the case where the Apple code writes jpeg's Apple code can't read properly. Could someone please identify whether this is a bug or a permanent feature ? If it's a &amp;quot;feature&amp;quot; I can just do If it's a bug that's going to get fixed I have to do something ugly like write out a tiny jpeg  at start-up and read it back in  to see whether or not I get back what I started with. Then I can decide whether to invert.</body>
  </mail>
  <mail>
    <header>Render CIImage and Back</header>
    <body>&lt;SPAN class=000052414-19022008&gt;Hi, &lt;SPAN class=000052414-19022008&gt; This problem has probably been discuused in the past, but I can not found any clear and complete answer: &lt;SPAN class=000052414-19022008&gt; To increase performances in a real time video application I would like to use the GPU (instead of the vImage librairy)&amp;nbsp;to make some picture transformations. I use CIImage to filter through a CIFilter and send to the GPU via a [NSOpenGLContext drawImage: atPoint: fromRect]. The problem is now to recover my transformed picture from the GPU. Any suggestion for Tiger and/or Leopard ?</body>
  </mail>
  <mail>
    <header>CVPixelBuffer memory suddenly goes bad ...</header>
    <body>Apologies if this isn't quite the right list. I'm writing some screengrabbing code, based quite closely on the OpenGLScreenCapture code. I have a set of FrameReader objects, which are pretty much identical to the sample code for all practical purposes. Each FrameReader has a CVPixelBuffer ref, and In one mode (Record), data is copied from the screen to the pixel buffer, and from there its written to disk. In another mode (Replay), data is copied from a file to the pixel buffer, and then the pixel buffer is fed to a movie compression session. All this seems to work fine. I create a new set of FrameReader objects for each record or replay operation. I can see logging messages in the dealloc methods, and see memory consumption rise and fall exactly as expected. I'm pretty certain that all of the associated NSOpenGLContexts etc are being correctly released and cleaned up when I've finished with the set of Frame Readers. However, after a few operations, when I'm trying to record, and reading the pixel buffer memory, I get a bad memory access. I'm definitely not over-running the buffer. I lock the CVPixelBuffer's base address before I access it, and have checked all the indexes and pointer arithmetic. The PixelBuffer is created from a pool, and the return values of each step in its creation are checked. Each FrameReader seems to have its own pool, even though it only creates one pixel buffer. I'm going to try using pixel buffers based on memory I allocate, but I just thought I'd ask, is there any obvious reason, dumb errors on my part apart, that would cause me to start getting pixelbuffers with bad memory? Here's the (unchanged sample code) that I'm using to create the CVPixel buffers ... #if __BIG_ENDIAN__ [attributes setObject:[NSNumber numberWithUnsignedInt:k32ARGBPixelFormat] #else [attributes setObject:[NSNumber numberWithUnsignedInt:k32BGRAPixelFormat] #endif [attributes setObject:[NSNumber numberWithUnsignedInt:width] [attributes setObject:[NSNumber numberWithUnsignedInt:height] //Create buffer pool to hold our frames theError = CVPixelBufferPoolCreate(kCFAllocatorDefault, NULL, if(theError != kCVReturnSuccess) // Create pixel buffer from pixel buffer pool theError = CVPixelBufferPoolCreatePixelBuffer(kCFAllocatorDefault, NSLog(@&amp;quot;CVPixelBufferPoolCreatePixelBuffer() failed with error %i&amp;quot;, --</body>
  </mail>
  <mail>
    <header>Re: Finding the fast path - Slow CGImage draw performance and	argb32_sample_ARGB32...</header>
    <body>Well, I've already submitted it as a bug report to the Qt guys and they seem to mostly agree with me in their response - they were already aware of the issue and gave me code for their patch.  They now draw everything to the screen using the display profile while things drawn to other devices get a generic RGB profile (I'm not sure I agree with that either).  As a generic framework Qt needs to provide consistent, identical functionality where possible so that those who use it know what to expect.  We ship our apps for Mac, Windows and Linux and we need Qt to work in the same way in order to provide a consistent application.  We can't have it doing some arbitrary and uncontrollable color management on the Mac while it does no color management on other platforms. Either way, it's not a topic for Quartz Dev so we should probably take it off list if you want to discuss it further. -- Dave Thorup Software Engineer</body>
  </mail>
  <mail>
    <header>Low latency video mapping on 3D objects</header>
    <body>I'm rather new to MacOSX programming and&amp;nbsp;I really could use some advise on how to handle the following problem. I'm working on an interactive video project,&amp;nbsp;for which&amp;nbsp;some video streams are mapped onto 3D objects like Different video streams should be mapped on the different sides of this rotating cube. The position and rotation of the cube is controlled by external signals (e.g from USB or FireWire ports). The biggest&amp;nbsp;problem I'm facing is that I need really low latency. So the time between a change of the control signal and the actual change on the screen should be as short as possible. I'm able to synchronize the control signals to the refresh rate of the screen updates. So when this control values are read from the USB or FireWire port, I have to calculate the positions and rotations of the objects. Then I now the coordinates to map the video to. But how do a manage to map the video with very short latency? Is it possible for instance to give the graphic chip instructions&amp;nbsp;such that it wil map video output from QuickTime onto some surfaces for the next frame it is going to display? So without double buffering. Could I setup a list of mapping instructions just before the graphic system starts to display the screen? Does Quartz-composer something like that? I really could use some advise here, so that I know where to search for more information.</body>
  </mail>
  <mail>
    <header>Re: Quartz iOS Performance</header>
    <body>Hello, Thanks for your reply. I do it like this: That's what Sander recommended too, isn't it? I'll try it. I don't quite understand this. The initial ctm of the current context is [2, 0, 0, -2, 0, 960] on a retina display device. So CGContextDrawImage() of a 320x480 image does a scale. When I change the ctm to [1, 0, 0, -1, 0, 960] and draw an 640x960 image there is no scale. Btw, this is how I get the initial ctm. Do I do something wrong? Haven't heard of these, I'll check it out. Well I need 20 fps for an animation that deserves its name, so that kind of performance is crucial. Basically what you say means not to use CGPath drawing on iOS except for very simple things, doesn't it? Thanks and regards, Sebastian Mecklenburg</body>
  </mail>
  <mail>
    <header>Re: Quartz iOS Performance</header>
    <body>Hi Sebastian, This was a regression in iOS 4. in previous versions, CGContextDrawImage was fast enough for my game Sjoelen, which draws up to 30 game elements this way.  I used it to scale the images as well (I couldn't realistically ore-scale the images as my game is pseudo-3D and the graphics scale as they move across the screen). For more information see for instance  and My solution after I found out about this (with my ratings plummeting in the mean time) was a complete rewrite of my graphics stuff to use CALayers instead.  The graphics look less nice when scaled (decidedly worse than on iOS &amp;lt;4) but the performance was back to normal. Those were a hectic few weeks... :-( Regards, Sander _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Quartz iOS Performance</header>
    <body>Probably won't help much, but it would certainly help to see how you are creating your bitmap context. You may get a slight boost doing this by assigning the created image to view.layer.contents instead of using -drawRect: and -setNeedsDisplay however (most noticeably on Retina devices since you will avoid the software scaler). You shouldn't need to do anything to the current context to get this. If you double the size of your buffer on retina devices and draw it into the same view's bounds, you should get a transfer. Be certain to set your blend mode to kCGBlendModeCopy to avoid reading from the buffer (this may also help with what you already have). It really depends on your goals. You may be able to do some things by taking advantage of CAShapeLayers instead of drawing them yourself, but you would have to check the performance difference yourself. Generally if performance is your primary goal, you want to be drawing with the CPU as little as possible. -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Quartz iOS Performance</header>
    <body>oh, crap, in the draw routine i draw into my bitmap context of course, so it looks something like //draw about 40 paths like this...</body>
  </mail>
  <mail>
    <header>Quartz iOS Performance</header>
    <body>Hello, I have a little app that uses CoreGraphics to draw a few simple bezier shapes.  I use a window sized CGBitmapContext as a buffer because I need to keep the contents of the buffer from one frame to the next. My drawRect routine basically looks like this: //draw about 40 paths like this... The problem is, while it easily runs at 60 fps on a desktop (probably much more, haven't tried) it is sluggish on older iPhones (~11 fps) and is unusably slow (~6 fps) on new iOS devices with the retina display. Here the screen size is still reported to be 320 x 480 (and so is my buffer) and the ctm of the context I get from UIGraphicsGetCurrentContext() scales everything up by the factor 2. The result is that according to the Time Profiler the app spends &amp;gt;70% in argb32_sample_RGBA32 (called from somewhere deep inside CGContextDrawImage). So my questions are: 1) Is there a better way to maintain my own backbuffer? CGLayer perhaps? Haven't used it yet. 2) If not, I can create my buffer at twice the size, render the content at twice the size and set the scale factor of the ctm of the current context to 1. This gets me about 9 fps. CGContextDrawImage is down to about 20% but the path rendering takes longer. Even when I disable anti aliasing (which still looks good on a retina display) I get only 13 fps. Can I improve this further? Also, since there is no CGContextSetCTM method (I have to do CGContextScaleCTM(ctx, 0.5, 0.5)) I think there might be something wrong with that. 3) Is CoreGraphics usable for animated content on iOS devices at all? I read several times that it renders really high quality images at the cost of speed. For the app I have in mind I need some good looking (i.e. anti aliased) animated (i.e. different shape in each frame) bezier curves at at least 20 fps. Can it be done? It would be so cool, because the quality is absolutely amazing! Thanks and regards, Sebastian Mecklenburg</body>
  </mail>
  <mail>
    <header>Send auto-repeated key using CoreGraphics methods (Mac OS X Snow Leopard)</header>
    <body>I have been successful sending keystrokes in order to automate a particular software package for drawing that I use. This software relies a lot of keyboard shortcuts so I wrote something that could call some of these keyboard shortcuts in order to streamline my workflow. As I said, this has worked out good. My library is a Cocoa library that is loaded as a plugin to the software package. Here is a snippet of code that I have been using for sending my keystrokes. For some procedures in the drawing package if you continue to hold the Shift key then you activate a special tool. I have been unable to simulate this. I thought I could send the Shift key and say that I wanted it to auto-repeat but that doesn&amp;#39;t seem to work. I have been using the following code to set the auto-repeat: In my testing I have been unable to make any key auto-repeat. It just send the key once and that is it. Is there anyone that have been successful autorepeating a key using the above method? I have searched the Internet for answers but all I have found are some unanswered questions from 2008... Any help is greatly appreciated. Thanks, mobbe</body>
  </mail>
  <mail>
    <header>Crash with infinite loop in +[CIFilter(CIFilterRegistry) filterWithName:compatibilityVersion:]</header>
    <body>I'm getting a number of crashes from an infinite loop in my app when it launches for some folks.  The users don't seem have anything special installed, and I'm not doing anything tricky with CIFilters as far as I know.  I've not been able to reproduce it myself either. Here's the stack trace, and this is for Mac OS X 10.6.7 (10J869) Thread 0 Crashed:  Dispatch queue: com.apple.main-thread 0  com.apple.CoreFoundation          0x00007fff8815d9e0 __CFGetConverter + 160 1  com.apple.CoreFoundation          0x00007fff8815d929 CFStringEncodingGetConverter + 9 2  com.apple.CoreFoundation          0x00007fff8816e3d9 CFStringEncodingIsValidEncoding + 9 3  com.apple.CoreFoundation          0x00007fff8816d419 __CFStringEncodeByteStream + 537 4  com.apple.CoreFoundation          0x00007fff881807e7 CFStringGetCString + 327 5  com.apple.Foundation              0x00007fff87ed8f30 -[NSCFString getCString:maxLength:encoding:] + 114 6  com.apple.Foundation              0x00007fff87ed8e64 NSClassFromString + 81 7  com.apple.QuartzCore              0x00007fff810c0ade +[CIFilter(CIFilterRegistry) filterWithName:compatibilityVersion:] + 41 8  com.apple.QuartzCore              0x00007fff810c0c22 +[CIFilter(CIFilterRegistry) filterWithName:compatibilityVersion:] + 365 9  com.apple.QuartzCore              0x00007fff810c0c22 +[CIFilter(CIFilterRegistry) filterWithName:compatibilityVersion:] + 365 ‚Ä¶ 510 com.apple.QuartzCore              0x00007fff810c0c22 +[CIFilter(CIFilterRegistry) filterWithName:compatibilityVersion:] + 365 511 com.apple.QuartzCore              0x00007fff810c0c22 +[CIFilter(CIFilterRegistry) filterWithName:compatibilityVersion:] + 365 Has anyone else seen this?  Or maybe offer some suggestions on debugging it? thanks, -gus -- August 'Gus' Mueller Flying Meat Inc.</body>
  </mail>
  <mail>
    <header>Using kCIContextUseSoftwareRenderer + default NSGraphicsContext's graphicsPort causes lots of console output</header>
    <body>), and I find myself having to revisit the topic because I haven't really found a good solution to this problem.) I'm trying to setup a CIContext which uses the software renderer, and which also draws directly to a NSGraphicsContext that is handed to my NSView. Here's the relevant bits of code from a sample project: NSDictionary *options = [NSDictionary dictionaryWithObjectsAndKeys: [NSNumber numberWithBool:YES], kCIContextUseSoftwareRenderer, (The whole class is here: ) When I create the cicontext from [[NSGraphicsContext currentContext] graphicsPort], this is printed out in the console: Which isn't too bad, and my images are drawn correctly.  But for every time I draw using _cicontext in my NSView's drawRect: this prints out: Since this happens for every redraw, thousands of lines are printed out.  I consider that a bad thing. So my question is- how can I create a CIContext using [[NSGraphicsContext currentContext] graphicsPort], which doesn't print out error messages on every redraw?  (I've got QuartzGL turned off in this window as well). Sample project here: As a side note- if I setup my CIContext with the option to use the old 10.5/non-opencl renderer (fe-context-sw), then I don't get the error messages.  But I'm willing to bet that's not a recommended practice. thanks, -gus -- August 'Gus' Mueller Flying Meat Inc.</body>
  </mail>
  <mail>
    <header>QTKit capture with video effects</header>
    <body>I'm experimenting with the QTKit capture API, and it seems to work OK, but I can't figure out how to record video with CIFilter-style video effects. I'm doing something very similar to Apple's QTRecorder demo, found at which is also very similar to what's described in the QTKit capture programming guide. That demo shows video effects in the preview window via QTCaptureView's delegate method -view:willDisplayImage:, but the recorded video doesn't have the effect applied.  How can I record the video with the same effects shown in the video preview? -- Tom Harrington email@hidden AIM: atomicbird1</body>
  </mail>
  <mail>
    <header>Re: Core Image renders out of texture memory...</header>
    <body>You probably already have, but please file the bug otherwise you can't expect it to improve. I can believe GC is a hornet's nest for subtle bugs with big impact.</body>
  </mail>
  <mail>
    <header>Re: Core Image renders out of texture memory...</header>
    <body>Hi, Raphael It seems that there is effectively a conflict between Core Image and garbage collection. If I go back to the classical memory scheme, there is no more slowdown. Sampler confirms this : available VRAM and textures count of the OpenGL driver remain stables. Cyril. &amp;nbsp;_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list &amp;nbsp; &amp;nbsp; &amp;nbsp;() Help/Unsubscribe/Update your Subscription: This email sent to</body>
  </mail>
  <mail>
    <header>Re: Core Image renders out of texture memory...</header>
    <body>It seems that there is effectively a conflict between Core Image and garbage collection. If I go back to the classical memory scheme, there is no more slowdown. Sampler confirms this : available VRAM and textures count of the OpenGL driver remain stables.</body>
  </mail>
  <mail>
    <header>Re: Core Image renders out of texture memory...</header>
    <body>Thank you, Moreover, the tree and all the objects that feed it are constructed only once, when source images are available. The only thing that is done repeatidly is a call to drawImage on the ouputImage of the root of my filter tree. Links between filters of the tree are defined by directly connecting inputImage and outputImage of filters. I do not set any intermediate CIImage or CIImageAccumulator. In the early days of Leopard, I read something describing conflicts between Core Image and garbage collection... Did somebody already experience such a problem (on the 10.5.2) ? Should I deactivate garbage collection ?</body>
  </mail>
  <mail>
    <header>Re: Core Image renders out of texture memory...</header>
    <body>I had that issue as well, and it turned out I was accidentally retaining a CIImage with every pass. Make sure if you retain any images that you release them properly during your rendering cycle.</body>
  </mail>
  <mail>
    <header>Core Image renders out of texture memory...</header>
    <body>My application constructs a moderately complex CIFilter tree once, before rendering occurs. Then I draw it via calls to CIContext drawImage on the outputImage of the root filter. During execution, I force the tree to render many times by stretching the output window. Then the texture count begins to increase rapidly. It never decreases. After the free VRAM roughly fell from 250 MB down to 30 MB, the system began to swap and the rendering slowed down consequently. As I understood the 'lazy evaluation' model of Core Image, CIImages are virtual containers, and filters execute only when rendering occurs. I thought texture resources was transparently managed and freed by the Core Image API between two renderings... For info : Garbage collection is on. Source images are 512 x 512 RGBA 32 b floats CIImages. System is a PowerPC dual G5 1.8 w/ ATI Radeon 9700. Thanks in advance. Cyril.</body>
  </mail>
  <mail>
    <header>Re: Core Image Color Tracking in GPU Gems 3</header>
    <body>Thank you Frank. That is excellent. Not sure what differs between the Gems book DVD and your projects. There is some talk on another contributors errata page that the DVD has some bad segs. Kind regards, Ian ******************************* Ian Grant Senior Lecturer in Digital Art Faculty of the Arts Thames Valley University Ealing, UK W5 5DX *******************************</body>
  </mail>
  <mail>
    <header>Re: CoreAnimation with NSTextField</header>
    <body>I would imagine that that would be as &amp;quot;simple&amp;quot; as creating a cross fade animation and then attaching it to the NSTextField using [textField addAnimation: crossFadeAnimation forKey: @&amp;quot;value&amp;quot;]</body>
  </mail>
  <mail>
    <header>Re: CoreAnimation with NSTextField</header>
    <body>I would like to have the old string fade out and the new string fade in. Jordan</body>
  </mail>
  <mail>
    <header>Re: CoreAnimation with NSTextField</header>
    <body>On Mar 28, 2008, at 8:25 AM, Jordan Breeding wrote: That would depend on what you mean by &amp;quot;animate the transition... from</body>
  </mail>
  <mail>
    <header>CoreAnimation with NSTextField</header>
    <body>Is it possible to animate an NSTextField with CoreAnimation so that when the string changes it will animate the transition of the string from the old string to the new string? Attachment:</body>
  </mail>
  <mail>
    <header>Re: Finding out which areas of a screen are being painted</header>
    <body>Take a look at CGRemoteOperation.h.</body>
  </mail>
  <mail>
    <header>Finding out which areas of a screen are being painted</header>
    <body>Is there a way in Quartz (or perhaps there is some hook in the window manager?) to find out which areas of the screen are being painted per refresh?  That is, an API similar to what you can observe in Quartz Debug using the Flash screen updates option (seeing the areas of the screen that are being painted, wish flashing yellow rectangles). -- Regards, Neil Clayton</body>
  </mail>
  <mail>
    <header>Re: Core Image Color Tracking in GPU Gems 3</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>CIImage + CV + ATI Radeon HD 2600XT</header>
    <body>I have a program that uses CIImage filters to render a quicktime file via CoreVideo.  It has worked perfectly fine for the last year with no problems. Suddenly I am getting memory &amp;quot;leaks&amp;quot; (explained below) when running it on the new Mac Pro w/ ATI Radeon HD 2600 XT video card (10.5.2 w/ all software updates installed except the Time Machine and Airport update, I think it came out after this last weekend). I have tested the program on my new MacBook Pro (4 weeks old) and it works fine. The new Mac Pro (3 weeks old) exhibits a strange memory leak.  Leaks does not show any memory leaking.  ObjectAlloc (both under Instruments) shows memory to be stable for a random amount of time (ranging from 10 minutes up to 35 minutes) and then suddenly jumping by about 200MB.  When I look at the large allocations happening at that time it shows that it is being allocated by calloc and owned by libSystem.dylib. (It doesn't show me a call stack for how it got allocated which is frustrating, I am posting to macosx-dev about that part but if somebody knows an answer here I would appreciate it). Has anybody run into the like before where a (I am assuming) video card causes system memory to be allocated in this way and never freed?  I will be running more tests trying to replicate this on iMac's and other machines.  Unfortunately I won't have this Mac Pro for more than a few days for full testing as it is not mine.  It will be a few cubicles down so I can run some tests on it, but I can't keep the guys machine forever and it won't be a &amp;quot;clean&amp;quot; test environment anymore. I realize this is not much info to go on, but as I said I am assuming this is hardware related and am hoping to hear that people have run into random things like this before.  If I can't figure it out I will post a bug report, but I don't know if I will even be able to build a sample application to show off this behavior as for all I know it is some funky combination of effects I am running that is causing the &amp;quot;leaks&amp;quot;.  Hard to tell when I can't see where memory is allocated. :/ Daniel</body>
  </mail>
  <mail>
    <header>QTMovieLayer with QTMovie leaking memory</header>
    <body>Hi I'm creating a qt movie player that will play movies in a QTMovieLayer from a simple playlist When running the application with Instruments using the Leaks template, it appears the function [QTMovie idleAllMovies] is leaking memory. This is the stacktrace from Instruments 0 CoreFoundation __CFDictionaryGrow 1 CoreFoundation CFDictionaryCreateMutableCopy 2 QuickTime ICMDecompressionSessionCreateForVisualContext 3  0x9475a2d7 4  0x94765533 5  0x947667c4 6  0x947672da 7  0x947684b7 8  0x9476a0df 9 CarbonCore CallComponentFunctionCommon 10  0x94758fd5 11 CarbonCore CallComponentDispatch 12 QuickTime MediaMoviesTask 13 QuickTime TaskMovie_priv 14  0x9481ae78 15  0x948294dc 16  0x9481acf7 17  0x948162b1 18 CarbonCore CallComponentDispatch 19 QuickTime MCIdle 20 QuickTime QTOMovieObject::SendCommand(unsigned long, void const*) 21 QuickTime DispatchQTMsg(void const*, unsigned long, unsigned long, unsigned long, QTOGenericObject*) 22 QuickTime QTObjectTokenPriv::SendMessageToObject(QTMessagePriv*, unsigned long) 23 QuickTime QTObjectTokenPriv::DispatchMessage(QTMessagePriv*, void const*, __CFAllocator const*, unsigned long, unsigned long, ComponentMsgParam*, unsigned long, unsigned long) 24 QuickTime QTSendToObject 25 QTKit QTObjectTokenExecuteCommand 26 QTKit -[QTMovie idle] 27 CoreFoundation CFSetApplyFunction 28 QTKit +[QTMovie idleAllMovies:] 29 Foundation __NSFireTimer 30 CoreFoundation CFRunLoopRunSpecific 31 CoreFoundation CFRunLoopRunInMode 32 HIToolbox RunCurrentEventLoopInMode 33 HIToolbox ReceiveNextEventCommon 34 HIToolbox BlockUntilNextEventMatchingListInMode 35 AppKit _DPSNextEvent 36 AppKit -[NSApplication nextEventMatchingMask:untilDate:inMode:dequeue:] 37 AppKit -[NSApplication run] 38 AppKit NSApplicationMain And this leaks 64 bytes of memory. Is there something I'm not doing correctly. I'm only using the QTKit framework for loading the movies and placing them in the movielayer directly, so no under the hood Carbon calls. Is there some missing function to call after the movie has finished playing? Freddie Tilley</body>
  </mail>
  <mail>
    <header>Core Image Color Tracking in GPU Gems 3</header>
    <body>Sorry to bother you all with this one. It may not be completely on topic: I received GPU Gems 3 yesterday and tried out the CIColorTracking example. The nib in the sample code in GPU Gems 3 appears to be zero bytes. Does anyone know of an errata page for GPU Gems 3 - I searched but couldn't find? I quickly constructed a new nib but seem to have an issue. I assume the duck should follow the pink ball ;-). I am building on Leopard (10.5.2) on a mac book pro. I assume the code is not nvidia specific. I even downloaded xcode 2.5 to see if there were simpler issues with the NIB. Also: I attempted to use the CI filter(s) in quartz composer, but couldn't quite work out the logic. I guess I could be witnessing the same problem exhibited in the application. The 'Color Tracking Area Mean Filter' outputs an image of bounds: I've also experimented by setting each CI filter to 'Use Normalised Coordinates'. ******************************* Ian Grant Senior Lecturer in Digital Art Faculty of the Arts Thames Valley University Ealing, UK W5 5DX *******************************</body>
  </mail>
  <mail>
    <header>Re: Color change, using QCRenderer under 10.4</header>
    <body>OK, that's fine.  I just wanted to know if it was something obvious first.   I had noted the changes to color on 10.5 and had wondered as much :) -- Regards, Neil Clayton,</body>
  </mail>
  <mail>
    <header>Re: Color change, using QCRenderer under 10.4</header>
    <body>It is quite possible that Tiger color was broken. Leopard added a brand new imaging system with great attention to color (see our release notes in the Quartz Composer.app) We appreciate the full example, and you should create a Radar and attach the zip file or a link. Otherwise, it's not going to get the right people looking at it if you just post on the mailing list :) Thanks, Troy</body>
  </mail>
  <mail>
    <header>Quartz Extreme drawing bug?</header>
    <body>The appearance of the bug is intermittent, but once it starts, it&amp;#8217;s difficult to get to to disappear. For example, in Safari, if one scrolls the content of a window, areas of the window are either copied multiple times or not redrawn. &amp;nbsp;However, if one were to resize the window, or do an operation that selects the window content, then the improperly drawn area is drawn again correctly. Restarting seems to temporarily make the problem go away. &amp;nbsp;In my case I have a brand-new 17&amp;#8221; MacBook Pro with the hi-rez LED backlighting screen. The reason I suspect Quartz Extreme involvement &amp;nbsp;is that the drawing defects can be temporarily eliminated by running Quartz Debug and either DISABLING QUARTZ EXTREME or ENABLING QUARTZGL. There&amp;#8217;s a Apple Discussions thread of users who have all had the same problem: Applecare techs seem baffled. Stephen Greenfield Write Brothers, Inc.</body>
  </mail>
  <mail>
    <header>Color change, using QCRenderer under 10.4</header>
    <body>I've done some more research and have come to the conclusion that Quartz QCRenderer must be performing color correction (or distortion, depending on your pt of view) under 10.4.    (I'd love to be wrong about this - really). I'm wondering if anyone has anything they can add to this, or even better a &amp;quot;oh yeah, do XYZ and you might find it'll work&amp;quot;.     I've created a reasonably small test harness that demonstrates the problem.    It simply grabs the screen and puts it into a SampleCIView. 1) The most basic test.   Clip a 128x128 area around the mouse (out of the CVPixelBufferRef that contains the image from the screen). Composite this back onto the final image before it's passed to the SampleCIView, but at an offset y + 2 (so that we can see that the compositing is taking place).    Indeed, we can see a bit of the image being composited on and the colors are correct. Note: The primary reason for testing this was to check that my own compositing code wasn't introducing the color change. 2) Do the same as (1) above, but before compositing pass the 128x128 buffer into a &amp;quot;do nothing&amp;quot; quartz patch.   This patch takes a CIImage as input, transforms it (upside down) using an Affine Transform and spits it out again.  It's got a (1) Clear and (2) Billboard of size 2.0.   I'd not expect this to have any real effect on the image.  What I see however is a lightening of the 128x128 image when it is composited. Is it the case that all ye who enter the realm that is QCRenderer on 10.4 should abandon all hope (of getting color correct)? BTW: This code works fine under 10.5. I've posted a full example, with controls to disable the QC part (so, perform step (1) above) and also the offset. So that whoever wants to can see the issues easily.     I'm happy to raise a radar for this if it's appropriate, but I'd like to know if I'm doing something silly/ stupid first. (for 10.4/5, XCode 2.5). Oh, the included qtz patch does more than just &amp;quot;nothing&amp;quot;.  It's a nice little splash of water :-) -- Regards, Neil Clayton,</body>
  </mail>
  <mail>
    <header>Re: Shadows, m34 sublayer transforms, and rotation</header>
    <body>Yes, this is a known bug with perspective transforms and filter/shadow effects,</body>
  </mail>
  <mail>
    <header>Shadows, m34 sublayer transforms, and rotation</header>
    <body>Hello. I'm seeing an issue I don't understand. If I add a layer, and a shadow - I see the shadow. [child setValue:[NSNumber numberWithFloat:-46] [child setValue:[NSNumber numberWithFloat:46] Then both the content of the layer, and it's shadow disappear. I have made an example to show just this problem alone.   Is this a known bug with CA, or am I doing something stupid?  The example contains about 77 lines of code total. -- Regards, Neil Clayton</body>
  </mail>
  <mail>
    <header>Re: CARenderer Memory Issue</header>
    <body>: slaps forehead : Hi, my name is Colin, I blindly trust the line number Instruments gives me.. Not clearing the depth buffer seems to have worked. I'm on my work Mac, so I can't trust quite as throughly, but Instruments is now showing memory usage going back to normal levels after rendering finishes. Thanks, Colin</body>
  </mail>
  <mail>
    <header>Re: CARenderer Memory Issue</header>
    <body>On Mar 23, 2008, at 11:49 PM, Colin Cornaby wrote: From the backtrace you provided below, it looks like the allocation is made by OpenGL when _you_ called glClear in your program. Maybe it's the depth buffer that is being allocated the first time you draw into the surface? (You probably don't need the depth buffer, CA definitely doesn't use it.) John</body>
  </mail>
  <mail>
    <header>CARenderer Memory Issue</header>
    <body>I sent an email to the list earlier asking about CoreAnimation memory use. I think I have that all resolved now, except for one memory usage issue with CARenderer. I am successfully rendering a layer tree that is never shown on the screen with a CARenderer. I set up a pixel buffer, and then render to the pixel buffer. But, when I call [CATransaction commit] before I actually do the rendering, there is a very large allocation of memory depending on the size of the image. Apparently glClear is being called internally by CoreAnimation, and I'm guessing it's allocating some sort of buffer internal to CoreAnimation. I can't figure out how to get CoreAnimation to let go of it, and it exists for the lifetime of my program. Here is the complete event log of the memory at the address in question (note the lack of a free event): #	Category	Event Type	Timestamp	Address	Size	Responsible Library Responsible Caller 0	GeneralBlock-36003840	Malloc	00:14.930	0x254cf000	36003840 libGL.dylib	glClear The stack trace from where the memory is allocated: 0 libSystem.B.dylib malloc 1  0x179b1e3c 2  0x179a37d8 3  0x179a6af1 4 libGL.dylib glClear 5 MyProgram -[GSOGLLayerRenderer imageByRenderingLayers:] /Users/.... 6 MyProgram -[GSImageExporterController prepareImage] /Users/.... 7 MyProgram -[MyDocument export:] /Users/.... 8 AppKit -[NSApplication sendAction:to:from:] 9 AppKit -[NSMenu performActionForItemAtIndex:] 10 AppKit -[NSCarbonMenuImpl performActionWithHighlightingForItemAtIndex:] 11 AppKit AppKitMenuEventHandler 12 HIToolbox DispatchEventToHandlers(EventTargetRec*, OpaqueEventRef*, HandlerCallRec*) 13 HIToolbox SendEventToEventTargetInternal(OpaqueEventRef*, OpaqueEventTargetRef*, HandlerCallRec*) 14 HIToolbox SendEventToEventTarget 15 HIToolbox SendHICommandEvent(unsigned long, HICommand const*, unsigned long, unsigned long, unsigned char, OpaqueEventTargetRef*, OpaqueEventTargetRef*, OpaqueEventRef**) 16 HIToolbox SendMenuCommandWithContextAndModifiers 17 HIToolbox SendMenuItemSelectedEvent 18 HIToolbox FinishMenuSelection(MenuData*, MenuData*, MenuResult*, MenuResult*, unsigned long, unsigned long, unsigned long, unsigned char) 19 HIToolbox MenuSelectCore(MenuData*, Point, double, unsigned long, OpaqueMenuRef**, unsigned short*) 20 HIToolbox _HandleMenuSelection2 21 HIToolbox _HandleMenuSelection 22 AppKit _NSHandleCarbonMenuEvent 23 AppKit _DPSNextEvent 24 AppKit -[NSApplication nextEventMatchingMask:untilDate:inMode:dequeue:] 25 AppKit -[NSApplication run] 26 AppKit NSApplicationMain 27 MyProgram main /Users/.... 28 MyProgram start Here are the relevant snippets of my code (with the line the large memory allocation occurs on noted): NSOpenGLPixelFormatAttribute attr[] = NSOpenGLPFAOffScreen, NSOpenGLPFADepthSize, 24, (CGLPixelFormatAttribute) 0 NSOpenGLPixelFormat *format = [[NSOpenGLPixelFormat alloc] m_glContext = [[NSOpenGLContext alloc] initWithFormat:format m_renderer = [CARenderer rendererWithCGLContext:[m_glContext [m_glContext setOffScreen:glData width:imageWidth height:imageHeight glOrtho (frame.origin.x, frame.origin.x+imageWidth, frame.origin.y, ....some more setup code here.... [CATransaction commit]; &amp;lt;-large memory allocation occurs here [m_renderer addUpdateRect:CGRectMake(m_renderFrame.origin.x, ...read the pixels here... If anyone had any suggestions, or could let me know what's going on here, I'd appreciate it. I'm really bad with pixel buffers, and I'm not sure why. Something about drawing something I can't see disagrees with me I guess. :) Thanks, Colin</body>
  </mail>
  <mail>
    <header>Re: Quartz idioms and Control Manager</header>
    <body>Actually it's probably been asked on carbon-dev before. :) This list is more specifically for Quartz drawing issues rather than integration with other system services. Many of the older Control Manager routines that took Rects, for example, have been gradually replaced with new routines in HIView.h (especially in Leopard) that take HIRects. So in many cases we have Quartz-based equivalents available now. If there is no Quartz-based equivalent, then yes, you just have to create a Rect/Point/whatever with the same values as the CGRect/Point/ etc.</body>
  </mail>
  <mail>
    <header>Quartz idioms and Control Manager</header>
    <body>How does one reconcile the newer Quartz paradigms and contexts to older APIs such as the Carbon Control Manager? For example, I can use an HIView or other HI Toolbox routines and structures in Carbon, but if those structures require older QuickDraw things like Rects, how to reconcile the two? Do I have to do transforms manually every time between the Quartz system and older APIs? Mike</body>
  </mail>
  <mail>
    <header>Re: CILinearGradient and 10.5 mess</header>
    <body>I&amp;#39;ve read the release note. It seems to be wrong. I link against 10.4 (but build on 10.5) and the behavior is 10.5 behavior when running on 10.5 and 10.4 behavior on when running 10.4. I&amp;#39;ve submitted a bug for this (#5811014) that shows it is NOT working. &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; [color &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; behavior_10_4 = &amp;gt; &amp;gt; I don&amp;#39;t know exactly what they assumed or how this was implemented, &amp;gt; &amp;gt; but I do think that the behavior you&amp;#39;re describing sounds like a bug &amp;gt; &amp;gt; in CoreImage. &amp;gt; At this point, fixing it is only going break any code that I write &amp;gt; to work around it. Not a great situation. Maybe I&amp;#39;ll just write code &amp;gt; that makes a gradient, samples it, and chooses based on empirical &amp;gt; evidence. Ugh! FWIW, we noticed that the documented behavior was broken when 10.5 came out and worked around it using // assume gradients are in sensible order // assume gradients are in 10.4 order If Apple fixes this to work as documented, it will probably break numerous apps. &amp;gt; &amp;gt; Most apps don&amp;#39;t need to know what SDK they were built against; they &amp;gt; &amp;gt; need to know what the current OS version is. That&amp;#39;s what &amp;gt; &amp;gt; NSAppKitVersionNumber is meant to provide - what version of AppKit &amp;gt; are &amp;gt; &amp;gt; you currently running against. It has _never_ been meant to indicate &amp;gt; &amp;gt; what version of the SDK you linked against. &amp;gt; OK. No reason to bang my head against a wall even though this &amp;gt; behavior seems to be of limited usefulness. But it is what it is. It&amp;#39;s useful for cases like this, where functionality changes based on framework version instead of linkage (as it was apparently supposed to). -- adam &amp;gt; &amp;gt; How would an application ever tell which SDK it was built against? &amp;gt; Most apps don&amp;#39;t need to know what SDK they were built against; they &amp;gt; need to know what the current OS version is. That&amp;#39;s what &amp;gt; NSAppKitVersionNumber is meant to provide - what version of AppKit are &amp;gt; you currently running against. It has _never_ been meant to indicate &amp;gt; what version of the SDK you linked against. &amp;gt; &amp;gt; Before you dismiss this message (Eric), I think the case in point is &amp;gt; &amp;gt; how CILinearGradient works. It is NOT backwards compatible. The Core &amp;gt; &amp;gt; Image guys probably (correctly) assumed that NSAppKitVersionNumber &amp;gt; &amp;gt; would return the version of AppKit _linked_ against rather than &amp;gt; &amp;gt; _available on the system_. &amp;gt; I don&amp;#39;t know exactly what they assumed or how this was implemented, &amp;gt; but I do think that the behavior you&amp;#39;re describing sounds like a bug &amp;gt; in CoreImage. &amp;gt; -eric &amp;gt; _______________________________________________ &amp;gt; Do not post admin requests to the list. They will be ignored. &amp;gt; Quartz-dev mailing list &amp;nbsp; &amp;nbsp; &amp;nbsp;() &amp;gt; This email sent to</body>
  </mail>
  <mail>
    <header>Re: CILinearGradient and 10.5 mess</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: image file as 1-bit mask</header>
    <body>In other words, I can't make a 1-bit mask, but I don't really need to. That does seem to work.  Thanks. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: CILinearGradient and 10.5 mess</header>
    <body>I believe we also encountered the problem when compiling on 10.4, since the developer building the releases was using 10.4.  We had to do a pure AppKit version check regardless of the compile platform or SDK. -- adam</body>
  </mail>
  <mail>
    <header>Re: CILinearGradient and 10.5 mess</header>
    <body>Here&amp;#39;s what I&amp;#39;m using to future proof this code (also attached to avoid line break issues): FWIW, we noticed that the documented behavior was broken when 10.5 came out and worked around it using // assume gradients are in sensible order // assume gradients are in 10.4 order If Apple fixes this to work as documented, it will probably break numerous apps. It&amp;#39;s useful for cases like this, where functionality changes based on framework version instead of linkage (as it was apparently supposed to). -- adam &amp;gt; _______________________________________________</body>
  </mail>
  <mail>
    <header>Re: Copying a JPEG image with CGImage changes file size</header>
    <body>As far as I can tell, Apple does not provide a way to edit just the metadata. As your prototype shows, Quartz recompresses the image when going from a source to a destination even when it's highly undesirable. I could really use this feature too. If you are working on an open- source project, you might want to look into the GPLed/C++ Exiv2 library. There is a libexif project in C at SourceForge that is at least LGPLed, still a pain and quite poorly documented. (The project doesn't seem to be active either, not sure if that's because their (code) work is done or what...) Stuck in the same boat, I'm not sure what I'm going to do either. There are a few public domain or suitably licensed example codes ( , ) in various languages. At this point I'm worried I might have to roll my own while looking at those. I'm sure that would be a Sisyphean task, so please let me know if you find a good solution. Oh, and in the meantime please file your own enhancement request requesting a built in solution to edit metadata somehow without needing to lose the user's image quality. hope this helps, -natevw</body>
  </mail>
  <mail>
    <header>Re: Morphing 2 B&amp;amp;W Images</header>
    <body>On Apr 10, 2008, at 9:03 PM, vance wrote: Core Image. That depends on what you mean by &amp;quot;morph&amp;quot;.  Core Image contains a cross- fade transition (CIDissolveTransition) that might work out very nicely.</body>
  </mail>
  <mail>
    <header>Re: Saving CGContext as png</header>
    <body>On Apr 11, 2008, at 12:08 AM, Roger Herikstad wrote: A context is not a drawing surface.  The context is the tool that you use to manipulate the drawing surface. A CGBitmapContext, for example, is a device that you use to draw into a pixel buffer.  A CGPDFContext is a device you use to draw to the pages of a PDF. The point is that you cannot &amp;quot;copy&amp;quot; a CGContext, that would be like saying &amp;quot;I want to copy my pencil and have another copy of the drawing I just made with it&amp;quot;.  The only way to copy the image made by the pencil is to redraw the image, or use a drawing surface that knows how to make copies of itself. CGLayer is a kind of drawing surface that knows how to make copies of itself, but it really only knows how to copy itself to one special type of graphics context, the one it was originally created with. Because of that, you don't own the surface that a CGLayer's context is drawing on.  With a CGLayer (to painfully continue the pencil metaphor) you can own the pencil, but you don't get the paper. In your case, it sounds like you want to use the pencil only once, but get two drawings out if it.  That's not really possible. Now what I would recommend you do is this.  Create a CGBItmapContext the size of the image you want to draw.  You'll own the pixel buffer (the &amp;quot;paper&amp;quot;) that that context knows how to draw on. Draw your image once into that bitmap context. Since you own the pixel buffer, create a CGImage from that pixel buffer.  You can then draw that CGImage into your window AND save that CGImage to disk as a PNG using Image IO's CGImageDestination.  Your long-running drawing routine will have run once, but you'll be reusing the image it created for both populating the screen and the PNG image. Scott</body>
  </mail>
  <mail>
    <header>Re: Saving CGContext as png</header>
    <body>Hi, My current context is a window context, and as I mentioned in my previous post, I'm using CGLayer to cache a background of a few thousand drawn lines, against which I constantly redraw a subset of those lines. Now, what I want to accomplish is to take a snapshot of one such redraw, meaning I want to capture the image of the smaller subset drawn against full set for later reference. ~ Roger</body>
  </mail>
  <mail>
    <header>Re: Saving CGContext as png</header>
    <body>Hi, Thanks. One thing, though. The drawing of these waveforms takes on the order of 30 s to complete and I'd rather not have to do it twice, if that's possible. I was wondering if there's a way of using CGLayers? I am currently drawing my waveforms to a layer to speed up redraws later, so if there's a way of creating a bitmap directly from CGLayer? Also, is there a way to combine layers, so that I can various part of my drawing separate while in interactive mode, and then combine them into a single layer before writing them to file? ~ Roger</body>
  </mail>
  <mail>
    <header>Re: Saving CGContext as png</header>
    <body>On Thu, Apr 10, 2008 at 6:52 PM, Roger Herikstad I also meant to ask why type of context do you currently have? The one you want to &amp;quot;copy&amp;quot;. If that is a bitmap context then you can copy the bits, etc. and/or create a CGImage form it (CGBitmapContextCreateImage) and write that out in any support image format (CGImageDestination). If it is a window related context you likely want to replay your drawing operations against a bitmap (or PDF) context with the resolution, etc. you want for the output file. You can capture a window related to context to an image but how to do that depends how the context was created. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Saving CGContext as png</header>
    <body>On Thu, Apr 10, 2008 at 6:52 PM, Roger Herikstad operations using that context. Note a PDF context be a better destination if you want to maintain vector aspects of the things you draw (depends on what and how you draw). -Shawn</body>
  </mail>
  <mail>
    <header>Morphing 2 B&amp;amp;W Images</header>
    <body>Is there a Code Image filter (apple or 3rd party) that can morph 2 images ? I have several black and white images. They are frames of infrared satellite clouds 30 minutes apart. So I am trying to smooth out the transition between them so that cloud animation looks better. Thank you! Vance</body>
  </mail>
  <mail>
    <header>Saving CGContext as png</header>
    <body>Hi list, I was wondering how I can copy the contents of the current CGContext to a bitmap and then save it to file? I have a simple graphing program that draws waveforms, a lot of them, and now I would like to save what I see to a png file so that I can examine it off-line, that is without having the reload the waveforms and draw them again. Any advice? Thanks! ~ Roger</body>
  </mail>
  <mail>
    <header>CARenderer won't render layers with filters</header>
    <body>I have some code that creates a bitmap snapshot of a bunch of layers by rendering them with CARenderer. This all works great until I try to apply filters to my layer. If I try and render a layer with filters, the following gets logged to the console: I've been trying to trace the issue to my code, and have come up with a few possibilities. CARenderer used to work with filters in my project, and it would seem a change I've made recently broke this. One change I recently made is I moved away from populating my layer content with NSImage and it's rep friends, and moved to CGImageSourceRef. According to the docs, error 10002 has to do with an invalid pixel format. Perhaps my images are being handled different by CGImageSourceRef, resulting in some sort of confusion when CI tries to render the image into a CIContext. If I don't set the content of my layer, but set filters, the CARenderer works fine. I'm not sure if I should form anything from that, given that CARenderer may not even be bothering to try to render the filters if the layer content is empty. I'm going to start pulling apart my code more, but if anyone had any ideas on where I should be looking it would be helpful. Thanks, Colin</body>
  </mail>
  <mail>
    <header>Copying a JPEG image with CGImage changes file size</header>
    <body>Hi, I am wanting to edit metadata in a JPEG, and am just testing code (below) to read and write out an image without modification using CGImage to make sure I can do so without changing the image.  I am unable to obtain an output file which is identical to the input file.  Any suggestions?  have tried setting kCGImageDestinationLossyCompressionQuality to 1.0. (Originally posted to cocoa-dev, where Scott Thompson suggested I try over here, as it might have been discussed before; although if it has, I have been unable to find the discussion) Thanks, Hugo // Load the image CGImageSourceRef source = CGImageSourceCreateWithURL((CFURLRef)absURL, // Create an image destination writing to `url' CGImageDestinationRef dest = CGImageDestinationCreateWithURL((CFURLRef)absURL2, for (i=0; i&amp;lt;numImages; ++i) CFMutableDictionaryRef mMetadata =</body>
  </mail>
  <mail>
    <header>Re: Function Keys from CGEvents</header>
    <body>I'm not seeing these issues in my event taps utility. Perhaps an issue with your code? -- Bill Cheeseman - email@hidden Quechee Software, Quechee, Vermont, USA www.quecheesoftware.com PreFab Software - www.prefabsoftware.com</body>
  </mail>
  <mail>
    <header>Re: Function Keys from CGEvents</header>
    <body>The keycodes for the function keys should be invariable across different keyboards (but of course each separate function key has its own keycode). In Leopard, you can consult HIToolbox/Events.h for a table of keycodes for each function key: kVK_F5                        = 0x60, kVK_F6                        = 0x61, kVK_F7                        = 0x62, kVK_F3                        = 0x63, kVK_F8                        = 0x64, kVK_F9                        = 0x65, etc. The constants are new in Leopard, but the keycodes were the same in earlier versions of Mac OS X. In Leopard, use [NSEvent eventWithCGEvent:cgEvent]. Prior to Leopard there's no way to create an NSEvent from a CG event, but I'd really recommend just identifying function keys by virtual keycode anyways.</body>
  </mail>
  <mail>
    <header>Re: CGDisplayCapture Not Capturing Second Monitor</header>
    <body>In a GUI a user inputs a display number they would like to use (1 based). I grab that value from that text box and do the following. int usersChoice; //for demonstration sake //capture the display @ dspys[usersChoice-1] //barf because the user entered a bad display number On Apr 8, 2008, at 6:38 PM, David Duncan wrote:</body>
  </mail>
  <mail>
    <header>Re: CGDisplayCapture Not Capturing Second Monitor</header>
    <body>How are you getting displayNumber? -- David Duncan Apple DTS Animation and Printing email@hidden</body>
  </mail>
  <mail>
    <header>CGDisplayCapture Not Capturing Second Monitor</header>
    <body>I have 2 active monitors on my system, my laptop monitor and an external display. When I pass CGDisplayCapture the id of the external display it returns with an error code of 1000 or kCGErrorFailure. Are you suppose to do something special when capturing a display other than the primary display? My code is below. It straight from a example. err = CGGetDisplayTransferByFormula (displayNumber-, &amp;amp;redMin, &amp;amp;redMax, &amp;amp;redGamma, &amp;amp;greenMin, &amp;amp;greenMax, &amp;amp;greenGamma, &amp;amp;blueMin, &amp;amp;blueMax, err = CGSetDisplayTransferByFormula (displayNumber, redMin, fade*redMax, redGamma, greenMin, fade*greenMax, greenGamma, blueMin, Thanks, Carmen</body>
  </mail>
  <mail>
    <header>The missing part of the Image Kit Programming Guide</header>
    <body>The Image Kit Programming Guide demonstrates how to use IKImageView to display an image and how to use IKFilterBrowser to open and display the Core Image filters. In the section of the guide &amp;quot;applying filters to an image&amp;quot; the guide cops out. It refers to the IUUIDemoApplication. That code uses an OpenGLView, not the IKImageView. I have gone around and around attempting to modify the code to apply the selected filter to the IKImageView (and without the complexity of the imagewell used in the IUUIDemo). Can anyone provide me with sample code that applies the core image filter to the IKImageView or even a strategy? The image is a CGImage which must be converted to a CIImage first.</body>
  </mail>
  <mail>
    <header>Re: Incremental loading of TIFF-based RAW images using	NSBitmapImageRep</header>
    <body>Hi, I have an application that needs to load pictures in various file formats, either from a local file or from a remote HTTP-based resource. &amp;nbsp;Pictures should be loaded incrementally whenever possible, so I&amp;#39;m using an NSBitmapImageRep initialized with initForIncrementalLoad:, pumping data with incrementalLoadFromData: as it becomes available from an NSURLConnection. &amp;nbsp;This works just fine with the most common file formats like JPEG, PNG or TIFF. &amp;nbsp;So far, so good. With TIFF-based RAW formats like .NEF (Nikon) or DNG, though, what I get at the end of the incremental load is the embedded thumbnail, which is usually much smaller (typically 160 x 120) than the &amp;quot;real&amp;quot; image synthesized from the CCD sensor data. In a way, this behavior makes sense, since an unqualified NEF or DNG data stream looks just like a TIFF, witness the fact that if I rename a NEF file in the Finder by giving it a .tiff extension, and I open it in Preview.app, I also get the thumbnail rather than the full-size image. So I guess what I need is a way to provide the incremental NSBitmapImageRep object with a hint that it&amp;#39;s really receiving a NEF or DNG data stream, not a regular TIFF. &amp;nbsp;Is there any way to do this? Thanks, &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;-- marco -- It&amp;#39;s not the data universe only, it&amp;#39;s human conversation. They want to turn it into a one-way flow that they have entirely monetized. I look at the collective human mind as a kind of ecosystem. They want to clear cut it. They want to go into the rainforest of human thought and mow the thing down. &amp;nbsp;_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list &amp;nbsp; &amp;nbsp; &amp;nbsp;() Help/Unsubscribe/Update your Subscription: This email sent to</body>
  </mail>
  <mail>
    <header>Re: Going round the bend with CGImageRef 16x16</header>
    <body>Turns out that the 16x16 cursors don't have alpha after all.  From what I can tell. Compositing is doing *exactly* what it's told to.   Good boy! -- Regards, Neil Clayton</body>
  </mail>
  <mail>
    <header>Re: CATiledLayer provider thread and deallocating layers</header>
    <body>this is a known bug, it should be fixed in the next OS update. I've not seen this backtrace before, but it's possible it has the same underlying cause as the other one (if the bug doesn't crash your program, it will probably corrupt your objects) yes, that is basically what is happening. No, I don't know of a workaround for this. The provider thread uses a number of locks and will never call your draw method after another thread has released the layer (the provider thread atomically retains the layer while asking you to draw into it.) Preview.app uses tiled layers for its image view in 10.5, so they've had a large amount of testing, but it doesn't use the path with the bug you ran into‚Ä¶</body>
  </mail>
  <mail>
    <header>Going round the bend with CGImageRef 16x16</header>
    <body>I wouldn't normally post such a simple question (er, maybe) - but this is driving me nuts: I'm getting cursors re the CGS API.   They are fine. I've even gone so far as to printf() their content to ensure the data is OK (it's OK in the debugger as well). [MouseCompositor] - Got cursor seed 7434, depth: 32, rowBytes: 68, [MouseCompositor] - Got cursor seed 7435, depth: 32, rowBytes: 64, The first is for some cursor from MS Word 08 (17x17) - and that is composited OK.  The second (16x16 case) is also from Word, this time it's IBeam cursor.  The data is OK, and from what I can see the row lengths, components and bits per component are the same as other cases.  The only thing that's different is the SIZE. It *seems* that anything I attempt to draw at 16x16 doesn't work. Have I hit some kind of special &amp;quot;now I won't paint anything from the CGImageRef&amp;quot; case?   Or am I missing something?   24x24, 32x32 and 17x17 cases work fine.  Just 16x16 doesn't. Here's the code that does the compositing: [frame bitmap] returns a CGBitmapContext (which is working for the other cases). cursorData is an unsigned char * which is filled with data from the CGSGetGlobalCursorData call (and I'm sure that data is OK). Oh - for some time I was paranoid that I'd done something daft with the hotspot, that maybe by some freak I was compositing it outside of the destination context.  I drew the same cgImageRef into 0,0 to make sure.  It's nothing to do with the hotspot. CGRect rect = CGRectMake(position.x - hotspot.x, position.y - CGDataProviderRef providerRef = CGDataProviderCreateWithData(self, CGImageRef cgImageRef = CGImageCreate(cursorSize.width, cursorSize.height, cursorBitsPerComponent, cursorBitsPerComponent * components, cursorRowBytes, [SWBColorSpaces genericColorSpace], kCGImageAlphaPremultipliedLast, providerRef, nil, NO, Any ideas anyone?  I feel kinda think as two planks right now. -- Regards, Neil Clayton,</body>
  </mail>
  <mail>
    <header>Re: To/from CIKernel colorspace?</header>
    <body>Distributing it (or an equivalent ICC) was meant for Tiger, where the constant for linear generic RGB is not defined. mark</body>
  </mail>
  <mail>
    <header>Re: To/from CIKernel colorspace?</header>
    <body>I think it's a fairly safe bet that this profile will be available on the user's system; redistributing it shouldn't really be necessary.</body>
  </mail>
  <mail>
    <header>Re: Fast Fourier Transform</header>
    <body>Amazing! &amp;nbsp;I gave a seminar on the FFT several decades ago at Purdue on the Cooley-Tukey algorithm. &amp;nbsp;We used to do this stuff on images during the night, monopolizing the campus CDC 6500 until they threw us off of it. &amp;nbsp;Now you can do this stuff on a laptop or even an iPad. I'm trying to make a convolution of an image with some filter. The best way I think is to do this in frequency domain. Apple provide Accelerate framework but I need a help with it. I have an image (8bit gray scale): bitmap2 = [[NSBitmapImageRep alloc] initWithBitmapDataPlanes: &amp;amp;imageData pixelsWide: width pixelsHigh: height bitsPerSample: 8 samplesPerPixel: 1 hasAlpha: NO isPlanar: NO colorSpaceName: NSCalibratedWhiteColorSpace // // Here should be the convolution // // First I should use &amp;quot;vDSP_vfltu8()&amp;quot; to make an vaector of my image and then I should create something like &amp;quot;FFTsetup mySetup&amp;quot; than &amp;quot;FFTSetup vDSP_create_fftsetup&amp;quot; and after that &amp;quot;vDSP_fft2d_zrip&amp;quot;. But because I am new to image processing and programming could someone show me some how to get through this? Thank to any responce in advice Best regards Wojciech</body>
  </mail>
  <mail>
    <header>Re: How can we find the position of context with respect to Window or Screen/Display?</header>
    <body>I have looked into some of the descriptions related to Quartz Compositor and have been trying to study how it maintain and compose windows.</body>
  </mail>
  <mail>
    <header>Re: How can we find the position of context with respect to Window or Screen/Display?</header>
    <body>Thank you David, That was very nice explanation. I have looked into some of the descriptions related to Quartz Compositor and have been trying to study how it maintain and compose windows.</body>
  </mail>
  <mail>
    <header>Re: How can we find the position of context with respect to Window or Screen/Display?</header>
    <body>Thank you David, That was very nice explanation. I have looked into some of the descriptions related to Quartz Compositor and have been trying to study how it maintain and compose windows. But there is very less documentation available on it. All I could found is this... "Each window in Mac OS X is stored as a bitmap associated with positioning (including z-index positioning),  transparency and anti-aliasing information.  The application that owns the window draws the window contents to the appropriate bitmap. Quartz Compositor then takes each window bitmap and composites them together using the associated information to create a display." I have few questions related to the same. 1. Coming back to contexts, as mentioned above window draws the window context to bitmap... If that is true then window should know about the location of different context where they needs to be drawn? 2. Also, You mentioned "You get the location of your drawing from the views in the window. Core Graphics knows nothing about how content in your window is positioned." In Architecture of OS X, Core graphics (quartz 2D &amp;amp; Quartz Compositor) is level below Cocoa/Carbon so they should know about the location where drawing is to be done. Right? 3. Where can I find more resources on Quartz Compositor? I mean somebody has to keep a track of location of context? Thank you very much for your time, Rahul Various UI elements could be draw in different context and but applications(window server) sync them so nicely on screen. I am trying to take the same approach for rendering UI (completely in quartz) of my application.</body>
  </mail>
  <mail>
    <header>Re: How can we find the position of context with respect to Window or Screen/Display?</header>
    <body>Thank you David, That was very nice explanation. I have looked into some of the descriptions related to Quartz Compositor and have been trying to study how it maintain and compose windows. But there is very less documentation available on it. All I could found is this... &amp;quot;Each window in Mac OS X is stored as a bitmap associated with positioning (including z-index positioning),  transparency and anti-aliasing information.  The application that owns the window draws the window contents to the appropriate bitmap. Quartz Compositor then takes each window bitmap and composites them I have few questions related to the same. 1. Coming back to contexts, as mentioned above window draws the window context to bitmap... If that is true then window should know about the location of different context where they needs to be drawn? 2. Also, You mentioned &amp;quot;You get the location of your drawing from the views in the window. Core In Architecture of OS X, Core graphics (quartz 2D &amp;amp; Quartz Compositor) is level below Cocoa/Carbon so they should know about the location where drawing is to be done. Right? 3. Where can I find more resources on Quartz Compositor? I mean somebody has to keep a track of location of context? Thank you very much for your time, Rahul Various UI elements could be draw in different context and but applications(window server) sync them so nicely on screen. I am trying to take the same approach for rendering UI (completely in quartz) of my application.</body>
  </mail>
  <mail>
    <header>Fast Fourier Transform</header>
    <body>I&amp;#39;m trying to make a convolution of an image with some filter. The best way I think is to do this in frequency domain. Apple provide Accelerate framework but I need a help with it. I have an image (8bit gray scale): bitmap2 = [[NSBitmapImageRep alloc] initWithBitmapDataPlanes: &amp;amp;imageData pixelsWide: width pixelsHigh: height bitsPerSample: 8 samplesPerPixel: 1 hasAlpha: NO isPlanar: NO colorSpaceName: NSCalibratedWhiteColorSpace bitmapFormat: 0 bytesPerRow: width</body>
  </mail>
  <mail>
    <header>Re: How can we find the position of context with respect to Window or Screen/Display?</header>
    <body>Various UI elements could be draw in different context and but applications(window server) sync them so nicely on screen. I am trying to take the same approach for rendering UI (completely in quartz) of my application.</body>
  </mail>
  <mail>
    <header>Re: How can we find the position of context with respect to Window or Screen/Display?</header>
    <body>Various UI elements could be draw in different context and but applications(window server) sync them so nicely on screen. I am trying to take the same approach for rendering UI (completely in quartz) of my application. I had an Idea that its possible to know the location of context on window or display by some mean. This will eventually help me put the context at proper location. Basically, the application doesn&amp;#39;t need to know any of the things that you believe it needs to know.</body>
  </mail>
  <mail>
    <header>Re: How can we find the position of context with respect to Window or Screen/Display?</header>
    <body>Basically, the application doesn't need to know any of the things that you believe it needs to know.</body>
  </mail>
  <mail>
    <header>Re: How can we find the position of context with respect to Window or Screen/Display?</header>
    <body>CGRect CGContextGetClipBoundingBox ( CGContextRef c); Gives the Rect starting from (0,0). I want Rect starting from actual location on window?</body>
  </mail>
  <mail>
    <header>How can we find the position of context with respect to Window or Screen/Display?</header>
    <body>Hello All, CGRect CGContextGetClipBoundingBox ( CGContextRef c); Gives the Rect starting from (0,0). I want Rect starting from actual location on window? Also, How does the applications/os sync these context to display the various controls on right locations? ¬† I mean, lets say window has one window context which does the painting on window and then if we add some Path Control, separate context for it is created and its painted at location we place it when work is done its released. How is this managed? Thank you for your time, Rahul</body>
  </mail>
  <mail>
    <header>Re: Can't duplicate Finder's rename image file</header>
    <body>My reading of the header file is that you should add NSFileWrapperWritingWithNameUpdating to the options argument if you want to update the file names. This is an obvious bug in the documentation. Use the link at the bottom of the class-reference page to report it. I'm curious to know why, if you simply want to rename one file, you're using this method instead of -[NSFileManager moveItemAtURL:toURL:error:] (or the equivalent &amp;quot;path&amp;quot; method)? Remember that in UNIX terminology, a rename is the same as a move to a different name in the same directory. ‚Äî F</body>
  </mail>
  <mail>
    <header>Re: Simple question, can't find answer</header>
    <body>Not sure what I'm doing wrong then. I just retested with the save/restore turned on and it definitely doesn't change the clipping path. I can zap any drawing I've done in the layer, but once it's set for  the layer's context, that seems to be it. I tried trashing my &amp;quot;content&amp;quot; layer and it suddenly started doing exactly what I want. Every time the user releases the mouse button after a window drag, the random bezier generator creates a new, wacky shaped, window. At this point, it's just play, but it's leading to something. Here's what worked - (void) drawContents NSRect			conFrame		= window.contentFrame, // the next two lines did the trick // draw content layer in background context</body>
  </mail>
  <mail>
    <header>Re: Simple question, can't find answer</header>
    <body>A clipping path is part of the current graphics state. Thats why I said that Save/RestoreGState should have resolved the issue. -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Simple question, can't find answer</header>
    <body>Forgot to include the draw path method. Here it is - (void) drawPath:(CGPathRef) inPath if (nsColor == nil) else</body>
  </mail>
  <mail>
    <header>Re: Simple question, can't find answer</header>
    <body>I have a CGLayer and draw a complex, randomly generated CGPath into it. That part works fine, but what I'd like to do is generate completely new paths at periodic intervals and draw them in the same layer. I know I have the periodic refresh mechanism working correctly because a bunch of NSLogs tell me when I've reached specific spots in the code. The problem I'm seeing is that the path is regenerated, but drawing it to the context has no effect. The only thing it displays is the first drawn Path.</body>
  </mail>
  <mail>
    <header>Re: Simple question, can't find answer</header>
    <body>Your going to have to show us what your doing I think before we can understand what is going wrong. -- David Duncan</body>
  </mail>
  <mail>
    <header>Simple question, can't find answer</header>
    <body>Hi I have a CGLayer and draw a complex, randomly generated CGPath into it. That part works fine, but what I'd like to do is generate completely new paths at periodic intervals and draw them in the same layer. I know I have the periodic refresh mechanism working correctly because a bunch of NSLogs tell me when I've reached specific spots in the code. The problem I'm seeing is that the path is regenerated, but drawing it to the context has no effect. The only thing it displays is the first drawn Path. How would I do that? Thanks for any help _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGContextSetShadow scrolling performance</header>
    <body>I would suspect you are drawing often then. Sampler is probably your next Instrument of choice for figuring out what is going wrong. -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: CGContextSetShadow scrolling performance</header>
    <body>Yeah I did that, but it's all green !</body>
  </mail>
  <mail>
    <header>Re: CGContextSetShadow scrolling performance</header>
    <body>You might want to to read Tim's blog post about shadow performance on the iPad: --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: CGContextSetShadow scrolling performance</header>
    <body>First step is to determine how much blending your doing. Attach the Core Animation instrument and turn on the Color Blended option. The more red you see, the more GPU power is necessary. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>In general for iOS related questions, you will want to also refer to the Dev Forums &amp;lt;&amp;gt;. The sticky regarding this topic is still up on the Cocoa Touch forums. It is not possible for your application to capture the contents of the screen (held by another application) while your application is in the background. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>Allegedly it was &amp;quot;unpublished&amp;quot; some time back, but Apple has sample code for achieving similar behavior using CoreGraphics: e I don&amp;#39;t find that API in any Apple-released documentation or current header. If it&amp;#39;s not documented or published, it&amp;#39;s not public, and if it&amp;#39;s not public, Apple won&amp;#39;t allow it in the App Store. ¬† ¬† ¬† ¬†‚Äî F ¬†_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list ¬† ¬† ¬†() Help/Unsubscribe/Update your Subscription: This email sent to</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>I don't find that API in any Apple-released documentation or current header. If it's not documented or published, it's not public, and if it's not public, Apple won't allow it in the App Store. ‚Äî F</body>
  </mail>
  <mail>
    <header>Is UIGetScreenImage banned by apple?</header>
    <body>Hi, Sorry, I don't know if here is the right place to ask this question. As title, Is UIGetScreenImage banned by apple? I am gonna use this API to capture the screen when my application is in background. Thanks a lot. Best regards, Sean _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: How can we find the position of context with respect to Window or Screen/Display?</header>
    <body>Hello Again! Sorry for delayed response. I had reworked over my design (it was mostly around the Idea that I can get location of context). And now it makes use of¬† views, and in -drawRect: to draw. @David, Scott, Ken: Thank you. Thank you for your time. However, I am still curious about the comment David posted that &amp;quot;As a rule, UI elements all draw to the same context (Core Animation changes the rules slightly, but importantly in terms of what the But I will post a different question for that as its not related to topic. Thanks Again. I have looked into some of the descriptions related to Quartz Compositor and have been trying to study how it maintain and compose windows.</body>
  </mail>
  <mail>
    <header>using CIFilters with QTMovieView</header>
    <body>I'm successfully using a QTMovieView and applying CIFilters via the delegates view:willDisplayImage: When I export the movie, how do I still invoke the CIFilters, soI can save my results? Is there a way to get view:willDisplayImage: to be invoked during export? Brian</body>
  </mail>
  <mail>
    <header>Re: CGPath occupying more memory.</header>
    <body>If you're just doing straight lines, you might look into using : CGContextStrokeLineSegments Generally speaking you can get better performance than using a single path with a lot of straight line segments.  Handling self-intersections in a path with a large number of segments can be expensive. CGContextStrokeLineSegments bypasses some of those self-intersection checks and might help. This is particularly true if your path has lots of &amp;quot;spikes&amp;quot; that are potentially left than one pixel wide (imagine a caret ^ shaped spike that is less than one pixel wide). This happens a lot with folks trying to draw noisy graphs (say audio spectrums).  In that case you can get a performance win by filtering your data set to reduce the number of points you are plotting. Scott</body>
  </mail>
  <mail>
    <header>willDisplayImage vs drawRect</header>
    <body>Two questions: 1. When I change my CIFilter (in response to a user menu item selection), I'd like to force a redraw of the QTMoviewView . Currently, I just call the movie view instance's setNeedsDisplay, as such: but this doesn't seems to make it redraw the view. 2. I have a typical MovieDocument class, and I'd like it to handle a drawRect: for it's associated QTMovieView object. How do I do that? I already have it handling the willDisplayImage: method. But, often the willDisplayImage doesn't get invoked (unless there's a frame size change or a time change). I'm a quartz dev newbie, unfortunately. I've looked through the docs already, and went down the rabbit hole ... a bit overwhelming. I could definitely use some help to point me in the right direction. -- Brian</body>
  </mail>
  <mail>
    <header>Re: CGPath occupying more memory.</header>
    <body>There isn't anything that jumps out in the code you present, but are you creating your own CGPaths (or UIBezierPaths) anywhere? How many points and lines do you have when your memory usage is that high? -- David Duncan</body>
  </mail>
  <mail>
    <header>Text drawing crooked</header>
    <body>Hey all, I have a NSCell that contains some text to be draw in a view.  Before calling the -drawWithFrame: method for the NSCell I am calling CGContextRotateCTM so the NSCell will be drawn at about 45 degrees. This seems to be working fine, but the text isn't drawn on a straight line.  A few characters are higher or lower than there neighbors.  I have tried to attach a screenshot so you can see what I am experiencing.  In the screenshot, notice how the digits in 2008 are not aligned properly.  Has anyone else noticed this or had to deal with this? Sincerely, Doug Penny Attachment:</body>
  </mail>
  <mail>
    <header>Re: Forcing menu bar window to update from background app</header>
    <body>On Jun 28, 2008, at 7:45 PM, David Duncan wrote: Thanks David, I'll file a bug. Not what I was hope for (as the window server can update the menu bar when it also updates the desktop image - it owns both windows though) - but it is what it is.</body>
  </mail>
  <mail>
    <header>Re: Forcing menu bar window to update from background app</header>
    <body>The Leopard translucent menu bar doesn't actually blend with what is underneath it. To the best of my knowledge, there is no way to affect what it blends with. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: rotation question</header>
    <body>Your cold ridden saturday morning brain is correct however :). For a sample code demonstration of this, you can also look up Quartz 2D Transformer which will rotate an image around its center -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: rotation question</header>
    <body>My guess is that you should read up on 3x3 transform matrices (known as a CTM in Quartz). You probably want to translate the ctm to the location you want to rotate around, rotate it, then translate back by the negative amount that you translated to in the first place. I might be thinking of something else. It's Saturday morning and I'm getting over a cold, so my brain's on standby. Experiment with the translations. :) _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Forcing menu bar window to update from background app</header>
    <body>I have a window that I'm using to replace the desktop image window from a faceless app (LSUIElement=1). It's working fine except with the menu bar on Leopard (10.5.3, Intel): If I set the level to kCGDesktopWindowLevel then the translucent menu bar becomes opaque and if I set the level to kCGDesktopWindowLevel-1, then the menu bar is transparent, but doesn't update when my desktop window does so it has a stale blend with my desktop window until the user switches foreground apps. Is there anyway to get around this? I tried creating a cover window at kCGScreenSaverWindowLevel (well above the menu bar) at an alpha of .05 that draws a fill color, yet the menu bar is still not updated when the cover window is removed.</body>
  </mail>
  <mail>
    <header>rotation question</header>
    <body>I am currently drawing some rotated text using CG, however the text is being rotated on an axis that is the origin in the lower left corner. I thought I came across something in the documentation that discussed how to change the axis point so you could rotate around the center, or in my case the lower right corner, but I can't find it now.  Is this possible or am I dreaming that I read that somewhere? Thanks, Doug</body>
  </mail>
  <mail>
    <header>Re: Strange Core Image kernel program behavior</header>
    <body>Thanks for the idea. It works great. Looks like problem was a compiler error.</body>
  </mail>
  <mail>
    <header>Re: Strange Core Image kernel program behavior</header>
    <body>Just tried it out, it fails just like you described. c.r = 0.0; // Image gets black c.r = c.r*0.0; // Works fine And also : c.r = 1.0; // Image gets white c.r = c.r*0.0 + 1.0; // Works fine</body>
  </mail>
  <mail>
    <header>Re: Strange Core Image kernel program behavior</header>
    <body>You don't actually need those unpremultiply/premultiply statements. They shouldn't do anything, balanced like that, but I'd be tempted to remove them and see if they are the cause.</body>
  </mail>
  <mail>
    <header>RE: Strange Core Image kernel program behavior</header>
    <body>My guess would be that you are sending in an ARGB image instead of RGBA. So zeroing out the red actually zeros out the alpha. Stephan -----Original Message----- From: quartz-dev-bounces+smarcoui=email@hidden [] On Behalf Of email@hidden Sent: Wednesday, June 25, 2008 8:37 PM To: email@hidden Subject: Strange Core Image kernel program behavior Hi, I have run into what seems to be very odd behavior from a very simple Core Image kernel program. Using Quartz Composer and the Core Image Filter patch, I wrote a simple kernel program that zero's out a single RGB component of an input image, then outputs the result. kernel vec4 removeColorComponent (sampler source_image) //pixValue.r = 0.0;     // Line A //pixValue.g = 0.0;     // Line B //pixValue.b = 0.0;     // Line C To zero out any component of the output image, I simply &amp;quot;uncomment&amp;quot; a single line, either Line A, Line B or Line C. When I uncomment Line B, all goes as expected--the green contribution disappears from the output image. Similarly if I uncomment Line C, the blue component disappears from the output. However, when I uncomment Line A (red component only), the whole output image turns black, not just the red contribution. Note, the input image is richly colored and contains all 3 RGB components. This problem happens under Leopard, Snow Leopard OS, using ATI or nVidia cards, on Laptop and Desktop Macs. Any ideas what I am missing?? Thanks in advance. Stan Stoneking AJA Video</body>
  </mail>
  <mail>
    <header>Strange Core Image kernel program behavior</header>
    <body>Hi, I have run into what seems to be very odd behavior from a very simple Core Image kernel program. Using Quartz Composer and the Core Image Filter patch, I wrote a simple kernel program that zero's out a single RGB component of an input image, then outputs the result. kernel vec4 removeColorComponent (sampler source_image) //pixValue.r = 0.0; 	// Line A //pixValue.g = 0.0; 	// Line B //pixValue.b = 0.0; 	// Line C To zero out any component of the output image, I simply &amp;quot;uncomment&amp;quot; a single line, either Line A, Line B or Line C. When I uncomment Line B, all goes as expected--the green contribution disappears from the output image. Similarly if I uncomment Line C, the blue component disappears from the output. However, when I uncomment Line A (red component only), the whole output image turns black, not just the red contribution. Note, the input image is richly colored and contains all 3 RGB components. This problem happens under Leopard, Snow Leopard OS, using ATI or nVidia cards, on Laptop and Desktop Macs. Any ideas what I am missing?? Thanks in advance. Stan Stoneking AJA Video</body>
  </mail>
  <mail>
    <header>Image Coloring and Compositing</header>
    <body>I'm new to using Core Graphics and Quartz directly and am trying to find a way to perform the following. In this case I can't use Core Image or Core Image Filters. Image 1 - Main Image Color Image 2 - Grey scale image that I'd like to use to paint specific pixels in Image 1 Color - Some color that the user would pick that can be used to paint Image 1 based on the grey scale value of Image 2 There is a one-to-one relationship between the pixels in Image 1 and Image 2. - Any pixel in Image 2 that is black I'd like the corresponding pixel in Image 1 to remain as it is. - Any pixel that is pure white would have the corresponding Color set for that pixel in Image 1 - Any pixel that is between black and white would be composited so that the Color would maintain aspect of the underlying shading (that is if the user Color is orange the resulting color would be some combination of orange + underlying grey tone) David W. Gohara, Ph.D. Center for Computational Biology Washington University School of Medicine 314-362-1583 (phone) 617-216-8616 (cell) Google Chat: sdg0919 iChat: sdg0919</body>
  </mail>
  <mail>
    <header>Re: NDevice colorspace</header>
    <body>I have not tried since the docs say it won't work, i will try it though. I suspect it wont work since ImageIO won't load an image that has a 5N space.</body>
  </mail>
  <mail>
    <header>Re: NDevice colorspace</header>
    <body>The documentation for CGColorSpaceCreateICCBased() says that nComponents must be 1, 3, or 4, but have you tried handing it a 5- component profile? The rest of the CGColorSpace API seems to support abstract N-dimensional profiles (I would expect that you would get a colorspace with the kCGColorSpaceModelDeviceN color space model, and 5 components).</body>
  </mail>
  <mail>
    <header>NDevice colorspace</header>
    <body>I have a 5-Color profile. I would like to create a CGColorSpaceRef with it. Is this at all possible? There does not seem to be a way to create  colorspace with profiles that contain more that 4 coordinates. AC</body>
  </mail>
  <mail>
    <header>Re: inter-process window parenting</header>
    <body>There is no public, or for that matter, private way to host a window an another process, or re-parent it.</body>
  </mail>
  <mail>
    <header>Re: inter-process window parenting</header>
    <body>Don't do it that way? :) Not sure I understand what you are trying to do and why you need to do something like this. -Shawn</body>
  </mail>
  <mail>
    <header>inter-process window parenting</header>
    <body>I'm looking for a way to make a remote process window the parent of a local process window - Cocoa or Carbon. Using the WindowList API's (CGWindow.h), I get a CGWindowID for a remote window.  However, I can't convert that CGWindowID into an NSWindow or WindowRef because these are not sharable across processes. Consequently I can't use NSWindow:setParentWindow or SetWindowGroup() to bind the windows. Trying a manual approach, I can track the remote window as it moves/ resizes using event taps, and manually move/resize the local window to match.  However, I don't know how to set the window ordering such that the local child window stays directly above the remote parent window. Thanks, -- Kevin Packard blackfrog software, inc.</body>
  </mail>
  <mail>
    <header>Re: Converting Core Image</header>
    <body>Wow! Just Wow! No wonder I couldn't make head nor tail of it.</body>
  </mail>
  <mail>
    <header>Getting selection in IKImageView</header>
    <body>I'm scanning an image in with the Image Capture API, and I put the result into an IKImageView to allow the user to select the area of the scan they want to keep (much like Image Capture does). The problem is that I can't find any way to actually get the selection area out of the image view. I know that the user can use the Copy command from the edit menu to get the selected image out, but I'd rather not try to jury rig a solution based on that. Is there a good way to either get the selected area, or the selected part of the image itself? Thanks, Colin</body>
  </mail>
  <mail>
    <header>Re: Converting Core Image</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Converting Core Image</header>
    <body>It's slightly more complicated for flipped/tiled textures. See for the best explanation I've found so far. Bug filed as rdar://6017698 - I suggest you do file documentation enhancement bugs whenever you encounter undocumented/badly documented functionality. In the past, Apple has been surprisingly good at addressing the ones I filed. A kernel is not a function. A kernel is a function applied to every pixel in an image, in total returning a new image. (Or sampler - it's not *quite* the same as a texture, but for all intents and purposes it is.) The confusion here comes from the fact that energyComputation is a kernel written in CoreImage's shader language, while the main function is JavaScript. (Gotta love Quartz Composer ;) Calling apply (on the JavaScript side) on a kernel returns a new sampler, since it applies the kernel to the entire sampler passed in. What main is doing is setting up a multipass filter chain.</body>
  </mail>
  <mail>
    <header>Fwd: Converting Core Image</header>
    <body>Realized I hadn't copied the list, even though I meant to. ---------- Forwarded message ---------- Subject: Re: Converting Core Image Wow! You found a doozy of a shader! I think you'll have fun porting this one to GLSL. You're missing something which I don't understand completely, because it's all Core Image extensions to the GLSL. Note that the &amp;quot;var energy&amp;quot; equals &amp;quot;energyComputation.apply()&amp;quot; and that energyComputation is a &amp;quot;kernel&amp;quot; routine. &amp;quot;energy.extents&amp;quot; is then passed into &amp;quot;iteration.apply()&amp;quot;. It's all outside of GLSL, and as far as I can tell, undocumented. It all suggests a subtext of objects that represent image areas, or something, but it's difficult to know. Again, samplerTransform is a CoreImage extension that just says that it transforms the coords into another working space. I think what it's doing is scaling the coordinates by the size of the the integer offsets applied become texel offsets. If anybody knows more (and is allowed to say) then please fill in the blanks.</body>
  </mail>
  <mail>
    <header>question on layer setup in Covertflow sample code</header>
    <body>Hi, I was reading the code (part of Core Animation sample code) and could not understand the part that the reflection layer is a sublayer of the desktop image layer. e.g,  in Controller.m .... .... .... ... CALayer *sublayer = [CALayer layer];  // for reflection the image ref will later to set as the content of desktopImageLayer, and sublayer layer |---------- desktopImageLayer |-------------------------------sublayer |------------ gradientLayer if (image != NULL) // main image [layer setContents:(id)image];    &amp;lt;------  Line A // reflection [sublayer setContents:(id)image];    &amp;lt;------------- Line B. Two questions 1)  I would think  sublayer above should be child of layer (and sibling of desktopImageLayer). what is the reason for the current setup? With this setup, is it true that a sublayer is not geometrically constraint to be inside the bounds of its superlayer? 2) In Line A noted above,  the content of layer is set. What is the impact of this to its sublayers?  Later, the sublayer's content is also set. Thanks for any insights on this. -- Wayne Shao</body>
  </mail>
  <mail>
    <header>Re: Scaling errors in event taps API</header>
    <body>Here's what I was remembering, at K&amp;amp;R (2d ed.), p. 45: &amp;quot;If x is float and i is int, then x = i and i = x both cause conversions; float to int causes -- Bill Cheeseman - email@hidden Quechee Software, Quechee, Vermont, USA www.quecheesoftware.com PreFab Software - www.prefabsoftware.com</body>
  </mail>
  <mail>
    <header>Re: Scaling errors in event taps API</header>
    <body>It has been pointed out to me privately that I may be the one who is confused about automatic type conversion. When I refer to &amp;quot;standard&amp;quot; C, I mean K&amp;amp;R (2d ed.) C. I was apparently misremembering something I had read there not too long ago about type conversion. Rather than &amp;quot;automatic type conversion,&amp;quot; what we may be seeing in these Apple bugs is some deliberate type conversion or cast that is inappropriate. In K&amp;amp;R (2d ed.) section 2.7 &amp;quot;Type Conversions,&amp;quot; at p. 42, it says &amp;quot;Expressions that might lose information, like assigning a longer integer type to a shorter, or a floating-point type to an integer, may draw a It seems clear to me that the bug stems from inappropriate truncating of the fractional part of a floating point value. This is because input of exactly 1.0 to any of the functions at issue yields correct output, while input of 0.99 or any other value between 0.0 and (but not including) 1.0 yield output of 0. Similarly, in the rotation case (the only case that allows input greater than 1.0), input of 2.0, 2.1 and 2.9, for example, all yield output of 2.0 (and the integer function yields the corresponding output of 128), while input of 3.0, for example, yields output of 3.0 (and 192). In other words, there is no scaling between whole-number values. Once I apply my workaround, I get the expected continuous scaling. Can anybody shed some light on this, for my edification? I would especially love to hear from an Apple engineer what the actual issue is. -- Bill Cheeseman - email@hidden Quechee Software, Quechee, Vermont, USA www.quecheesoftware.com PreFab Software - www.prefabsoftware.com</body>
  </mail>
  <mail>
    <header>Scaling errors in event taps API</header>
    <body>I reported earlier on scaling errors in the CGEventSetDoubleValueField() function for several fields: mouse pressure, tablet pointer pressure, pointer tiltX and pointer tiltY. I have now acquired pointer devices that enable me to confirm that the same scaling errors affect the pointer rotation and tangential pressure fields. All of the scaling errors I have reported appear to be due to failure to appreciate the effect of the standard C automatic type conversion rules when multiplying an integer scale factor by a double input value. Type conversion truncates the input from 0.x to 0 when input is less than 1.0, unless the first multiplier in the statement is typed as float or double. Since almost all of the scaled event tap functions take input ranging from 0.0 to 1.0, they all set their fields to 0 for any input value less than 1.0 This bug has a particularly interesting effect on the kCGTabletEventRotation field. Rotation is supposed to be scaled from the integer range 0 to 23040, per Wacom developer documentation, to the double range 0.0 to 360.0 in degrees. Apple's implementation of CGEventSetDoubleValueField for the kCGTabletEventRotation field sets correct values for input ranging from 1.0 to 360.0 degrees in whole numbers. However, for input ranging from 0.0 to (but not including) 1.0 degrees, it sets the incorrect value of 0, apparently due to standard C's type conversion rules truncating the input from 0.x to 0 when input is less than 1.0. Presumably non-whole number inputs above 1.0, such as 259.817, are also truncated to the integer part, i.e., to 259.0, which will result in a subtly incorrect integer value being stored in the field. Here's my rotation workaround, where the 'rotation' input is a double between 0.0 and 360.0 degrees. Instead of using this statement, which is buggy: Use this: CGEventSetIntegerValueField(eventRef, kCGTabletEventRotation, 64.0 * Explanation: the double scaling factor 64.0 will cause the double 'rotation' input to remain a double according to standard C type conversion rules, the multiplication will then yield a correctly scaled double value with full precision, and standard C will automatically cast that result in the parameter to an integer in the range 0 to 23040, which the function will store in the field. Then CGEventGetDoubleValueField(eventRef, kCGTabletEventRotation) will return a double result correctly scaled between 0.0 and 360.0 degrees. Use similar workarounds for the other scaled fields. radr://5967488 -- Bill Cheeseman - email@hidden Quechee Software, Quechee, Vermont, USA www.quecheesoftware.com PreFab Software - www.prefabsoftware.com</body>
  </mail>
  <mail>
    <header>Re: invalid context for CoreText</header>
    <body>On Jun 13, 2008, at 11:15 PM, Adam R. Maxwell wrote: ...and now I find the CTStringAttributes.h header, which says that underline uses foreground text color, and doesn't mention strikeout. I see now there's an API doc page for CTStringAttributes as well (which the CFAttributedString docs unfortunately don't mention) and apparently CoreText only supports a subset of attributes.  I guess I'll probably end up trading this bug for a documentation bug. -- Adam</body>
  </mail>
  <mail>
    <header>Re: invalid context for CoreText</header>
    <body>On Jun 13, 2008, at 10:16 PM, David Duncan wrote: [...] Thanks!  That explains the invalid context error; pushing an NSGraphicsContext does swallow the error messages, and the purple text is drawn.  The red underline and strikeout attributes still aren't drawn. It seems like CoreText should handle this when drawing a CFAttributedString, especially since it seems to use NSAttributedString attribute keys and values (can I set a CGColorRef attribute?).  In this case I'm not setting the color attributes manually, so it has whatever NSAttributedString read from the RTF file. I filed a couple bugs on CoreText today, based on a sample project [1].  I've also discovered that if I set the font to Monaco 10 pt, most of the glyphs don't draw unless I disable antialiasing in the context.  Maybe I'm doing something dumb... thanks, Adam</body>
  </mail>
  <mail>
    <header>Re: invalid context for CoreText</header>
    <body>I think this might be because your attributed string includes an NSColor property. However, there is no current NSGraphicsContext, from which NSColor will attempt to get its CGContextRef (but since the current NSGraphicsContext is nil, the graphics port it gives will also be nil) thus you get an invalid context error. You can create and push an NSGraphicsContext to resolve this, or you can specify the colors without NSColor (I presume this is possible, but I don't know for sure...). -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>invalid context for CoreText</header>
    <body>I'm trying to draw a block of text in a CGBitmapContext using CoreText included with dev tools examples, I'm seeing some error messages logged by CG: The text draws, but it's missing all color attributes for text and underlines (and outlined text is drawn as normal).  The context I'm drawing to was created with device RGB colorspace; using a generic RGB colorspace doesn't seem to matter.  Any ideas? #0	0x96868627 in asl_send #1	0x96867b81 in asl_vlog #2	0x943d06f5 in default_log_message #3	0x94491a4b in CGPostErrorWithArguments #4	0x94491a31 in CGPostError #5	0x91dbdd49 in -[NSCalibratedRGBColor set] #6	0x924dbd28 in TAttributes::BuildRenderAttributes #7	0x924dbbac in TRun::DrawGlyphs #8	0x924dbaa6 in TLine::DrawGlyphs #9	0x9250121d in TFrame::DrawInRect #10	0x000baab0 in -[FVCoreTextIcon _lockedRenderAttributedString:withDocumentAttributes:] at FVTextIcon.m: 630 ...where line 630 is just a call to CTFrameDraw().  I can provide a link to the source if anyone's curious enough to look at it.  I figured quartz-dev was reasonable for this question since it's going through CGPostError and complaining about the CGContext. thanks, Adam</body>
  </mail>
  <mail>
    <header>Simple kernel runs on cpu instead of gpu</header>
    <body>Hello again quartz-dev-list, I&amp;#39;ve further simplified my kernel to a very very simple x-direction&amp;nbsp;only, mean filter and&amp;nbsp;still get errors&amp;nbsp;causing execution on the CPU. I am no longer concerned with working on the Gaussian/Bilateral filter. For now I would just like to figure out why even the most basic kernel does not run on the gpu. This is my kernel: kernel vec4 xMean(sampler src) This is the error: CoreImage: xMean: error in program 0x1ee8200: position 0, Error on line 18: malformed destination mask (hint: &amp;#39;,&amp;#39;) CoreImage: xMean: falling back to software Thank you, Jordan</body>
  </mail>
  <mail>
    <header>CILanczosScaleTransform sometimes giving blank output - Maddening</header>
    <body>I have a situation in my app where a CILanczosScaleTransform is producing blank output. It's reproducible only by running through an Applescript, if I do the same steps by hand, I can't replicate the issue. (That isn't to say I haven't seen the issue in my software when doing things by hand, but the only way I can definitely reproduce it is when I script my app. Timing issue perhaps?) I do real time processing of an image, and display the output to the user to a CALayer, among other things. When I create a new document, and set the document's image to a certain file, the lanczos scaling filter produces blank output. Again, I can't reliably replicate this when I set things by hand, but I can reliably make it happen when I run an AppleScript that automates my app. The AppleScript has very little overhead, and therefor very little place for it to go wrong. Additionally, I know that the input being given to the filter, a CGLayerRef, is good. This is driving me crazy. I've tried to tackle this bug before to little success because it was so hard to replicate. Now that I have an AppleScript to replicate the bug, I'm still having trouble. An additional weird thing is if I run the script again (which creates a second new document, with it's own CoreImage layer tree, CIContext, etc) the scale filter properly produces output. But if I go back to the first document and run the filter again it still produces bad output in the first document. The only way to get the first document to start rendering again is to change the input image. Slowly going insane, Colin</body>
  </mail>
  <mail>
    <header>Re: How to implement a regionOf method.</header>
    <body>Read the docs on creating the method: As I understand it, the ROI method only gets called once per invocation of the filter, not once per pixel as you seem to be assuming. Since your filter samples pixels 10 away in all directions, your ROI is basically just outsetting the input rectangle by 10. Cheers, Brian On Jun 11, 2008, at 11:07 AM, Jordan Woehr wrote:</body>
  </mail>
  <mail>
    <header>Re: How to implement a regionOf method.</header>
    <body>I am not at the WWDC but I will keep look out for the code. Other suggestions? Sent from my iPhone Original thread found here: After doing more reading and research on CIFilter&amp;#39;s I have come to a block. Specifically, how to use the regionOf method within a filter. From what I understand, the regionOf method is used to pass to the CIKernel information about pixels other than the source pixel. The documentation says a regionOf method is required whenever there is not a 1:1 mapping from source to destination. This is somewhat misleading because as was seen with my kernel I provided no regionOf method was required. So in an effort to reduce the number of texture lookups the regionOf method seems like the way to go. On a high level I understand what the regionOf method is suppose to do. What I don&amp;#39;t know is how to actually implement it. First, the regionOf method is suppose to take a CGRect and also return one. The CGRect that is passed to the method is &amp;quot;the extent of the region, in working space coordinates&amp;quot; [1]. So does this mean the whole image because I want the blur to apply to the whole image? Second, how is the CGRect which is returned accessed through the kernel? This wouldn&amp;#39;t require more texture look ups would it? Lastly, from the reading I did on general purpose computing on the GPU [2], I understand that transferring data from the CPU to the GPU is quite expensive time wise. Because the regionOf method is obj-C code it would have to be executed on the CPU, then the resulting texture transferred to the GPU for every pixel. For a blur this would be extremely expensive. The other possibility I see is that I&amp;#39;ve misunderstood what the regionOf method is suppose to do. Instead of provide multiple source pixels for the kernel&amp;#39;s destination, it specifies only a portion of the image that is suppose to be processed by the kernel. If this is the case, how can I reduce the number of texture look ups my kernel is performing. I&amp;#39;m already working on precalculating the Gaussian weights and separating the filter into multiple passes. Thanks Jordan</body>
  </mail>
  <mail>
    <header>Re: What happened to the Python Bindings to Quartz 2D</header>
    <body>&amp;gt; I'm not a Python expert, but the scripting bridge is the exact same API as I believe David was really referring to BridgeSupport, which is the foundation of the Python and Ruby bridges. &amp;quot;Scripting Bridge&amp;quot; is indeed a technology for generating Objective-C wrappers to Apple Events. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: What happened to the Python Bindings to Quartz 2D</header>
    <body>So, as Michael asked, are there some example scripts I can read to see how to do this? Could I really read in an image from a file, operate on it, then write out another image to a file using the Scripting Bridge???</body>
  </mail>
  <mail>
    <header>Re: What happened to the Python Bindings to Quartz 2D</header>
    <body>suppose that Apple put &amp;quot;sips&amp;quot; on github - and they adjudicated the &amp;quot;pull requests&amp;quot;. So many of us would contribute, they could pick and choose what to take. sips cannot be anything more than good demo code - what they post on their dev sites all the time. Ah - &amp;quot;Shangri-La&amp;quot; - for those of you old enough to remember what that means...</body>
  </mail>
  <mail>
    <header>Re: What happened to the Python Bindings to Quartz 2D</header>
    <body>If you are trying to use Quartz from python have you tried obj-c bridge? in interactive mode.. (python -i in terminal) from Quartz import * help(CIImage) CIImage.imageWithContentsOfURL_(&amp;quot;somevalidpath&amp;quot;) help(CIImage.imageWithContentsOfURL_) fun! m</body>
  </mail>
  <mail>
    <header>Re: What happened to the Python Bindings to Quartz 2D</header>
    <body>well, that was mostly an attempt at humor, given the whole Apple/Jobs/Pixar thing, before Disney got involved. However, i'd be much less depressed if Apple had any Quartz scripting bridge sample code snippets on par with the excellent python Quartz code samples that used to install alongside Xcode.... Any chances of that? michael</body>
  </mail>
  <mail>
    <header>Re: What happened to the Python Bindings to Quartz 2D</header>
    <body>Hopefully it's not depressing that you have access to the same help as Pixar tools developers, plus the help from them too from time to time :) _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: What happened to the Python Bindings to Quartz 2D</header>
    <body>wait, you're at Pixar, and you don't have any better contact to Apple than this list?? How depressing. michael</body>
  </mail>
  <mail>
    <header>Re: What happened to the Python Bindings to Quartz 2D</header>
    <body>We use sips here at Pixar all the time. We love the fact that it embeds an ICC profile in its output if it is in the src image. We'd love, love, love to see Apple spend some more time on it. I've expressed this to Apple folks in the past - if there's something more formal I could do, please, Apple folks, let me know.</body>
  </mail>
  <mail>
    <header>Re: pdf to very large image....</header>
    <body>----- Original Message ----- Cc: Sent: Wednesday, September 28, 2011 3:25 AM Subject: Re: pdf to very large image.... Hi, James. allocation fails at drawing context creation... ....&amp;nbsp; @600 dpi we explode the memory ... You cannot allocate large bitmap buffer more than 2GB under 32 bits programing mode. You can do it in 64 bits mode, up to 65500 pixels for each width and height. You can allocate more than that, but some drawing api will fail. I did create 40000 x 40000 NSBitmapImageRep, draw into it, and save as jpeg, tiff, png and others, tested on Mac OS X 10.6. Yoshiaki Katayanagi</body>
  </mail>
  <mail>
    <header>Re: What happened to the Python Bindings to Quartz 2D</header>
    <body>Let me second Michael's suggestion. The whole reason this seemed of interest is as follows. I'm trying to build a &amp;quot;Service&amp;quot; that downloads images from my companies web sites, saves them in a folder named after the current web page, then modifies the images. In my case I need to crop the images - that is, to extract a subimage in the interior of the downloaded images. Now, I told my boss that this was a piece of cake, Apple has this powerful Automator and the &amp;quot;sips&amp;quot; (Scriptable Image Processing System) command line tool, and I could knock this out in a few hours. Well - to my chagrin, sips cannot be coerced into extracting arbitrary crops - it does so from the center and there is no way to do what I needed :-( In the end I wrote a small command line program to do what was needed - its like 10 lines of code - but now I cannot distribute the Automator action as it needs an executable (well, I probably COULD embed that program in the Automator based service but it seems like cheating. So, today, I tripped on a page referring to Python bindings, so I thought - ahah - I can do this with a Python script. --- So, as Michael asked, are there some example scripts I can read to see how to do this? Could I really read in an image from a file, operate on it, then write out another image to a file using the Scripting Bridge??? PS: in case anyone cares, it would be just GREAT if someone could enhance sips - which appears to have not been touched since Tiger. There are many people who have complained on the web of the inability to specify a crop offset:   rdar://10185319</body>
  </mail>
  <mail>
    <header>Re: What happened to the Python Bindings to Quartz 2D</header>
    <body>i would love more info on this. Can anyone point me to some sample scripts that use this newer approach to the Quartz functionality? thanks, michael</body>
  </mail>
  <mail>
    <header>Re: What happened to the Python Bindings to Quartz 2D</header>
    <body>The python bindings were deprecated in favor of general Scripting Bridge support that was introduced in Mac OS X 10.5 (iirc). Through the Scripting Bridge you have access to all of Mac OS X, as such you can use the Quartz C API directly rather than via the bridge. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>What happened to the Python Bindings to Quartz 2D</header>
    <body>I read with some interest about the Python bindings, and even found a early 2010 version of the Quartz 2D Programming Guide with a section on then. Then, in late 2010, it disappeared from the guide, and seems to have been totally erased. David</body>
  </mail>
  <mail>
    <header>Re: pdf to very large image....</header>
    <body>Are you a 32-bit application then? Does this happen if you make your app 64-bit? -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: pdf to very large image....</header>
    <body>&lt;span Monday, September 26, 2011 2:17 AM Re: pdf to very large image.... &lt;span class="yiv1991121068Apple-style-span"</body>
  </mail>
  <mail>
    <header>Speeding up image drawing</header>
    <body>I'm looking for tips to speed image drawing on OS X ‚Äî this is a primary function of my application. I started out naively with NSImage, switched to CGImage, added layer backing, then drew directly into that layer with Quartz. I have an implementation using CATiledLayers but that doesn't add speed in most cases. At this point, large images still draw significantly slower than in Preview. For example, I can display a 25MB test image (~9k x 9k pixels) in ~2 seconds while Preview can do it in ~1 second. I've looked at Preview in Shark and can't make much of it. I've combed the documentation, the internet, and the list archives pretty thoroughly and I feel like I'm about out of things to try. Can anyone on the list give me any leads or advice? Thanks</body>
  </mail>
  <mail>
    <header>pdf to very large image....</header>
    <body>I&amp;nbsp; (na√Øvely ?) tried this CGImageDestinationAddImageFromSource (imageDestination, src, 0, JpegPropertiesDic ) expecting that quartz would automatically create an imaging stream from file to file to honor the request. This produces no runtime error , but does absolutely nothing ! the JPEG output is empty, despite the fact that the JpegPropertiesDic dictionary Obviously , the following code</body>
  </mail>
  <mail>
    <header>Re: How to optimize rendering to CGContext from an in-memort bitmap?</header>
    <body>Because your previous technique was CGBitmapContext -&amp;gt; CGImage -&amp;gt; draw to CGContext.  Creating the CGImage directly avoids the first step.  Either way involves creating a CGImage each time you draw it to the destination context, which is going to have similar implications regarding copying data. When using Cocoa, you could work with NSBitmapImageRep, which is nominally a mutable image.  However, if you read the AppKit release notes, you'll find that it creates immutable CGImages under the hood.  It gives you direct access to the bitmap memory, but it has to decode from a CGImage to do so.  Then, when you go to draw, it creates a CGImage from the bitmap and discards/invalidates the bitmap pointer it previously provided to you. If that's what Apple chose to do with NSBitmapImageRep, it's probably the best you can do. In your original message, you didn't mention anything about the image &amp;quot;constantly changing&amp;quot;.  Your code suggested you were only creating the bitmap graphics context for the sole purpose of eventually drawing a bitmap image to some other context.  Are you actually rendering to the bitmap graphics context?  If you are, then it does make sense to keep using a bitmap graphics context and occasionally obtaining an image from it to draw elsewhere.  (If you're using AppKit and NSGraphicsContexts, then you may be able to avoid the intermediate image object by using NSCopyBits() to blit from one context to the other.) Regards, Ken</body>
  </mail>
  <mail>
    <header>Re: How to optimize rendering to CGContext from an in-memort bitmap?</header>
    <body>Thanks folks. I need to use both Carbon and Cocoa, because it is for AudioUnit plugins... what a mess... Anyway you suggested  instead of CGBitmapContextCreate. Why do you think so? I mean the image is constantly changing and CGImageCreate looks like creating a bitmap from some data, but it looks like it would copy the data. The documentation doesn&amp;#39;t say anything about actual behaviour as usual, so I can just guess... Cheers! Vojtech &amp;gt; Are you referring to some mechanism for obtaining graphics contexts CGBitmapContextCreate() isn&amp;#39;t Carbon. ¬†I&amp;#39;m referring to graphics contexts obtained from HIViews or parameters of Carbon events (the kEventParamCGContextRef parameter of a kEventClassControl/kEventControlDraw event). That said, I was looking at an older local copy of the Quartz 2D Programming Guide which still mentions Carbon and HIViews. ¬†The one online that I linked to does not. ¬†So, sorry about that. ¬†If one is still programming to Carbon, though, one should have a version of that guide which does document this stuff. Regards, Ken</body>
  </mail>
  <mail>
    <header>Re: How to optimize rendering to CGContext from an in-memort bitmap?</header>
    <body>CGBitmapContextCreate() isn't Carbon.  I'm referring to graphics contexts obtained from HIViews or parameters of Carbon events (the kEventParamCGContextRef parameter of a kEventClassControl/kEventControlDraw event). That said, I was looking at an older local copy of the Quartz 2D Programming Guide which still mentions Carbon and HIViews.  The one online that I linked to does not.  So, sorry about that.  If one is still programming to Carbon, though, one should have a version of that guide which does document this stuff. Regards, Ken</body>
  </mail>
  <mail>
    <header>Re: How to optimize rendering to CGContext from an in-memort bitmap?</header>
    <body>2011/9/25 Ken Thomases &amp;lt;email@hidden&amp;gt;: I don't see where this is documented. From : Are you referring to some mechanism for obtaining graphics contexts from Carbon, other than CGBitmapContextCreate? --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: How to optimize rendering to CGContext from an in-memort bitmap?</header>
    <body>Have you looked at CGImageCreate()? Yes.  A CGContext obtained from Carbon has its origin in the top-left, with positive Y going down.  This and other stuff relevant to what you're trying to are documented in the Quartz 2D Programming Guide &amp;lt;&amp;gt;. Regards, Ken</body>
  </mail>
  <mail>
    <header>How to optimize rendering to CGContext from an in-memort bitmap?</header>
    <body>Hi folks, I&amp;#39;m implementing a crossplatform (not-only GUI) library, where everything is drawn internally to an RGBA bitmap and then just copied to screen. On Windows it works like charm especially since there is a support for &amp;quot;in-memory&amp;quot; bitmaps. But I don&amp;#39;t see such a feature on Mac. I ended up with this code: ¬†¬†¬† ¬†¬†¬† CGContextRef bmpcontext = CGBitmapContextCreate((void*)desc.Bitmap.GetData(), ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† It basically converts the whole bitmap into CGImageRef and then copies a part of it into the NSView&amp;#39;s context. But it&amp;#39;s quite slow. Is there a better way to do this? And is there a way to create the in-memory images like they are on Windows? One more thing - I do the same thing in Carbon (I just need Cocoa because of x64...) and I noticed that the coordinates for the CGContext is inverted vertically, is that correct?? I mean the same context object type, but different Y handling... Thanks in advance. Vojtech</body>
  </mail>
  <mail>
    <header>Re: Pdf to jepg</header>
    <body>CFNumberRef&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp; CFNum300DPi;&lt;span &amp;nbsp;DataOptionsDict = CFDictionaryCreate( NULL , (const void**)Key, (const void**)Value, 2, NULL, NULL );JF ----- Original Message ----- Sent: Monday, September 19, 2011 5:50 PM Subject: Re: Pdf to jepg to the final CGImageDestinationAddImage(imageDestination, ImageRef, iDataOptionsDict). Have you tried using kCGImagePropertyDPIWidth/Height instead? -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: Pdf to jepg</header>
    <body>Have you tried using kCGImagePropertyDPIWidth/Height instead? -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: How to best create a 2x image containing images and text</header>
    <body>The relatively standard way to do this is to use the image as a mask, then draw a white rectangle through that mask. Just CGContextClipToMask() followed by the normal drawing to generate the final content. This method uses the image's alpha values as the alpha values to modulate drawing through (so 10% alpha leads to drawing through that pixel as if CGContextSetAlpha was passed 0.1 alpha for just those pixels). -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Pdf to jepg</header>
    <body>I&amp;nbsp; have to convert a multi-page PDF&amp;nbsp; into multiple jpegs @ 300 dpi.. Pretty easy to do with quartz... but I cant succeed in telling the jpeg files their density , aka storing a JFIF Dictionary within the Jpeg files, which should be done by the dictionary attached to the final CGImageDestinationAddImage(imageDestination, ImageRef, iDataOptionsDict). My code is : CFDictionaryRef&amp;nbsp;&amp;nbsp; iDataOptionsDict = CFDictionaryCreate( NULL /*allocator*/, &amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; (const &amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; (const void**)Value, &amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; 2, No error detected but the final Jpeg file is still @ 72 dpi when examined by GraphicConvertor or Photoshop. Could not find appropriate documentation or directions&amp;nbsp; neither in quartz docs&amp;nbsp; nor on dev-lists Could someone help on this ? Thanks and regards James email@hidden _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: How to best create a 2x image containing images and text</header>
    <body>Well, I released version one of my UIBarButtonItem this AM, and while it works perfectly in UIToolBars, in UINavigationBars the images do not get the white mask treatment that the toolbar images get. I've played around with trying to use blendModes to duplicate the UIToolBar effect (where every pixel of the image having a non-zero alpha value appears white) to no affect. Is there some way to use blendModes to get this effect, or is my only option to go poking around in the context changing bits? David</body>
  </mail>
  <mail>
    <header>[ANN} DHBarButtonItem</header>
    <body>Announcing a new UIBarButtonItem subclass that lets you create, update, and modify custom images created from an array of image, text, and spacing specifications. As an example, you could have a &amp;quot;Stop&amp;quot; or &amp;quot;Go&amp;quot; icon followed by some bits of text - the first image is the &amp;quot;Stop&amp;quot; icon and the text &amp;quot;Stop!&amp;quot;, and an alternate image might be the &amp;quot;Go&amp;quot; icon followed by the words &amp;quot;OK to Proceed&amp;quot;. The control determines the maximum width needed for all possible options, and sizes itself initially to this size - so that changes do not affect the item's width. Within each &amp;quot;column&amp;quot; you can align items to the left, center, or right side as well as change the item's baseline (vertical position). Images greater than 20 pixels are proportionally scaled to have a height of 20. PSS: many thanks to David Duncan for all his help on this list!</body>
  </mail>
  <mail>
    <header>Re: How to best create a 2x image containing images and text</header>
    <body>e If you use UIGraphicsBeginImageContextWithOptions(), then you can have everything taken care of for you (just pass 0 for the scale and it will select based on the screen).</body>
  </mail>
  <mail>
    <header>Re: How to best create a 2x image containing images and text</header>
    <body>So if we use UI* functions/classes, we get retina scaling for free, but if we use Quartz functions, we do not ‚Äì is that correct? e If you use UIGraphicsBeginImageContextWithOptions(), then you can have everything taken care of for you (just pass 0 for the scale and it will select based on the screen).</body>
  </mail>
  <mail>
    <header>Re: How to best create a 2x image containing images and text</header>
    <body>WOW! That worked great! I had been having to flip the text, use an affine transform, etc etc. My rendering code is now 1/3 of what it was! = [someNsString drawInRect (or drawAtPoint)] - same thing - scales and flips the text for you Really nice!!! David</body>
  </mail>
  <mail>
    <header>Re: How to best create a 2x image containing images and text</header>
    <body>If you use UIGraphicsBeginImageContextWithOptions(), then you can have everything taken care of for you (just pass 0 for the scale and it will select based on the screen). If thats not an option, scaling the CTM will generally work, but there are edge cases (such as shadows, which use the base coordinate system) where this is not the case. I would highly recommend using UIGraphics to generate the contexts for creating graphics to be used by UIKit. Another advantage is you automatically get a UIImage. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>How to best create a 2x image containing images and text</header>
    <body>Specifically, I want to create a UIImage from a CGImageRef that I create from other CGImageRefs and text. So, for retina displays, I want to create a 2x image. I'm making the assumption that if I have a UIImage of say 20x20 points, but the bit map is 40x40 and the UIImage size is 2, that the system (in my case a UIBarButtonItem) will treat propertly deal with it. Anyway, so in the case of retina display, I am going to make the bit map have 2x the pixels as points. I will figure out how to get the 2x images and draw them into the bit map (one way or the other). The real question is: how to draw the text? Should I set the affine transform on the bit map to 2x? [Meaning, I tell the string to draw into a frame of say 20x100, when in fact the bit map is 40x200]? Or would I just use a 2x font size and draw &amp;quot;normally&amp;quot; into the larger bit map? David</body>
  </mail>
  <mail>
    <header>Re: How best to create a CGImage by compositing an image and some	text</header>
    <body>Just keep in mind that main thread is only required on iOS 3.x. iOS 4.0+ can do this on any thread you like. -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: How best to create a CGImage by compositing an image and some	text</header>
    <body>Well, I only need a few ASCII words, but to bring this thread to a more general close: - get width of text using UIKit Additions I get the width of the text using the UIKIt additions - create bit map context - draw the image - UIGraphicsPushContext(my bit map context) draw the string UIGraphicsPopContext David</body>
  </mail>
  <mail>
    <header>Re: How best to create a CGImage by compositing an image and some	text</header>
    <body>Don't use CGContextShowText. There are a large variety of reasons that all come down to &amp;quot;it doesn't scale to your users&amp;quot;. It does not. I would recommend that you draw and measure your text with the UIKit additions unless for some reason you need to support iOS 3 and need to run this on a background thread. If you end up needing more complex layout, CoreText is also a possibility on 3.2 or later. Either way, don't use CG to draw or measure your text ‚Äì the limitations are not worth it. -- David Duncan</body>
  </mail>
  <mail>
    <header>How best to create a CGImage by compositing an image and some text</header>
    <body>iOS issue. What I want to do is create a bit map image where I draw an image and some text into a bit map context. I have what appears to be a chicken and egg issue: to know the width of the image, I need to know the width of the text. But without a context in which to track the text position, I cannot get it using CGContextShowText. I could use the UIKit Additions to NSString &amp;quot;sizeWithFont&amp;quot; and assume that the widths are more or less the same. However, I have this sinking feeling what that routine does is to create a context, draw into it, then measure the change in text position (exactly what I want to do) - but of course do not know for sure. So, if I use CGContextShowText to draw invisible text, then I assume I would make some very small context, measure the text width, then delete that context and create the real one. David</body>
  </mail>
  <mail>
    <header>Re: CGImageSourceCreateWithURL leaves files open?</header>
    <body>What happens if you don't raise the limit?  There's a soft limit and a hard limit, and the hard limit is &amp;quot;unlimited&amp;quot;.  I've seen my program go above 1000 open files without calling setrlimit and without any obvious ill effects. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: CGImageSourceCreateWithURL leaves files open?</header>
    <body>Thnks -- You (and Gus) were right on the money &amp;#8212; it is ONLY by copying data that the file reference can be freed up! The example code (below) now works. This code is for browsing a large collection (thousands) of images &amp;#8212; of course the browsed images are subject to resizing on-screen, so perhaps there's a better way ( CGImageSourceCreateThumbnailAtIndex?) Stephen &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;CGImageRef theCGImageRef = &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;CGImageSourceRef sourceImageRef = CGImageSourceCreateWithDataProvider(myCopyOfImageDataRef, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;theCGImageRef = CGImageSourceCreateImageAtIndex(sourceImageRef,</body>
  </mail>
  <mail>
    <header>Re: CGImageSourceCreateWithURL leaves files open?</header>
    <body>I recently ran into something similar, but only when running on 10.7.  My workaround was to use CGImageSourceCreateWithData with an NSData object that wasn't mapped to the file system. -gus -- August 'Gus' Mueller Flying Meat Inc.</body>
  </mail>
  <mail>
    <header>Re: CGImageSourceCreateWithURL leaves files open?</header>
    <body>Are you sure you don't have open file system objects as long as the CGImageRef exists?  I don't see anything fundamentally different between what you're doing and what the OP is doing. I'm guessing that CGImageRef has some sort of caching scheme, in which it may sometimes dump its bitmap data and later reload it from the image file. If it's important to avoid the open files, it may be necessary to make a deep copy of the image.  You might think that CGImageCreateCopy would do that, but no.  It can be done using CGDataProviderCopyData, CGDataProviderCreateWithCFData, and CGImageCreate. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>CGImageSourceCreateWithURL leaves files open?</header>
    <body>Stephen, I suggest you use only URLs. This snippet works for me for thousands of images without any leaks or open file system objects left over. @interface imageStuff: NSObject @end @implementation imageStuff - (CGImageRef) readOneImage:(NSURL *)url // I previously check that the url is a valid file system url and that the file exists NSUInteger	imageIndex = 0;	// which image in the wrapper url (there could be any number if it's TIFF) // do the right thing // you should probably check that the image type is one you can work with. //	do the right thing //	do the right thing //	do the right thing @end // imageStuff Hope this helps. Fred Fred Glover, P.E. Machine Vision, Image Processing, OS X and iPhone development Visicon Inc Los Gatos, CA cell: 408-761-1551</body>
  </mail>
  <mail>
    <header>CGImageSourceCreateWithURL leaves files open?</header>
    <body>I'm looping through a a ton of JPEG files and creating corresponding CGImageRefs. &amp;nbsp;However, even though I release the URL and the CGImageSourceRef, a reference to the file itself still remains open. &amp;nbsp;This of course becomes a problem because I assume I shouldn't need to raise the default open file limit from 255 (which I have done). Is there something wrong with this whole approach: &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;CGImageRef theCGImageRef = &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;CGImageSourceRef sourceImageRef = CGImageSourceCreateWithURL(cfUrl, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;theCGImageRef = CGImageSourceCreateImageAtIndex(sourceImageRef, Best Regards, Stephen Greenfield Write Brothers, Inc.</body>
  </mail>
  <mail>
    <header>How to get a IKImageBrowserView to &amp;quot;display&amp;quot; on command?</header>
    <body>My IKImageBrowserView has a enclosing NSScrollview, and uses CGImageRefs. After a user selects a directory full of images, I'd like to have the IKImageBrowserView display each images as its loaded: - load one CGImageRef, and insert into an array - inform IKImageBrowserView that the array of images is updated - scroll to the bottom of the scrollView's document view (the IKImageBrowserView) - command the IKImageBrowserView/ScrollView to update the display - repeat until all images loaded However, I cannot figure out anyway to do this - using [scrollView display] or [ikBrowserView display] does nothing. What happens is that all images are loaded, and only then does the browser view start painting the thumbnails (or full images, zoomValue has no affect). David</body>
  </mail>
  <mail>
    <header>Re: How do I draw circular text in a CG graphics context?</header>
    <body>To be quite honest, you would rarely use only CG calls to draw text of any non-triviality. Since your using layers, I would recommend CoreText if your interested in replicating the functionality of the CATextLayer. The Core-Text mailing list can assist you with functionality. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>CIPageCurlTransition oddity with large view area</header>
    <body>CIPageCurlTransition displays an odd behavior when run inside of a large view. The page curl seems to break at the mid-point of the view and start over. You can see this in Core Image Fun house by going full screen on a MBP 17, 24 CD or 30 CD. The default images don't exhibit the behavior but they are only 1200x900. If you use images larger than the screen (I'm using 3888x2588 and higher) than the break occurs. Using the same images on a smaller screen (for instance an iMac/ATI 1400x900) does not exhibit the issue. Adjusting the angle and radius did not help. I also tried shrinking the input extent rect and that didn't help either. Any ideas?</body>
  </mail>
  <mail>
    <header>Re: How do I draw circular text in a CG graphics context?</header>
    <body>This seems like a work-around and it is definitely something I will fall back on, if I can't do the whole thing with CG API calls.  Still, can anyone out there show me some sample code using only CG API calls to accomplish drawing text on a CALayer? Attachment:</body>
  </mail>
  <mail>
    <header>trapping on errors in Core Animation code?</header>
    <body>So I'm been mucking with my code, and while I think I may have a handle on what I did to cause this, for future reference, I would very much like to know how I can trap on stuff like: superlayer=0x0 display]: Ignoring bogus layer size (19940.000000, 8500.000000) CoreAnimation: rendering error 500 CoreAnimation: 11178 by 4618 image is too large for GPU, ignoring I stuck a breakpoint at -[NSException raise], with no luck.  Perhaps I'm misremembering the break point syntax?  These are throwing exceptions, right? --&amp;gt; Michael B. Johnson, PhD --&amp;gt;  (personal) --&amp;gt;  (alum) --&amp;gt; MPG Lead --&amp;gt; Pixar Animation Studios</body>
  </mail>
  <mail>
    <header>Re: How do I draw circular text in a CG graphics context?</header>
    <body>I have mixed NS-based drawn methods with CG-based drawing methods in some on my layer delegates.  You'll need to make sure that you have the correct context-object to draw in to. The delegate with be passed a CGContextRef which you can use to create an NSGraphicsContext.  use the CGContextRef for your CG-based drawing and NSGraphicsContext for you NS-based drawing</body>
  </mail>
  <mail>
    <header>Re: drawing special characters (e.g. omega) using MacRoman encoding and CGShowTextAtPoint?</header>
    <body>On Oct 30, 2008, at 7:03 PM, Rua Haszard Morris wrote: You might try HIThemeDrawTextBox, after converting your MacRoman string to a CFString.</body>
  </mail>
  <mail>
    <header>drawing special characters (e.g. omega) using MacRoman encoding and	CGShowTextAtPoint?</header>
    <body>I am trying to draw certain special characters using CGShowTextAtPoint. This is part of ongoing development of an Carbon app that is now beginning to use some of Cocoa and Quartz; the app uses Pascal MacRoman strings and I need to render such strings containing things like omega and delta (Œ©, ‚àÜ) in NSViews. CGShowTextAtPoint is documented as being only able to use kCGEncodingMacRoman and kCGEncodingFontSpecific encodings, but things like omega don't render correctly (render as a box). This may be in part due to the function taking a signed char* parameter. See the documentation here: What is the best/easiest way to draw these kinds of characters into a CGContext? The API needs to be available &amp;amp; supported back to 10.4. Thanks, Rua HM. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: How do I draw circular text in a CG graphics context?</header>
    <body>Ok, I've been looking at the CircleView example and I'm not sure which NS methods I can mix with the CALayer and CG methods that I'm currently using.  Anyone have any idea how to leverage the CG text and glyph functions to accomplish the same thing as the CircleView sample application? Attachment:</body>
  </mail>
  <mail>
    <header>Re: How do I draw circular text in a CG graphics context?</header>
    <body>Attachment:</body>
  </mail>
  <mail>
    <header>Re: How do I draw circular text in a CG graphics context?</header>
    <body>Did you take a look at the CircleView sample code in your /Developer/ Examples/Appkit folder?</body>
  </mail>
  <mail>
    <header>How do I draw circular text in a CG graphics context?</header>
    <body>I need to draw some text around a compass-rose.  Sample code, anyone? Pointers to relevant articles or documentation also welcome. Attachment:</body>
  </mail>
  <mail>
    <header>Re: right pattern for updating alongside animation,	including cancelling?</header>
    <body>On Oct 21, 2008, at 9:07 AM, John Harper wrote: That fixed it, thanks.</body>
  </mail>
  <mail>
    <header>Re: right pattern for updating alongside animation, including cancelling?</header>
    <body>this looks like the problem: you will have to set the fromValue of the animation to where you want it to animate from: animation.fromValue = [[self.layer presentationLayer] valueForKey:@&amp;quot;position&amp;quot;] CAAnimation.h claims that when fromValue, toValue and byValue are all nil the animation will &amp;quot;interpolate between the previous value of the property in the render tree and the current presentation value of the property&amp;quot; but I'm afraid that is a bug in the header file‚Äîthe behaviour is effectively undefined when all values are nil. (As you saw, the render thread does the right thing, but the objective C API is unable to.) On Oct 19, 2008, at 11:37 AM, Michael B Johnson wrote:</body>
  </mail>
  <mail>
    <header>Core Video rendering frames in display link</header>
    <body>GetFrameForTime() is pretty much the same as the sample, except I added the &amp;quot;else&amp;quot;. &amp;nbsp;I was hoping to provide my frames for the inserted segment, but&amp;nbsp;QTVisualContextIsNewImageAvailable() always returns FALSE for that duration, so I try to do it in &amp;quot;else&amp;quot;, but it&amp;#39;s always crash, even with a glFlush(). &amp;nbsp;I also try to insert a black frame with 3 second duration (instead of empty movie) and hope to provide real frames in callback,&amp;nbsp;QTVisualContextIsNewImageAvailable() doesn&amp;#39;t return TRUE during this period either. &amp;nbsp;My questions are:</body>
  </mail>
  <mail>
    <header>Single Bit editable Image Mask (and other strangeness)</header>
    <body>I am working on pixel based drawing tool, and could use some hint on how to handle selection areas. In the program, each document has a couple different Bitmap context (24 bit RGP space CGBitmapContext ) at the same time. I draw into a scroll view using a Layer (CGLayer).  Whenever one of the bitmap context changes, I copy the changes into the layer, but scrolling within the view is done only with the layer except for drawing the selection area. As always, I want to make sure there isn't a better way of doing what I am doing now. To handle the selection, I have been using a CGBitmapContext  (created with method CGBitmapContextCreate with a bitsPerComponent of 8, a color space of GreySpace, and CGBitmap info of kCGImageAlphaNone). This gives me an editable context, so that I can create the selection graphic using Quartz draw rectangles and ovals. When I need to draw using the selection, I convert the selection into an image (CGBitmapContextCreateImage), clip the context to that image (CGContextClipToMask) and then do whatever drawing I want to do (ex. invert, special fill, you name it). First of all, it is possible to create a single bit bitmap context (only black and white)?  I could only get 8 bit greyscale working correctly. Should I be using ImageMask for these operations? I did not create a bitmap mask (using CGImageMaskCreate) because I needed to be able to constantly modify the mask using rectangle and oval commands (and I could not get that call to work). Lastly, when when drawing between bitmap context (basically copying one section to another), is the best way to create an Image from the bitmap info (CGBitmapContextCreateImage) and then draw using the image.  Apple documentation suggest that this is quick enough. If I create an image, and then modify the context, the image I created would have a copy of the data, not any changes. Steve Sheets</body>
  </mail>
  <mail>
    <header>Re: Pointer on how to create a filter similar to Leopard's	CIAreaMaximum</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Rounded corners on QTMovieLayer</header>
    <body>I'm trying to playback a movie in a QTMovieLayer that I want to have rounded corners and a shadow. Setting the 'cornerRadius' property on the QTMovieLayer does not show any effect. I can show the corners if I enable 'maskToBounds' though, but when I do this, the playback performance seriously drops (the movie does not playback smooth anymore) and the shadow is gone also. Is there a way to enable rounded corners on a QTMovieLayer that does not affect the playback performance? Thanks, Oliver Attachment:</body>
  </mail>
  <mail>
    <header>Re: ImageIO and multipage TIFF</header>
    <body>Yes, but you need know the the count of images CGImageDestinationCreate* time in advance. -- Ren√© Rebe - ExactCODE GmbH - Europe, Germany, Berlin</body>
  </mail>
  <mail>
    <header>Re: CGWindowListCreateImage and Adobe Flash</header>
    <body>One option could be for the OS to defer the capture until the next flush (if a window is dirty). But that would have be an enhancement request. I have an application that grabs portions of the screen with CGWindowListCreateImage(). &amp;nbsp;In situations where the screen contains browser content with Adobe Flash applications, sometimes the images returned will show solid white in the area of the flash app.</body>
  </mail>
  <mail>
    <header>Re: CGWindowListCreateImage and Adobe Flash</header>
    <body>I'm not aware of any known issues with this, so I would recommend that you file a bug. It may have to do with the interaction of Safari and Flash (assuming your using Safari), so you may want to try it with the content rendered by other plugins as well to see if you can come up with any others that exhibit the same issue. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>CGWindowListCreateImage and Adobe Flash</header>
    <body>I have an application that grabs portions of the screen with CGWindowListCreateImage().  In situations where the screen contains browser content with Adobe Flash applications, sometimes the images returned will show solid white in the area of the flash app. Is there anything known about this problem?  Ideally it would be great if there was a fix, but I'd be just as happy if I could detect that I have a partial image and throw it away. Thank you.</body>
  </mail>
  <mail>
    <header>Re: right pattern for updating alongside animation,	including cancelling?</header>
    <body>On Oct 15, 2008, at 2:32 PM, John Harper wrote: Finally got around to trying this.  Unfortunately, it's not working, so I must be missing something. The problem is that even if I'm only halfway through the animation (say it's moving from 0,0 to 1000,1000 over 10 seconds) when I ask for the position of the layer's presenationLayer, it always returns (1000,1000). I set up a timer to monitor the value of the layer's presentationLayer's position as it animates, and it remains at the final value the whole time (even as I watch the animation happen). // print position out 10 times over the course of the animation self.checkTimer = [NSTimer scheduledTimerWithTimeInterval: (self.driftSeconds/10.0) target:self selector:s userInfo:self.layer CABasicAnimation* animation = [CABasicAnimation animation.timingFunction = [CAMediaTimingFunction // set it to its final value that we want it to animate to NSLog(@&amp;quot;layer position is %@&amp;quot;, Unfortunately, the position is always the same as the finalPosition - it's not varying over time (even though the animation is working as expected). --&amp;gt; Michael B. Johnson, PhD --&amp;gt;  (personal) --&amp;gt;  (alum) --&amp;gt; MPG Lead --&amp;gt; Pixar Animation Studios</body>
  </mail>
  <mail>
    <header>Re: Pointer on how to create a filter similar to Leopard's	CIAreaMaximum</header>
    <body>You can use the &amp;quot;filter function&amp;quot; to make loops for running your kernels. In Quartz Composer, you can make a new Core Image Kernel effect, select 'edit filter function', and use loops to hit your kernel, and then average out your result. I not sure how one would do that in straight Core Image/Cocoa, but the functionality is in theory there. I imagine that will be very slow. You might be better off with using the GPU to downsample the image for you, since in a way that is doing averaging. It may not be exactly correct but for your purposes, and speed, it may be useful?</body>
  </mail>
  <mail>
    <header>Pointer on how to create a filter similar to Leopard's CIAreaMaximum</header>
    <body>Leopard has some new filters that puzzle me. I have written a few CIFilters  but  am really not an expert on the matter. So please bear with my ignorance. How can a filter like  CIAreaMaximum return in one pixel the max of all pixels? In my view you'd have to iterate though all of them to check that, but since you're only getting one pixel as the output, there's no way I can figure how this is implemented. Is the loop implemented elsewhere or is it possible to do this just from a single kernel? My idea is that if I could understand that, I could write a filter that returns bounds as a 4 pixel image (based on some conditions). Damn I wish Apple opensourced the standards filters, that would be a great help resource for learning. -------------------------------------------- Santiago (Jacques) Lema - link-u</body>
  </mail>
  <mail>
    <header>Re: CAOpenGLLayer won't recomposite to the screen</header>
    <body>...except that it is now significantly slower.  Way too slow to use for full screen playback. -C</body>
  </mail>
  <mail>
    <header>Re: (sort of SOLVED) Layer won't recomposite to the screen</header>
    <body>I converted my layer class from a CAOpenGLLayer to a CALayer and provided the layer content in a different way. CALayer does not exhibit this bug, so everything is now working correctly. Sorry for all the bandwidth I used; hopefully the archive of my conversation with myself will be useful to someone else later on.</body>
  </mail>
  <mail>
    <header>Layer won't recomposite to the screen (was: bizarre	CABasicAnimation behavior)</header>
    <body>Indeed.  I should have spent more time reading the documentation: /apple_ref/occ/instm/CALayer/initWithLayer: Anyway, more information: Reading John Harper's recent discussion of the layer drawing mechanism, I went back and removed CABasicAnimation entirely.  Instead I created my own rudimentary animation class and used it to directly update the layer properties over time.  I've turned off all actions for the layer by returning NSNull in my layer actionForLayer:forKey: method.  I've also simplified things by loading my video frame into only a single layer. Therefore my setup is now simply two layers on two different views on two different screens, one of which has some content to draw.  One of the layers is positioned on screen, the other is positioned off screen. The result remains:  When I adjust any property on the layer that is off screen, the layer that is on screen will stop re-compositing. -C</body>
  </mail>
  <mail>
    <header>Re: bizarre CABasicAnimation behavior</header>
    <body>Which, on benefit of a night's sleep, I now see was doomed to failure anyway (at least in the way I tried it) since aborting the setting of the new position value would never allow the layer to change from a visible position to an invisible position, and vice-versa. Also on benefit of sleeping on it, I assume these must be presentation layers doing their work behind the scenes. At any rate, this all still leaves me with my original problem:  the layers won't redraw properly.  I'm pretty stumped. Best, Chris</body>
  </mail>
  <mail>
    <header>ImageIO and multipage TIFF</header>
    <body>Does ImageIO handle output to multipage TIFF i.e. call CGImageDestinationAddImage several times to add images to a multipage TIFF file? Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen</body>
  </mail>
  <mail>
    <header>CGContextShowGlyphsAtPoint errors</header>
    <body>When I use CGContextShowGlyphsAtPoint with known correct CGGlyphs on the released iPhone OS 2.0, it spits out these errors Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen</body>
  </mail>
  <mail>
    <header>Re: bizarre CABasicAnimation behavior</header>
    <body>In an earlier version of the code the same failure to redraw all visible layers would occur if I called setNeedsDisplay on any layers that were not in the visible rect.  Once I enforced calling setNeedsDisplay only when a layer was actually visible, all the visible layers would continue to refresh. Thinking that the CABasicAnimation may be triggering the same behavior (bug?) by committing the same sin (triggering a redraw when the layer is not visible), I thought it might help to override a CALayer method and attempt to abort the redraw if the layer wasn't visible. - (BOOL) visibleInView CGRect layerRect = CGRectMake(0, 0, presentationLayer.frame.size.width, CGRect superLayerRect = [presentationLayer convertRect:layerRect if (NSIntersectsRect(NSRectFromCGRect(superLayerRect), NSRectFromCGRect(self.superlayer.frame))) else - (void) setValue: (id) value forKeyPath: (NSString *) keyPath NSLog( @&amp;quot;%p setValue:forKeyPath: %@ on %@, video: %@&amp;quot;, self, keyPath, if ([self visibleInView]) The result was completely unexpected:  the layers getting these calls are not the layers I allocated.  Wherever they came from, I didn't make them.  Their instance variables are empty.  They're just dozens of new, previously unseen CALayers, receiving new positions during the animation.  They may pop up one or two or three times before they are never seen again.</body>
  </mail>
  <mail>
    <header>bizarre CABasicAnimation behavior</header>
    <body>I have two NSWindows positioned to fill two screens.  The content view of each window is backed by a CALayer.  In this CALayer I add CAOpenGLLayer sublayers.  These CAOpenGLLayers are a subclass that display CVImageBuffers generated from a QuickTime movie.  A single CVImageBuffer produced by the movie is drawn in one or more layers simultaneously, depending on which layers are visible on their respective screens.  (I don't bother pushing the frame into the layer if the layer is not visible.) This all works great.  I can place the layers anywhere I want, rotate them, scale them, whatever, and I get my movie playing back on both screens in the independent layers. Now I'm trying to add the ability to animate those layers to new positions/opacities/etc.  My goal is to apply an explicit CABasicAnimations to each layer.  Here's where things get weird. If I apply a single CABasicAnimation to a single layer (doesn't matter which), it works as expected. If I create two separate CABasicAnimations, and apply them to two separate CAOpenGLLayers, the following behavior occurs: - if BOTH CAOpenGLLayers are visible on screen, both layers continue to update - if ONE CAOpenGLLayers is visible on screen, it will update for a few frames, and then simply STOP updating.  I continue to push frames into each layer and call setNeedsDisplay on the layer, but this call is completely ignored.  The calls to drawInCGLContext come to a halt.  I get a still image in the layer instead of a playing movie.  (The animation is, however, being applied to that still image.) - MOREOVER, in the case where two layers are being animated to new positions, this &amp;quot;freeze-frame&amp;quot; effect is instantly fixed at any moment both layers become simultaneously visible.  In other words, if both layers are visible, everything works.  As soon as one becomes invisible, the playback on the remaining visible layer halts after about a half second.  If both become visible again it instantly starts working again. Again, simply creating a second (independent) CABasicAnimation to add to a second (independent) layer causes this behavior to start. I can't make heads or tails of this, but it will sink my app if I can't figure out a way to fix it / work around it.  :(  Any advice or insight gratefully accepted. Best, Chris</body>
  </mail>
  <mail>
    <header>Re: Measuring text with CGFontGetGlyphAdvances</header>
    <body>I don't think this will include kerning pairs. For me CGFont is just a direct way to have information on the font, and the kerning pairs are in a different table. What I mean is that I don't think that the fact that there is an array of Glyph means that it is meant to handle a piece of text. You can have graphemes which are made of multiple glyphs, with placement not only determined by an offset on the x-axis. Raphael</body>
  </mail>
  <mail>
    <header>Re: Measuring text with CGFontGetGlyphAdvances</header>
    <body>After doing some OpenType research and observing how CoreGraphics advances the text position, it seems that the following is a decent text measuring algorithm: CGFontRef font = ... CGGlyph glyphs[n] = ... // sum all the advances for (int i = 0; i &amp;lt; n; ++i) Did I miss out anything? Will this incorporate any kerning pairs? Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen</body>
  </mail>
  <mail>
    <header>Measuring text with CGFontGetGlyphAdvances</header>
    <body>The function writes out an array of ints, so how do you scale this with the font size to get the right measurement? And will the advances include any kerning pairs in the input glyphs? I do know of CoreText and ATSUI, alas the um, platform I'm using doesn't have these niceties. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen</body>
  </mail>
  <mail>
    <header>Re: right pattern for updating alongside animation,	including cancelling?</header>
    <body>On Oct 15, 2008, at 2:32 PM, John Harper wrote: sorry, I meant CALayer*.  I mistyped (literally :-)) Okay, that's what I went ahead and wrote - just wanted to make sure I wasn't missing something. Hmm, the problem with that is that I'm trying to conserve vram by paging stuff in, but I'm seeing the false economy of it. If I just enlarge my clip region appropriately, I can start the paging work when I launch the animation...</body>
  </mail>
  <mail>
    <header>Re: right pattern for updating alongside animation,	including cancelling?</header>
    <body>On Oct 15, 2008, at 11:05 AM, Michael B Johnson wrote: It returns id so you don't need to cast to your subclass of CALayer (it returns some kind of CALayer not CAAnimation.) Yes. No, we don't provide anything like that, a periodic timer is the best solution. One other way you could try is to make sure that before you start the animation you have content available along the entire animation path?</body>
  </mail>
  <mail>
    <header>right pattern for updating alongside animation,	including cancelling?</header>
    <body>So I have a layer (with many sublayers) that the user can pan around by clicking and dragging with the mouse. When the user mouses up, I start up a CABasicAnimation to continue &amp;quot;drifting&amp;quot; in the direction the user was panning in the last few property.  This animation runs for a few seconds, and has an ease out, which has a nice &amp;quot;settling&amp;quot; feel to it. There's two problems, though - one of which I think I have the right idea about, but the other I don't have have a clue. The first problem is that if the user mouses up, and the layer drifts for a bit, but before it's done, the user mouses down and want to start dragging around again. So I expect that I need to: -  grab the layer's &amp;quot;presentationLayer&amp;quot; (aside: why is that (id), rather than (CAAnimation*)?) and get its position - stop the &amp;quot;drift&amp;quot; animation - set my layer position property to that saved position The one I don't know how to handle is that as the layer drifts along, some number of its sublayers come into view, and I need to reevaluate the content I'm drawing in them.  Up till now, since the view is the single point of entry for manipulating the layer, I was able to do this in my event handling code, but now, the layer is animating, and I need to get updated as it goes along. I already set the view to be the delegate of the animation, and I handle: but that doesn't help me as the animation goes along - only when it finishes.  I can obviously set up a timer to fire and accompany the animation, but I'm expecting there must be some sort of animation progress callback that the delegate can get, but I'm not seeing it. --&amp;gt; Michael B. Johnson, PhD --&amp;gt;  (personal) --&amp;gt;  (alum) --&amp;gt; MPG Lead --&amp;gt; Pixar Animation Studios</body>
  </mail>
  <mail>
    <header>Scaling transformation centered around an anchor point</header>
    <body>I'm trying to do a scaling transformation that's centered around an arbitrary (dynamic) anchor point.  The context for this is that I have a layer-backed NSView that's embedded inside an NSScrollView, and I have a zoom tool where a user can adjust the zoom level of the overall view.  The zoom's done by adjusting the NSView's bounds (which in turn adjusts the bounds of its layer), followed by a perspective transform on the scroll view's documentView's layer.  So, zooming in by 1.5x would change the view &amp;amp; layer's bounds from e.g. (0, 0, 400, 300) to (0, 0, 600, 450), and a perspective transform's then done so that there's scaling done of 1.5 on the layer contents. It all works nicely if I disable animations.  If I enable animations, the animation of the bounds and the perspective transform is &amp;quot;anchored&amp;quot; at the bounds origin, i.e. (0, 0).  I'd like the animation to be centered around the center of the scroll view's current content view instead, as one would expect from most Mac apps. Is there anything I can do besides trying to make (0, 0) the center point for the layer and having the bounds go into negative coordinate space? -- Attachment:</body>
  </mail>
  <mail>
    <header>Re: Render to CVOpenGLBuffer on background thread  -</header>
    <body>For those interested, the solution was to separate out the iChat render context from the main Quartz Composer rendering context, share them, and duplicate the incoming texture  from Quartz Composer, to avoid it being released/written to when iChat renders. With ample use of [NSRecursiveLock tryLock] and locking contexts, all seems happy, and window server lockups/crashing ati x1000 driver instances have vanished (knock on wood). On Oct 10, 2008, at 3:58 PM, vade wrote:</body>
  </mail>
  <mail>
    <header>Precision of Core Image floating point numbers</header>
    <body>What is the bit resolution of Core Image floats?  It appears that the smallest number I can represent is 0.003922, which corresponds to 1/255.  When I try to divide that down further, I get zero.  If I forcibly enter 0.002, it rounds up to 0.003922. kernel vec4 test(sampler gx) That returns values that all show to be 0.003922.</body>
  </mail>
  <mail>
    <header>Render to CVOpenGLBuffer on background thread  -</header>
    <body>I am attempting to implement a video provider for iChat Theater, by implementing the IMAVmanager video data source protocol: My code lives inside of a custom Quartz Composer plugin, as a consumer patch. It takes in a QCPluginInputImageSource, and attempts to render for iChat according to the above videoDataSource protocol for OpenGL. I *must* have thread safe rendering to a CVOpenGLBuferRef that the callback provides to me, since the callbacks happen on a background thread. I currently am able to output an image to iChat, but after some time, my image is lost, and I get garbage video memory, and the image never recovers. I also have noticed LOTS of ATI driver crashes/window server hangs, which tells me I am not properly synchronizing the two threads. I am wondering what the proper strategy for syncing resources across threads via OpenGL and Core Video GL buffers. Currently, what I think is happening (in email ascii markup :) Capture incoming QC image -&amp;gt; map it to a texture -&amp;gt; send texture to iChat video data source object - &amp;gt; release texture -&amp;gt; repeat Ichat callback for rendering - &amp;gt; Enter CVOpenGLBuffer -&amp;gt; render with the above texture - &amp;gt; repeat So I clearly need to make sure that: The texture from QC is still around, and somehow locked so that isnt being written or deleted when iChat calls it. The two threads render at vastly different frame rates as well. The other issue is that whenever I receive an image from QC, in the main QC rendering thread, *within that frame* I also have to release it, or I leak memory and other nastiness happens. Im close, I can taste it, but these threading nuances are a bit above my hobby programmer wiles. I was under the impression one couldnt even render to GL in a background thread at all, but I guess to a pbuffer its ok, because it is not on screen? Thanks *very much* for any hints, I know this is a terribly complicated issue.</body>
  </mail>
  <mail>
    <header>Re: copying Exif ...</header>
    <body>Hi, I am trying to copy Exif from one file to another with the following code, but it does'nt seems to work... The data are copied, but not the metadata... Did i missed something ? &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;// create the source &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;NSURL *_url = [NSURL fileURLWithPath:sourcePath]; // for exif &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;NSURL *_outurl = [NSURL fileURLWithPath:outputfile]; // dest &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;NSURL *_tmpurl = [NSURL fileURLWithPath:tempfile]; // for image &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;// get Exif from source?</body>
  </mail>
  <mail>
    <header>Re: using Quartz Debug to help understand/optimize CALayer drawing</header>
    <body>On Oct 10, 2008, at 12:17 AM, John Harper wrote:</body>
  </mail>
  <mail>
    <header>copying Exif ...</header>
    <body>Hi, I am trying to copy Exif from one file to another with the following code, but it does'nt seems to work... The data are copied, but not the metadata... Did i missed something ? // create the source NSURL *_url = [NSURL fileURLWithPath:sourcePath]; // for exif NSURL *_outurl = [NSURL fileURLWithPath:outputfile]; // dest NSURL *_tmpurl = [NSURL fileURLWithPath:tempfile]; // for image // get Exif from source? // create the destination CGImageDestinationRef destination = CGImageDestinationCreateWithURL((CFURLRef)_outurl, CGImageSourceGetType(source), CGImageSourceGetCount(source), // copy data from temporary image ... CGImageDestinationAddImageFromSource(destination, source, i, Ce message et les pi?ces jointes sont confidentiels et r?serv?s ? l'usage exclusif de ses destinataires. Il peut ?galement ?tre prot?g? par le secret professionnel. Si vous recevez ce message par erreur, merci d'en avertir imm?diatement l'exp?diteur et de le d?truire. L'int?grit? du message ne pouvant ?tre assur?e sur Internet, la responsabilit? du groupe Atos Origin ne pourra ?tre recherch?e quant au contenu de ce message. Bien que les meilleurs efforts soient faits pour maintenir cette transmission exempte de tout virus, l'exp?diteur ne donne aucune garantie ? cet ?gard et sa responsabilit? ne saurait ?tre recherch?e pour tout dommage r?sultant d'un virus transmis. This e-mail and the documents attached are confidential and intended solely for the addressee; it may also be privileged. If you receive this e-mail in error, please notify the sender immediately and destroy it. As its integrity cannot be secured on the Internet, the Atos Origin group liability cannot be triggered for the message content. Although the sender endeavours to maintain a computer virus-free network, the sender does not warrant that this transmission is virus-free and will not be liable for any damages resulting from any virus transmitted.</body>
  </mail>
  <mail>
    <header>Re: using Quartz Debug to help understand/optimize CALayer drawing</header>
    <body>On Oct 9, 2008, at 11:09 PM, Michael B Johnson wrote: I think you need to move the call to setNeedsDisplayInRect to inside the first branch of the if statement, i.e. instead of: do this: what's happening in the first case is that after setting &amp;quot;contents&amp;quot; to nil to remove the image you call -setNeedsDisplay, which before the layer is committed to the screen will arrange for the &amp;quot;contents&amp;quot; to be set back to a non-nil image value, which since the delegate is nil will be a transparent (solid black if &amp;quot;opaque&amp;quot; = YES) image. the second version will only mark the layer to redraw its contents when you've set a non-nil delegate and have something to draw.</body>
  </mail>
  <mail>
    <header>Re: using Quartz Debug to help understand/optimize CALayer drawing</header>
    <body>On Oct 9, 2008, at 6:18 PM, John Harper wrote: Okay, that all makes sense. I think this is the part that is confusing me.  Here's what my NSOperation looks like.  It has a &amp;quot;layers&amp;quot; array of NSDictionaries, that have a layer and optionally have a delegate - if the delegate is set, then we want to have the layer draw its image, if it's nil, we want it to revert back to its backgroundColor. But if this is what my NSOperation subclass looks like, I get black rectangles when I go through the delegate == nil path in the code: [CATransaction setValue:[NSNumber numberWithInt:1] layer.backgroundColor = CGColorCreateGenericRGB(layer.storyboard.red, layer.storyboard.green, [CATransaction setValue:[NSNumber numberWithInt:1] If I change the inner part to look like this, though: layer.backgroundColor = CGColorCreateGenericRGB(layer.storyboard.red, layer.storyboard.green, It works as expected (the layer reverts back to being drawn with its background color, not black).</body>
  </mail>
  <mail>
    <header>Re: using Quartz Debug to help understand/optimize CALayer drawing</header>
    <body>On Oct 9, 2008, at 6:03 PM, Michael B Johnson wrote: Or maybe I don't understand exactly how your code is working‚Ä¶ 1. you call -setNeedsDisplay or -setNeedsDisplayInRect. This sets a &amp;quot;display&amp;quot; bit in the layer that tells us to call the -display method before the end of the current runloop iteration. 2. -display is called, it ensures the layer backing is allocated and calls -drawInContext: with a CGContext pointing at the backing. 3. the default -drawInContext: method calls your delegate's - drawLayer:inContext: if defined 4. after -drawInContext: returns -display calls -setContents: to associate the backing with the layer and trigger any implicit animations (and to unmark the &amp;quot;display&amp;quot; bit in the layer until you next call -setNeedsDisplay.) One important thing here is that -setNeedsDisplay is not the trigger to make the layer recomposite itself to the screen, setting the individual layer properties (i.e. backgroundColor, contents, ‚Ä¶) takes care of that.</body>
  </mail>
  <mail>
    <header>Re: using Quartz Debug to help understand/optimize CALayer drawing</header>
    <body>On Oct 9, 2008, at 5:25 PM, John Harper wrote: Yes I am.  I thought I needed to. Hmm.  this doesn't work for me.  If I don't call setNeedsDisplay, my layer doesn't get redrawn.</body>
  </mail>
  <mail>
    <header>Re: Is CGColorSpaceCreateWithName supported?</header>
    <body>File an enhancement request as a duplicate of #7685385 if you'd like color management support in iOS. That's the one mine was marked as a duplicate for.</body>
  </mail>
  <mail>
    <header>Re: Is CGColorSpaceCreateWithName supported?</header>
    <body>Yes, dead API. In fact, color management as a whole is pretty much unimplemented on iOS. Sandy</body>
  </mail>
  <mail>
    <header>Is CGColorSpaceCreateWithName supported?</header>
    <body>iOS 5.0 SDK, Xcode 4.2 I'd like to use CGColorSpaceCreateWithName(), which I assume lets me choose among a repertoire of valid spaces, but I can't find the eligible names. The documentation directs me to some article or section called &amp;quot;Color Space Names,&amp;quot; but there's no link, and searching the docset turns up nothing. I've looked in the header file, and there are no string externs in it. The availability macros say CGColorSpaceCreateWithName has been in iOS since 2.0, and has not been deprecated, but it's of no use without the keys. Dead API? ‚Äî F</body>
  </mail>
  <mail>
    <header>Re: CGEventTapCreate [and CFRunLoop]</header>
    <body>Thanks for the response ... I did figure that out and more and have the tap working great. -db</body>
  </mail>
  <mail>
    <header>Re: CGEventTapCreate</header>
    <body>You shouldn't be calling CFRunLoopRun.  A Cocoa app's main thread already has a run loop that is running, courtesy of NSApplicationMain. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: CGEventTapCreate [and CFRunLoop]</header>
    <body>If you're inside an application, which you presumably are since you have an IBAction method there, then the application is already invoking CFRunLoopRun (or equivalent); you don't usually want to recursively invoke it. If you simply return from -createTap:, the application's event loop will continue and will dispatch events to your cgEventCallback() as well as all the other event-handling routines necessary for the app to function. In general, you'll only need to call CFRunLoopRun() yourself if you've written a command-line tool or something similar that doesn't already have a run loop. There's probably a highlevel introduction to CF/NSRunLoops in the Conceptual Documentation part of the developer docs somewhere which will give you all the details you need.</body>
  </mail>
  <mail>
    <header>Re: Capture Pixels on any Display</header>
    <body>CGWindow APIs (10.5+) or CGDisplayCreateImageForRect() (10.6+). The latter are preferred if you can require 10.6 or later. See the SonOfGrab sample for an example of the former. -- David Duncan</body>
  </mail>
  <mail>
    <header>Capture Pixels on any Display</header>
    <body>Here is what I am trying to do. My users may scan a thread color say Robinson Anton Poly 40, by wrapping the thread around some cardboard stock and then placing this on the scanner bed. As you might imagine there will  variations in the color due to the thread being wrapped ‚Ä¶ peaks , valleys, reflections, etc. Now we want to bring that thread color into our embroidery program.  The scanned image may be on any connected display. I want to capture a rect centered on the point where the user clicked so I can get a best fit for the color. This is easy to do on Windows using the SetCapture() functions which directs all events to my app. How do I get this accomplished on OSX? -koko</body>
  </mail>
  <mail>
    <header>[ANN] PhotoScrollerNetwork: networked and blazingly fast PhotoScroller</header>
    <body>Apple's PhotoScroller sample code for iOS looks to many as a perfect starting place to display a scrolling list of photos that can each be zoomed significantly. It uses a CATiledLayer as a backing store so it does not have to load whole images into memory. That said, after you see the three pretty jpeg images in the project, you lift up the covers and find approx 800 pre-tiles png files - its the pretiling of the jpegs that makes this project work. So, I've taken that project and greatly enhanced it: This project has two targets and offers several technologies: 1) Tiling The code takes a jpeg image, decompresses it into a mapped file, rearranges the bits for efficient tiling, then shrinks it by half, recreating a smaller files etc etc. You can specify the number of levels. This would be useful for camera images or other images in your bundle. When the view needs a tile the code maps just the bits needed and does one memcpy. 2) Concurrent Downloading Using concurrent NSOperations (based on another of my open source projects), it downloads the same three image from DropBox, then tiles as above. 3) By incorporating the open source libjpeg-turbo library (which uses the NEON SIMD unit in the arm chips), it can incrementally decode images as they download, so that when each image is completely received it's already decoded. The primary view lets you select on-disk or network, and the turbo target lets you choose the decoding method: CGContextDrawImage, libjpeg-turbo (instead of CGContextDrawImage), and incremental (which uses libjpeg-turbo). The incremental decoding lops about a second off the final image processing on the three sample images (when downloaded and decoded concurrently). It would be even more impressive if you only download and decode one image. See the project page README for more detailed information. LICENSE is BSD, no attribution required.</body>
  </mail>
  <mail>
    <header>Re: CGBitMapContext: high performance pixel layout is ARGB, BGRA,	or other on iPhone/iPad when vending CGImageRefs</header>
    <body>I can't say anything about an official recommendation, but I've found ARGB in 32-bit host byte order to easily be the most performant format for bitmap contexts. Profiling it in the Core Animation instrument also reveals that it doesn't have to do copying (including any channel swapping) before displaying. Justin Spahr-Summers</body>
  </mail>
  <mail>
    <header>CGBitMapContext: high performance pixel layout is ARGB, BGRA,	or other on iPhone/iPad when vending CGImageRefs</header>
    <body>Tried to jam all the keywords into the subject line as I could not find the answer to this in the archives, nor googling, even though I know I recently saw it somewhere. I'm talking about vending big images so this could potential make a big difference. I could test on my iPhone 4 running iOS 5, but would not know for sure if what I measure is actually representative of the community of devices or not, nor whether I should use one format going forward because &amp;quot;its the best way to do it&amp;quot; (ie directive from Apple). So, what IS the best format to use when vending CGImageRefs destined to be shown in a view? Thanks! David</body>
  </mail>
  <mail>
    <header>[WITH DTS] CGImageSourceCreateIncremental: is it possible to have it	continually render to its cached image?</header>
    <body>Well, I just burned a DTS incident on this. As far as I can see, it does not start caching images until it gets the final chunk (and the completed flag set to yes). I'm not hopeful there is a solution (outside of building and using libjpeg or equiv directly). I tried everything under the sun so to speak, and cannot find anything that even remotely speeds it up. In the end, my users will just have to wait a few extra seconds while the image decodes into memory. If they use 3G they'll never even notice the delay! David</body>
  </mail>
  <mail>
    <header>CGImageSourceCreateIncremental: is it possible to have it continually	render to its cached image?</header>
    <body>I'm downloading some huge files to an iOS device from the internet, in JPEG format. What happens now is I get all the data, decode it into memory, then show part of it. It takes several seconds to download (where the iOS device is idling), then when the whole image arrives, I draw it into a context, which takes another 1.5 seconds on a iPhone 4. What I'd dearly love to do is use a CGImageSourceCreateIncremental imageSource, and somehow prod it into incrementally updating the image, so when the final chunk arrives the final draw-into-context is simply a copy operation. Nothing I do seems to improve the performance. I've tried asking the incremental source for an image, then drawing those into a context, but the timing does not improve. Supposidly caching is the default behavior, but to be sure I am setting it as well. The only solution I've come up with is to pretile my big image as a series of jpegs, then put them into simple archive (binary plist?), so as I get a full tile I can decompress that and load it into memory, so that when the final tile arrives the time between the last incoming data chunk and showing the image is a few milliseconds. The archive would a simple header with a count and the sizes of each image then the image data. Its killing me that I got this powerful device just idling until I get the whole image!!! [I looked at PNGs - the files are just SO much larger even crushed. JPEG2000 tiles take tens of seconds to decompress.] David</body>
  </mail>
  <mail>
    <header>Serialize processed image for web service</header>
    <body>Hey guys, Im working on a small cropping application that once the user has cropped the selected image, I need to serialize it somehow and then send it to my java servlet. I've done a lot of googling, and reading of docs, but I can seem to work out how I can serialize the image in a standard way? Any help / advice would be great Thanks Tim</body>
  </mail>
  <mail>
    <header>Re: Flicker when several subviews need to be refreshed</header>
    <body>Strangely enough, although I subscribed to this list about two days ago, I did not receive one email yet... (I checked, Mail delivery is enabled though.) I could read the answer of Shawn Erickson though thanks to the list archive when that one appeared, and it was quite enlightening. The answer lied in my -drawRect method of the PHGraphView object indeed, as I finished the drawing with a CGContextFlush, instead of using CGContextSynchronize... Pierre-Henri Jondot</body>
  </mail>
  <mail>
    <header>Re: Drawing size limits</header>
    <body>Hi Shawn, I had encounterd the same issue. The picture can be created correctly but failed to draw in a view if picture width is greater than 32767 pixels. I wrote a test program to check this issue. Attached document is written in Japanese but just try compile and run it. if you specify more than 32767 pixels to piicture width, you will see white space at the both side. My application limits pixel width to 32767 to avoid this issue. Yoshiaki Katayanagi</body>
  </mail>
  <mail>
    <header>Re: Drawing size limits</header>
    <body>//Drawing into the context takes place here, then The fact that the image always ends at precisely 32768 pixels width makes me think this is not a memory issue but a limit imposed by something somewhere measuring with a 16-bit word or a Fixed data type. Thanks,&amp;nbsp;Jeff Which works fine until the image width reaches 32768 pixels, when drawing</body>
  </mail>
  <mail>
    <header>Re: Flicker when several subviews need to be refreshed</header>
    <body>On Fri, Jan 30, 2009 at 12:04 AM, Pierre-Henri Jondot ...snip... If you use setNeedsDisplay: to dirty a view then at the end of the current event cycle all dirty views will be asked to draw themselves in a back to front order based on the super-view / sub-view containment hierarchy for a window (and as of 10.5 sibling views are also drawn based on their ordering relative to their siblings) . Normally only once all of this drawing has taken place will the window buffer be flushed to the quartz compositor and become visible to the user. That is unless some how you are causing immediate drawing to take place. It seems more likely to me that you are causing the flicker yourself by some incorrect drawing taking place on secondary drawing passes. Can you explain what you meant by &amp;quot;although the views have already been blanked before calling the -drawRect&amp;quot;? Can you better explain what &amp;quot;Quartz 2d primitives&amp;quot; you are using and how you are using them? Can you outline what you are doing in a typical drawRect: call? -Shawn</body>
  </mail>
  <mail>
    <header>compatibility check in Quartz Composer</header>
    <body>In Quartz Composer, if I check &amp;quot;Display 10.4 Compatibility Information&amp;quot;, then I see the &amp;quot;stop&amp;quot; icon on every Core Image Filter patch, including an unmodified one.  Why?  I know the name was different in Tiger (it was called a Core Image Kernel patch) but that shouldn't make it incompatible. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: HSL and CoreImage</header>
    <body>For a small source area: I&amp;#8217;d create a 1 pixel temp image. On the first filter, read the RGB values from the source image, scale it by the number of pixels in the area, read the &amp;nbsp;&amp;nbsp;temp image, add the values and write back to the temp image. When you&amp;#8217;re done, the temp image will have the average values. On the second pass, read the temp image values and use to process the source image, in what ever way. For large images you&amp;#8217;ll end up with a lot of rounding error, maybe accurate enough ... You could also try using a Greg VanSickle wrote: Hmm... you mean like, write a filter that averages blocks of 4 pixels, reducing each dimension by a factor of 2, and then run the image through it repeatedly until it's down to 1x1? -- &amp;nbsp;&amp;nbsp;&amp;nbsp;James W. Walker, Innoventive Software LLC &amp;nbsp;_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;(email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Drawing size limits</header>
    <body>Can post a little more information about how (code for example) you are creating the bitmap context? Also explain what you mean by 32,768 pixels... Is that a width pixel count? height count? ...or width x height count? If you are talking about a 32k by 32k image with 4 bytes a pixel the you are beyond what you could allocate in a 32-bit virtual address space (bitmap would take up 4,294,967,296 bytes, aka 4 GiB). -Shawn</body>
  </mail>
  <mail>
    <header>Re: HSL and CoreImage</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: HSL and CoreImage</header>
    <body>Hmm... you mean like, write a filter that averages blocks of 4 pixels, reducing each dimension by a factor of 2, and then run the image through it repeatedly until it's down to 1x1? -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: HSL and CoreImage</header>
    <body>Andy Finnell wrote: Thanks, that helps. Here's another question about CoreImage: &amp;nbsp;One of the algorithms I'm dealing with takes an average of all the RGB values in an image. &amp;nbsp;That's exactly what the CIAreaAverage filter does... except that CIAreaAverage requires 10.5, and I need to support 10.4. &amp;nbsp;I don't see how to do that simply in a kernel function, since there would be loops with non-constant limits... is there a less obvious way? -- &amp;nbsp;&amp;nbsp;&amp;nbsp;James W. Walker, Innoventive Software LLC &amp;nbsp;_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;(email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Drawing size limits</header>
    <body>Which works fine until the image width reaches 32768 pixels, when drawing ends. Is there a limit on the size of a bitmap? When setting up the bitmap &amp;nbsp;I set the rowbyte arguments like this:</body>
  </mail>
  <mail>
    <header>Re: HSL and CoreImage</header>
    <body>Thanks, that helps. Here's another question about CoreImage:  One of the algorithms I'm dealing with takes an average of all the RGB values in an image.  That's exactly what the CIAreaAverage filter does... except that CIAreaAverage requires 10.5, and I need to support 10.4.  I don't see how to do that simply in a kernel function, since there would be loops with non-constant limits... is there a less obvious way? -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: HSL and CoreImage</header>
    <body>Check out the &amp;quot;HSL Kernel&amp;quot; sample composition, it's using a custom filter. /Developer/Examples/Quartz Composer/Compositions/Core Image Filters/ HSL Kernel.qtz (Pasting) vec4 rgb2hsl(vec4 color) //Compute min and max component values //Make sure MAX &amp;gt; MIN to avoid division by zero later //Compute luminosity //Compute saturation float s = (l &amp;lt; 0.5 ? (MAX - MIN) / (MIN + MAX) : (MAX - MIN) / (2.0 - //Compute hue float h = (MAX == color.r ? (color.g - color.b) / (MAX - MIN) : (MAX == color.g ? 2.0 + (color.b - color.r) / (MAX - MIN) : 4.0 + (color.r</body>
  </mail>
  <mail>
    <header>Re: HSL and CoreImage</header>
    <body>You can't do if statements, but you can use the ternary operator, which is what I used. Maybe the sample code will give you some ideas. Hope this helps, Andy On Jan 30, 2009, at 1:45 PM, James Walker wrote:</body>
  </mail>
  <mail>
    <header>HSL and CoreImage</header>
    <body>I am contemplating porting some image processing functions to the Mac, and wondering if CoreImage is up to it.  One of the functions involved basically converts each pixel from RGB to HSL, adds input parameters to each of the H, S, L values, and then converts back to RGB.  I don't see any single CoreImage filter that can do such a thing.  It looks like I could adjust the hue alone with CIHueAdjust, and the saturation alone with CIColorControls, but I don't know about the luminance.  The formulas I've seen for converting between RGB and HSL involve a bunch of ifs, which as I understand it mean that they could not be used in a CI kernel function.  Any ideas? -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Flicker when several subviews need to be refreshed</header>
    <body>I developed a small framework for scientific 2d graphing (named PHGraph : One user informed me of one problem : he placed several PHGraphView which needs to be refreshed in real-time. Only one is refreshed smoothly (always the same, that is, the first such view created from the nib), the others &amp;lt;&amp;lt;flicker&amp;gt;&amp;gt;. Indeed I could reproduce this problem, with only two such views : it appears both views are blanked at the same time, first being repainted immediately, the second with a small delay, so the display of the second view is a bit dimmed compared to the first. I tried several things : by using -setNeedsDisplay:YES method to each subview : flicker by using -setNeedsDisplay:YES method for the superview : kind of worse by sending -display to each subview, no flicker, but the views won't refresh at the same time. What I don't get is that the drawing should be very fast. I even reduced dramatically the sizes of the views to exclude a possibility that the flickering could be reduced with optimization of the drawing code (even with quite small views, the flickering still appears) So it appears (this is my best guess so far) that there is some delay between the calls of the -drawRect methods of each view, although the views have already been blanked before calling the -drawRect method, which induces the flickering one can experience. With -display, my guess would be that, according for example to my LCD (using DVI cable, but an analog SVGA cable didn't make much of a difference as concerns flickering) showing a refresh rate of 60Hz, only one view is refreshed every 1/60second, so if the window has 5 views, it takes 1/12 second to refresh all of them, which is a big price to pay for a non-flickering display, and of course the 5 windows never are perfectly synchronized... Pierre-Henri Jondot</body>
  </mail>
  <mail>
    <header>Re: ImageIO BMP conversion issue</header>
    <body>I'm only speculating here as I haven't done anything with PDFs. I believe it is because PDF and BMP have inverted coordinate systems. PDF puts y=0 at the bottom and increases upwards like a Math textbook or OpenGL. BMP puts y=0 at the top and increases downwards. (FYI, inverted images are a typical issue for loading images into OpenGL.) My guess is your inversion code is not aligned properly. As I understand it (and I welcome a better explanation for my own understanding), image formats may pad rows with extra bytes for word alignment for performance. This means the number of pixel bytes you have in an image may be less than the actual number of bytes that fill a row. So if your code doesn't account for this, then everything gets distorted. -Eric</body>
  </mail>
  <mail>
    <header>Change the properties of a CGContextRef myBitmapContex</header>
    <body>CGContextRef  myBitmapContext = CGBitmapContextCreate ( bitmapData, pixelsWide, pixelsHigh, bitsPerComponent, bitmapBytesPerRow, colorSpace, when saved as a TIFF creates the kCGImagePropertyTIFFResolutionUnit = 72 dpi. will create the CGContext at pixelsWide and pixelsHigh equal to the pica width and height of the PDF. (It is unfortunately confusing that both pixel per inch and picas per inch is 72.) The image within the PDF has a width and height in pixels greatly larger than the kCGPDFMediaBox expressed in picas.  In this case pixel width/ (mediaBox width / 72) provides the dpi. Let's pretend that all PDFs have bitmaps with a width = 300 * (mediaBox width / 72). The result is the CGContextRef pixelsWide property -- and 300 is the dpi. So I know the bitmap size in pixels, its dpi, and its size in inches (pixelsWide /dpi and pixelsHigh/dpi) how do I use these properties to save a TIFF or a PDF at 300 dpi?</body>
  </mail>
  <mail>
    <header>quartz and CMYK profile...</header>
    <body>I am trying to draw an CIimage in a context created with a CMYK colorspace and get these : ImagePrinter: CGImageCreate: invalid image bits/pixel: 32. ImagePrinter: CGImageCreate: invalid image bits/pixel: 64. ImagePrinter: CGImageCreate: invalid image bits/pixel: 128. CGImageRef finalCgImage = [finalCIImageContext createCGImage:result ImagePrinter: CGBitmapContextCreate: unsupported parameter combination: kCGImageAlphaPremultipliedLast. any ideas of what i am doing wrong ? thanks valery</body>
  </mail>
  <mail>
    <header>Problems with [CIImage imageWithCVImageBuffer:...]</header>
    <body>What I want to do is take a video frame laid out as 24-bits per pixel (RGB) and create a CVPixelBufferRef that will later be passed to CIImage: CVReturn cvRet = CVPixelBufferCreateWithBytes ( NULL, pCodecCtx-&amp;gt;width, pCodecCtx-&amp;gt;height, k24RGBPixelFormat, nbuf, pCodecCtx-&amp;gt;width*3, releaseCallback, NULL, (CFDictionaryRef)dict, &amp;amp;pixelBufferOut dict = [NSDictionary dictionaryWithObjectsAndKeys: [NSNumber numberWithInt:k24RGBPixelFormat], kCVPixelBufferPixelFormatTypeKey, [NSNumber numberWithBool:YES], kCVPixelBufferCGBitmapContextCompatibilityKey, So, using the PixelBuffer, I wrap it in a CIIimage (for later rendering in an OpenGL context: What happens is that when the image is rendered, its 3/4th the width it should be (among other issues), so I NSLoged the image using &amp;quot;%@&amp;quot;: So, I do more tests. Right now, I switched from real video input to using random() on the image buffer, so I can see the noise on the left 3/4 of the view, and the right 1/4 is black. CVReturn cvRet = CVPixelBufferCreateWithBytes ( NULL, pCodecCtx-&amp;gt;width, pCodecCtx-&amp;gt;height, k32ARGBPixelFormat, // NEW - ARGB nbuf, pCodecCtx-&amp;gt;width*4, // NEW switched to 4 releaseCallback, NULL, (CFDictionaryRef)dict, &amp;amp;pixelBufferOut So, why did CIImage accept the CVPixelBufferRef if given a format it cannot handle? Why not nil? Is this a bug or am I just missing something? David</body>
  </mail>
  <mail>
    <header>Re: CALayer and antialiasing problem</header>
    <body>I think this is something I can work with. I would need to resize the image to it's target CALayer dimensions, before I pass it. I am going to try this!</body>
  </mail>
  <mail>
    <header>Re: CALayer and antialiasing problem</header>
    <body>I hadn't noticed that he was doing that, but this is true - Core Animation uses OpenGL to do its downsampling, so your results will be graphics card dependent. Right, no LCD/subpixel antialiasing. Some people like it, some people go to great lengths to get it :). -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: CALayer and antialiasing problem</header>
    <body>I had a chance to look at Rob's source, and noticed that the CGImageRef he was passing to the CALayer was the size of the original image, letting CALayer scale it down to final presentation size. After changing that to create the bitmap at the size of the CALayer--that is, scaling it in CoreGraphics instead of on the graphics card--it looked like it should. So it seems the problem is that CALayer's downsampling isn't as good as that in CoreGraphics. Correct me if I'm wrong, but I don't think it's doing any kind of subpixel antialiasing--there's none of those horrible color fringes that drive me absolutely batty. :)</body>
  </mail>
  <mail>
    <header>multi color profile image printing</header>
    <body>I've not been able to create a CGImage from a CGImageSource that is multi-color ( 5/6/7/8 CLR ). Si this not possible in quartz? AC</body>
  </mail>
  <mail>
    <header>Re: CALayer and antialiasing problem</header>
    <body>I would lookup information on Blur filters (Gaussian is a popular one, but there are others). Core Image would make this easy to do as well if you can get the effect you desire. If you are doing this for (eventual) export, you could just render in the higher quality for export. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>CGContextShowTextAtPoint Matrix Disobedience</header>
    <body>Has anyone else noticed that translation factors in the text matrix aren't observed by this function? Scale factors and rotation work but whatever is set for tx and ty behaves as if 0. What I'm talking about can be demonstrated by altering the sample JustDraw (). can be changed to, for instance: The reflection-scaling built into the demo works as expected, but the translation has no effect. This behavior seems to be the same on iPhone or OS X. I don't know if this is documented somewhere to be functioning as intended, but it sure seems wrong.</body>
  </mail>
  <mail>
    <header>Flipped views and layers not happy together</header>
    <body>NSView has some magic for dealing with -isFlipped when layer backing is on. Sadly, this magic isn't sufficient and if you put extra layers (other than those for the subviews managed by the view), the other layers get composited incorrectly. Instead, you have to jump through a bunch of hoops to set up a flipped layer inside the view's layer. I've tried various approaches to this problem, but haven't found anything else that works well. Really, it seems like NSView would set up the flipped rendering state when drawing and the layers would just follow along -- it's doing something of the sort for its subview layers, but not for all layers. alternate approach that others have had success with. I've logged this bug as Radar #6503347, as well.</body>
  </mail>
  <mail>
    <header>ImageIO BMP conversion issue</header>
    <body>Hello All We are using &amp;#39;ImageIO&amp;#39; framework APIs to convert PDF file (has multiple pages) into BMP file. The &amp;#39;Xerox Scanner Driver - TWAIN DS(Data Source)&amp;#39; was using QuickTime APIs earlier for this purpose. But the API&amp;nbsp;&amp;#39;GetGraphicsImporterForFileWithFlags&amp;#39; supports only for multipage TIFF&amp;nbsp;and not for multipage PDF. For a multipage PDF the API &amp;#39;GraphicsImportGetImageCount&amp;#39; just returns&amp;nbsp;the page count as 1. To overcome this we introduced &amp;#39;ImageIO&amp;#39; framework APIs usage. Here we&amp;nbsp;are able to convert the PDF pages to BMP images as expected. But we are having a strange problem here, after importing the BMP in&amp;nbsp;this case, the image imported to application gets vertically flipped. (Note: this happens even for other format images say conversion from JPG to BNP using ImageIO) We have no clue why the BMP data gets imported vertically flipped. We&amp;nbsp;assume the &amp;#39;ImageIO&amp;#39; approach saves the image with some settings (may be metadata) and we need to take care while reading raw data before import&amp;nbsp;from DS. ---- Code snippet to convert PDF to BMP using ImageIO ---- // create an image destination (ImageIO&amp;#39;s way of saying we want to save&amp;nbsp;to a bmp file format) // finalize: this writes image data and properties to the image&amp;nbsp;destination bStatus =CGImageDestinationFinalize(imageDestRef) ------- Are we missing anything here? Kindly give some inputs to solve this. One approach we used to solve this is, flip vertically the raw BMP data&amp;nbsp;before sending from DS to Client. This works well for the pages other than A4 and ISO A5. For these dimension images the import operation ends with distorted image. We are&amp;nbsp;flipping the raw data pixel by pixel. Questions are: 1. Why imported BMP (saved using ImageIO) getting flipped? 2. Why flipping the raw data for a particular image dimension ends with&amp;nbsp;distorted image? (Note: We rendered a NSImage (A4 dimension 595 x 842 pixels) &amp;nbsp;with same logic and it works as expected.) Thanks in advance. Regards Lokesh</body>
  </mail>
  <mail>
    <header>Re: Sequential CGDataProvider call backs - access to cocoa variables</header>
    <body>The Data Provider API provide this as the first parameter (when you call CGDataProviderCreateSequential you pass it an info pointer along with the callbacks - this pointer is passed to each of the callbacks as the first parameter, see the docs for specific information). I would just recommend using C callbacks to call into the class (whose instance pointer is passed as the info parameter). -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Core Image CIImage sampler dimension limitations?</header>
    <body>Oh man Brian, that's not a good sign. Poking around it a bit I found this to be interesting: if i change the sampleIndex check to 0 instead of 1 I get no errors, but the same pixel cropping. I googled around for this error too and I see no discussion about working around this or what it even really means, just a lot of speculation.  Can someone in Core Image development discuss what causes this CoreImage: ROI is not tilable error conceptually and what can be done to work around it? This is defiantly blocking me from going forward with my research and I don't want to have to abandon the Core Image platform just because of what seems to be a simple bug. Thanks, exception is thrown or error returned (just a &amp;quot;CoreImage: ROI is not tilable&amp;quot; message sent to the console), you can't report the problem to users or try to recover from it. -- - Jim</body>
  </mail>
  <mail>
    <header>Re: Core Image CIImage sampler dimension limitations?</header>
    <body>Oh no, the dreaded &amp;quot;ROI is not tilable&amp;quot;! Be sure to let the list know if you discover anything useful. I posted a question regarding this issue in May 2007, and Christopher Ashworth had a similar question/issue in March of 2008, but neither of us received informative or useful replies. My conclusion/assumption is that there is some mystery limit to how many pixels CoreImage can handle. I'm not really sure what the limit is, but avoiding it gets very complicated if you allow users (as our application does) to chain multiple filters together. The output of some filters have infinite extent, while the input to others increases the ROI by 25% or 50% on either side in both dimensions. Since no exception is thrown or error returned (just a &amp;quot;CoreImage: ROI is not tilable&amp;quot; message sent to the console), you can't report the problem to users or try to recover from it. Cheers, Brian SmileOnMyMac, LLC</body>
  </mail>
  <mail>
    <header>Incorrect colors when extracting images from PDF streams</header>
    <body>When parsing a PDF content stream, I need to extract images. For this I wrote the code below. It seems for the most part to be working correctly, but with one file I find that the colors of several images are not correct. The images are monochrome and should range from white to black, but the but it is rendered with the white point being sand- colored. I cannot find anything in my code that would cause this to happen. I'm not even sure that the problem is with the code quoted below, but would seem the most likely culprit. I am trying to narrow down what could be causing this effect.  My hunch would be that I may be extracting the color spaces incorrectly, but I can't find anything wrong with the code when I double check  it against the PDF reference. All the values I see in the debugger would appear to be correct. The color spaces are either ICC based or Indexed, where the indexed color space is also ICC based, and effectively the color space is always CMYK. Does my description of the incorrect coloring set off an alarm with anyone? Any hints as where to look for leads? By the way, the code is written for garbage collection, and I am quoting it (almost) in full. The code should be useful to others trying to extract images from PDFs, as it fleshes out significantly from what is provided by the Voyeur example that Apple provides, and it works for most images I have tried to date. CGColorSpaceRef colorSpaceFromPDFArray(CGPDFArrayRef colorSpaceArray) // First obtain the alternate color space if present if (CGPDFDictionaryGetName(dict, &amp;quot;Alternate&amp;quot;, alternateColorSpace = alternateColorSpace = alternateColorSpace = // Obtain the preferential color space CFDataRef			colorSpaceDataPtr = CGPDFStreamCopyData(stream, CGDataProviderRef	profile = retrieved = CGPDFDictionaryGetInteger(dict, &amp;quot;N&amp;quot;, // Deduce an alternate color space if we don't have one already case 1: alternateColorSpace = case 3: alternateColorSpace = case 4: alternateColorSpace = default: Debugger(); // TODO: Need to test a file where a Range entry is present. cgColorSpace = CGColorSpaceCreateICCBased(numberOfComponents, // Since we have a preferential color space, we no longer need the hang on to the alternate color space // // // TODO: Raise some error state? cgColorSpace = CGColorSpaceCreateIndexed(baseSpace, highValue, CGFloat *decodeValuesFromImageDictionary(CGPDFDictionaryRef dict, CGColorSpaceRef cgColorSpace, NSInteger bitsPerComponent) case kCGColorSpaceModelMonochrome: case kCGColorSpaceModelRGB: case kCGColorSpaceModelCMYK: case kCGColorSpaceModelLab: // ???? case kCGColorSpaceModelDeviceN: case kCGColorSpaceModelIndexed: default: static NSImage * getImage(CGPDFStreamRef myStream, NSString *nodeName) const char				*name = NULL, *colorSpaceName = NULL, if (myStream == NULL) // obtain the basic image information if (!CGPDFDictionaryGetName(dict, &amp;quot;Subtype&amp;quot;, &amp;amp;name)) if (strcmp(name, &amp;quot;Image&amp;quot;) != 0) if (!CGPDFDictionaryGetInteger(dict, &amp;quot;Width&amp;quot;, &amp;amp;width)) if (!CGPDFDictionaryGetInteger(dict, &amp;quot;Height&amp;quot;, &amp;amp;height)) if (!CGPDFDictionaryGetInteger(dict, &amp;quot;BitsPerComponent&amp;quot;, &amp;amp;bps)) if (!CGPDFDictionaryGetBoolean(dict, &amp;quot;Interpolate&amp;quot;, &amp;amp;interpolation)) if (!CGPDFDictionaryGetName(dict, &amp;quot;Intent&amp;quot;, &amp;amp;renderingIntentName)) else still one we can infer from bps decodeValues = decodeValuesFromImageDictionary(dict, cgColorSpace, // pdf image row lengths are padded to byte-alignment if (rowBits % 8 != 0) if (format == CGPDFDataFormatRaw) sourceImage = CGImageCreate(width, height, bps, bps * spp, rowBytes, cgColorSpace, 0, dataProvider, decodeValues, interpolation, if (format == CGPDFDataFormatJPEGEncoded)	// JPEG data requires a CGImage; AppKit can't decode it sourceImage = CGImageCreateWithJPEGDataProvider(dataProvider, decodeValues, interpolation, // note that we could have handled JPEG with ImageIO as well else if (format == CGPDFDataFormatJPEG2000)	// JPEG2000 requires ImageIO CFDictionaryRef dictionary = CFDictionaryCreate(NULL, NULL, NULL, CGImageSourceRef cgImageSource = cgImage = CGImageSourceCreateImageAtIndex(cgImageSource, 0, else										// some format we don't know about or an error in the PDF // &amp;quot;convert&amp;quot; the CGImage to an NSImage CGContextDrawImage([[NSGraphicsContext currentContext] graphicsPort], ----------------------------------------------------------- And you would accept the seasons of your heart, even as you have always accepted the seasons that pass over your field. --Kahlil Gibran -----------------------------------------------------------</body>
  </mail>
  <mail>
    <header>Re: Core Image CIImage sampler dimension limitations?</header>
    <body>So after re-reading up on ROI, I found a parallel between what i am doing and their &amp;quot;environment map&amp;quot; example. (listing 3-11) In that example they specify 3 samplers, one of which is an environment map that does not coincide with the domain of definition of the output.  So when i attempted to emulate that style I get the following error streaming into the console when applying the kernel: CoreImage: ROI is not tilable: APPLY volumeComposite DOD [0,0 320x240] ROI [0,0 320x32] RGBA_F APPLY matrix_nobias DOD [0,0 320x240] opaque RGBA_14 APPLY csync_curve0 DOD [0,0 320x240] opaque RGBA_14 IMAGE CIImage:0xdcf00 DOD [0,0 320x240] opaque BGRA_8 IMAGE CIImage:0x1506b800 DOD [0,0 320x24000] ROI [0,0 320x24000] RGBA_F the ROI function looks like this: - (CGRect)regionOf: (int)samplerIndex  destRect: (CGRect)destination userInfo: (NSNumber*)height The first sampler to the function is the image that shares the same scale as the output DOD, the second is the large  one that I need access to in the sampler and is getting cut off. In my kernel i only need access to the columns of pixels below each destination pixel, so i return the same width as the destination but the entire height. Is this a sign that the image is just to big to be fed into a single kernel call, or is something messed up with the way I am calculating this ROI? Thanks so much! -- - Jim</body>
  </mail>
  <mail>
    <header>Re: Core Image CIImage sampler dimension limitations?</header>
    <body>Just as an experiment, I'd be inclined to write a simple little ROI function for the input image that just specified the whole input image. My highly simplistic understanding of ROI functions is that if the kernel has anything more complex than a one-to-one pixel mapping between input and output, you probably need one. Specifying the whole image is perhaps not very efficient, but it would tell you if that was the problem.</body>
  </mail>
  <mail>
    <header>Re: Core Image CIImage sampler dimension limitations?</header>
    <body>Hey Jeff &amp;amp; Sandy, perhaps, yet even a single 640x480 has more than 32768 pixels... but it may the same type of problem but your response did motivate me to find more specific numbers. working now with 320x240000 images, If I sample anywhere beyond 2760.0 in the Y axis i start getting blank rows. so i can sample up to 320x2760.  This just seems odd, as CoreImage should be able to handle very large images, it must be something wrong with my filter. Also, i checked the sampleExtent function and it is the value that it is supposed to be (240000), so it isn't like core image is cropping it. @sandy: I am not specifying a region of interest function in this case. I thought it wasn't needed because i'm not doing anything funky with coordinates....  But this is where my knowledge of implementing core image is a bit shaky.  I have two samplers, the first is exactly the size of the output image (say 640x480) and the second has the same width but the height is very large.  The output always needs to have access all pixels of the large image, but will only create an image the size of the first image (640x480). What would a proper ROI function look like for this? If i could get this working I would be so pumped, seems like it must be something simple. thanks! -- - Jim</body>
  </mail>
  <mail>
    <header>Re: Core Image CIImage sampler dimension limitations?</header>
    <body>Where there is a known bug that will give you blank output with large images is [CIContext drawImage:]</body>
  </mail>
  <mail>
    <header>Re: Core Image CIImage sampler dimension limitations?</header>
    <body>I wonder if this relates to the bug discovered earlier that limits the sizes of offscreen images in the same way that QuickDraw used to. Another guy and myself both reported that one - seems like somewhere in the Quartz imaging there's a 16-bit integer counting pixels. In our case, everything stops at 32768 pixels. Your number is different, but there might be some connection? I am writing an Image Unit that needs to work on images of very large dimensions (sizes of over 480,000 pixels in height, but small width of 640).  Although the input is very big, the output definition is still just a 640x480 image. I have written an image unit to process these images, but if I ever sample above around 3600 &amp;gt; the samples just return black.  If I convert the large CIImage to a bitmap and write it to file the entire thing is intact and i can see all pixels, so the image is definitely there. It must have something to do with the process of giving the image to the filter kernel for sampling. the kernel code to replicate the problem looks like this: where the src is a CIImage that has a height of over 3600.  This image will return blank, It seems like core image should be able to handle sizes of  3600 or bigger. if I change the sample down to 800, the pixels show up... so it seems to be the size causing the problem.  Is this a known limitation or is there some sort of thing I am missing or doing wrong? -- - Jim</body>
  </mail>
  <mail>
    <header>Core Image CIImage sampler dimension limitations?</header>
    <body>Hello, I am writing an Image Unit that needs to work on images of very large dimensions (sizes of over 480,000 pixels in height, but small width of 640).  Although the input is very big, the output definition is still just a 640x480 image. I have written an image unit to process these images, but if I ever sample above around 3600 &amp;gt; the samples just return black.  If I convert the large CIImage to a bitmap and write it to file the entire thing is intact and i can see all pixels, so the image is definitely there. It must have something to do with the process of giving the image to the filter kernel for sampling. the kernel code to replicate the problem looks like this: where the src is a CIImage that has a height of over 3600.  This image will return blank, It seems like core image should be able to handle sizes of  3600 or bigger. if I change the sample down to 800, the pixels show up... so it seems to be the size causing the problem.  Is this a known limitation or is there some sort of thing I am missing or doing wrong? any advice would be great! thanks -- - Jim</body>
  </mail>
  <mail>
    <header>Re: CGDataProviderCopyData bug [SOLVED]</header>
    <body>Ok, turns out that the byte padding in the buffer (the difference between the actual amount of real data per row and the CGImageGetBytesPerRow value) changes for the same source image, depending on transforms applied, but only in the case of larger images. A bit unexpected if you're trying to compare images, but not actually a bug.</body>
  </mail>
  <mail>
    <header>Re: CGDataProviderCopyData bug</header>
    <body>The image in question is a CGimage that is rendered from a CIImage using CreateCGImage. The original CIImage comes from a CMFloatBitmap.</body>
  </mail>
  <mail>
    <header>Re: CGDataProviderCopyData bug</header>
    <body>The image in question is a CGimage that is rendered from a CIImage using CreateCGImage. The original CIImage comes from a CMFloatBitmap.</body>
  </mail>
  <mail>
    <header>Re: CGDataProviderCopyData bug</header>
    <body>Where are you getting the image from? -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>CGDataProviderCopyData bug</header>
    <body>I'm using CGDataProviderCopyData to get access to pixel data. Mostly this works fine, but if (a) the image is over 512 pixels in any dimension AND it has had a CGAffineTransform (of 90 degrees) applied to it prior to the CGDataProviderCopyData, the data organization in the buffer is all over the place - it appears to be tiled in some way that I can't work out, at least partially on 512 pixel boundaries. The image still displays fine, btw. Sandy</body>
  </mail>
  <mail>
    <header>Re: CGL-backed CIContext for render:toBitmap:</header>
    <body>Ok, thank you. In case of a hardware-accelerated renderer, is there a specific way how I should align the bitmap memory for render:toBitmap: in order to increase performance? Like, on a 16, 32 or 64-byte boundary? Currently, I simply malloc() it, which means (on MacOS X) that it's aligned on a 16-byte boundary. Thanks again, Georg</body>
  </mail>
  <mail>
    <header>Re: CGL-backed CIContext for render:toBitmap:</header>
    <body>That is the expected behavior because with the render:toBitmap: API no work actually happens on the client context. Pretty much the only thing that has a performance impact here is the render you pick in the pixel format</body>
  </mail>
  <mail>
    <header>CGL-backed CIContext for render:toBitmap:</header>
    <body>I want to render an image to a bitmap after applying a couple of custom CIFilters to it. To do so, I use Leopard's render:toBitmap: method of CIContext. My question is how to set up the CIContext for use with render:toBitmap: in a way that drawing happens hardware-accelerated and at the best possible performance (for the creation of an 8-bit per channel ARGB bitmap -&amp;gt; CIFormatARGB8). Right now, I'm creating the CIContext from a CGLContext, which I create myself using CGLCreateContext(). However, no matter which pixel format options I request for the CGLContext (fullscreen, pb-backed, etc...), drawing performance is always the same. It's also strange that render:toBitmap: works fine even if I create the CIContext from a pb-backed CGLContext to which I have not even attached an actual pixel buffer yet. Could somebody please shed some light on the relation of CIContext and its backing CGLContext in respect to render:toBitmap: Many thanks, Georg</body>
  </mail>
  <mail>
    <header>Measuring Core Animation performance</header>
    <body>I'm looking for ways to programmatically assess CA's performance/frame rate. It'd be very useful to make a change and then be able to objectively judge what's the effect on the frame rate, and see how different hardware responds. What I'm effectively looking for is be able to have an objective measure of 'smoothness'. As an aside, I haven't been able to find official documentation on improving CA performance (optimisation techniques, what things are expensive and so on). I've found some useful information here: but not much after that and google doesn't seem to be helping. Thanks, Orestis -- email@hidden</body>
  </mail>
  <mail>
    <header>understanding quartz debug</header>
    <body>I have a window that covers my [[NSScreen mainScreen] visibleFrame] In this window I have a single view, and manage multiple CALayers in the view. I was looking at it with Quartz Debug, and I noticed every time a CALayer moves, the whole view is updated and turns yellow. Does this mean that it is not hardware accelerated (i.e. not green)? ANd also the whole view is painted every time a tiny CALayer moves. My view is a standard NSView and I'm only updating CALayer transforms, do I need to manage the view myself to tell it which parts to redraw?</body>
  </mail>
  <mail>
    <header>Re: Smoother mouse tracking for brush strokes</header>
    <body>Thanks everyone for the help.  After much fiddling, I am relatively sure that I am processing mouse drag events as fast as the system can send them, that layers are working efficiently (despite showing large update regions), and that if I want smoother brush strokes, I'm going to have to look at interpolating the mouse locations from drag events.</body>
  </mail>
  <mail>
    <header>core animation slow down</header>
    <body>I have an NSTimer which fires every couple of seconds. Every time it fires it applies little animations to 15 layers. These animations are flip, rotate, slide etc. When I first run the app it runs very smoothly, but if I leave it running for a while it eventually slows down. Sometimes it slows down after a 10-20 seconds. Sometimes it runs very smooth for half an hour without slowing down, but then slows down. If I leave it running for hours it will always speed up again, then randomly gradually slow down, but eventually go back to being super smooth. I have not managed to detect a pattern. My code is very simple and I'm sure there are no leaks (nothing is allocated or retained after the init functions). I can post code if need be. Any ideas what it may be? Or pointers to look out for? It seems like a system thing but I can't figure it out. P.S. I've noticed that when the slowdown happens, other things like dock animations, cover flow etc. slow down to the same pace.... P.P.S This does not happen on very high spec machines (GF8800), but does happen on lower spec (ATI1600, intel etc.).</body>
  </mail>
  <mail>
    <header>Re: Smoother mouse tracking for brush strokes</header>
    <body>On Mar 24, 2009, at 3:25 PM, Steven Frank wrote: It's been a few years since I wrote this code, but it looks like it bottoms out into that.  Of course, I am calling: - wave</body>
  </mail>
  <mail>
    <header>Re: Smoother mouse tracking for brush strokes</header>
    <body>On Mar 24, 2009, at 2:49 PM, Michael B Johnson wrote: Ahh, that's very helpful, thanks. However it does confirm that the entire CALayer is redrawing when I invalidate a 20x20 rect. I've re-posted my sample project with this environment variable added, so you can see. Michael, thanks for posting your mouseDragged:.  Since you're using CGLayer, I assume your view also has a drawRect: which simply calls CGContextDrawLayerAtPoint/InRect()? I think, except for the modal mouse event loop, I'm back where I started.  :)</body>
  </mail>
  <mail>
    <header>Re: Smoother mouse tracking for brush strokes</header>
    <body>Since Marc is very smart, you can probably get smoother info if you go and do an event tap, but my code (my CGLayer based sketching code), which is considered very smooth by all our artists here, just turns off mouseCoalescing in the view earlier and looks like this: _MPGPointWidth* pw = [[[_MPGPointWidth alloc] while (e = [NSApp nextEventMatchingMask:NSLeftMouseDraggedMask untilDate:[NSDate distantPast] inMode:NSEventTrackingRunLoopMode pw = [[[_MPGPointWidth alloc] initWithPoint:locationInView so what I'm doing here is grabbing all the points and sticking them (as instances of a private class to hold their width and location) and then when I've finally pulled off all the events on the queue, I hand those to the view to add them into the drawing code in one swell foop.  In the mouseDraggedAlongPointsWidths: methods I accumulate a dirtyRect for the view, and use that to set the dirtyRect for the view at the end of the routine. I believe I got this mouse coalescing idea from Raleigh Ledet, previously of Wacom, and now I believe at Apple. If anyone has a better idea (I'm going to take a look at Marc's idea about the event tap), I'd love to know, but this works very well for me, and gives me good input on my Cintiq. --&amp;gt; Michael B. Johnson, PhD --&amp;gt;  (personal) --&amp;gt;  (alum) --&amp;gt; MPG Lead --&amp;gt; Pixar Animation Studios</body>
  </mail>
  <mail>
    <header>Re: Smoother mouse tracking for brush strokes</header>
    <body>Right - sorry, while I've got some CALayer stuff going on in my sketching stuff, I was conflating CGLayer and CALayer. My bad.  In penance, here's the info you do want to debug this in Core Animation world, from the ever helpful John Harper, in response to a question I posed on this very list a year ago: There's currently no way that Quartz Debug can tell you what Core Animation is drawing. CA draws like any OpenGL application into a surface attached to your window, and Quartz Debug and the Quartz window server will either not be told about each screen update at all (if the surface is unoccluded by other windows) or will be told that the entire surface was updated (most likely what you're seeing.) The best solution is to run your program with the environment variable CA_COLOR_FLUSH=1 set, to turn on Core Animation's own version of &amp;quot;flash screen updates&amp;quot; for the individual parts of the window that it redraws. CA_COLOR_NO_WAIT=1	like the &amp;quot;no delay after flash&amp;quot; option in Quartz Debug CA_COLOR_OPAQUE=1	shows opaque vs non-opaque regions in red and green - minimizing &amp;quot;redness&amp;quot; is a good way to improve performance CA_PRINT_TREE=1		logs the CA render tree to stderr every frame</body>
  </mail>
  <mail>
    <header>Re: Smoother mouse tracking for brush strokes</header>
    <body>Actually in this case QuartzDebug won't tell you the information your looking for. Because a CALayer is rendered via OpenGL, it will always look like the entire area is repainted - this however isn't necessarily slow as this rendering is done in hardware. The only way to track this is to check the clip area of the context passed to your -drawLayer:inContext: delegate call. Set the view as the layer's delegate and add this code to see this in action. -(void)drawLayer:(CALayer*)l inContext:(CGContextRef)c NSLog(@&amp;quot;%f %f %f %f&amp;quot;, clipRect.origin.x, clipRect.origin.y, -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Smoother mouse tracking for brush strokes</header>
    <body>I just checked my current sketching code, and I used layer backed it shows that I'm doing minimal rect redrawing, so it is possible. Sorry I don't have more time to look, but I thought the existence proof might be helpful... --&amp;gt; Michael B. Johnson, PhD --&amp;gt;  (personal) --&amp;gt;  (alum) --&amp;gt; MPG Lead --&amp;gt; Pixar Animation Studios</body>
  </mail>
  <mail>
    <header>Re: Smoother mouse tracking for brush strokes</header>
    <body>I'm sorry to continue to be dense about this, but I cannot get a layer to redraw anything less than its entire content, no matter how small of a portion is invalidated. A layer-backed NSView subclass in a window.  When you click on it, it you'll see the _entire_ layer redraws every time you click. If a layer's content is equivalent to an OpenGL texture, then it occurs to me that a layer-backed paint program may simply not be feasible.  You can't efficiently track a brush stroke in real time if every change to the drawing requires a re-upload of the texture to the GPU and the entire texture to be re-displayed.  Am I trying to do something that's impossible here?  Should I go back to NSViews? - (void)awakeFromNib - (void)mouseDown:(id)sender @end</body>
  </mail>
  <mail>
    <header>Re: Smoother mouse tracking for brush strokes</header>
    <body>Yes, but you don&amp;#39;t have to stop with mouseDragged: . In our &amp;quot;applications&amp;quot; when we need robust, high resolution mouse input to accompany live drawing we install an Event Tap (CGEventTapCreate) and pull the mouse (or Wacom Tablet) events out in a completely separate thread. The result is device resolution coordinates with non-coalesced events at very frame-rates. It&amp;#39;s pretty straightforward (once you&amp;#39;ve gone down that architectural route) to play with latency / accuracy trade-offs. Marc.</body>
  </mail>
  <mail>
    <header>Re: Smoother mouse tracking for brush strokes</header>
    <body>OK, so, correct me if I'm wrong, but it seems like setting the contents property will never be efficient for a painting program, as the contents are changing constantly as the user drags the brush across the canvas, which will cause the entire layer to be pushed to the GPU and redrawn every time a small section of the brush stroke is stamped.  Right so far? So, the only other way to provide layer content is to implement one of the draw-in-context methods.  However, those won't get called until part of the layer is invalidated via setNeedsDisplayInRect: and friends. My problem then is there's a disconnect between the mouse events I'm getting and the drawing code.  I get mouseDragged:s, which contain the information I need to draw the stroke, but I won't have a context in which to draw the stroke until the layer calls its draw method. - mouseDragged: received - Calculate the rect that this part of the stroke will invalidate - Invalidate it - Cache off the details of the mouse event (start and end point) and transfer that information to the layer, so it will know what to draw when the time comes - Wait for the layer to call its draw method and then draw the stroke there Does this sound like the right &amp;quot;flow&amp;quot;?  It's quite a bit different than my original NSView-based implementation. Steven</body>
  </mail>
  <mail>
    <header>NCWConcatColorWorld() error</header>
    <body>Could someone elucidate on the -171 (cmMethodError) error I get after calling NCWConcatColorWorld()? Only one ICC Profile gives this error. All other profiles work correctly. This particular profile was edited using some package, however the ColorSync Utility can open it and display its tags. When visually comparing it to another ICC Profile they seem very similar. Lastly I Verified this ICC Profile using ColorSync Utility so we know it has valid tags. Aaron Alpher</body>
  </mail>
  <mail>
    <header>Re: Smoother mouse tracking for brush strokes</header>
    <body>Think of layers as 2 items, content and geometry. Content is akin to an OpenGL texture. You can share content between layers, and you can set content by assigning the contents property, or drawing to a bitmap that Core Animation provides (and that it then uploads as a texture). Layers do have a -setNeedsDisplayInRect: method, but I don't recall if it actually chops up areas. That said, if you wanted to support that, you'd go back to drawing the image rather than setting contents (don't use -drawlayer:inContext: to set contents, use -displayLayer: instead or just assign the contents at your whim instead). -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Smoother mouse tracking for brush strokes</header>
    <body>OK, so I moved a few things around in order to try this. My painting NSView now has three CALayers: a root layer (just a container for other layers) which contains an opaque background layer, which in turn contains a clear canvas layer.  The user's brush strokes live on that canvas layer. Then we have toolContext -- a CGBitmapContext where the actual brush strokes are getting rendered into. - (void)mouseDragged:(NSEvent*)event NSRect updateRect = [currentTool mouseDragged:p context:toolContext];  // drawing done here, returns changed rect Brush strokes are still choppy.  With Quartz Debug turned on, I can see that the entire view is redrawing every time through mouseDragged:.  So that's no good. - (void)mouseDragged:(NSEvent*)event NSRect updateRect = [currentTool mouseDragged:p context:toolContext];  // drawing done here, returns changed rect I've confirmed that updateRect is very small, only 12x12 -- the size of the brush, which is all that needs to be drawn. - (void)drawLayer:(CALayer*)layer inContext:(CGContextRef)context CGContextDrawImage(context, CGRectMake(0, 0, CGBitmapContextGetWidth(toolContext), I thought maybe this call would be clipped to the invalidated region, but Quartz Debug still shows the whole view updating every time through mouseDragged:, and thus still choppy. I am clearly not getting some basic fundamental about layers.  I'm still thinking in NSViews.  What's the trick to drawing just the changed sub-rect instead of the whole kaboodle?  Or is that not how we do things around here?</body>
  </mail>
  <mail>
    <header>Re: Smoother mouse tracking for brush strokes</header>
    <body>Turning in layer backing will get you this behavior where your always asked to paint the entire area. However, since your using a bitmap context for your imaging, you can do better than drawing the image inside of -drawRect: by doing the following: 1. Enable layer backing mode 2. Call -setLayer: on your view with a plain CALayer 3. When your ready to update the content, create a new CGImageRef and assign it as the contents of the CALayer ala: As for your dragging, disabling mouse coalescing will probably get you smoother mousing. See the NSEvent reference for the methods to do this. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Smoother mouse tracking for brush strokes</header>
    <body>If you're really drawing the whole rect instead of just your dirty rect, you're screwed. On Mar 23, 2009, at 10:35 PM, Steven Frank wrote:</body>
  </mail>
  <mail>
    <header>Smoother mouse tracking for brush strokes</header>
    <body>So in my simple paint program, I'm using mouseDragged: messages to follow the brush stroke.  Works well enough, but curved lines tend to get choppy angular edges to them if you drag the mouse too quickly. /apple_ref/doc/uid/10000060i-CH6-SW4 Also, during vigorous painting, Shark shows me spending 10-15% of the time in CGContextDrawImage(), drawing my offscreen bitmap context into the on-screen NSView, which is currently layer-backed.  I'm wondering if this is the bottleneck.  I've noticed that if I request the NSView be layer-backed, it seems to always call drawRect: with the full dimensions of the view, even though I'm only invalidating the small sub-rects of the brush stroke.  This seems like it would kill performance, and makes me wonder if I'm doing something wrong architecturally. I guess this is sort of pushing off-topic for Quartz-dev... but probably someone here has been down this road.</body>
  </mail>
  <mail>
    <header>Re: Can I load an image unit in my app bundle?</header>
    <body>-- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: CIContext's render:toBitmap: - Does the CIContext matter?</header>
    <body>Thank you! As a follow-up question then: Provided I turn HW rendering back on, I suppose a CGL-context backed CIContext would perform better with render:toBitmap:, no? If so, which pixel format options would you recommend to use in CGLChoosePixelFormat() in order to achieve the best performance with render:toBitmap:? Thank you again, Georg</body>
  </mail>
  <mail>
    <header>Can I load an image unit in my app bundle?</header>
    <body>The Core Image docs just talk about installing image units in /Library/Graphics/Image Units and ~/Library/Graphics/Image Units, but since there exists the method -[CIPlugIn loadPlugIn:allowNonExecutable:], I thought I ought to be able to use an image unit that's not &amp;quot;installed&amp;quot;.  However, it doesn't seem to work. There's no console message or anything, I just get nil back from -[CIFilter filterWithName:keysAndValues:].  What's the deal? -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>CIContext's render:toBitmap: - Does the CIContext matter?</header>
    <body>I'm using some custom CIFilters in a computer vision system. I need to access the bitmap data of the rendered images to feed them into the computer vision algorithm, and I'm using CIContext's render:toBitmap:rowBytes:bounds:format:colorSpace: method to render the CIContext to memory in the kCIFormatARGB8 pixel format. Color matching is off (all colorspaces (in the CIImage and CIContext) are set to [NSNull null]), and rendering is forced to happen on the CPU, since getting the rendered bitmap back from the CPU is too expensive. What I'm wondering is is whether the way the CIContext is created matters in terms of performance of the render:toBitmap: method? I can't find anything about this in the docs. I've experimented with different ways of creating the CIContext (either from a CGContext or a CGLContext), but I didn't get any conclusive results (using mach_absolute_time() to do some simple measurements). If the way a CIContext is created indeed *does* matter in terms of rendering performance, what would be the absolutely best way to create the context to achieve the fastest rendering performance? Would a CGContext or CGLContext-backed CIContext be better, and which pixel format etc. options should I use to create the backing context? Cheers, Georg</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextGetData() returns NULL</header>
    <body>There have been a number of instances in the past when a bitmap context was needed, but the need to track the memory allocated by that bitmap context made it more complex to use. If you don't need access to the actual pixels (on 10.3-10.5) then you can pass NULL and be freed of the need to track this memory. That is the entire feature. On 10.3-10.5 if you need pixel access (and the many of clients of a bitmap context do not) then just pass NULL and be done with it. The documentation wasn't updated for CGBitmapContextGetData() when it was updated for CGBitmapContextCreate(). Feel free to provide feedback in the Quartz documentation about that. However, not being able to access the pixel buffer on 10.3-10.5 does not make it a good rule of thumb to always create your own buffer. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextGetData() returns NULL</header>
    <body>&amp;gt; The doc says: &amp;quot;This frees you from managing your own memory, which But then it says : &amp;quot;Quartz has more flexibility when it manages data storage for you. For example, it‚Äôs possible for Quartz to This had some problems like knowing the length of the scanline, and even possibly Quartz could change internally the pixel format to accomodate the driver ? I'm OK with that. It seems that Apple is in between architectural theory and practical needs. I'm very surprised for this thing on 10.6, as it means that the implementation will follow your commands, meaning that you're probably going to give a non optimized pixel format, when you're probably not even going to touch the pixels... Raphael</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextGetData() returns NULL</header>
    <body>&amp;gt; Maybe I've missed something, but I'm very sorry I disagree. The doc says: &amp;quot;This frees you from managing your own memory, which So I expected I could do: context = CGBitmapContextCreate(NULL, ...) // a buffer is created buffer = CGBitmapContextGetData(context) // I can actually use the buffer that was created, I don't expect it to be NULL CGContextRelease(context) // the buffer is released, I don't have to care about it Maybe I misunderstood the documentation and this is the expected behaviour, but Ken Ferry says on 10.6, CGBitmapContextGetData returns the buffer even if you passed NULL as the first argument of CGBitmapContextCreate. I expected this behaviour in 10.5 also. Managing my own buffer was not a big deal, just a matter of malloc/free. My point is that the documentation is misleading. C√©dric</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextGetData() returns NULL</header>
    <body>Hi C√©dric, Maybe I've missed something, but I'm very sorry I disagree. Well actually letting Quartz manage memory can have a benefic impact as it might organize the memory exactly how it is recommended for texture transfers on the OpenGL part. If you're going to do exactly the same but yourself, then I don't understand why to do it in the first place. (and in that very special case then I don't understand why not directly use OpenGL) The only time where it is necessary is when you really need to access pixels for some reasons. Thanks, Raphael</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextGetData() returns NULL</header>
    <body>&amp;gt; FYI, this is changed in 10.6. ¬†There¬†CGBitmapContextGetData retrieves the Unfortunately, this is documented to work from 10.3 as I understand it. CGBitmapContextCreate reference says: Starting in Mac OS X v10.3, you can pass NULL if you don‚Äôt care where the data is stored. This frees you from managing your own memory, which reduces memory leak issues. Quartz has more flexibility when it manages data storage for you. For example, it‚Äôs possible for Quartz to use OpenGL for rendering if it takes care of the memory. Also see  ;-)</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextGetData() returns NULL</header>
    <body>Not all of us do.  ;-) --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: Newbie MFC/GDI expat - a simple question</header>
    <body>CGImages (and UIImages) are immutable, so to draw on top of one you have to create a bitmap context, draw that image into the bitmap context, and then extract a new image from that context. In UIKit this is fairly easy using UIGraphics functions // do additional drawing Then you can assign the new image to the UIImageView. Alternatively if what you want to draw on top of the image changes (or the image underneath the drawing changes) then you can do this faster by keeping your drawing in another view that is a subview of the UIImageView and then just update your drawing normally inside your view's -drawRect: (or change the image on the UIImageView). -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Newbie MFC/GDI expat - a simple question</header>
    <body>I've looked for hours now, and can't find a straight forward example of drawing on top of an existing CGImage. My (iPhone) app has a UIImageVIew object that is loaded from a .png. I can get a CGImageRef from that object. I simply want to draw on top of that CGImageRef, so when ever the UIImageVIew is shown ( rotated, etc ), my added drawing shows on top. In MFC/GDI, I would simply create a device context and select the bitmap object into it and start drawing using the context. I also tried to find a way to add a layer to the CGImageRef, but dead- ended there too. Thanks - and if there are any other Windows GDI expats that can offer some suggestions on adjusting to the Mac approach, I'd appreciate that.</body>
  </mail>
  <mail>
    <header>Re: CoreImage Filter Question with Conditionals</header>
    <body>___ Andrew Lindesay www.lindesay.co.nz</body>
  </mail>
  <mail>
    <header>Re: CoreImage Filter Question with Conditionals</header>
    <body>Thanks for that; it looks like it would have worked, but now I realise that there is a better approach to what I am trying to achieve. At a conceptual level, I am a little confused as to why those 'inline construct?  I'd be glad to understand a bit more about why this is. ___ Andrew Lindesay www.lindesay.co.nz</body>
  </mail>
  <mail>
    <header>Re: CoreImage Filter Question with Conditionals</header>
    <body>Andrew, CoreImage doesn't support Data-Dependent &amp;quot;if&amp;quot; statements, meaning that the compiler needs to be able to evaluate conditionals at compile-time.  However, after a little noodling I managed to come up with this workaround. I tested it in Quartz Composer and it worked as expected. vec4 pix = (cs.x &amp;lt; se.z) ? ((cs.y &amp;lt; se.w) ? sample(image,c) : Cheers, David</body>
  </mail>
  <mail>
    <header>looking for some Quartz/CoreGraphics consulting help</header>
    <body>i've got a project where i'll likely need some additional expertise, and am looking for someone who may be able to provide that. If you're interested and can offer even just intermittent help via IM and phone, would you please contact me? I may be looking at a project that needs some CoreGraphics/Quartz compositing that goes beyond my meager abilities. Even if you don't think you're very good, you're almost certainly more experienced than i am, so i'd welcome the connection. michael geary email@hidden</body>
  </mail>
  <mail>
    <header>optimizing rowBytes on iPhone</header>
    <body>The Programming in Quartz book makes a recommendation (&amp;quot;current as of Tiger&amp;quot;) that CGBitmapContext rowByte  allocations should be in multiples of 16. I recall reading somewhere or other tho that multiples of 8 are fine for iPhone. Can somebody verify that? It'd be nice to save a little RAM. Thanks, Howard</body>
  </mail>
  <mail>
    <header>CALScrollLayer:  Flickers while scrolling</header>
    <body>In a layer-hosted view, I have created a CAScrollLayer that has bounds of ( 0, 0, 5200, 500).  This CAScrollLayer has a number of sublayers that each contain images (which represent various products that can be selected).  Each of these sublayers is approximately 300 by 500.  The sublayers content is being drawn using a delegate method.  The images are positioned like a filmstrip across the larger CAScrollLayer. When I horizontally scroll my CAScrollView to the right or the left I am experiencing a stutter or jerkiness instead of a nice smooth scroll. Has anyone seen this before?  If so, how can I make it go away?  Is this issue related to the size of CAScrollLayer? If so, what other options do I have for creating an arbitrarily large CAScrollLayer that needs to scroll left and right? douglas</body>
  </mail>
  <mail>
    <header>[CALayer visibleRect] not proper property? KVO or CAAction</header>
    <body>hello everyone i'm no working for over two months with Core Animation and i like the API a lot, it's very well implemented and also documentation very good. i have issue with CAScrollLayer. Each time a layer becomes &amp;quot;visible&amp;quot; while scrolling, i'd like to make it blink or some other action. The Layer is my own Subclass MyItem of CALayer. i tried to: A) add an action for Key-Property visibleRect the Action is called for all properties except visibleRect B) add an Observer to Key-Property visibleRect Both of it don't get called. Is visibleRect somehow not a &amp;quot;proper&amp;quot; property? is it only calculated on request? sorry my poor english. thank's everyone who's looking at it :-), greetings from japan, Sapporo mahal ---- my code A: @implementation MyItem // snip - (id&amp;lt;CAAction&amp;gt;)actionForKey:(NSString *)key if ([key isEqualToString:@&amp;quot;visibleRect&amp;quot;]) @end @implementation VisibilityAction // singleton + (id)visibilityAction if (sharedVisibilityAction == nil) // should be called whenever an layers visibleRect property is changed - (void)runActionForKey:(NSString *)key object:(id)anObject arguments:(NSDictionary *)dict // if layer becomes partly visible: at the moment: do nothing // if layer becomes fully visible: send message // if a layer is zoomed closer (when fully visible): //  do nothing, might be fancy to load content at higher resulution // if a layer becomes invisible (out of scrolling bounds): send message (so it set it's last visited date and detach from listening to FileSytemEvents) // do nothing ---- code B @implementation MyItem - (id) init //TODO somehow the property visibleRect is not KVO compliant... ask apple [self addObserver:[VisibilityAction visibilityAction] options:(NSKeyValueObservingOptionNew | NSKeyValueObservingOptionOld) @end @implementation ZFVisibilityAction // ... snip ... same singleton as in A // implemented as KVO because internally changed (read-only) visibleRect somehow does not send actions - (void)observeValueForKeyPath:(NSString *)keyPath ofObject:(id)object change:(NSDictionary *)change context:(void *)context // be sure to call the super implementation // if the superclass implements it @end</body>
  </mail>
  <mail>
    <header>RE: Byte Order in CGBitmapContextCreate</header>
    <body>Ah the light dawns. What a small but important thing to learn! ;) Many thanks, Howard &amp;gt; -----Original Message----- &amp;gt; From: David Duncan [] &amp;gt; Sent: Friday, May 15, 2009 11:20 AM &amp;gt; To: Howard Katz &amp;gt; Cc: email@hidden &amp;gt; Subject: Re: Byte Order in CGBitmapContextCreate &amp;gt; &amp;gt; Does that mean this is the ticket: &amp;gt; &amp;gt;    kCGImageAlphaLast | kCGBitmapByteOrder32Big &amp;gt; &amp;gt; That causes a big hiccup. &amp;gt; Yea, that won't work because bitmap contexts can only render RGB &amp;gt; formats that use premultiplied alpha or no alpha. I was using &amp;gt; AlphaFirst simply as short hand for all of the kImageAlpha &amp;gt; constants &amp;gt; that had 'First' in them (SkipFirst, PremultipliedFirst and &amp;gt; just plain &amp;gt; First). Ditto for AlphaLast. &amp;gt; -- &amp;gt; David Duncan &amp;gt; Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Byte Order in CGBitmapContextCreate</header>
    <body>Yea, that won't work because bitmap contexts can only render RGB formats that use premultiplied alpha or no alpha. I was using AlphaFirst simply as short hand for all of the kImageAlpha constants that had 'First' in them (SkipFirst, PremultipliedFirst and just plain First). Ditto for AlphaLast. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>RE: Byte Order in CGBitmapContextCreate</header>
    <body>Dave, That very combo has been working all along for me. I think you can even omit the kCGBitmapByteOrderDefault. I was just confused to see ABGR, expecting RGBA (which is what I had wanted). David @ Apple has enlightened me on the endidness issue, and I'm laughing at myself for not seeing it. Turns out that ABGR is just fine, as long as I know where things are. Ta, Howard &amp;gt; -----Original Message----- &amp;gt; From: Dave MacLachlan [] &amp;gt; Sent: Friday, May 15, 2009 7:11 AM &amp;gt; To: Howard Katz &amp;gt; Cc: 'David Duncan'; email@hidden &amp;gt; Subject: Re: Byte Order in CGBitmapContextCreate &amp;gt; Howard, &amp;gt; (long time no type!) &amp;gt; I tend to get good results on the iPhone with: &amp;gt; kCGImageAlphaPremultipliedLast | kCGBitmapByteOrderDefault &amp;gt; Does that work for you? &amp;gt; Cheers, &amp;gt; Dave &amp;gt; &amp;gt; Does that mean this is the ticket: &amp;gt; &amp;gt;    kCGImageAlphaLast | kCGBitmapByteOrder32Big &amp;gt; &amp;gt; That causes a big hiccup. &amp;gt; &amp;gt; Howard &amp;gt; &amp;gt;&amp;gt; -----Original Message----- &amp;gt; &amp;gt;&amp;gt; From: David Duncan [] &amp;gt; &amp;gt;&amp;gt; Sent: Thursday, May 14, 2009 5:03 PM &amp;gt; &amp;gt;&amp;gt; To: Howard Katz &amp;gt; &amp;gt;&amp;gt; Cc: email@hidden &amp;gt; &amp;gt;&amp;gt; Subject: Re: Byte Order in CGBitmapContextCreate &amp;gt; &amp;gt;&amp;gt;&amp;gt; I pass in kCGImageAlphaPremultipliedLast for example, but when I &amp;gt; &amp;gt;&amp;gt;&amp;gt; dump any of &amp;gt; &amp;gt;&amp;gt;&amp;gt; my stroked pixels in the bitmap to the console, they're &amp;gt; &amp;gt;&amp;gt; 0xFF00FF00, &amp;gt; &amp;gt;&amp;gt;&amp;gt; which &amp;gt; &amp;gt;&amp;gt;&amp;gt; looks like green, but alpha-first. &amp;gt; &amp;gt;&amp;gt; Your getting little-endian pixels, and this is really giving &amp;gt; &amp;gt;&amp;gt; you alpha &amp;gt; &amp;gt;&amp;gt; last (ABGR to be exact). What it sounds like you want is BGRA, or &amp;gt; &amp;gt;&amp;gt; perhaps RGBA, in which case you need to also set the byte &amp;gt; &amp;gt;&amp;gt; order flag &amp;gt; &amp;gt;&amp;gt; appropriately. &amp;gt; &amp;gt;&amp;gt; Here are some rules of thumb to help remember this &amp;gt; &amp;gt;&amp;gt; AlphaFirst =&amp;gt; The Alpha channel is next to the Red channel &amp;gt; &amp;gt;&amp;gt; (ARGB and &amp;gt; &amp;gt;&amp;gt; BGRA are both Alpha First formats) &amp;gt; &amp;gt;&amp;gt; AlphaLast =&amp;gt; The Alpha channel is next to the Blue channel &amp;gt; &amp;gt;&amp;gt; (RGBA and &amp;gt; &amp;gt;&amp;gt; ABGR are both Alpha Last formats) &amp;gt; &amp;gt;&amp;gt; LittleEndian =&amp;gt; Blue comes before Red (BGRA and ABGR are &amp;gt; &amp;gt;&amp;gt; Little endian &amp;gt; &amp;gt;&amp;gt; formats) &amp;gt; &amp;gt;&amp;gt; BigEndian =&amp;gt; Red comes before Blue (ARGB and RGBA are Big endian &amp;gt; &amp;gt;&amp;gt; formats). &amp;gt; &amp;gt;&amp;gt; -- &amp;gt; &amp;gt;&amp;gt; David Duncan &amp;gt; &amp;gt;&amp;gt; Apple DTS Animation and Printing &amp;gt; &amp;gt; _______________________________________________ &amp;gt; &amp;gt; Do not post admin requests to the list. They will be ignored. &amp;gt; &amp;gt; Quartz-dev mailing list      (email@hidden) &amp;gt; &amp;gt; Help/Unsubscribe/Update your Subscription: &amp;gt; gmail.com &amp;gt; &amp;gt; This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Byte Order in CGBitmapContextCreate</header>
    <body>Cheers, Dave</body>
  </mail>
  <mail>
    <header>RE: Byte Order in CGBitmapContextCreate</header>
    <body>Does that mean this is the ticket: kCGImageAlphaLast | kCGBitmapByteOrder32Big That causes a big hiccup. Howard &amp;gt; -----Original Message----- &amp;gt; From: David Duncan [] &amp;gt; Sent: Thursday, May 14, 2009 5:03 PM &amp;gt; To: Howard Katz &amp;gt; Cc: email@hidden &amp;gt; Subject: Re: Byte Order in CGBitmapContextCreate &amp;gt; &amp;gt; I pass in kCGImageAlphaPremultipliedLast for example, but when I &amp;gt; &amp;gt; dump any of &amp;gt; &amp;gt; my stroked pixels in the bitmap to the console, they're &amp;gt; 0xFF00FF00, &amp;gt; &amp;gt; which &amp;gt; &amp;gt; looks like green, but alpha-first. &amp;gt; Your getting little-endian pixels, and this is really giving &amp;gt; you alpha &amp;gt; last (ABGR to be exact). What it sounds like you want is BGRA, or &amp;gt; perhaps RGBA, in which case you need to also set the byte &amp;gt; order flag &amp;gt; appropriately. &amp;gt; Here are some rules of thumb to help remember this &amp;gt; AlphaFirst =&amp;gt; The Alpha channel is next to the Red channel &amp;gt; (ARGB and &amp;gt; BGRA are both Alpha First formats) &amp;gt; AlphaLast =&amp;gt; The Alpha channel is next to the Blue channel &amp;gt; (RGBA and &amp;gt; ABGR are both Alpha Last formats) &amp;gt; LittleEndian =&amp;gt; Blue comes before Red (BGRA and ABGR are &amp;gt; Little endian &amp;gt; formats) &amp;gt; BigEndian =&amp;gt; Red comes before Blue (ARGB and RGBA are Big endian &amp;gt; formats). &amp;gt; -- &amp;gt; David Duncan &amp;gt; Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Byte Order in CGBitmapContextCreate</header>
    <body>Your getting little-endian pixels, and this is really giving you alpha last (ABGR to be exact). What it sounds like you want is BGRA, or perhaps RGBA, in which case you need to also set the byte order flag appropriately. Here are some rules of thumb to help remember this AlphaFirst =&amp;gt; The Alpha channel is next to the Red channel (ARGB and BGRA are both Alpha First formats) AlphaLast =&amp;gt; The Alpha channel is next to the Blue channel (RGBA and ABGR are both Alpha Last formats) LittleEndian =&amp;gt; Blue comes before Red (BGRA and ABGR are Little endian formats) BigEndian =&amp;gt; Red comes before Blue (ARGB and RGBA are Big endian formats). -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Byte Order in CGBitmapContextCreate</header>
    <body>I'm confused. I'm drawing text into a bitmap context for an iPhone app. I set the RGB fill color to an opaque green, set the color space to CGColorSpaceCreateDeviceRGB(), draw my text into the bitmap context and capture that to a CGImage and post it to the screen. Works like a charm. However, I don't understand what's going on with byte ordering. It doesn't seem to matter what argument I pass in as the CGBitmapInfo argument to CGBitmapContextCreate(), I always get alpha-first, and I'm trying to get alpha-last. I pass in kCGImageAlphaPremultipliedLast for example, but when I dump any of my stroked pixels in the bitmap to the console, they're 0xFF00FF00, which looks like green, but alpha-first. It also turns out that if I pass in kCGImageAlphaLast, the debug info I get back from the image says the color space model is UNKNOWN, and the alpha channel info returns kCGImageAlphaNone. What do I pass in to get alpha-last? Am I supposed to bitwise-or the last parameter with something else? TIA Howard</body>
  </mail>
  <mail>
    <header>Re: CIImage memory usage issue.</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Tracing image</header>
    <body>The bits per pixel value is how many bits to travel between pixels (and generally is generally a power of 2, although this isn't required). The bits per component is the bit width of each color channel and alpha if present. Row bytes is how many bytes you need to cross to go from the y=0 row to the y=1 row for the same column. You also need to know layout and color scheme. The bitmap info contains the layout information (which includes the alpha information so you don't need to query that separately). And the color scheme (Gray, RGB, CMYK) is obtainable via the color space. Typically you have control over the source images when you use CGDataProviderCopyData(), so you don't have to worry about all the queries (you just use the layout format you expect from the source image). -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>CIImage memory usage issue.</header>
    <body>&amp;nbsp;&amp;nbsp;&amp;nbsp;CGRect extent =[ mImage extent ]; //mImage is of type CIImage &amp;nbsp;&amp;nbsp;&amp;nbsp;[ mImage &amp;nbsp;&amp;nbsp;&amp;nbsp;twice but releases only once.&amp;nbsp;This Issue is also found in Core image fun house application. Disclaimer: This email may contain confidential material. If you were not an intended recipient, please notify the sender and delete all copies. Emails to and from our network may be logged and monitored. This email and its attachments are scanned for virus by our scanners and are believed to be safe. However, no warranty is given that this email is free of malicious content or virus.</body>
  </mail>
  <mail>
    <header>Re: Tracing image</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Tracing image</header>
    <body>In the QA, follow the link in the first warning note (Getting Information About an image)</body>
  </mail>
  <mail>
    <header>Re: Tracing image</header>
    <body>My images are perfectly clean with strict masks so determing the edge is not an issue, I just don't understand how to use the CGImage API to iterate through pixel maps, determine color values and finally change the value. There has to be some source code some place... Thanks again.</body>
  </mail>
  <mail>
    <header>Re: Tracing image</header>
    <body>CGImages are read-only, so you get the pixels from an image, modify them, then create a new image (if necessary). -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Tracing image</header>
    <body>I'm attempting to trace the edges of an image but I'm pretty lost on how to</body>
  </mail>
  <mail>
    <header>Re: Tracing image</header>
    <body>Have a look at  for an outline of various methods. Hamish</body>
  </mail>
  <mail>
    <header>Tracing image</header>
    <body>Josef</body>
  </mail>
  <mail>
    <header>Re: Bitmap printing context</header>
    <body>Yes, by drawing the whole thing once rather than in bands. #if/ #else'ing our kNumBands for each platform was much easier and safer than messing up a large portable routine with special code that only works around this odd mechanism in Quartz pdf contexts. Thanks for the great explanation. The point about data providers had not yet been embedded in my brain, as this is the first time I've used one that didn't actually get consumed right away when used. The fact that drawing to a context that is a pdf works differently than when drawing to more common types of contexts (windows and bitmaps) is what caused the problem/confusion. In that light, wouldn't it be a great idea if there was a flag that we could set on contexts that says &amp;quot;consume the drawing commands right away instead of holding onto data providers and such&amp;quot;? I can easily see this scenario coming up again and again, and having a simple way to draw *now* would sure help, no matter what type the context is. (Besides, one's first assumption when drawing to a bitmap printer is that the context would naturally be a bitmap context, not a pdf context. But I can see where it might sometimes be useful to hang onto actual drawn vectors rather than rasterizing right away.) _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Bitmap printing context</header>
    <body>Re: On May 4, 2009, at 1:12 PM, Steve Mills wrote: You have apparently worked out a way to do this correctly however I want to reinforce something that might not have been made clear. When you create a data provider you have the opportunity to provide a release function to be called when the data provider is released. You then use that data provider when you create a CGImageRef whose data corresponds to the data your data provider &amp;quot;provides&amp;quot;. Since CGImageRefs are immutable, you are promising that the data provided by that data provider will be unchanged and available until the data provider itself is released and your data provider's release function is called. Even though *you* are creating your image and drawing it once and releasing it, when you are drawing to a PDF context, Quartz retains a reference to your image and keeps it around until it is finished with it. At that point it will release the image and the underlying data provider will be released and your data provider's release function will be invoked. THEN you can release the data you are providing. You are making the mistake of modifying the data the data provider provides before your release function is invoked. Unfortunately this kind of problem is common enough that I wrote quite a bit about it in the &amp;quot;Programming with Quartz&amp;quot; book. The publisher online so you can read it without buying the book: ~Chapter_Seventeen.pdf On page 614 is Table 17.1 &amp;quot;Drawing Problems and Debugging Tips&amp;quot;. Your particular problem in that table is &amp;quot;Images look identical, but should be different.&amp;quot;. The debugging tip is &amp;quot;See &amp;quot;Checking for Immutability Violations&amp;quot; (page 620).&amp;quot; And page 620 indeed has a section about this. Hope this helps, David</body>
  </mail>
  <mail>
    <header>Re: Core Animation and scaling images</header>
    <body>On May 5, 2009, at 2:48 AM, Orestis Markou wrote: Yes, that's right. That might be problematic, since you don't always know when the constraints will be applied, though I suppose you could subclass - layoutSublayers, call the layout manager, then correct for integralness. Or implement the layout algorithm directly. Requesting the presentation layer initializes a second temporary copy, the original model layer is never reinitialized. All these things end up creating an image that the compositor has to render. The second two (and the text layer) will always create that image in the optimal format, whereas explicit image layers may have to copy the data first if it's provided in a format that's not ideal for the destination. There should be no performance difference at all between the delegation and subclassing models.</body>
  </mail>
  <mail>
    <header>Re: Core Animation and scaling images</header>
    <body>Hi John, When you say 'draw the image into a suitably-sized layer', do you mean implementing 'drawInContext:' and drawing the image on the context directly, sized to the layer's bounds or is this some other technique? About the pixel alignment - we are using CAConstraints at some points, which makes it difficult to ensure whole pixels are used. Is there a way of ensuring that after positions and bounds are set, they are 'nudged' in place? I presume we can just walk the layer hierarchy and use NSRectIntegral or similar. What kind of performance impact does subclassing CALayer and adding instance variables have? From my understanding, copies of layers are made for the presentation layers - does that effectively re-init the model layer? * Have a tree of simple layers (CATextLayers, image layers and so on) * Subclass CALayer and draw manually * Create delegates to draw and assign them to plain CALayer instances. Thank you again, this has been invaluable in confirming and expanding our understanding of CA. Orestis _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Core Animation and scaling images</header>
    <body>The image you provide is always uploaded to the GPU in its original size. Any scaling happens as the image is composited to the screen using the GPUs filtering options. Most of the time it's best to pre- scale the image so that it's close to the size it will be displayed on screen before handing it to CoreAnimation, this can give you higher quality scaling and lower memory requirements. The easiest way to resize an image is to draw it into a suitably-sized layer, this will also ensure the scaled version is in the most optimal format for uploading. Clipping the context you're drawing into will only affect the contents of the bitmap you're drawing as you're drawing it, and so has a one- time cost. Setting masksToBounds and/or the mask property can require clipping to be performed every time the layer is composited to the screen, which can be expensive. (One optimization you can make here is to ensure that your layer's geometry is aligned to whole pixels, in which case masksToBounds uses a cheaper rendering method than when subpixel or non-rectangular clipping is needed) Some basic performance guidelines are to provide as little image data as possible, and to make sure that anything that can be opaque is opaque ‚Äî images shouldn't have alpha channels unless necessary, set the &amp;quot;opaque&amp;quot; property of layers you draw into that don't need alpha.</body>
  </mail>
  <mail>
    <header>Re: Are 2 displays connected to the same video card?</header>
    <body>Indeed. The contexts don't need to be made current, or for that matter you don't need to do much of anything with them and should only need to test them once per display configuration (as per the Core Graphics display reconfiguration event callbacks etc). -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Core Animation and scaling images</header>
    <body>I was wondering whether setting the contents of the layer to a large image and then changing the bounds to something smaller is more expensive than pre-scaling an image to the required size and setting that as contents, explicitly asking to not resize anything. That is, is the image pushed to the GPU memory in its scaled size, or in its original? In a similar vein, does clipping the CGContext or setting the masksToBounds property make any performance difference? We are having performance problems with our CA application (tearing, dropped frames etc) and are wondering what's the best approach to deal with them, given that there's not much information available on CA. Thanks, Orestis -- email@hidden</body>
  </mail>
  <mail>
    <header>Re: Bitmap printing context</header>
    <body>Certainly feel free to draw in multiple bands if you prefer, but if you do that then I would recommend creating a data provider &amp;amp; image for each band rather than doing it one at a time. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Bitmap printing context</header>
    <body>Man, the list has been really slow lately, which gets everybody's replies out of order with reality when they also send replies to the OP. Nevermind this one. I'm just drawing one band instead of many now. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Are 2 displays connected to the same video card?</header>
    <body>So you're saying I do need to create OpenGL contexts, I can't just do some test with the displays? -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: Are 2 displays connected to the same video card?</header>
    <body>You need to check if the two contexts screens share a virtual screen. The OpenGL Programming Guide for Mac OS X discusses this in the Techniques for Working with Rendering Contexts section. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Bitmap printing context</header>
    <body>But the the image drawn for each band is only drawn *once* and then destroyed. It just happens to use the same source pointer for the data provider each time. That's why it seems like a really goofy way to handle &amp;quot;drawing&amp;quot;, pdf or not. It's portable code, but I'll see if we can easily draw in 1 band instead of many when building on Mac. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Bitmap printing context</header>
    <body>When you print, your drawing to a PDF. When you draw into a PDF, the contents you draw are only referenced until the page is completed (an EndPage call is made). As such, the contents will be queried until then. This indicates that your breaking the contract of a CGImageRef (CGImageRefs are immutable and must always vend the same content). As such you've drawn X copies of the same CGImageRef to the page and the PDF then produces X copies going down the page. Might I recommend that instead of doing the banding manually, you just define a single CGImageRef that is the size of the entire page, and then create a data provider that vends the requested pixels? The something like what it sounds like you are doing. It should be conceptually simpler. PS: There is one minor nit that I see in the sample looking at it again - you don't have to provide the full count of bytes that Quartz requests, you can provide fewer as long as you provide more than 0. This might help you make your banding logic a bit simpler. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Bitmap printing context</header>
    <body>Use the page size to get the context's size. The printer context will be setup (by default) to use the page's size in points as its basic coordinate system. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Bitmap printing context</header>
    <body>That's weird. When one draws, one expects the drawing will occur, not some kind of pointer hoarding. How can I make this work correctly, i.e. drawing a CGImage copies the bits instead of holding onto my data provider? _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>RE: Bitmap printing context</header>
    <body>&amp;gt; OK, now here's where it gets weird. If I then fill that Right. It stores a pointer to the CGImage (and provider, etc), which doesn't actually get called (you can check this by hanging breakpoints in your provider) until ClosePage is called. -DaveP</body>
  </mail>
  <mail>
    <header>Bitmap printing context</header>
    <body>Something is really weird here. Our drawing code renders itself to a bitmap in horizontal bands, and then draws each band to the printer's CG context. We have our own internal bitmap class that we render to. The data of this bitmap is then provided to a CGDataProvider, which is used to create a CGImage. That image is then drawn to the print context with CGContextDrawImage, and the image and provider are both deleted. Repeat for each band down the page, reusing the bitmap each time. Follow me so far? Pretty straight forward. OK, now here's where it gets weird. If I then fill that original bitmap with random data, that same random data will end up in the print context in the same location that we had previously drawn the CGImage above. What the hell? It's like the call to CGContextDrawImage did nothing more than add a pointer to our data provider to the print context rather than really copying the bits. Is it retaining the image, data provider, and original bits instead of copying the bits? If so, how can I make it not do that? If I just let the program run naturally without filling the bitmap with random data, what I get is a page filled with the same band over and over, and the band shown at each position is the image that was in the very last band. So if the page being printed had a triangle at the top and an oval at the bottom, I see a bunch of ovals from top to bottom. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Bitmap printing context</header>
    <body>I'm switching our Carbon app to use PMSessionBeginCGDocumentNoDialog instead of PMSessionBeginDocumentNoDialog, and then calling PMSessionGetCGGraphicsContext instead of PMSessionGetGraphicsContext(sess, kPMGraphicsContextQuickdraw, &amp;amp;port). The problem is, I need to flip it to draw top-down, not bottom up. But in order to flip the context, I need its height. How can I do this, since there is no routine for getting a context's size? _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Are 2 displays connected to the same video card?</header>
    <body>I see that I can get a lot of information about displays using CGDisplay APIs and IODisplayCreateInfoDictionary, but is there any way to tell whether two displays are hooked to the same video card?  (Not the same model, the exact same card.) What I ultimately want to know is whether windows on the displays will be able to share OpenGL textures with each other.  So I guess I could just try creating windows and OpenGL contexts sharing texture data, but I thought there might be a nicer approach. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>NSLoging CIContext fixes my problems?</header>
    <body>We have a video capture app that has been working fine for us on our PPC iMac and even on my MacBookPro.  However, when we switched to a new Intel iMac (and also tested on a new Intel Mac Mini), our video capture returns 640x480 blank frames.  The capture app runs without a UI.  After much gnashing of teeth, I found the line: myCIContext = [[CIContext contextWithCGLContext:myCGLContext So I thought &amp;quot;must be a timing problem&amp;quot; and put some sleeps around just to try to make it break or work differently, and it gives me a blank frame every time unless i NSLog the CIContext.  So then I moved the NSLog of the CIContext out of that method entirely and moved it somewhere else (the constructor of the class that this call is in) and that ALSO fixes it. So my question is whether there is anything in NSLog'ing the CIContext that might cause something to be initialized that otherwise might not be initialized when run without a user interface?  This same code when called from a Java application with a user interface DOES NOT fail even without NSLog'ing the CIContext. Thanks alot! ms</body>
  </mail>
  <mail>
    <header>URL vs. CFData For Image Source</header>
    <body>I'm playing with ImageIO and I've found a strange case with a particular Nikon raw image.  Apparently the image file contains a thumbnail image.  When I import the image by creating an image source from a URL I get the normal high resolution image, but when I import the image by first reading it into a CFDataRef and creating my image source from that I only get the thumbnail. Is this a bug or am I doing something stupid?  I've used the same code to import a larger Canon raw image without trouble. Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: CGPathContainsPoint problem?</header>
    <body>On Aug 22, 2006, at 11:48 PM, David Catmull wrote: Several people (myself included) have noticed problems with CGPathContainsPoint returning incorrect results in some cases. In my case, I was able to reduce the number of false reports by testing against the bounding box of the curve first (as I recall) (so if the point is not in the bounding box of the curve, don't bother checking to see if it's inside the curve).</body>
  </mail>
  <mail>
    <header>Re: Quartz for Quicktime Display?</header>
    <body>On Aug 23, 2006, at 12:58 AM, Colin Doncaster wrote: It's probably the best way to do it.  Core Video provides a mechanism for letting you decompress video frames and get them to the video card very quickly.  OpenGL is the lingua franca of the video card and is the tool that lets you actually use those video frames after they've been shipped across the graphics card bus.</body>
  </mail>
  <mail>
    <header>Re: Newbie question about the Quartz Python binding.</header>
    <body>Hello. Ok, I understand that it wasn't very clear what I were trying to do. And it was not my intention to be impolite - sorry if someone got this impression. I have a couple of html pages with a lot of more or less complicated tables (rowspan, colspan, you name it). I would like to convert those tables into PDF. Until now, I am doing this with the PDFlib (PHP binding). Although I feel rather comfortable working with the library, until now PDFlib had very limited support when it comes to tables (with 7.0 it seems to be another story - I haven't had the time yet to test the new version). I was very exited when I saw that there was a Python binding for Quartz, because it seemed to be a very easy way to do this task. import os from CoreGraphics import * from urllib2 import * pageOutline = CGRectMake(0, 0, 612, 792) context = CGPDFContextCreateWithFilename(&amp;quot;/Users/myself/myPDF.pdf&amp;quot;, pageOutline) context.beginPage(pageOutline) test = context.drawHTMLTextInRect(CGDataProviderCreateWithString (html), pageOutline, 12) context.endPage() As long as all CSS is included (copy and paste the CSS into the html source), the page is rendered properly into the output pdf - which means that I am able to set the font, row lines, background color of the table and so on with the style sheet. But when I link the CSS to the html, the CSS file is not recognized/included - neither are linked graphics, such as background pic or any other linked graphics by the way. Quartz is able to render html pages (because &amp;quot;drawHTMLTextInRect&amp;quot; is part of CoreGraphics, which is part of Quartz - as far as I understand). My problem is: How can I get the script to include linked CSS and linked graphics when rendering the html page? When I open the linked CSS document with &amp;quot;css = urlopen(&amp;quot;http:// 127.0.0.1/test.css&amp;quot;).read()&amp;quot;, include it with &amp;quot;test = content.drawHTMLTextInRect(CGDataProviderCreateWithString(css), pageOutline, 12)&amp;quot; I only get plain text from the CSS. Scott Thompson suggested using WebView in order to render the html page and dump the output to a pdf. Are there any other alternatives? I hope that I was able to explain my question more precisely now for you guys. With best regards and thank you for your time, /frank arensmeier 21 aug 2006 kl. 18.36 skrev Jeffrey Oleander:</body>
  </mail>
  <mail>
    <header>Quartz for Quicktime Display?</header>
    <body>I'm starting an app that will be displaying quicktime movies in a custom view, I noticed that a lot of the quartz CoreVideo apps that utilize quicktime actually use an NXOpenGLView and opengl for drawing the quicktime video as a texture to a square poly.  Is this the standard, quickest/best way to draw/display the current quicktime frame or was that chosen for other reasons?  Would it be better to use Quartz class for this? Thanks.</body>
  </mail>
  <mail>
    <header>CGPathContainsPoint problem?</header>
    <body>I'm working on an open source Cocoa SVG-based drawing app (see http:// xvg.sourceforge.net for details - developers wanted!), and at the moment I'm having a problem with hit testing on my shapes. It works on one computer but not on the other, and the call I'm using is CGPathContainsPoint. The computer where it works is a PowerBook on 10.4.7. The one where it fails is a G4 also on 10.4.7 (I have since installed the Leopard seed on the G4, but the failure predates that so I'm tentatively ruling that out as a cause). I tried a fresh checkout of the code, found that it still fails on the G4, copied the project folder to the PowerBook, and saw it succeed there. Of course I can't say for sure that my problem lies in my use of CGPathContainsPoint, but I'm running out of ideas. If you're willing to have a look at the code, the offending method is -[XVGShape hitTestImpl:inContext:] -- David Catmull email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGContextDrawImage error catching</header>
    <body>On 15 Aug 2006, at 20:12, Marc Van Olmen wrote:</body>
  </mail>
  <mail>
    <header>Re: vImage - Histograms - beginning</header>
    <body>On Aug 21, 2006, at 11:00 AM, George Warner wrote: Just a &amp;quot;me too&amp;quot; agreeing with George. When I've needed help with vImage I've gotten it on the performance list: Scott</body>
  </mail>
  <mail>
    <header>re: vImage - Histograms - beginning</header>
    <body>Your issue isn't Quartz related... I'm guessing you probably want the performance mailing list. If nothing else I know that several of the Accelerate engineers hang out on that list. This shouldn't be a pointer (you just pass it by address). It should be a record with four values: the address of your data, its height &amp;amp; width and stride (bytes per row). 3rd parameter should be 256. -- Enjoy, George Warner, Schizophrenic Optimization Scientist Apple Developer Technical Support (DTS)</body>
  </mail>
  <mail>
    <header>Re: Newbie question about the Quartz Python binding.</header>
    <body>a) stupid (go, get and read &amp;quot;Quartz for beginners like you&amp;quot;) b) ignorant (RTFM) c) impossible? I am desperately trying to learn more about Quartz. I would appreciate if someone could share a litte amount of your knowledge with me. regards, /frank</body>
  </mail>
  <mail>
    <header>vImage - Histograms - beginning</header>
    <body>I hope that is the right list for vImage related stuff, if it isn't maybe someone can point me in the right direction. I am an extreme beginner in this... I have a question about the proper care and handling of vImage_Buffer, In my vImage_Buffer I have a single channel image of type Planar_F histogram calculation (hence using vImage) of this image(localvIMG). All I get are problems. vImage_Buffer *localvIMG; //initializers etc... the buffer does contain data since I am able to display it via CGImageRef status = vImageHistogramCalculation_PlanarF((vImage_Buffer for ( i = 0; i &amp;lt; 256; i++) the status returns okay but... on the printf the hist values are all zero. It all seems pretty straight forward.  Can anyone enlighten me on the use of vImageHistogramCalculation_PlanarF? thanks -- Bruce Johnson email@hidden</body>
  </mail>
  <mail>
    <header>Obtaining the color profile of a CGImage</header>
    <body>I have a CGImageRef obtained from QuickTime (i.e., GraphicsImportCreateCGImage).  I want to display the name of the color profile and color model (i.e., RGB, CMYK, etc.), but I can't find a way to get the color profile back from a CGColorSpaceRef.  The closest I've been able to get is to call CFCopyDescription() for the colorspace object.  This sort of gives me some information but it's formatted more for debugging than for user information. Is there some way to get the color profile or its name from a CGImageRef or its CGColorSpaceRef? Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <body>On 18 Aug 2006, at 16:18, Shawn Erickson wrote: rdar://4688040</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <body>On Aug 18, 2006, at 7:21 AM, Jerry wrote: vImage provides channel permute operations. So you can convert as needed (of course that comes with a cost). It would be helpful if vImage could work with BGRA, etc.... I suggest folks file an enhancement request with Apple (may get it in time for Leopard if folks file now). -Shawn</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <body>On 18 Aug 2006, at 15:08, Steve Mills wrote: For most operations this will be true, but for some it won't e.g. the matrix multiply operators where you'd have to create a different matrix. Many of the operations only seem to support ARGB though, so there's not even the option of XXXA. This seems to be a bit of a gap and I was wondering if I'd misread the documentation and there was some hidden flag equivalent to kCGBitmapByteOrder32Host in Quartz. All our images are in host byte order (i.e. ARGB on PPC, BGRA on Intel).</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <body>I can't imagine that it would matter which order the rgb fields are in, as long as they're in the correct XXX- or -XXX end of the 32- bits. I'm pretty sure that the algorithms work on each channel individually and one doesn't affect the other. So it doesn't matter if byte 0 is red, blue, or green. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <body>On 17 Aug 2006, at 22:41, Shawn Erickson wrote: Glancing at the vImage documentation, it appears that vImage handles ARGB or RGBA data, but I couldn't see any mention of BGRA data as you'll get on Intel Macs if you keep your data in native order. Is this true or can it handle BGRA data?</body>
  </mail>
  <mail>
    <header>Newbie question about the Quartz Python binding.</header>
    <body>Actually, I have just begun to understand the basic concept and possibilities of the Quartz engine (I am reading &amp;quot;Quartz 2D for Mac OS X Developers&amp;quot; by Scott Thompson right now). Therefore, I would like to apologize if my questions are obvious for you (sorry for my bad English as well). I am thinking about to integrate some Quartz functionality as a web- service (converting HTML pages to PDF) and the Python binding seems to be the path to go. I have seen that a data provider for generating PDF documents is able to accept common http URLs. Further more, when I include some CSS in the source document, I am also able to give the content some style attributes like background color, position and so on. So, obviously, the HTML page gets rendered when it is provided as a data provider. My problem for now is that I am not able to get linked images or linked CSS documents to be rendered as well. Is it possible to have several different data providers and combine them together? My idea is to parse a HTML document, look for linked files (CSS, images and so on) and add them to the originally provided HTML source. Is this possible? Actually, I have seen a Python script on the Internet that converts HTML pages into PNG images. As far as I can understand, the script uses webkit and some special C binding for Python. It renders the HTML document with webkit and outputs a PNG image. The problem is that this script also uses a newer version of Python which is not supported by the Quartz binding. So, the script will not be able to output a PDF document. If someone could point me to the right direction or could give me some links to relevant documentation for example, I would be very grateful. With best regards, /frank arensmeier</body>
  </mail>
  <mail>
    <header>Re: CGWaitForScreenRefreshRects and CGWaitForScreenUpdateRects</header>
    <body>Well I got them to work! My test code was in a foundation only tool without an NSApplication. Once I use NSApplicationLoad() events fire like mad. The non-functioning makes sense given my testing tool had no connection to the window server. To bad that the functions don't return an error when they find themselves in that situation. -Shawn</body>
  </mail>
  <mail>
    <header>CGWaitForScreenRefreshRects and CGWaitForScreenUpdateRects</header>
    <body>I am playing around with alternate ways to get information about screen updates from CG services so I decided to see how CGWaitForScreenRefreshRects and CGWaitForScreenUpdateRects work. So far when I call either of those methods they block indefinitely. I assume I need to enable them somehow so that the CG services 1) knows that I want it to watch for screen updates on my behalf (otherwise I don't see how they would work without missing updates) and 2) get them to return in some reasonable amount of time. I know I can use matching callbacks but I want to understand what I may be doing wrong with these APIs... and if I am not doing something wrong file a defect or two about them. Note I have tried with and without my main thread running a runloop with not change in behavior. Thanks, -Shawn System Version: Mac OS X 10.4.7 (8J135) Kernel Version: Darwin 8.7.0 Machine Name: Power Mac G5 Quad Machine Model: PowerMac11,2 CPU Type: PowerPC G5 (1.1) Number Of CPUs: 4 CPU Speed: 2.5 GHz L2 Cache (per CPU): 1 MB Memory: 2 GB Bus Speed: 1.25 GHz Boot ROM Version: 5.2.7f1 Chipset Model: GeForce 6600 Type: Display Bus: PCI Slot: SLOT-1 VRAM (Total): 256 MB Vendor: nVIDIA (0x10de) Device ID: 0x0141 Revision ID: 0x00a4 ROM Revision: 2149 Displays: Cinema HD Display: Display Type: LCD Resolution: 1920 x 1200 Depth: 32-bit Color Core Image: Supported Main Display: Yes Mirror: Off Online: Yes Quartz Extreme: Supported Cinema Display: Display Type: LCD Resolution: 1680 x 1050 Depth: 32-bit Color Core Image: Supported Mirror: Off Online: Yes Quartz Extreme: Supported</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <body>On Aug 17, 2006, at 3:22 PM, Steve Mills wrote: There was some discussion about this on the SciTech mailing list about a year ago.  The short answer is to just do (alpha * channel + 127) / 255.  GCC is smart enough to optimize division by numbers close to a power of two into just a modified multiply followed by a shift.  It turns out to be pretty fast.  (You can't fake it in C because you really need just the mulhw (multiply high word) instruction, so you have to write / 255 and hope that your compiler can do the right thing.) Brendan</body>
  </mail>
  <mail>
    <header>Re: Re: image byte order problem</header>
    <body>vImage will utilize AltiVec/SSE/etc, memory stream hinting, and even multiple threads (tiling) if multiple cores exist to carry out its various operations. Apple hand tunes it for the hardware they support. -Shawn</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <body>Why does it not make sense? I have a CGBitmapContext with kCGImageAlphaFirst data, and I want to create a CGImage that *needs* to be kCGImageAlphaPremultipliedFirst (because the Drag Manager only wants premultiplied data). I said it's too bad there is a version of CGImageCreate that takes the CGBitmapData AND a target alpha type that would cause the function to do the math on the bits and end up with premultiplied data. That makes perfect sense to me. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <body>On Aug 17, 2006, at 1:22 PM, Steve Mills wrote: First, you should make sure that both formulas give you correct results ‚Äî they don't look equivalent to me.  Second, when it comes to speed, opinions are irrelevant.  The only way to find out the truth is to measure the two cases and compare the results.</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <body>Re: On Aug 17, 2006, at 11:42 AM, Steve Mills wrote: If your bits were created with non-premultiplied data and the first byte is the alpha data then you should create your CGImage using kCGImageAlphaFirst. Quartz can handle image data with alpha that is either premultiplied or not premultiplied, you just have to tell it which type you are supplying. Re: I don't see that this makes any sense. When you create a CGImage object using CGImageCreate you tell Quartz what type of alpha data you are supplying, premultiplied or non-premultiplied. If you are creating the CGImage yourself then you should specify the correct alpha info type. There should be no reason to change it.</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <body>I know nothing about the efficiency of math operations and bit manipulation. Here's a formula I took from a 3rd party graphic lib (32-bit data): Anybody have any opinions on which is faster? I would guess that the bit shift would be faster than the division. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <body>The CGBitmapContext is not drawn into with Quartz at all, but with a 3rd party graphics engine (we're cross-platoform), so the data really does contain relevant values in the alpha channel. Yes, it *should* have been created with kCGImageAlphaFirst, but it wasn't and it doesn't matter. I mean the that you give it an alpha info of what the source bits are, and if the alpha info for the CGImage to be created is premultiplied and the source bits are not premultiplied, then CGImageCreate would do the math for you. That wouldn't help with what I suggest, because it doesn't allow you to change the alpha info type of the resulting CGImage. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <body>Re: On Aug 17, 2006, at 6:55 AM, Steve Mills wrote: If I understand you correctly, you are creating your data by drawing to a bitmap context that was created with kCGImageAlphaNoneSkipFirst and are creating a CGImage from those bits but using a BitmapInfo value of kCGImageAlphaPremultipliedFirst when you create the image. This makes no sense to me. I would expect you to create a CGImage from those bits using kCGImageAlphaNoneSkipFirst since that is how you generated them. If you do want to capture alpha in the bits that you want to create an image from then I suggest you use kCGImageAlphaPremultipliedFirst when you create your bitmap context, clear the bits using CGContextClearRect prior to drawing to them and then when you create your CGImage from those bits you should supply kCGImageAlphaPremultipliedFirst as the BitmapInfo value passed to CGImageCreate. Re: I'm not sure what &amp;quot;currentAlphaType&amp;quot; means. However you can call CGBitmapContextGetBitmapInfo on a Bitmap Context and get the CGBitmapInfo value that you used to create the context. That would be the correct value to use if you were creating an image from that context. Note that in Mac OS X v10.4 and later, Quartz has the function CGBitmapContextCreateImage which creates a CGImage from the bits in a bitmap context. David</body>
  </mail>
  <mail>
    <header>Re: How to render CIImage into CGBitmapContext or NSBitmapImage</header>
    <body>On Aug 17, 2006, at 9:26 AM, Jaret Hargreaves wrote: To render a CIImage you have to draw it into a CIContext.  You can create a CIContext from a CGContext (analogously to how you create a CIImage from a CGImage).  So to render into a CGBitmap you first have to allocate memory for the backing store, create the CGBitmap, create a CIContext from the CGBitmap, and render the CIImage into the CIContext. Nick</body>
  </mail>
  <mail>
    <header>Re: Re: image byte order problem</header>
    <body>For your pre-multiply operation consider (it will ensure it is fast on the hardware you are on)... -Shawn</body>
  </mail>
  <mail>
    <header>How to render CIImage into CGBitmapContext or NSBitmapImage</header>
    <body>Using Core Image, I've been able to render the result of an image filter I wrote.  For the purposes of post-processing, however, I need to return the resultant image to python.  To do this, I need to generate a CGBitmapContext or NSBitmapImage to return the filtered image. How do I render the product of the CIImage into either a CGBitmapContext or NSBitmapImage?  Any suggestions or pointers to example code (ex. from the ADC sample code base) would be much appreciated. -Jaret</body>
  </mail>
  <mail>
    <header>Re: image byte order problem</header>
    <body>That did the trick. Thanks for the confirmation. Too bad CGImageCreate doesn't take a currentAlphaType so it can be smart enough to do the math itself. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>RE: image byte order problem</header>
    <body>Even I have faced the same kind of problem while implementing the Opacity filter. The problem was when the image is semi transparent the image colors appears to be distorted. The problem got solved when I pre-multiplied the components... But I am not sure how not premultiplying them would make only the last channel come out wrong. -----Original Message----- From: quartz-dev-bounces+vinayprabhu=email@hidden [ n Behalf Of Steve Mills Sent: Thursday, August 17, 2006 7:26 PM To: email@hidden quartz-dev Subject: image byte order problem I recently noticed that our drag image looks wrong. A few months ago I made some changes to ensure that we're creating it correctly for PPC and Intel, and it was working correctly on both. When we create our internal CGBitmapContext, the alpha type is kCGImageAlphaNoneSkipFirst. When we create the CGImage to give to the Drag Manager, the alpha type is kCGImageAlphaPremultipliedFirst, because that's what I was told is the proper type to use. I do a straight byte copy, so the colors really are NOT premultiplied. The problem shows up on PPC and Intel when any part of the bitmap has visible transparency (alpha 1-254) and some color. If it's opaque, the color looks correct. The case I noticed involves a transparent blue area (argb = 0x7f ee ee ff), but it shows up as what I guess is 0x7f ee ee 7f. I say &amp;quot;guess&amp;quot; because I brought up a color picker and found a color that looks close to what I see when I drag the transparent blue object. As an experiment, I created the CGImage with kCGImageAlphaFirst instead, and this caused it to show up correctly (again, on both architectures). So it *appears* that the last channel value is being replaced with the alpha channel value, perhaps in CGImageCreate. I don't know if I previously tested the transparent blue case back when I was initially getting this to work on PPC and Intel. Can anyone tell me if anything was changed in 10.4.7 that would cause this? Or am I doing something wrong by not manually premultiplying the colors when creating the CGImage? I don't see how not premultiplying them would make only the last channel come out wrong. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>image byte order problem</header>
    <body>I recently noticed that our drag image looks wrong. A few months ago I made some changes to ensure that we're creating it correctly for PPC and Intel, and it was working correctly on both. When we create our internal CGBitmapContext, the alpha type is kCGImageAlphaNoneSkipFirst. When we create the CGImage to give to the Drag Manager, the alpha type is kCGImageAlphaPremultipliedFirst, because that's what I was told is the proper type to use. I do a straight byte copy, so the colors really are NOT premultiplied. The problem shows up on PPC and Intel when any part of the bitmap has visible transparency (alpha 1-254) and some color. If it's opaque, the color looks correct. The case I noticed involves a transparent blue area (argb = 0x7f ee ee ff), but it shows up as what I guess is 0x7f ee ee 7f. I say &amp;quot;guess&amp;quot; because I brought up a color picker and found a color that looks close to what I see when I drag the transparent blue object. As an experiment, I created the CGImage with kCGImageAlphaFirst instead, and this caused it to show up correctly (again, on both architectures). So it *appears* that the last channel value is being replaced with the alpha channel value, perhaps in CGImageCreate. I don't know if I previously tested the transparent blue case back when I was initially getting this to work on PPC and Intel. Can anyone tell me if anything was changed in 10.4.7 that would cause this? Or am I doing something wrong by not manually premultiplying the colors when creating the CGImage? I don't see how not premultiplying them would make only the last channel come out wrong. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>CGContextDrawImage error catching</header>
    <body>My applications opens up large PSD files (1.5GB big) Before my code was using a QuickTime Graphics Importer and when the GraphicsImportDraw run out of memory I got an error back and did the necessary steps to open these files different. Now that I converted my code to ImageIO I notice that CGContextDrawImage  doesn't return an error and the image that it is drawn is black and in the console I notice something like this: iProcessor2(7251,0x130b9e00) malloc: *** vm_allocate(size=652402688) failed (error code=3) iProcessor2(7251,0x130b9e00) malloc: *** error: can't allocate region iProcessor2(7251,0x130b9e00) malloc: *** set a breakpoint in szone_error to debug iProcessor2(7251,0x130b9e00) malloc: *** vm_allocate(size=326176768) failed (error code=3) iProcessor2(7251,0x130b9e00) malloc: *** error: can't allocate region iProcessor2(7251,0x130b9e00) malloc: *** set a breakpoint in szone_error to debug Is there a way I can catch these errors? Is there a function that i can use that gives me back something like global errors like in the QuickDraw days. ? I also use cocoa exceptions but they don't seem to help in this case. marc</body>
  </mail>
  <mail>
    <header>Re: Getting pixel values from CIImage processed data</header>
    <body>You need to create a CIContext out of a CGBitmapContext. Then you draw in the CIContext and just get the bytes in the array you allocated. Here's basically what I use (might not compile, just copy/pasted and removed the useless details). Of course if you want floating point data you'd need a 'workingPixelData' buffer in float*. CGColorSpaceRef colorSpace = CGColorSpaceCreateWithName if (bitsPerComponent==32) bitmapInfo = kCGImageAlphaPremultipliedLast | CGContextRef workingCGContextRef = CGBitmapContextCreate((unsigned char*)workingPixelData,w,h, bitsPerComponent,bytesPerRow, CIContext* cicontext = [CIContext contextWithCGContext:workingCGContextRef  options:[self //and just draw in the context [cicontext drawImage:[filterBlack valueForKey:@&amp;quot;outputImage&amp;quot;] ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Getting pixel values from CIImage processed data</header>
    <body>Thanks to some great feedback, I am closer to solving the problem of returning pixel values from the result of a CIImage. I have little experience in ObjC and would appreciate any suggestions for how to solve the following problem or where example code could be found (I have all the ADC sample code). 1. How do I render the result of a CIImage into a CGBitmapContext? The constructor for this class listed in the ADC docs have it's contents listed, but how the arguments are passed is not clear to me. 2. Once data has been retrieved as an NSData object from the CGBitmapContext, how can it be converted to a string for post-processing? -Jaret</body>
  </mail>
  <mail>
    <header>Saving results of CI filter</header>
    <body>I am performing image filtering in a python framework by calling the objC method listed below.  Currently, the python code converts the image slice data into a raw string(&amp;quot;data&amp;quot;), which is handed to the method, and expects a modified string of the same length to be returned. I know that CIImage is a recipe, not actually image data. How do I save the processed image data (represented by &amp;quot;outputImage&amp;quot;, a CIImage) to return back to python?  Essentially, how can I reconstitute a string of processed output after filtering (for use when comparing pre- and post-filtered data in python)? Thanks in advance for any help.  Any suggestions of a simpler or more elegant solution are also welcome. ***ObjC filtering method*** - (id) initWithData:(char*)data xDim:(int)x yDim:(int)y bytesPerPixel:(int)pixelSize k:(float)filterParam iter:(int)iterations // Copy the image into inputData via memcpy // Create a CIImage from inputData for processing NSData *imageData = [NSData dataWithBytes:(char*)inputData length:(x // Initialize the aniso filter [anisoFilter setValue: [NSNumber numberWithFloat: filterParam] // Perform first iteration of filtering // Perform subsequent iterations of filtering as needed for(i=0;i&amp;lt;(iterations-1);i++) // How do we get CIImage &amp;quot;outImage&amp;quot; back into NSData &amp;quot;inputData&amp;quot;? //inputData = // Copy the filtered image directly back to data</body>
  </mail>
  <mail>
    <header>Re: Draw JPEG2000 images into CG PDF contexts</header>
    <body>Yes, Image I/O is fine, but I need the image Jpeg2000 encoded for &amp;quot;highly compressed&amp;quot; PDFs. Using bitmap data in a cgImage it will only be zlib compressed as far as I can see. -- Rene Rebe - ExactCODE - Berlin - Europe, Germany +49 (0)30 / 255 897 45</body>
  </mail>
  <mail>
    <header>Re: Draw JPEG2000 images into CG PDF contexts</header>
    <body>On Aug 9, 2006, at 6:18 AM, Ren√© Rebe wrote: You should be able to use the functionality in Image I/O to read a JPEG 2000 image into a CGImage.  Once you have done so, you can draw it into any CGContext you like, including a CGPDFContext. CGImageSource/index.html CGImageDestination/index.html If you don't want to use Image I/O (because you need to run on pre- Tiger systems) then you can also try using the QuickTime Image Importers and create a CGImage that way.  I don't know for sure that there's a JPEG 2000 importer, but it's a good bet.</body>
  </mail>
  <mail>
    <header>Using PDFKit to move page from a document to another</header>
    <body>Hi, i'm trying to use PDFKit to make an application which also allows importing pages from other documents. My problem is that even if i managed doing the import work without crashing, sometimes it just insert a white page, more little than the others and sometimes it insert the page, bigger than the others and all the ones of the original document are made blank. I guess this happens when the documents are not of the same size, but i cannot be completely sure of this either. What could i use to adjust the page to the receiving document before inserting it? I tried the setBoundsForBox: of NSPage passing by setting the NSRect size to the one of a page of the receiving document. I hope i was clear explaining my problem. Anybody knows a workaround? Attachment:</body>
  </mail>
  <mail>
    <header>Draw JPEG2000 images into CG PDF contexts</header>
    <body>I wonder if there is a possibility to draw JPEG2000 data into CG PDF contexts or if we have to wait for a JPEG2000DataProvider to be able to do so? -- Rene Rebe - ExactCODE - Berlin - Europe, Germany +49 (0)30 / 255 897 45</body>
  </mail>
  <mail>
    <header>Missing fonts</header>
    <body>I am not sure this is the right list to ask this question, so pardon me if this should be asked elsewhere. Where should I look if I want to know if an NSTexView's content (loaded from RTF) requires a font that is currently not available? ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <body>On 7 Aug 2006, at 16:13, Nick Nallick wrote: Oh yes. I was thinking of Java2D instead of Quartz.</body>
  </mail>
  <mail>
    <header>Re: Can CGImageDestination write 16-bit per component TIFF files?</header>
    <body>I've had the same problem, which I also reported here, and Johannes Fortmann too... In case you're interested, our respective messages are archived there: I've personally filed a bug (rdar://problem/4304932) but it apparently hasn't been touched yet. My hope is that this problem will be solved by a software update before Aperture's release (10.4.3?). I can't imagine Aperture being limited to 8 bits per channel (although Aperture's page says absolutely nothing about bit depth, which I find ... surprising). Michel. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Can CGImageDestination write 16-bit per component TIFF files?</header>
    <body>I built an image batch processing app using CGImageDestination to output files. Unfortunately documentation for this powerful (Tiger only) function is almost non-existent. Lots of trial and error on my part, but I am stuck with these problems: - How can I output 16-bit per component TIFF files with CGImageDestination? Seems to work fine when using CGImageDestinationAddImageFromSource  (and the source file of course is a high-bit TIFF). This allows me to set some image properties (metadata) but image data itself remains untouched. If I run that same source file through some Core Image Filters and draw it to a CGBitmapContext I end up with 8 bits or 32 bits per channel, depending on how the context was set up. No 16-bit data in sight ... - Is there a way for CGImageDestination to NOT recompress JPEG image data when only metadata properties need to be changed? Thanks for any suggestions! Jojo ______________________________________________________________ Verschicken Sie romantische, coole und witzige Bilder per SMS! Jetzt bei WEB.DE FreeMail:</body>
  </mail>
  <mail>
    <header>Re: Caching of CGImage</header>
    <body>On Oct 24, 2005, at 4:16 PM, email@hidden wrote: CGDataProvider doesn't allow me to do background update of the image for I have no control over when it's used. Also, the image size is big as in 100K pixels in width and height. Quartz seems to like all the bits in memory when creating the image. My data provider callback is then ask to fill a buffer several GB in size. Needless to say, it dogs the system to an unusable state. --</body>
  </mail>
  <mail>
    <header>Caching of CGImage</header>
    <body>I am moving a QuickDraw-based image file display application to Quartz. I used to read in one scan line of data into a Bitmap then copy it to the scaled destination Bitmap. This way, QD does the scaling and anti-aliasing for me in at least the horizontal direction. Now I am trying to do the same thing in Quartz with CGImage and friends. I find that the CGImage I use as scan line buffer is cached so the same single-line image is copied over and over again to the whole destination context. I don't want to create and destroy the scan line CGImage for each lines I read for it's really, really slow that way. So is there a way to tell Quartz not to cache a CGImage? BTW, the image file is in our own private format, so I don't think the new CGImageSource thingy will work. --</body>
  </mail>
  <mail>
    <header>Re: What does the returned value of CGDataProviderGetBytesAtOffsetCallback do?</header>
    <body>On Oct 21, 2005, at 11:52 AM, Stephen Chu wrote: I can only guess that it will call back again for more data, trying to get the data you didn't provide on the first pass. I believe it only stops when you return 0. Of course that is me just reading the tea leaves provided in the documentation... and you have the edge case of being asked for data yet have none available just yet but returning zero is out since that stops things. -Shawn</body>
  </mail>
  <mail>
    <header>What does the returned value of	CGDataProviderGetBytesAtOffsetCallback do?</header>
    <body>I am curious. What does the return value of CGDataProviderGetBytesAtOffsetCallback do? If I can't provide as many bytes as requested in the callback, what will happen? --</body>
  </mail>
  <mail>
    <header>Re: Using a CGDataProvider for my own purposes.</header>
    <body>Both of these are close, but they are more likely to wind up being a CRC of the &amp;quot;image data as drawn&amp;quot; (or in the PDF case potentially the image data as drawn and then compressed), rather than the image data that is the source of the image. For example, in this particular problem, if I use a JPEG data provider then I want to CRC the JPEG data, not the image that data describes.   It's a very unusual thing to want to do, I'll admit. Option 1 would also CRC the PDF information that surrounds the image... things like the headers, and the PDF catalog.  That might not be a bad thing... but it is a thing :-) I probably will. Since I'm creating my CGImages as part of a larger C++ class, what I've done is capture the data source when the C++ class is created and I'm running the CRC on the data then, rather than waiting until after it has been encapsulated in a CGImage.  This solves my particular need... but I agree with you that being able to encapsulate filters for data providers/consumers as custom data providers/consumers would be a handy feature in some instances.</body>
  </mail>
  <mail>
    <header>Re: Using a CGDataProvider for my own purposes.</header>
    <body>On Oct 21, 2005, at 8:06 AM, Shawn Erickson wrote: So after a little more coffee I can think of two ways (out of likely a few more)... 1) Create a data consumer that does your CRC (doesn't even have to store the bytes passed in, etc.). Then uses this data consumer to create PDF context (CGPDFContextCreate) with some reasonably sized media box. Then focus that context, draw your image into it, and flush/close the contex. 2) Create a bitmap image context (CGBitmapContextCreate) of reasonable size (doesn't have to be the fully image likely) and backed by a buffer of larger enough size. Then focus that context, draw your image into it, and flush/close the context. Then do your CRC. Option (1) likely avoids to much additional memory allocation that is implied in option (2) but both incurs a copy and image transformation. It would be nice to be able to more directly chain a data provider to a data consumer and/or access the data consumer directly (of course concurrent access of a data consumer is an issue) when all you care about is the bytes not exact what it represents (bitmap, vector, etc.). It would also be nice to be able to chain data providers together, including for ones you didn't author (implies you need a stable way to access a data providers callbacks). Consider filing an enhancement request. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Using a CGDataProvider for my own purposes.</header>
    <body>On Oct 21, 2005, at 8:06 AM, Shawn Erickson wrote: Of course my suggestion still has you fighting with the fact that a data consumer reference is basically opaque, so looking past that isn't likely the best thing. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Using a CGDataProvider for my own purposes.</header>
    <body>On Oct 21, 2005, at 7:23 AM, Scott Thompson wrote: Off the top of my head... why not define your own data providers that wraps a system provided data provider. You basically would pass calls into the real data provider doing your CRC calculations with the bytes it returns. See CGDataProviderCreate and the various callbacks you would implement and provide to that method. Of course use of a data consumer may also make sense... don't know how you are working with the CGImages. -Shawn</body>
  </mail>
  <mail>
    <header>OpenGL, CI, and transparency</header>
    <body>I've got a project that, simply put, renders CIImages to a OpenGLView. I'd like to replace some pixels on with transparent ones.  So, a white pixel is fully transparent, etc.  I'd like to do this with as little manual pixel tweaking as possible. Here is what I've done so far. 1) overriden isOpaque to return NO in the view 2) [[self openGLContext] setValues:&amp;amp;notOpaque forParameter: NSOpenGLCPSurfaceOpacity]; // to let clearcolor render clear What I'd like is to have white pixels rendered as fully transparent. I've played with some CI filters.  It seems the color effects filters might be what I want (map white -&amp;gt; clear).  But I saw no documentation on using these.  Before I experiment too much solo, I'd like to make sure I'm going about this the best way. As a more general question, I'd like to know how closely vImage and CI are integrated.  I'd like to do a convolution operation on some frames.  CI doesn't seem to offer an emboss filter right now, but vImage does.  I'm not too familiar with the nuts and bolts of the two frameworks; would doing a convolution using vImage require graphics card system memory copying?  Should I just shoot for writing a custom CIFilter?</body>
  </mail>
  <mail>
    <header>Re: Transforming 16 bits images with CoreImage</header>
    <body>Am 13.10.2005 um 14:56 schrieb Michel Schinz: This looks a lot like my recent problem (&amp;quot;CGContext munging hdr images?&amp;quot;). For the record, I couldn't find a problem with my code, and filed a bug report. the more, the merrier. Greetings, Johannes Fortmann</body>
  </mail>
  <mail>
    <header>Transforming 16 bits images with CoreImage</header>
    <body>I'm having troubles when trying to transform 16 bits images with CoreImage, as I can't get a 16 bits image as a result. I have written the following test program, which opens an image using ImageIO, passes it through the CIColorControls filter using CoreImage, and saves the result using ImageIO again. I observe the following behaviour: if I give as input a 16 bits image and leaves the filter with its default configuration (which does not change the image), I get back a 16 bits file, as expected. However, as soon as I try to set a filter parameter to a non-default value (i.e. I try to increase the contrast using a value different than 1.0 for the &amp;quot;inputContrast&amp;quot; parameter), I get an 8 bit file. Therefore, it seems to me that CoreImage cannot transform 16 bits images without downgrading them to 8 bits. Am I correct? And if I am, is it actually possible to do something about that and obtain a 16 bits image as a result? Here is my sample code (I'm using a 3*1 pixels image as a test, which explains the various constants appearing here): CGColorSpaceRef cs = CGColorSpaceCreateWithName NSDictionary* opt = [NSDictionary dictionaryWithObject:cs // the following line forces a transformation of the image, which brings it down to 8 bits [flt setValue:[NSNumber numberWithFloat:1.1] CGContextRef bmContext = CGBitmapContextCreate(NULL, CIContext* ctx = [CIContext contextWithCGContext:bmContext options: [NSDictionary dictionaryWithObject:cs CGImageRef cgImg = [ctx createCGImage:[flt CGImageDestinationRef dst = CGImageDestinationCreateWithURL Thanks, Michel.</body>
  </mail>
  <mail>
    <header>Re: Trying out CGEventTapCreate, but it appears to be broken!</header>
    <body>Thanks a lot for your help!  This fixed the problem like a charm.  (I'm not nearly as familiar with CoreFoundation apps as I am with Carbon and Cocoa.) On Sun, 09 Oct 2005 13:00:46 -0400, Mike Paquette &amp;lt;email@hidden&amp;gt; wrote:</body>
  </mail>
  <mail>
    <header>Re: Trying out CGEventTapCreate, but it appears to be broken!</header>
    <body>On Oct 9, 2005, at 2:49 AM, Brian Kendall wrote: A listen-only event tap listens to events, but does not filter them. Without the kCGEventTapOptionListenOnly option, the event tap acts as a filter, which can effectively stall the flow of events through the system unless it acts on and processes events promptly.  At the very low level that event taps operate at, the system guarantees that events will be delivered sequentially.  (We wouldn't want that modifier key for your mouse click to arrive after the click, right?) The symptoms you are describing are all indicative of the tapping application not yet actually listening for events on the event tap's CFMachPortRef.  In your source code, nothing was done to actually place the tap's CFMachPortRef in the Carbon MainEventLoop. The CGEventTap code doesn't do this automatically, because there may be many different kinds of run loops in applications.   In general, for fitering event tap applications, I recommend running the event tap filter in it's own thread, with it's own CFRunLoop, rather than in the main application thread.  Trying to filter all the system's events in the main application thread effectively throttles the flow of events through the  system to the rate at which your application can process, draw, and fetch the next event. So far, so good.  You have an event tap port.  Now you need to turn it into a runloop source, and in this case, add it to the runloop behind the Carbon main event loop.  Note that I normally don't recommend doing this, as it will throttle the flow of events to the rate at which the main loop can process and draw.  In this case, the main event loop will just be servicing the CGEventTap, so we might be able to get away with this. if ( eventSrc == NULL ) // Get the CFRunLoop primitive for the Carbon Main Event Loop, and add the new event souce CFRunLoopAddSource(GetCFRunLoopFromEventLoop(GetMainEventLoop if ( eventSrc ) In a command line tool, faceless app, or daemon, you can do something similar purely at the CoreFoundation level: CGEventRef printEventCallback(CGEventTapProxy proxy, CGEventType type, CGEventRef event, void *refcon) printf( &amp;quot;Got event of type %d\n&amp;quot;, type ) int main(int argc, char ** argv) eventPort = CGEventTapCreate(kCGSessionEventTap, kCGHeadInsertEventTap, kCGEventTapOptionListenOnly, CGEventMaskBit(kCGEventOtherMouseDown), printEventCallback, if ( eventPort == NULL ) if ( eventSrc == NULL ) if ( runLoop == NULL )</body>
  </mail>
  <mail>
    <header>Re: Trying out CGEventTapCreate, but it appears to be broken!</header>
    <body>I would suspect that this API is the core of how GetEventMonitorTarget () works, as such I bet you won't get anything unless Access for Assistive Devices is turned on. But if you are just trying to monitor keyboard &amp;amp; mouse actions while not frontmost, just use GetEventMonitorTarget() with your normal Carbon Events code and your life will be simpler. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Trying out CGEventTapCreate, but it appears to be broken!</header>
    <body>I just recently discovered the new CoreGraphics events API that are in Tiger, and in particular I'm interested in event taps.  So far they're undocumented, but there was enough information in /System/Library/Frameworks/ApplicationServices.framework/Frameworks/CoreGraphics.framework/Headers/CGEvent.h for me to get the basic jist of it. So I tried throwing together a quick Carbon app that would make an event tap that outputs a message when it's triggered (for say, pressing the middle mouse button) but it doesn't work.  If I set the event tap to be a listener only, everything on my computer behaves normally but no event is ever received by the tap.  More alarming though is when I set it not to be a listening-only tap.  As soon as I clicked a mouse button &amp;gt; 2, the entire system would lock up.  No keypresses or mouse events would be registered by anything at all, though the cursor would still move around on the screen. This problem persists until a few seconds after the application terminates or the event tap is released.  I've tried making the function the event tap calls do various different minor things like beeping or just doing nothing, but the UI lock still occurs.  Also, changing what events are being monitored seems to have no effect - tapping any kind of event causes this to happen. So... what can I do?  There's no documentation about these functions so I can't check if I'm doing something wrong, and no one else seems to have used them yet.  I would have a hard time believing that they're just broken. CGEventRef eventTapFunction(CGEventTapProxy proxy, CGEventType type, CGEventRef event, void *refcon) int main(int argc, char* argv[]) // window and menus set up here machPortRef =  CGEventTapCreate(kCGSessionEventTap, kCGTailAppendEventTap, 0, CGEventMaskBit(kCGEventOtherMouseDown), (CGEventTapCallBack)eventTapFunction, if (machPortRef == NULL) if (machPortRef) Any help would be greatly appreciated! - Brian</body>
  </mail>
  <mail>
    <header>Populating CGLayers with Adobe Illustrator Layers?</header>
    <body>Hello... Is it possible to populate CGLayers with Adobe Illustrator Layers? How?</body>
  </mail>
  <mail>
    <header>ImageIO capabilities</header>
    <body>Hello, I searched for information about image file types supported by ImageIO but could not it in documentation. I would like to support as many file formats as I can and I was wondering if ImageIO is a one-stop solution on Tiger, specifically if it falls back to QuickTime importer for images it cannot read by its core libraries (or is it vice versa: QT using ImageIO in Tiger?). Thanks, Tom</body>
  </mail>
  <mail>
    <header>Re: Best way to display/manupulate large images</header>
    <body>On Sep 29, 2005, at 2:14 PM, Scott Thompson wrote: Yes. I remember that, too. But I can't find any reference to it on Apple site. Does anyone have a pointer to any documentation about it? --</body>
  </mail>
  <mail>
    <header>CGContextShowTextAtPoint and Alpha Background</header>
    <body>Hi All, I don't have large experience on Quartz, so sorry if it is a basic question or a FAQ. I have a strange problem when I call the CGContextShowTextAtPoint API to draw some text on my background. I see the problem only when my background is filled with a color that contais 0 value to alpha (entire transparent). In this case, the drawed text comes with a filled black square behind the caracter. The square has exactly the same size (width and height) of a caracter. Apparentely, It works fine when I change the alpha value of the color to 1 (solid color) and call the API again. In this case I cannot see the black square. Has anyone already seen anything like that ? Following is the source code I'm using to do it. Am I doing anything wrong ? //Getting a Second Display, Capture It and Create a Context to Draw. /* Get a small 32 bit display mode */ mode = CGDisplayBestModeForParameters(display[1], 32, 720, 486, stripeRect  = CGRectMake (0, 0, 720, 486); // stripe //Set the back color with 0 alpha value CGContextSetRGBFillColor (context, 1, 0, 0, 0); &amp;lt;------- See alpha value here CGContextFillRect (context, stripeRect); &amp;lt;---- Clean the back with the fill color //Set the text color to white //Draw the white text on the transparent background. CGContextShowTextAtPoint (context, 15, (5 + pos), (const Thanks for your attention and time. Any help will be really appreciated. William</body>
  </mail>
  <mail>
    <header>Quartz 2D Extreme</header>
    <body>The application I develop is now Quartz 2D extreme savvy.&amp;nbsp; There are a few places in the UI that look incorrect due to invalid clipping or a path is fill instead of &lt;SPAN class=593322716-03102005&gt; When will Quartz 2D extreme be accessible by non-developers?&amp;nbsp; Will 10.4.3 likely fix more Q2DX &lt;SPAN class=593322716-03102005&gt; -Jim</body>
  </mail>
  <mail>
    <header>Re: Understanding how to use the NSAffineTransform (Scaling) for an EPS vector image</header>
    <body>This isn't a Quartz question but a Cocoa question since all classed you are using are provided by the Cocoa framework. It is better to use the cocoa-dev list[1] for such questions. The following saves (pushes it the stack) the graphics state before you change the graphics states current tranform matrix (CTM). The following line concatenates your scaling transform the CTM. The following restores (pops it off the stack) the graphics state that you save two lines up. This is a graphics state that doesn't have your scaling transform applied. The following asks you super class to draw itself (in this case NSImageView). No scaling transform is in place. I see two issues, one is outlined above... basically your scaling transform is not in place when you attempt to draw your image. I suggest trying the following. [scaleTransform scaleBy:0.5];  // The scaling factor '0.5' is used for testing. [super drawRect:rect];   // This is necessary to actually draw the image. The other issue I see is that you appear to be using NSImageView which is a control simply as a way to display an image. This is likely over kill and possibly problematic for the scaling you are attempting to do. I suggest instead subclassing NSView instead and dropping that custom view into your window/containing view. So something like (ignores any possible use of scroll view, etc.)... @interface epsImageView : NSView ... ... @end ... ... [scaleTransform scaleBy:0.5];  // The scaling factor '0.5' is used for testing. // Assumes myImage is an NSImage instance existing someplace... // (if you want image to blend with background then use NSCompositeSourceOver) [myImage drawAtPoint:NSZeroPoint fromRect:srcRect Note all code examples are written in mail from memory and not tested, bug likely exist.</body>
  </mail>
  <mail>
    <header>Re: javascript called every N frames/seconds</header>
    <body>Am 02.10.2005 um 04:20 schrieb Aaron Wallis: I'm not sure what you mean exactly (can't decipher your English grammar, I'm probably too tired ;-). I would expect it would be possible to do the same thing the Quartz Compose application does in your own app. I haven't done anything in that area though, sorry. Mike -- Mike Fischer         Softwareentwicklung, EDV-Beratung Schulung, Vertrieb Note:                 I read this list in digest mode! Send me a private copy for faster responses.</body>
  </mail>
  <mail>
    <header>Re: javascript called every N frames/seconds</header>
    <body>Am 02.10.2005 um 02:23 schrieb Aaron Wallis: I see. Didn't even know it could do JavaScript. Cool! Well the window.setTimout() function doesn't work because there is no window object/variable. Also the comments in the main window of Quartz Compser suggest that the JavaScript support is rudimentary at best. So I don't think that would be the way to go. Here is an idea: Use the System Time Tool as a generator, connect it to a Numeric Math component and use a modulo operation (to get a managable frequency). That will give you a fast counter with a fixed range. Then use a Numeric Conditional component with &amp;gt;= to ge a 0 or one result (essentially a square wave generator alternating between 0 and 1) and connect that to a Numeric Counter component as an example. Feed that into &amp;quot;Image with String&amp;quot; and watch the display count up. You'll need to figure out how to put your calculation somewhere into that chain. AFAICT all objects react as soon as an input changes. But I could be wrong. Assuming this is correct then you need some time generator. The above approach using the System Time Tool seems to work fine. Use the JavaScript Tool instead of the counter and do your thing. HTH Mike -- Mike Fischer         Softwareentwicklung, EDV-Beratung Schulung, Vertrieb Note:                 I read this list in digest mode! Send me a private copy for faster responses.</body>
  </mail>
  <mail>
    <header>Re: javascript called every N frames/seconds</header>
    <body>I was referencing the Javascript object in quartz composer. I need the equation I have created within the object to be processed every frame, rather than once... Send instant messages to your online friends</body>
  </mail>
  <mail>
    <header>Re: javascript called every N frames/seconds</header>
    <body>Am 01.10.2005 um 13:08 schrieb Aaron Wallis &amp;lt;email@hidden&amp;gt;: JavaScript and Quartz have no connection at all. You should ask this question in a general JavaScript forum or on the web- email@hidden mailing list. Check out the JavaScript function setTimeout(). HTH Mike -- Mike Fischer         Softwareentwicklung, EDV-Beratung Schulung, Vertrieb Note:                 I read this list in digest mode! Send me a private copy for faster responses.</body>
  </mail>
  <mail>
    <header>Fwd: What are these errors?  kCGErrorTypeCheck, kCGErrorFailure,	kCGErrorIllegalArgument</header>
    <body>Setting breakpoint at&amp;nbsp;CGErrorBreakpoint() doesn't help as the whole stack trace is library stuff and not my code. June 21, 2010 0:31:02  GMT+03:00</body>
  </mail>
  <mail>
    <header>Re: Instruments complains about misaligned images for CAShapeLayers,	when to rasterize?</header>
    <body>Unfortunately, I don't know. you might try filing a bug to find out, but are you seeing a performance degradation (vs rasterizing the shape yourself in a regular CALayer)? -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Instruments complains about misaligned images for CAShapeLayers,	when to rasterize?</header>
    <body>Unfortunately, I don't know. you might try filing a bug to find out, but are you seeing a performance degradation (vs rasterizing the shape yourself in a regular CALayer)? -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Instruments complains about misaligned images for CAShapeLayers,	when to rasterize?</header>
    <body>CALayer (UIScrollView) CALayer (UIScrollView) CALayer (UIScrollView) How do I fix it so that in #3, the shape layer is NOT misaligned? (Or is this a bug?) --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>CGSetDisplayTransferByTable</header>
    <body>Does anyone know if the call CGSetDisplayTransferByTable is immediate or does it wait for the VBL. Thanks much,</body>
  </mail>
  <mail>
    <header>Re: Instruments complains about misaligned images for CAShapeLayers,	when to rasterize?</header>
    <body>Ah no, the view is actually stable 99% of the time, I only animate when I need to show changes. Checking with Instruments, I notice that shouldRasterize: YES on the deep sublayers doesn't really affect the FPS of the actual animation much. But it does improve the scrolling speed tremendously, thus I'd want to keep it. For your reference, those simple layer arrangements that caused misaligned image under Instruments: CALayer (UIScrollView) CALayer (UIScrollView) CALayer (UIScrollView) --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Re: Instruments complains about misaligned images for CAShapeLayers,	when to rasterize?</header>
    <body>This doesn't really sound like a good use case for shouldRasterize, at least not on the view containing the animated contents, especially since it sounds like your view may be blowing the cache size. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Instruments complains about misaligned images for CAShapeLayers,	when to rasterize?</header>
    <body>On 18/06/2010, at 12:38 AM, David Duncan wrote: I've tried several different simple arrangements to reproduce the case. Here's the results: CALayer (UIScrollView) CALayer (UIScrollView) CALayer (UIScrollView) Note that all layers are at default position and bounds. Zoom level is allowing the UIScrollView to zoom i.e. the system sets the transform of the next layer down, and I don't mess with it. My actual use case is closest to #3, since I use several plain vanilla CALayers to group together the shape layers. Also keeping shouldRasterize: YES on the custom view layer slows down scrolling dramatically, presumably because of the canvas-sized bitmap that has to be used, and there is a lot of animation happening at the shape layer level. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Re: Instruments complains about misaligned images for CAShapeLayers,	when to rasterize?</header>
    <body>Are you seeing this when you have shouldRasterize turned on or always? shouldRasterize can be a performance boost or a performance sink. It depends a lot on both what you are doing and how much rasterization you are doing (there is a limited amount of cache). In particular, shouldRasterize should only be used for layer trees that are relatively static in nature, as it will have a negative performance impact if you modify the appearance of the layer tree (such as by changing layer style properties or animate a sublayer). -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Instruments complains about misaligned images for CAShapeLayers,	when to rasterize?</header>
    <body>When I have several CAShapeLayers on screen (iOS project), Instruments often colors them purple for misaligned images. I've managed to avoid these for text layers by carefully aligning the transformed bounds to integer values, but how do I do this for CAShapeLayers? Presumably it's all vectors in there and the layer bounds aren't even used. Another related performance issue: turning on shouldRasterize seems to speed up my CAShapeLayers but only in certain circumstances. Are there some rules of thumb we can use to know when to turn it on? Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Re: Quartz Global Display Coordinate System</header>
    <body>This is how you do it, but I suppose I'm not sure what else you were expecting to exist :). The origin of the display system is the upper-left corner of the menu bar. The positive direction is downwards and rightwards, hence the expectation is that a display positioned below the main display would be at some large positive offset. NSScreen would give you a coordinate system like this IIRC, but CG deals in this in the opposite handedness. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Quartz Global Display Coordinate System</header>
    <body>Hi, I'm using CGDisplayBounds() to check the origin's of each of my 2 displays which are in extended mode (i.e. NOT mirrored).  I'm writing a method to determine where one display is in relation to another by examining their origins and comparing them to one another.  Just do be sure, no official API exists for doing this, correct? When the displays are side-by-side in the arrangement view in the system preferences, all works as I would expect.  For instance, if the main display (with the menubar) is to the right of the secondary display (without the menubar), the x-coordinate of the secondary display is negative the width of the screen.  However, if the displays are stacked on top of each other and the main display is on the top and secondary display is on the bottom, the main displays origin is (0,0) like expected, however the secondary display below has an origin of (0,1200) instead of (0,-1200) like I would expect. Is this normal behavior?  If so, how come?  Why doesn't the display under the main display have a negative y-coordinate when it's below the main display and a positive y-coordinate when it's positioned above the main display?  I'm having trouble visualizing why I am getting the results I am. Thanks in advance for any insights you may be able to provide! Andrew _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>adding zoom to NSImageView</header>
    <body>Hi Everyone, I've displayed a bitmapped image in a NSImageView but need to add control features (zoom, rotation, etc). I would like to include the IKImageView class to support the control features but struggling to integrate it under the NSImageView. Any tips or examples on how to access the IKImageView class from NSImageView? Best, Joe</body>
  </mail>
  <mail>
    <header>Re: Applying a CIFilter - nil value for argument #0 src?</header>
    <body>Hi,</body>
  </mail>
  <mail>
    <header>Re: Applying a CIFilter - nil value for argument #0 src?</header>
    <body>-- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: CoreImage leak when drawing offscreen</header>
    <body>For posterity here is a work-around. CoreImage appears to leak when using a CIContext that is not the screen. Thus code like this leaks memory: NSGraphicsContext *nsgc = [NSGraphicsContext However if you use the screen context (not the bitmap context) to render to a CGImage first, then there are no memory leaks: NSGraphicsContext *nsgc = [NSGraphicsContext Yes this is slower but it works, and it doesn't leak memory like a sieve. thanks - Will</body>
  </mail>
  <mail>
    <header>Scaling PDF data to a CIIMage</header>
    <body>is there any simple way to scale a PDF output to a CIIMage? I get PDF dafa out of a NSTextView, pass it through a NSImage in order to get iin in my CIIMage at the end. Now the question is how do I scale the data returned by dataWithPDFInsideRect:bounds ? I want an uniform scale of the result, not modify the content of the textview. And of course I don't want to use an NSAffineTransform, I want high quality text. Here's what I do: any better idea to get a CIIMage* from the output of textView? And what is the correct way to scale the PDF output? ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>maximum radius</header>
    <body>font-family:Arial'&gt;CGContextAddArcToPoint takes a radius parameter.&amp;nbsp; I am see that some large radius values cause the path to jump off to far away places instead of going straight. &amp;nbsp;I was wondering where the limit was on this radius. font-family:Arial'&gt; font-family:Arial'&gt;2.3e+7 seems to be normal font-family:Arial'&gt;2.3e+8 starts to form a small bulge protrusion font-family:Arial'&gt;2.3+11 shoots the path segment off to never land. font-family:Arial'&gt; font-family:Arial'&gt;We get these radius values when the curve is nearly straight.</body>
  </mail>
  <mail>
    <header>Re: CoreImage leak when drawing offscreen</header>
    <body>Unfortunately no particular objects are leaking. I have a test app that just draws a CIImage to a CGBitmapContext then to the screen. Just by causing it to redraw in live-resize I can get it to eat several MB per second. I've eliminated most other possibilities, to get a leak all I need to do is draw a CIImage to some offscreen bitmap context. It seems to be CGLCreateContext that is leaking. Using OmniObjectMeter I get a lot of anonymous byte leaks like this: 0x900083b6 in _calloc 0x15a9aca6 in _gldGetString 0x15a304b0 in _gldGetString 0x15a8bc34 in _gldGetString 0x15a11674 in _gldGetQueryInfo 0x159ec497 in _gldCreateContext 0x025b4b4e in _gliCreateContext 0x932f66ed in _cglInitializeContext 0x932f57d1 in _CGLCreateContext 0x9402c685 in _allocate_context 0x9402c598 in _fe_cgl_create_context 0x94002a0b in _create_context 0x93ffd0d6 in _fe_accel_new 0x93ffd03e in _fe_context_accel 0x93ffcf20 in _fe_context_gl_new 0x94031a3b in -[CICGContextImpl _feContext] 0x9401ccbe in _provider_ensure_data 0x940315dd in _provider_get_byte_pointer 0x90336156 in _CGAccessSessionGetBytePointer 0x903350a6 in _img_data_lock 0x903330bd in _CGSImageDataLockWithReference 0x943f3534 in _ripc_AcquireImage 0x943f12cb in _ripc_DrawImage 0x90331773 in _CGContextDrawImage 0x93fe6d39 in -[CICGContextImpl render:] 0x94014bf7 in -[CIContext drawImage:inRect:fromRect:] or like this 0x900083b6 in _calloc 0x15a9aca6 in _gldGetString 0x15a304b0 in _gldGetString 0x15a8bc34 in _gldGetString 0x15a11674 in _gldGetQueryInfo 0x159ec497 in _gldCreateContext 0x02585b4e in _gliCreateContext 0x932f66ed in _cglInitializeContext 0x932f57d1 in _CGLCreateContext 0x9402c685 in _allocate_context 0x9402c598 in _fe_cgl_create_context 0x94002a0b in _create_context 0x94002935 in _fe_accel_context_ 0x9400281d in _fe_accel_context 0x94071d16 in _fe_gl_current_context 0x94004479 in _fe_gl_bitmap_collect 0x940156f6 in _fe_gl_purge 0x94015547 in _fe_context_purge 0x94015909 in _context_finalize 0x93ff1ab3 in _fe_release 0x93fea673 in -[CICGContextImpl dealloc] 0x94015464 in -[CIContext invalidate] 0x940153fa in -[CIContext dealloc] thanks -- Will</body>
  </mail>
  <mail>
    <header>Re: CoreImage leak when drawing offscreen</header>
    <body>On Sep 26, 2006, at 6:48 AM, Will Thimbleby wrote: How have you determined that something is leaking... (i.e. what method are you using)? Scott</body>
  </mail>
  <mail>
    <header>CoreImage leak when drawing offscreen</header>
    <body>I need to render a CIImage into a bitmap, and everything I've tried leaks memory. Does anyone know of any way to render a CIImage offscreen without leaking memory? Anything would do. These are the methods I have tried so far (all of them leaking on my MacBook Pro) Using CIContext's createCGImage method: CGContextDrawImage([[NSGraphicsContext currentContext] Using CIContext's drawImage method to draw dirextly into a CGBitmapContext NSGraphicsContext *nsgc = [NSGraphicsContext Any ideas would be much appreciated. thanks -- Will</body>
  </mail>
  <mail>
    <header>Re: Getting data out of an image.</header>
    <body>For float pixel processing you need to read back the pixels from the GPU using glReadPixels or texture readback. Hopefully we have some sample for that soon.</body>
  </mail>
  <mail>
    <header>Re: Getting data out of an image.</header>
    <body>Thanks for the info.  I'll try my code with 16 bit data and see how it goes. I posted this problem to Apple's dev support group and they claim that you can get float data if you base your CIContext on an OpenGL context.  I haven't been able to try that yet so i don't know. I personally think it's a bug in the code.  The API allows you to specify float pixel formats but the final rendering isn't in floats. fishcamp engineering 105 W. Clark Ave. Orcutt, CA  93455 TEL: 805-937-6365 FAX: 805-937-6252</body>
  </mail>
  <mail>
    <header>Re: Getting data out of an image.</header>
    <body>There is, AFAIK, no public way of obtaining something else than 8 bit output out of Core Image right now. However, the non-public method &amp;quot;createCGImage:fromRect:format:&amp;quot; in CIContext enables you to obtain at least a 16-bits image by passing kCIFormatRGBA16 as the format (I tried). I suppose you could get your floats out by passing kCIFormatRGBAf, but I didn't try. BTW the reason why you get your input image back when you set the brightness to 0 is that Core Image ignores filters that are configured in such a way that they have no effect on the incoming image (search for CIAttributeIdentity in the documentation if you're interested).</body>
  </mail>
  <mail>
    <header>Re: Getting data out of an image.</header>
    <body>Le 24 sept. 06 √† 01:39, bob piatek a √©crit : ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Is this a RADAR was Re: rect from ATSUGetGlyphBounds no aligned	with text, am I missing something?</header>
    <body>Hello, So in response to this, I did some testing here is what I've come up with. I used the sample code from Quartz2DBasics, and replaced &amp;quot;drawStrokedAndFilledRects()&amp;quot; with this: //test // style first // get a font and set the font ID GetThemeFont(kThemeSystemFont, smSystemScript, fontName, NULL, status = ATSUFindFontFromName( &amp;amp;fontName[1], StrLength(fontName), kFontFullName, kFontMacintoshPlatform, kFontNoScriptCode, // set size // set color // layout // set the context //set the text #define kTestStringLengthinUnicode 24 ATSUSetTextPointerLocation(layout, theText, kATSUFromTextBeginning, // copy to layout 2 to show both unangled and angled issues // set angle // set the run style ATSUSetRunStyle(angledlayout, style, kATSUFromTextBeginning, ATSUGetGlyphBounds(layout, FloatToFixed(40), FloatToFixed(100), kATSUFromTextBeginning, kATSUToTextEnd, kATSUseDeviceOrigins, 1, CGContextMoveToPoint(context,FixedToFloat CGContextAddLineToPoint(context,FixedToFloat CGContextAddLineToPoint(context,FixedToFloat CGContextAddLineToPoint(context,FixedToFloat CGContextAddLineToPoint(context,FixedToFloat ATSUGetGlyphBounds(angledlayout, FloatToFixed(200), FloatToFixed (100), kATSUFromTextBeginning, kATSUToTextEnd, kATSUseDeviceOrigins, CGContextMoveToPoint(context,FixedToFloat CGContextAddLineToPoint(context,FixedToFloat CGContextAddLineToPoint(context,FixedToFloat CGContextAddLineToPoint(context,FixedToFloat CGContextAddLineToPoint(context,FixedToFloat ATSUDrawText(layout, kATSUFromTextBeginning, kATSUToTextEnd, ATSUDrawText(angledlayout, kATSUFromTextBeginning, kATSUToTextEnd, In doing so I get what I describe. The bounding box is rotated in the opposite direction as the text, and in a non-rotated state the top of the bounding rect is centered on the text. Cheers Rob</body>
  </mail>
  <mail>
    <header>Re: Getting data out of an image.</header>
    <body>You can check out the CIAnnotation sample from the Apple Developer website. It shows how to write out a CIImage through ImageIO</body>
  </mail>
  <mail>
    <header>Getting data out of an image.</header>
    <body>I'm writing a network server that takes images and runs them through a stack of core image filters for an image processing system we are developing. I'm a beginner at this so excuse the simple question. It's easy to read in the data from an network, place it into an NSData object and then get call CIImage +initWithData to build the image which I can then feed it through my image pipeline. What I want to do is take the output of the process, which is a CIImage, back into a tiff or jpeg or whatever. It seems to be fairly complicated. does anyone have a simple example or explanation of the process? I'm trying to make heads or tails of ImageIO at this point.</body>
  </mail>
  <mail>
    <header>Boxwork: has several Mac Software engineer positions in NYC</header>
    <body>We are currently interviewing for the following job openings. Please send resume‚Äôs to: email@hidden We are looking for a Software Engineer with OpenGL experience that can implement creative ideas. Join a team of Software engineers that design commercial software in the field of digital photography, designing the human interface of the software with OpenGL on Mac OSX. Job Qualifications include: * OpenGL * Human Interface design * C/C++ * Object-Oriented design * Interest in image processing * Good communication skills (reporting/documentation) Good but not required: * Mac OSX API‚Äôs * Cocoa Client Server Software Engineer (full time position) This job opening is more specific to develop the next generation of our database client for our Image Assessment Management software solution. Job Qualifications include: * Cocoa/Objective-C * Multi-threading in Cocoa * MySQL * Java (to understand the older version of the software) * Good communication skills (reporting/documentation) Software Engineer (full time position) This job opening is more specific to assist in the different software‚Äôs that are being developed at Boxwork. Job Qualifications include: * Cocoa/Objective-C * C/C++ * Object-Oriented Design * Good communication skills (reporting/documentation) * Able to develop specific parts of the software by following strict guidelines Mathematician in the field of Image Processing (part time position)</body>
  </mail>
  <mail>
    <header>Re: rect from ATSUGetGlyphBounds no aligned with text,	am I missing something?</header>
    <body>On Sep 21, 2006, at 5:53 PM, Robert Kukuchka wrote: It sounds like the text matrix that you use to measure your text is flipped (on the y axis) relative to your drawing context. Try shifting the origin to the opposite side of your drawing context and then reflecting the y axis.</body>
  </mail>
  <mail>
    <header>CIKernel Particle Dynamics</header>
    <body>I'm using CoreImage to do a particle dynamics simulation on the GPU rather than the CPU.  This gives a very large performance gain for the computation portion of the simulation. The images I'm using are not bitmap data - each pixel is a float[4] that contains state information for the particle at the given pixel location, most specifically, the velocity of the particle and the mass of the particle. Once during each timestep, I need to move each particle (located at a certain pixel location in my 2D &amp;quot;bitmap&amp;quot; of particles).  The new position of the particle is oldPosition + velocity * timestep.  Mass at the new location is summed - multiple particles might contribute to a given destCoord during a given timestep. The problem is that if I treat the CIKernel &amp;quot;destCoord&amp;quot; as &amp;quot;the particle I'm going to compute trajectory for&amp;quot;, then I potentially need to be able to read/write to any arbitrary destination coordinate in the outputImage. I don't see any good way of doing this in a CIKernel, since CIKernels work on the assumption that nothing is mutable and the return value is the only place where anything can be changed.  And, the operation I require basically boils down to a seed fill, which the CIKernel docs expressly say is not something that CoreImage supports.  So I suspect I'm out of luck. But the CIImageAccumulator docs talk about fluid dynamics, which is very similar to particle dynamics, so I wonder whether I'm missing something.  I think fluid dynamics uses the same assumption that the particle being processed could wind up anywhere after a timestep. - Rather than treat the destCoord as the particle to which I'm going to apply trajectory, treat it as the location a trajectory might arrive at.  This implies that for each destCoord, I need to perform particle dynamics on every possible srcCoord.  This is ugly, obviously - it's N^2 rather than N.  But more importantly, the limitations on &amp;quot;for&amp;quot; loops in a CIKernel might make it impossible.  I haven't tried it, but it might work.  Yuck. - Copy from VRAM to system memory and do the transformation there. Then go back to VRAM and do everything else on the GPU.  Doing this decimates my performance.  Kills it completely. Could I write a custom fragment shader that would do what I need, without pulling the data out of VRAM?  Is there something about CIImageAccumulator I'm missing (keeping in mind that I need random access to the destination image)? I've implemented the &amp;quot;move to system memory&amp;quot; option using a cached pbuffer and glReadPixels.  Is there any more efficient way of doing it?  Every little bit of performance counts if I have to go this route. Is there some completely different approach I should be using that still gives me the performance benefit of doing most of my computation on the GPU? Attachment:</body>
  </mail>
  <mail>
    <header>Re: CGGLCreateContext: one time only deal?</header>
    <body>Are you calling CGLSetCurrentContext? -- ______________________________________________________________________ Michael Sweet, Easy Software Products           mike at easysw dot com Internet Printing and Document Software</body>
  </mail>
  <mail>
    <header>CGGLCreateContext: one time only deal?</header>
    <body>In my application, I have an OpenGL context that I need to render a CGImage into.  I looked into ways to do this, and it seemed to me that the simplest way to do this was to use CGGLCreateContext and render to the resulting CGContext.  At first this seemed to work quite well.  Unfortunately, whenever I try to do this more than once in an application, it just doesn't work.  That is, the resulting CGContext just doesn't render CGImages -- they don't show up, even though CGClearRect and CGFillRect work fine. In the test case that I have failing, what I do is create an offscreen OpenGL context, do random setup on it, save GL state, set all matrices to the identity, create the CGGLContext, draw the CGImage, release the CGGLContext using CGContextRelease, restore the matrices to what I normally want them to be, restore GL state, and finally read the pixels using glReadPixels.  I then destroy the GL context. Does anyone know why this isn't working?  Always the second and later attempts just don't draw the image even though I know with certainty (having saved them) that they are valid. Damian Frank</body>
  </mail>
  <mail>
    <header>Core Image Developer required</header>
    <body>Hi, I run a small UK based web development company and am currently seeking to sub-contract(not in-house) the services of a talented core image programmer to develop a custom sharpen filter for an OS X based cocoa application. We have a been developing an application that takes video frames and outputs static images(jpeg, png etc). We're using the basic core image filters at present to apply effects. However, it turns out that we are unhappy at the quality of the core image sharpen filters and would therefore like to develop our own custom sharpen filter. I require the services of a developer on a sub contract basis who can create a custom sharpen filter to drop into our application. The developer must know GLSLang and write a kernel function according to CoreImaging/index.html#//apple_ref/doc/uid/TP30001185. Our goal is to have a sharpen filter comparable in quality to that of the new 'Smart Sharpen' filter that is present in Adobe Photoshop CS2. More details can be provided upon response. You will be required to communicate(via email) with my other developer on this project in order to ensure the new filter meets the requirements... please feel free to email if you have any queries about the nature of this enquiry. mark</body>
  </mail>
  <mail>
    <header>CGPostKeyboardEvent problem</header>
    <body>I'm trying to send keystrokes to the front application using CGPostKeyboardEvent. I'm using the function at bottom of this post. It takes as parameters a modifier mask and a virtual key code. For instance, to send 'command-N' one would use: This works fine, _except_ when the shift modifier is combined with other modifiers. In that situation the shift modifier event seems to be ignored. To build on the example above (assuming text edit is front application): pressKeyCombination(0, 0x2d);			//  -&amp;gt; prints 'n' pressKeyCombination(NSShiftKeyMask, 0x2d);	//  -&amp;gt; prints 'N', shift works pressKeyCombination(NSCommandKeyMask, 0x2d);	//  -&amp;gt; creates new document but: also creates new document Also, shift-command-p should bring up page setup: print dialog... // //  code used: // void pressKeyCombination(int modifiers, CGKeyCode key) CGSetLocalEventsSuppressionInterval(0.0);	//	don't wait after posting CGEnableEventStateCombining(false);		//	ignore real keyboard state //	depress modifiers //	press/release key //	release modifiers CGSetLocalEventsSuppressionInterval(0.25);	//	default</body>
  </mail>
  <mail>
    <header>Re: CGImageDestinationCreate* and image count</header>
    <body>In absence of any response I gave it a try to find out neither is supported: Sep 16 18:36:36 rene-rebes-computer /Users/rene/.../Contents/ MacOS/...: CGImageDestinationCreate capacity parameter is zero\n Also passing some fictive value, such as let's say 666 does not work either: Sep 16 18:37:26 rene-rebes-computer /Users/rene/.../build/Debug/.../ Contents/MacOS/...: CGImageDestinationFinalize image destination does not have enough images\n So there is no way to write multi page TIFF files with an unknown page count at creation time in OS X? -- Ren√© Rebe - ExactCODE - Berlin (Europe / Germany) +49 (0)30 / 255 897 45</body>
  </mail>
  <mail>
    <header>Synthesizing Scroll Wheel Events with Quartz Event Services</header>
    <body>I need to synthesize low level events for automating GUI-level app testing, and I had success with key up/down and mouse up/down events. Anyways, not so with scroll wheel events. Below is my code. F.ex., when passing the center point &amp;quot;textViewCenterPoint&amp;quot; of some NSTextView, i.e. calling postScrollWheelEvent(textViewCenterPoint, 1, 1, 0, 0), it correctly moves the cursor to the desired location, but no scrolling happens, it behaves just like kCGEventMouseMoved. void postScrollWheelEvent(CGPoint point, int64_t xDelta, int64_t yDelta, int64_t zDelta, CGEventFlags modifiers) CGEventSetIntegerValueField(event, kCGScrollWheelEventDeltaAxis1, CGEventSetIntegerValueField(event, kCGScrollWheelEventDeltaAxis2, CGEventSetIntegerValueField(event, kCGScrollWheelEventDeltaAxis3, CGEventSetIntegerValueField(event, Thanks a lot and best regards, -- Martin</body>
  </mail>
  <mail>
    <header>Re: ImageIO Exif GPS</header>
    <body>I am going to take the lack of response as an answer to my question and consider writing GPS Exif data using ImageIO not possible and find another solution. I can totally understand why ImageIO doesn't deal with this currently as it seems rather difficult to do correctly and the number of applications requiring this functionality is most likely pretty low. On Sep 10, 2006, at 11:59 PM, Byron Wright wrote:</body>
  </mail>
  <mail>
    <header>Re: CIImage from RGB data / CoreImage OGL texture basic questions</header>
    <body>You could possibly use Core Video to do this.  Create a CVPixelBuffer with your existing data, using the correct pixel format.  Then create a CVOpenGLTexture from the CVPixelBuffer.  You can then create a CIImage from this CVOpenGLTexture directly (+[CIImage imageWithCVImageBuffer]).  You'll need to create a CVOpenGLTextureCache just to create one texture, but it may be workable.</body>
  </mail>
  <mail>
    <header>Re: Image rescaling (20-30fps)</header>
    <body>Actually, Im using glCopyTexSubImage2D and glGetTexImage for the capturing.  However; my OpenGL knowledge is probably less than my Quartz knowledge, so I've no idea how to scale using OpenGL.  I've already got a Q on the list about it tho, in case it's a better avenue to explore.</body>
  </mail>
  <mail>
    <header>Re: Image rescaling (20-30fps)</header>
    <body>&amp;gt; Yes, I tried turning of all interpolation (using the None setting) I see. I'm assuming you're capturing with glReadPixels(), so you can't simply use OpenGL to resize using a texture. If you haven't already, you might want to have a look at vImage: Best regards, Robert van Uitert</body>
  </mail>
  <mail>
    <header>Re: Image rescaling (20-30fps)</header>
    <body>Yes, I tried turning of all interpolation (using the None setting) and also anti-aliasing (just in case). It's better - but still quite slow.   e.g: it's faster for me to capture full screen and then simply resize the video in quicktime (at runtime, not as an export).   I was hoping that by resizing a full screen capture down to 1/2 (or somethin\g like that)  I'd be able to get a faster capture rate (the actual OpenGL capture is taking only about 12% CPU).</body>
  </mail>
  <mail>
    <header>RE: Image rescaling (20-30fps)</header>
    <body>Did you try different interpolation settings? For example: Best regards, Robert van Uitert</body>
  </mail>
  <mail>
    <header>Image rescaling (20-30fps)</header>
    <body>I'd like to be able to scale images that I'm capturing from the screen (using OpenGL), so that when I put them into a QT movie, they don't have to be the original size.        (I doing this on an Intel MBP, btw) I've done what is probably a very naive implementation, using CGImage/ Bitmap, which is this: // Make a new bitmap to hold the data newBitmap = CGBitmapContextCreate(newData, scaledRect.size.width, scaledRect.size.height, 8, byteWidth, [frame colourSpace], // Create an image from the existing frame bitmap data, so that we can draw this image to the new bitmap Some notes: 1) The frames being captured use pre-allocated buffers (so they are not allocated for each frame, which would hurt). 2) newData is also a piece of pre-allocated memory. 3) From what I can tell from the docs the CGBitmapContextCreateImage should be fast - as it potentially doesn't do much.  Shark seems to confirm this, as all the time is spent inside CGContextDrawImage - and then within that rgba32_image_rgba32 Thanks, Neil Clayton</body>
  </mail>
  <mail>
    <header>Re: Thread safety of drawing a PDF page in an offscreen context</header>
    <body>Generally speaking, this is safe.  The key thing is that only a single thread may access the contents of a CGPDFDocument at any given time. It's okay, however, for multiple threads to access the document as long as you put in locks to enforce this constraint.</body>
  </mail>
  <mail>
    <header>CIImage from RGB data / CoreImage OGL texture basic questions</header>
    <body>I am having some trouble finding out how to create a CIImage from RGB data (my app is receiving RGB data buffers as unsigned char *). If I understand it correctly, CIImage's +imageWithBitmap +imageWith only work with data in ARGB format (32 bytes per pixel), so to use any of these I would need to convert my buffer to ARGB first. Doesn't CoreImage ultimately create a OGL texture from the pixel data provided? Can I upload my RGB data directly to an OpenGL texture and then use CIImage +imageWithTexture:size:flipped:colorSpace - and thus bypass the convertion to ARGB? Unfortunately I could not find any information on how to create OGL textures compatible with Core Image. The CIImage documentation refers to three CIImage pixel formats (kCIFormatARGB8, kCIFormatRGBA16 , kCIFormatRGBAf) - do these relate to the OGL texture &amp;quot;internal format&amp;quot;? What are the parameters that need to be passed to glTexImage2D for any of the CIFormats above? Thank you &amp;amp; best regards, Mark</body>
  </mail>
  <mail>
    <header>Thread safety of drawing a PDF page in an offscreen context</header>
    <body>Is it safe to call CGContextDrawPDFPage() from a separate thread, if the CGPDFPage and offscreen CGContext are also created and later released in that same thread and are never used by any other thread? Thanks, Robert van Uitert</body>
  </mail>
  <mail>
    <header>Re: Correct way to find out if CIFilter supports infinite input?</header>
    <body>On Sep 14, 2006, at 8:18 AM, Santiago (Jacques) Lema wrote: I think the only way to know is to look at the extent of the image you get back.  One way to deal with this is to crop the image to the intersection of its extent and some arbitrarily large rectangle. Nick</body>
  </mail>
  <mail>
    <header>Correct way to find out if CIFilter supports infinite input?</header>
    <body>Is there a way to know if a CIFilter accepts to render with a given input (eg. an infinite one) ? Typically if your input are checkers (infinite) and you give them to a CICircularWrap it won't work, which is normal. But how do I know before that it won't ? Is  there any attribute for this in the filter? Currently I just do the following: try to set the inputImage and see if output is nil.  Otherwise crop it. Is there any better way' if ([filter valueForKey:@&amp;quot;outputImage&amp;quot;]==nil) ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Re: CGImageRef from CIImage</header>
    <body>So assuming the CI filter chain doesn't change geometry (i.e. I can reuse the width/height of the input image in this example), would the code below be the proper/fastest way to go from input CGImageRef to an output CGImageRef? Which leads me to another question -- if I CICrop as one of my filters, geometry changes.  What is the &amp;quot;proper&amp;quot; way to get the resultant x/y/width/height on the way out for rendering the final image?  When I did this in my code, it seemed that my code had to &amp;quot;be smart&amp;quot; about keeping track of its geometry changes and there wasn't an obvious (to me) way to just say &amp;quot;give me the final CGImageRef and figure out the bounds yourself&amp;quot;. ... do stuff in CoreImage ... // assuming no image size changes, that we can reuse the width/height from the original CGColorSpaceRef colorSpaceRef = CGColorSpaceCreateWithName CGContextRef outputCGImageContext = CGBitmapContextCreate(outputData, width, height, bitsPerComponent, bytesPerRow, colorSpaceRef, NSDictionary *outputContextOptions = [NSDictionary dictionaryWithObjectsAndKeys: [NSNumber numberWithBool: NO], CIContext *ciContext = [CIContext contextWithCGContext:outputCGImageContext options: CGImageRef outputCGImage = [ciContext createCGImage:outputCIImage</body>
  </mail>
  <mail>
    <header>Re: CGImageRef from CIImage</header>
    <body>On Sep 13, 2006, at 9:49 AM, Frank Doepke wrote: Generally the answer is twofold.  Sometimes you want to do something other than display these images on the screen so you have to export them, and with the current implementation its usually substantially faster to do the CI rendering once and cache the results than to draw it over and over. Nick</body>
  </mail>
  <mail>
    <header>Re: CGImageRef from CIImage</header>
    <body>Got the fish on the hook now (but then again I don't like seafood). Anyhow: I am quite aware of the desire to get to the pixels of a CIImage. To do that you have to create a CGBitmapContext, then create a CIContext from it and render into that. Or for those inclined to venture into OpenGL can read the pixels back from the graphic card. Here is the point where I should say: Yes you can do it but I would like this to be easier and I have a bug in my court regarding this. When you are done fiddling around with the pixels, you can either create a CGImage from that again and then a CIImage as you stated correctly or use imageWithData. And now comes the the 'but' part: When you just want to use CI on images that get combined with some CG rendering, there is no need to wrap those image into a CGImage as you can create the CIContext based on the CGContext and just draw the CIImage into it. Having a CIContext also allows you to create a CGImage or a CGLayer using the following APIs: /* Render the region 'r' of image 'im' into a temporary buffer using * the context, then create and return a new CoreGraphics image with * the results. The caller is responsible for releasing the returned * image. */ /* Create a CoreGraphics layer object suitable for creating content for * subsequently rendering into this CI context. The 'd' parameter is * passed into CGLayerCreate () as the auxiliaryInfo dictionary. */ - (CGLayerRef)createCGLayerWithSize:(CGSize)size info: These are helpful for operations, where you definitely need a CGImageRef, like writing them into a file - as demonstrated in the CIAnnotation example. Or when you want to use the result in CG operations as a mask or something like that. Thats where feedback is always welcomed so we can make it less confusing This contradicts the rest of the email :)</body>
  </mail>
  <mail>
    <header>Re: CGImageRef from CIImage</header>
    <body>- (void)displayImage:(Image*)tempImage CGContextTranslateCTM(ctx,bounds.size.width/2.0 CIContext	*myCIContext = [CIContext contextWithCGContext:ctx CIImage		*myCIImage = [CIImage imageWithCGImage:currentImage [hueAdjust setValue:[NSNumber numberWithFloat: 1.1] [myCIContext drawImage:result inRect:CGRectMake(-w/2.0,-h/2.0,w,h) // Image without CIFilters... My problem is that it is very slow when I try to move the image by dragging it, so I thought that I could apply the transformation to the CIImage, and store the result in a CGImageRef to draw it when I need. But my approach is probably wrong. This is the first time I use CoreImage, so I may have missed something... -- AstroK Software Arthur VIGAN email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGImageRef from CIImage</header>
    <body>On Sep 13, 2006, at 8:49 AM, Frank Doepke wrote: Because that seems to be the only way to get access to the raw pixel data.  For instance when  you wish to do further processing or be able to store the resulting image to a file. Bob</body>
  </mail>
  <mail>
    <header>Re: CGImageRef from CIImage</header>
    <body>On Sep 13, 2006, at 8:49 AM, Frank Doepke wrote: Okay, I'll bite :-) Let's say I have a CIImage, and I want to do something with that I can't express as some CIFilter operation (like flood fill on the pixels that will result when that CIImage is evaluated). In other words, if I need to get pixels from something that up until now was happy to be expressed algorithmically with a CIFilter/CIImage chain, the shortest path from a CIImage to pixels is a CGImageRef, right?  Then once I've done work in pixels I want to loft it back up into a CIImage so I can continue on in the filter chain, which I can do with [[CIImage alloc] initWithCGImage:], but I'm not sure how expensive that is... --&amp;gt; Michael B. Johnson, PhD --&amp;gt;  (new &amp;amp; improved!) --&amp;gt;  (alum) --&amp;gt; MPG Lead --&amp;gt; Pixar Animation Studios</body>
  </mail>
  <mail>
    <header>Re: CGImageRef from CIImage</header>
    <body>Generally the question is, why do you need a CGImage, as you can create a CIContext from your CGContext and draw the CIImage into that.</body>
  </mail>
  <mail>
    <header>Re: CGImageRef from CIImage</header>
    <body>On Sep 13, 2006, at 9:15 AM, AstroK Software wrote: I don't have any sample code handy, but I can give you the high-level view... You have to create a CGBitmapContext and use that to create a CIContext, render your CIImage into the CIContext, and finally create a CGImage from the backing store of the CGBitmapContext (or use CGBitmapContextCreateImage). Nick</body>
  </mail>
  <mail>
    <header>CGImageRef from CIImage</header>
    <body>in my application I work mostly with Quartz 2D for drawing and images, but I would like to use the CoreImage filters. My first question is: is it possible to use CoreImage directly from Quartz 2D? The researches I've made seem to tell me that it's not. So here is second question: how can I get an CGImageRef from a CIImage? It's probably not trivial, because CIImages can have infinite extent, but the CIImages I would manipulate would be of finite extent. -- AstroK Software Arthur VIGAN email@hidden</body>
  </mail>
  <mail>
    <header>CMYK &amp;amp; CGImage</header>
    <body>I'm having problems creating a CGImage with CMYK data. My application right now creates an imageRef via imageRef = CGImageCreate(	width, height, bitsPerComponent, numOfComponents*bitsPerComponent, bytesPerRow, colorSpace, kCGImageAlphaNone, dataProvider, NULL, NO, with dataProvider providing the CGImage with CMYK data. However, it seems the black component of the CMYK data is not getting used when the app renders the CGImage. Black looks rather grayish there, then. Just like a CMY image would. Having a closer look at the kCGImageAlpha group of constants, I discovered this description in the docs kCGImageAlphaNone There is no alpha channel. If the total size of the pixel is greater than the space required for the number of color components in the color space, the least significant bits are ignored. This value is equivalent to kCGImageAlphaNoneSkipLast. So, it seems, the K component is simply ignored. I also tried kCGImageAlphaLast and kCGImageAlphaPremultipliedLast, but they really just interpreted the K as alpha then. Best regards, Felix Schwarz</body>
  </mail>
  <mail>
    <header>CGGLContextCreate confusion</header>
    <body>I'm trying to use CGGLContextCreate on my existing (and working) GL context. Unfortunately for me, my entire view becomes black - also my ATSUI text that I'm trying to render isn't showing (I've also tried just draw rects and things using CG but to no avail - nothing shows). NSOpenGLPixelFormatAttribute viewGLPixelFormatAttributes[] = NSOpenGLPFANoRecovery, NSOpenGLPFAAccelerated, NSOpenGLPFAColorSize, (NSOpenGLPixelFormatAttribute)24, NSOpenGLPFADepthSize, (NSOpenGLPixelFormatAttribute)16, NSOpenGLPFADoubleBuffer, (NSOpenGLPixelFormatAttribute)0 Any pointers? I searched through the lists and note someone's comment on setting up an orthogonal projection matrix but that doesn't appear to work for my circumstance. What'd be really helpful is a working example of CGGLContextCreate in action. Cheers, -C</body>
  </mail>
  <mail>
    <header>Re: can a command-line tool use Core Image?</header>
    <body>On Nov 17, 2005, at 12:47, Frank Doepke wrote: Thanks. I started to dig into how to do this but was more focused on having something working. I'll digest this later. It took me a while to work out that this meant ApplicationServices/ ImageIO, but I got there eventually. (Haven't had much reason to do any Cocoa development, and so much has changed since my brief excursion into the NeXT world...) Thanks again; something else to digest later.</body>
  </mail>
  <mail>
    <header>Re: can a command-line tool use Core Image?</header>
    <body>You can create your CIContext like this: CIContext* getCIContext(void) if(!gCIContext) (CGLPixelFormatAttribute)24, kCGLPFAAlphaSize, (CGLPixelFormatAttribute)8, error = CGLChoosePixelFormat( attribs, &amp;amp;pixelFormat, gCIContext = [[CIContext contextWithCGLContext:cglContext And use ImageIO as I stated yesterday for saving the image out to disk. That saves you from pulling in AppKit. Not that AppKit is a bad thing but it is a little heavier than ImageIO. Frank</body>
  </mail>
  <mail>
    <header>Re: can a command-line tool use Core Image?</header>
    <body>On Nov 16, 2005, at 16:55, Frank Doepke wrote: I also ended up having to link against AppKit, in order to bring in NSGraphicsContext and NSBitmapImageRep so I could render and save the resulting image, using code nicked from Samuel DeVore's post yesterday :-) Being a Quartz newbie I'm not sure if there's a better way to have done that, but this seems to work (and on my machine, about 60% faster than Imagemagick, which is pretty sweet to see.)</body>
  </mail>
  <mail>
    <header>Re: Creating a fake display</header>
    <body>Ian Archer: You may well be able to do something working from the Desktop Manager source. -- Jens Ayton</body>
  </mail>
  <mail>
    <header>Re: Resizing a 1 bpp image</header>
    <body>samiam work: Quite possibly the best way to do this will be QuickDraw. Blasphemy, I know, but it was originally designed for 1bpp. Note that QD‚Äôs 1bpp blitters are designed for black-on-white images and prefer black over white to preserve detail. If your images are predominantly white on black, you may get better results inverting them. -- Jens Ayton</body>
  </mail>
  <mail>
    <header>Re: Creating a fake display</header>
    <body>Possible or not, it's dangerous... MacOS X is pretty dumb about detecting monitor disconnection as is, which often results in windows being lost... I'm not sure it's a good idea to go provoking this kind of behaviour, except in extremely controlled circumstances. ICQ: 40056898 AIM, Yahoo &amp;amp; Skype: wadetregaskis MSN: email@hidden iChat &amp;amp; email: email@hidden Jabber: email@hidden Google Talk: email@hidden</body>
  </mail>
  <mail>
    <header>Re: can a command-line tool use Core Image?</header>
    <body>On Nov 16, 2005, at 16:55, Frank Doepke wrote: OpenGL was the one I was missing - I can now  at least run a &amp;quot;Hello world&amp;quot; with QuartzCore included. Thanks very much!</body>
  </mail>
  <mail>
    <header>Creating a fake display</header>
    <body>I'm interested in creating a &amp;quot;fake&amp;quot; display to use as a desktop for offscreen windows whose pixels and updates I am still interested in obtaining.  The &amp;quot;CGDisplayReconfigurationCallBack&amp;quot; is a nice function for maintaining state of screen updates as they occur, and it would be quite useful for windows that are not visible.  It seems that creating an alternate display and moving windows to that display would allow such tracking to occur, even if that display didn't actually correspond to a real monitor.  Is such a thing possible?</body>
  </mail>
  <mail>
    <header>Resizing a 1 bpp image</header>
    <body>I've got a bunch of 1200 dpi tiff bitmap images at 1 bpp that I'm resizing down to 600 and 300 dpi but so far I can't see any good way to resize them and keep the resulting CGImage at the same 1 bpp as the source. All I've been able to manage is 8 bpp gray. The way I'm doing it is to create a bitmap context at the lower dpi and draw the source image into it but CGBitmapContextCreate only lets me do 8 bits per component with a gray colorspace. Is there a better way to do this that will give me 1 bpp CGImages? Is the only way to do it to convert the resized data in the bitmap context myself and create a CGImage from that? Thanks, -Sam</body>
  </mail>
  <mail>
    <header>Re: can a command-line tool use Core Image?</header>
    <body>Which frameworks are you linking against? It should be: OpenGL ApplicationServices QuartzCore Foundation</body>
  </mail>
  <mail>
    <header>Re: can a command-line tool use Core Image?</header>
    <body>On Nov 16, 2005, at 15:12, Rick Innis wrote: Ack. Just realised that should have said &amp;quot;importing QuartzCore/ QuartzCore.h leads to a SIGBUS&amp;quot;.</body>
  </mail>
  <mail>
    <header>Re: speeding up jpeg writing to disk from CIImage</header>
    <body>Have you looked at using ImageIO to write out the file? I haven't compared the numbers but it could be a little bit more efficient. Check out the CI Annotation sample code.</body>
  </mail>
  <mail>
    <header>Re: can a command-line tool use Core Image?</header>
    <body>Not sure what your problem with CoreFoundation is but you can most definitely use CoreImage in a command line tool - the Image Units Validation Tool ( agreements/imageunits.html) is a command line tool that uses CI.</body>
  </mail>
  <mail>
    <header>speeding up jpeg writing to disk from CIImage</header>
    <body>So I am processing some large ( &amp;gt;30Meg) tif files by applying some filters and then writing out to disk as jpegs and it seems to be pretty slow.  I am using the following to do the writing (resultImage is the CIImage after the filters)  Are there any things I can try to squeeze a little more performance out of this? theBitMapToBeSaved = [[NSBitmapImageRep alloc] initWithBitmapDataPlanes:NULL pixelsWide:newSize.width pixelsHigh:newSize.height bitsPerSample:kBitsPerSample samplesPerPixel:kSamplesPerPixel hasAlpha:YES isPlanar:NO NSGraphicsContext *nsContext = [NSGraphicsContext [[nsContext CIContext] drawImage:[self resultImage] atPoint:imageDestinationRect.origin fromRect:[[self resultImage] theFileSaveDictionary = [NSMutableDictionary dictionaryWithObjectsAndKeys:[NSNumber numberWithFloat: ([jpegQuality [[theBitMapToBeSaved representationUsingType:NSJPEGFileType</body>
  </mail>
  <mail>
    <header>can a command-line tool use Core Image?</header>
    <body>I have a requirement to do some on-the-fly image composition. It's a server-side process, so needs to be a headless application. I also have a solution; it's called ImageMagick. However, I'm curious to see if CoreImage can do the job any faster, hence the question. I haven't seen anything in the documentation that suggests this can't be done, but I also haven't seen anything suggesting it can't. archives/quartz-dev/2005/May/msg00177.html), but I haven't see any responses.) I've made a start at exploring this by creating a new &amp;quot;Command Line Utility&amp;quot; project in XCode. However, the simple act of including/ importing CoreFoundation/CoreFoundation.h leads to a SIGBUS at run time. If anyone has any insights on this, I'd love to hear 'em - hopefully before I burn too many more cycles on this approach.</body>
  </mail>
  <mail>
    <header>Getting the alpha/pixel data from a CGImageSource without quality	loss</header>
    <body>I'm currently implementing a loader that utilizes CGImageSource for formats my other code does not already handle. As far as I can see, I can only obtain CGImageRefs that way. I also know I can use CGBitmapContextCreate to copy their pixel data into a buffer of my choice. So far so good. However, what I'm currently lacking is a possibility to obtain un- premultiplied data. Looking at qa1037.html I can't see a possibility to achieve this. Did I miss something or is this just not possible? Also, does CGImageSource support alpha information in JPEG 2000 files? (I couldn't find a file containing alpha information to try it out myself.) Felix</body>
  </mail>
  <mail>
    <header>Re: CGLayer size, etc.</header>
    <body>On Nov 16, 2005, at 9:49 AM, Stephen Chu wrote: Weird.  I don't know.  I suppose it would be easy enough to figure it out.  Hmmm...</body>
  </mail>
  <mail>
    <header>Re: CGLayer size, etc.</header>
    <body>On Nov 15, 2005, at 4:32 PM, Scott Thompson wrote: I am creating the layer from a CGContext I get in HIView draw event. It does have the origin at the top-left corner with y axis pointing down. Is this a special case? --</body>
  </mail>
  <mail>
    <header>Re: CGLayer size, etc.</header>
    <body>On Nov 15, 2005, at 4:32 PM, Scott Thompson wrote: Thanks. That clears things a lot. --</body>
  </mail>
  <mail>
    <header>Re: CGLayer size, etc.</header>
    <body>On Nov 15, 2005, at 3:06 PM, Stephen Chu wrote: The size you pass in is the size, in Points, of the layer you would like to create. The Layer does not take on the CTM of the reference context.  When the layer is passed to you, it will have an identity matrix and the origin will be in the lower left corner of the layer with the positive y axis oriented &amp;quot;up&amp;quot;. The reference context is used to choose the most efficient caching scheme for the layer.  For example, if your layer is attached to a screen context, the computer will set up an offscreen bitmap with the same color space as the screen.  Theoretically, at least, if you had a context attached to an OpenGL context (for example) the computer might choose to store the layer as an OpenGL display list, or a p- buffer. or something along those lines.</body>
  </mail>
  <mail>
    <header>CGLayer size, etc.</header>
    <body>What exactly is the size in CGLayerCreateWithContext? By that I mean is it in device space or user space of the context it's created with? Does the new CGLayer take on the CTM of the context? --</body>
  </mail>
  <mail>
    <header>Re: CGPathContainsPoint and &amp;quot;Open&amp;quot; Paths</header>
    <body>Okay, thanks.  Technically this makes the documentation wrong because it says: and a non-closed path is painted exactly the same as a closed path when filled. On Nov 11, 2005, at 4:27 PM, Haroon Sheikh wrote:</body>
  </mail>
  <mail>
    <header>Re: CGPathContainsPoint and &amp;quot;Open&amp;quot; Paths</header>
    <body>This is by design. You should close the path if you want to test &amp;quot;insideness&amp;quot;.</body>
  </mail>
  <mail>
    <header>Core Image and Camera RAW</header>
    <body>Searching around, I haven't been able to find an answer to this question, so hopefully I can find it here. When Core Image performs its operations on an image, such as exposure, does it do these actions on the actual RAW file or a pre- computed version of that file? I'm inclined to think the latter because of experiments with RAW conversion in photoshop vs. using Core Image yield much clearer results on a dark image with photoshop.  The results from the Core Image test looked exactly like it had only increased the exposure on the non-black pixels rather than the data from the file. If this is indeed the case, does the RAW support provide any interface to adjust how it renders the image? I used CIExposureSample included in the Core Image examples for my benchmark of that.</body>
  </mail>
  <mail>
    <header>CGPathContainsPoint and &amp;quot;Open&amp;quot; Paths</header>
    <body>In the following code fragment the variable &amp;quot;inside&amp;quot; is set to true as I would expect.  However if I remove the call to CGPathCloseSubpath &amp;quot;inside&amp;quot; becomes false.  This isn't what I would expect since the point is well within the path even if the path isn't closed.  Is it fair to conclude that CGPathContainsPoint requires all subpaths to be closed?  If so, is this by design or should I file a bug? Thanks, Nick CGPathCloseSubpath(path);   // this line is apparently required</body>
  </mail>
  <mail>
    <header>Re: CGPDFContextBeginPage dictionary</header>
    <body>You can't set the creation date; that's set for you automatically when the document is created.  Unfortunately, neither the subject nor the keywords can be set with public API in Tiger.</body>
  </mail>
  <mail>
    <header>CGWaitForScreenRefreshRects / CGWaitForScreenUpdateRects questions</header>
    <body>Why would CGWaitForScreenRefreshRects or CGWaitForScreenUpdateRects return kCGErrorRangeCheck?  Is there anything special we should do in this case? What about other errors? Why would these calls return kCGErrorSuccess, but return a count of zero? Should we do something special in this case? What is kCGScreenUpdateOperationReducedDirtyRectangleCount for?  Should we ask for it?  What should we do if we get one? -Marc</body>
  </mail>
  <mail>
    <header>Re: CGPDFContextBeginPage dictionary</header>
    <body>Actually I had looked in the header file before, but did only look for the keys right next to CGPDFContextBeginPage :-) When I use CGPDFDocumentGetInfo, I can get additional information such as CreationDate, Subject and Keywords. However, I can't set those neither via CGPDFContextCreate nor via CGPDFContextBeginPage. Yet, Preview can change the keywords of a PDF document (which I'm interested in the most). What is the interface for this functionality? -- Postal address: IOSPIRIT GmbH, Berckhauser Stra√üe 11, 90409 N√ºrnberg, Germany Phone: +49 (0)911 / 3677 423 Fax: +49 (0)911 / 3677 424 Homepage: Trade register number and place of registration: HRB 21960, Amtsgericht N√ºrnberg Attachment:</body>
  </mail>
  <mail>
    <header>CGPDFContextBeginPage dictionary</header>
    <body>void CGPDFContextBeginPage( CGContextRef context, CFDictionaryRef pageInfo pageInfo A dictionary that contains key-value pairs that define the page properties. I'd really love to use the pageInfo parameter to pass the dimensions of the different boxes (media box, crop box, etc.), but I can't find any information on the contents of this dictionary anywhere. Not even, whether I can pass the dimensions for the different boxes to this call or this dictionary is there for other metadata. Any hints as to how to use this function's second parameter and what it expects and accepts in its dictionary are very welcome :-) Attachment:</body>
  </mail>
  <mail>
    <header>Draw a CIImage to an offscreen and onscreen context</header>
    <body>Hello, I'm doing the following: I'm getting an CVPixelBuffer from an QT ICMDecompressionSession, creating a CIImage from it, applying several filters to it and drawing it to on CIContext based on an NSOpenGLView's context, so the processing happens on the GPU. Afterwards I'm reading the context back to ram, packing it into a CVPixelBuffer and passing it to a QT ICMCompressionSession. When drawing to the NSOpenGLView's context, I'm scaling the output to fit the size of the view (preserving proportions). The problem is, I would like to be able to access the output (the drawn CIImage) before scaling, to send it to the ICMCompressionSession. How can I accomplish this, efficiently? Can I create an offscreen OpenGLContext, draw unscaled to it and generate the CVPixelBuffer. And on a second step, just draw the offscreen context scaled to the NSOpenGLView's context. Is this possible, does it make sence? Or should I just create the offscreen context and have 2 CIContexts, one for it and one for the NSOpenGLView and draw the CIImage to both? Wouldn't this mean the CIImage gets processed twice by the GPU? Mark</body>
  </mail>
  <mail>
    <header>CIImageAccumulator mangling image data?</header>
    <body>I have the following problem with CIImageAccumulator: when I set the image of such an accumulator to an image in any colour space different from CoreImage's internal colour space (say Generic RGB), and then extract it again, the pixel data are slightly different than those of the original image. I've written a test program, reproduced below, to illustrate this problem. The program creates a CIImage containing 3 pixels, whose individual values are (in RGB, 8 bits, with alpha=255 in all cases): (1,2,3), (126,127,128) and (253,254,255). This image is then rendered into a bitmap context, and the pixel values are extracted from there. At this stage, they are equal to the input values. However, if I then store the CIImage inside an accumulator, extract the output image from that accumulator, and extract the pixel values from there, I now get (0,0,0), (125,126,128), (254,255,255). This close, but not equal, to the original data. Here is what I think is happening: the accumulator stores its data internally in the linear variant of the Generic RGB colour space that is used by CoreImage. The problem I observe is due to rounding errors after the two-way conversion (non-linear -&amp;gt; linear -&amp;gt; non-linear). Is that analysis right? If yes, does anybody know of a work-around? Ideally, I'd like to be able to specify the colour space to use for the accumulator, but there doesn't seem to be an API for that, unfortunately. I'd be grateful for any help, thanks. Michel. CGContextRef cgContext = CGBitmapContextCreate(data, CIContext* context = [CIContext contextWithCGContext:cgContext [context drawImage:image atPoint:CGPointZero fromRect:[image for (int i = 0; i &amp;lt; 3; ++i) fprintf(stderr, &amp;quot;[= = =] &amp;quot;, data[i*4 + 1], data[i*4 + 2], CGColorSpaceRef genColorSpace = CGColorSpaceCreateWithName NSData* bitmapData = [NSData dataWithBytes:bitmapBytes CIImage* image = [CIImage imageWithBitmap bytesPerRow:3*8 size:CGSizeMake(3,1) format:kCIFormatARGB8 CIImageAccumulator* accu = [CIImageAccumulator [  1   2   3] [126 127 128] [253 254 255] [  0   0   0] [125 126 128] [254 255 255]</body>
  </mail>
  <mail>
    <header>Minimum time between CGEventPost calls?</header>
    <body>Is there a minimum time I should wait between calls to CGEventPost?  It looks like if I post an event too soon after a previous event, the first event gets lost.  I'm seeing this when posting several key ups for modifier keys in quick succession.  The next-to-last modifier gets stuck down if the last modifier up comes too soon.  Each event is created with CGEventCreateKeyboardEvent on our own event source. -Marc</body>
  </mail>
  <mail>
    <header>Re: AW: Re: How to draw a CIImage to CVPixelBufferRef</header>
    <body>I'm working on a similar issue.  I'm using the ICM to encode frames. I'd like to apply filters to video frames before they're encoded. The method I've taken is to encode frames during the decompression sequence callback, which provides a CVPixelBuffer.  I create a CGBitmapContext, create a CIContext from that, then apply filters. The format of the CVPixelBuffer is a bit weird; there appears to be 2 bytes per pixel.  I've managed to create the CGBitmapContext using this: CGContextRef context = CGBitmapContextCreate(CVPixelBufferGetBaseAddress(pixelBuffer), CVPixelBufferGetWidth(pixelBuffer), CVPixelBufferGetHeight(pixelBuffer), 5, CVPixelBufferGetBytesPerRow(pixelBuffer), colorSpace, The color seems to be off though, as though this isn't the correct color setting.  However, I can't create the bitmap context with any other parameter (the kCGImageAlphaNoneSkipFirst is the only setting I have seen for 16 bps image data). Does anyone know what could be going wrong here? More generally, is there any better way to apply filters to frames in the ICM that aren't rendered?  This method unnecessarily requires rendering in the CPU, and I'd like to do this to the images while in the GPU.</body>
  </mail>
  <mail>
    <header>Re: AW: Re: How to draw a CIImage to CVPixelBufferRef</header>
    <body>Core Image will use the GPU when your target context allows it to use the GPU - simplified an OpenGL context on a CI capable graphics card - otherwise it will render in software.</body>
  </mail>
  <mail>
    <header>Re: AW: Re: How to draw a CIImage to CVPixelBufferRef</header>
    <body>Does this mean that Core Image will always fallback to using the GPU, when the target context is not located on video ram?</body>
  </mail>
  <mail>
    <header>Re: AW: Re: How to draw a CIImage to CVPixelBufferRef</header>
    <body>On Nov 8, 2005, at 11:39 AM, Mark Munte wrote: Slightly different: Lock the base address of the pixel buffer and create a bitmap context based on that. Unlock the base address when done rendering. There is a difference that the first approach uses fast hardware rendering while the second happens on the CPU. The first comes with the cost of reading back from the GPU the second doesn't. So which to choose depends on the complexity of the filters used and the GPU</body>
  </mail>
  <mail>
    <header>Re: AW: Re: How to draw a CIImage to CVPixelBufferRef</header>
    <body>thank you for the help so far - I'm new to CoreImage and would like some more details on this. 1) -create a OpenGL context -create a CIContext based on the OpenGL context -draw the CIImage, so it's rendered to the OpenGL context -read the results from the OpenGL context to the CVPixelBufferRef.... how can I do that? Can you give me some more info on that please. 2) -create a CGBitmapContext based on my CVPixelBufferRef... Can I pass the CVPixelBufferRef as the void * data parameter of of CGBitmapContextCreate? Should I set the kCVPixelBufferCGBitmapContextCompatibillityKey attribute on creation of the CVPixelBufferRef for that... is this nescessary? -draw the CIImage, so it's rendered to the CGBitmapContext context and thus, on the CVPixelBufferRef What would be the preffered way of doing it? And what about performance, is it right that the OpenGL context will exist on video ram, while the CGBitmapContext and CVPixelBufferRef will always reside on system ram? At the end I will be displaying the rendered CIImage on a NSOpenGLView, and sending the CVPixelBufferRef to a ICMCompressionSession, whith that on mind, what approach is best? Kind regards, Mark On 08.11.2005, at 19:58, Frank Doepke wrote:</body>
  </mail>
  <mail>
    <header>Re: AW: Re: How to draw a CIImage to CVPixelBufferRef</header>
    <body>These attributes of pixel format or pixel buffer indicate, that the buffer is in a format that allows conversion into the desired format. Eg. yuv buffers are not CG compatible while other formats are not compatible with OpenGL so a buffer of that type cannot be converted into a CVOpenGLTexture.</body>
  </mail>
  <mail>
    <header>AW: Re: How to draw a CIImage to CVPixelBufferRef</header>
    <body>Its me again, I've looked at CVPixelBufferCreate's attributes. What do the following attibutes mean exactly? kCVPixelBufferCGBitmapContextCompatibilityKey Indicates whether the pixel buffer is compatible with Core Graphics bitmap contexts (type CFBoolean). kCVPixelBufferCGImageCompatibilityKey Indicates whether the pixel buffer is compatible with CGImage types (type CFBoolean). kCVPixelBufferOpenGLCompatibilityKey Indicates whether the pixel buffer is compatible with OpenGL contexts (type CFBoolean). Regards Mark</body>
  </mail>
  <mail>
    <header>Only one image cached in CGImageSource ?</header>
    <body>While experimenting with CGImageSource I enabled image caching in the options and opened multiple large (10MB) JPEGs. I came to the conclusion that CGImage would cache only one image. In a document based app, the first image is cached and redrawn very fast, all further images seem to be re-decoded while drawing. Allocating a BitMapContext and drawing the image into it to provide my own cache seems a lot like reinventing the CG wheel and does not make things faster or less complicated.</body>
  </mail>
  <mail>
    <header>PDFKit and page insertion</header>
    <body>I am experimenting a problem concerning the insertion of page in a PDFDocument using PDFKit. I tried to insert a PDFPage in a PDFDocument with the method - insertPage:atIndex:. If the page comes from another PDFDocument than the one I am inserting in, the method doesn't throws an exception but if I ask the object for the page I previously inserted, by calling - pageAtIndex: I get an exception. As the source of the PDFPage isn't specified in the documentation i figured out that it worked for every PDFPage. I kept on trying to insert a page from one doument to another and I tried this : // doc1 = [[PDFDocument alloc] initWith this works. I guess that by creating a new PDFDocument I reseted some private attributes that made the object have a bad behavior. If the method -insertPage:atIndex: doesn't support insertion of a page from another PDFDocument, it would be nice to say it in the documentation. I'd like to know if there is a proper way (without allocating a new object) to insert a page from a document to another one. -- Jean-Fran√ßois GRANG _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: How to draw a CIImage to CVPixelBufferRef</header>
    <body>There is no direct way of doing it. You would have to render the CIImage into a context and read back the results (when you use an OpenGL context) or use a CGBitmapContext based on your pixel buffer and render into it.</body>
  </mail>
  <mail>
    <header>Re: Incremental image loading using CG ?</header>
    <body>I was also thinking about CGLayer(s) but my problem is mostly that I want CoreGraphics to do the decoding because I want to offer all formats supported by CG to the user while having access to the decoded image and progress information at all times. On 08.11.2005, at 15:19, Stephen Chu wrote:</body>
  </mail>
  <mail>
    <header>Re: Real time bitmap scroll update.</header>
    <body>I solved the issue with using CG/Quartz to render my bitmap instead of a copybits. Since others may have similar issues (and I hate reading forums where questions are asked and never answered.) I was able to use CGDataProviderCreateWithData to convert my image into a representation that CGImageCreate could handle, a call to CGContextDrawImage and I now see my bits when I scroll.  I found this in the CTMDemo sample code, which did not show when I did a search for &amp;quot;drawing images with Quartz.&amp;quot;  I found the latter when looking for something  unrelated. The other issue of a real time window update may be more of a carbon/HiView question.  HIViewSetNeedsDisplay and HiViewRender do not work when I use HiViewGetRoot to get the front document class window. I still need to click on the scrollbar in order for the window to update. I really want a function I can call from within my scanning loop that will cause the window to redraw periodically and set the scroll controller. I may have to use a timer, but I need to poll the USB every 4 or so milliseconds. The tight loop is working but it would be nice for the user to have some feedback other than the spinning ball of doom while the scan is progressing. Later after I have captured the data, I will need to update the window from a MIDI callback.  With Quickdraw I could just call the window draw content function and be done with it.  Now I am totally confused. Should I ask this over in the carbon group? It seems that to force a window to render is more of a Quartz issue. -julieP</body>
  </mail>
  <mail>
    <header>Re: clipping path woes</header>
    <body>Well I think your path is a little strange in how it jumps around... if I am reading what you are doing correctly you appear to draw a line from a center point out to an edge of circle. Then move back to the center point and draw another line out to the edge at a different angle. Then draw an arc along the edge of the circle. Then close the path. In other words this path is disjoint in the way it is constructed... second line doesn't start at the end of the first line, etc. So what happens if you do something like the following...? CGContextMoveToPoint(context, c.x, c.y); // move to center point CGContextAddLineToPoint(context, p.x, p.y); // add a line from center to edge CGContextAddArc(context, c.x, c.y, _radius, a, a-radians(_beam), TRUE); // add arc from end of first line along edge of circle CGContextClosePath(context); // close (line from end of arc back to center)</body>
  </mail>
  <mail>
    <header>clipping path woes</header>
    <body>I'm trying to create a clipping path. In the code below I create a path,  if I call CGContextFillPath() as a test, everything works just fine, so my path is looking good. So next I want to use this path to clip some further drawing, problem is the path is ignored and the whole context is drawn into. What am I missing? The CGContextMoveToPoint() / CGContextAddLineToPoint() calls seem to be the problem, if I remove them the clip works, all be it the wrong shape. Thanks Nick CGContextAddArc(context, c.x, c.y, _radius, a, a-radians(_beam), /* CGContextFillPath(context);   works ok */ CGContextFillRect(context, CGRectMake(0, 0, _width, _height)); 		 / * fills whole context */ ---------------</body>
  </mail>
  <mail>
    <header>QuartzComposerTV crashes</header>
    <body>Could anyone shed some light on why the Apple sample code QuartzComposerTV might be crashing on one machine (OSXS 10.4.5, PM 2GHz x 2, 8GB) but not on others? The only substantive difference seems to be that we have a specialty camera (Optronics MacroFire - works fine with other apps) hooked up on the crashing machine, and iSights on the others. I can't change the configurations without a lot of red tape, but maybe if there's a compelling reason. renderer = [[QCRenderer alloc] initWithOpenGLContext:_glContext The parameters to the call appear to be valid, yet it crashes: path: /Users/roland/QuartzComposer_WWDC_QCTV/QCTV_03/build/ Development/QuartzComposerTV.app/Contents/Resources/Contents.qtz Host Name:      Proteon Date/Time:      2006-03-13 06:17:03.210 -0800 OS Version:     10.4.5 (Build 8H14) Report Version: 4 Command: QuartzComposerTV Path:    /Users/roland/QuartzComposer_WWDC_QCTV/QCTV_03/build/ Development/QuartzComposerTV.app/Contents/MacOS/QuartzComposerTV Parent:  WindowServer [10985] PID:    14123 Thread: 0 Exception:  EXC_BAD_ACCESS (0x0001) Codes:      KERN_PROTECTION_FAILURE (0x0002) at 0x00000004 Thread 0 Crashed: 0   libstdc++.6.dylib                   0x946a3ff8 std::_Rb_tree_decrement(std::_Rb_tree_node_base*) + 20 1   ...pple.QuickTimeIIDCDigitizer      0x093c29a8 APWVDOIIDCDigitizerEntry + 46352 2   ...pple.QuickTimeIIDCDigitizer      0x093b715c dyld_stub_getpid + 154772188 3   ...pple.QuickTimeIIDCDigitizer      0x093d91f0 APWVDOIIDCDigitizerEntry + 138584 4   ...ple.CoreServices.CarbonCore      0x90b5a684 CallComponent + 260 5   ...ickTimeComponents.component      0x8f5f4f6c VDSetCompression + 60 6   ...ickTimeComponents.component      0x8f2596ac _SGVideoPrepare + 4348 7   ...ple.CoreServices.CarbonCore      0x90b5ab1c CallComponentFunctionCommon + 1076 8   ...ple.CoreServices.CarbonCore      0x90b5a684 CallComponent + 260 9   ...ickTimeComponents.component      0x8f5f5dc4 SGPrepare + 56 10  ...ickTimeComponents.component      0x8f249994 _SGPrepare + 280 11  ...ple.CoreServices.CarbonCore      0x90b5ab1c CallComponentFunctionCommon + 1076 12  ...ple.CoreServices.CarbonCore      0x90b5a684 CallComponent + 260 13  com.apple.QuickTime                 0x8fbfd8e0 SGPrepare + 56 14  ...QuartzComposer.ExtraPatches      0x016b80dc dyld_stub_pthread_cond_init + 23751120 15  com.apple.QuartzComposer            0x9834a7c8 -[QCPatch(Private) _setup:runtime:] + 396 16  com.apple.QuartzComposer            0x9834a888 -[QCPatch(Private) _setup:runtime:] + 588 17  com.apple.QuartzComposer            0x9834a004 -[QCPatch(Runtime) startRendering:options:] + 192 18  com.apple.QuartzComposer            0x98390074 -[QCRenderer initWithOpenGLContext:pixelFormat:file:] + 520 19  AppController.ob                    0x0004f730 -[AppController _reloadRenderers] + 472 (AppController.m:107) 20  AppController.ob                    0x0005170c -[AppController applicationDidFinishLaunching:] + 1864 (AppController.m:344) 21  com.apple.Foundation                0x928e6cf8 _nsnote_callback + 180 22  com.apple.CoreFoundation            0x907844c4 __CFXNotificationPost + 368 23  com.apple.CoreFoundation            0x9077c5a0 _CFXNotificationPostNotification + 684 24  com.apple.Foundation                0x928d1100 - [NSNotificationCenter postNotificationName:object:userInfo:] + 92 25  com.apple.AppKit                    0x9368f5b8 -[NSApplication _postDidFinishNotification] + 112 26  com.apple.AppKit                    0x9368f4a4 -[NSApplication _sendFinishLaunchingNotification] + 92 27  com.apple.AppKit                    0x9368efec -[NSApplication (NSAppleEventHandling) _handleAEOpen:] + 264 28  com.apple.AppKit                    0x9368eb94 -[NSApplication (NSAppleEventHandling) _handleCoreEvent:withReplyEvent:] + 92 29  com.apple.Foundation                0x928e7d04 - [NSAppleEventManager dispatchRawAppleEvent:withRawReply:handlerRefCon:] + 380 30  com.apple.Foundation                0x928e7b64 _NSAppleEventManagerGenericHandler + 92 31  com.apple.AE                        0x914b1960 aeDispatchAppleEvent(AEDesc const*, AEDesc*, unsigned long, unsigned char*) + 208 32  com.apple.AE                        0x914b17fc dispatchEventAndSendReply(AEDesc const*, AEDesc*) + 44 33  com.apple.AE                        0x914b1654 aeProcessAppleEvent + 284 34  com.apple.HIToolbox                 0x93191940 AEProcessAppleEvent + 60 35  com.apple.AppKit                    0x9368d2dc _DPSNextEvent + 856 36  com.apple.AppKit                    0x9368cdc8 -[NSApplication nextEventMatchingMask:untilDate:inMode:dequeue:] + 116 37  com.apple.AppKit                    0x9368930c -[NSApplication run] + 472 38  com.apple.AppKit                    0x93779e68 NSApplicationMain + 452 39  main.ob                             0x0000dfa8 main + 60 (main.m:55) 40  com.apple.QuartzComposerTV          0x0000224c start + 404 41  com.apple.QuartzComposerTV          0x000020f4 start + 60 Thread 0 crashed with PPC Thread State 64: srr0: 0x000000000004f730 srr1: 0x100000000200f030                        vrsave: 0x0000000000000000 cr: 0x24004422          xer: 0x0000000000000000   lr: 0x000000000004f724  ctr: 0x0000000000000001 r0: 0x00000000c0000000   r1: 0x00000000bfffe0f0   r2: 0x00000000c0000000   r3: 0x0000000000370610 r4: 0x0000000000000002   r5: 0x0000000001800000   r6: 0xffffffffffffffff   r7: 0x0000000000000003 r8: 0x00000000003700e1   r9: 0x0000000000000010  r10: 0x0000000000000001  r11: 0x0000000000000e01 r12: 0x00000000900066c0  r13: 0x0000000000000000  r14: 0x0000000000000000  r15: 0x0000000000000000 r16: 0x0000000000000000  r17: 0x0000000000000000  r18: 0x0000000000309960  r19: 0x0000000000315310 r20: 0x00000000a368b130  r21: 0x00000000a0734364  r22: 0x0000000000000001  r23: 0x0000000000000000 r24: 0x00000000003062b0  r25: 0x0000000000000000  r26: 0x0000000000000000  r27: 0x0000000000348710 r28: 0x0000000090a31040  r29: 0x000000000033e3c0  r30: 0x00000000bfffe0f0  r31: 0x000000000004f588</body>
  </mail>
  <mail>
    <header>Real time bitmap scroll update.</header>
    <body>I have been working a number of years on an open source custom piano roll scanning application.  This code was based on the One Scanner STK demo application. A piano roll can be 15 inches wide  with a length as long as a football field 100 yards or more unrolled. Typically they are scanned at 300 dots per inch. I have written a simple application that reads my custom USB scanner interface. This is working as far as I can read the data and save the data as a single line of  one bit deep bi-level bitmap packed 8 bits per byte. Now I want to use Quartz (Under panther) to add this line to a window. The scanning array can provide a new line every 3.8 milliseconds. This works out to a data rate of one megahertz. I want to draw this into the display window as it arrives into the array. The window fills from bottom to top as the top fills the window needs to scroll down, this simulates the effect of a player piano. Using the ScrollAndZoom sample, I am able to install a copybits.  I am unclear as to how to get the actual window from the context. To test the The USB code fills an array with data bit packed with 8 bits per byte. The array for testing is 4000x5000 which represents one tile. A code fragment from the carbonized app  follows, Note that I am already in a Quartzlike reference with 0,0 at the bottom right. Since the raw data is buffered in the array I use that to create overlays of the bitmap. This seems Ideal for Quartz, but I am too much of a Quickdraw programmer to know how to set up as simple background blit data source. It is important that the data in the window is updated and re drawn every few lines.  The app also needs to autoscroll the data from time to time, So I am unsure as to where to put the draw window function. In the scroll routines like the demo? Inside the USB read callback? Or does Quartz have some sort of invalRect/Rgn that will flush the bits from the array into the window buffer when called from either the scroll callback or the USB callback? void PaintBackground(WindowRef window) float scaleF, zoomF; //scPortSep if(!window) if(doc) else segment = /* calculate how much the segment is scrolled */ delta = (*dr).tile[segment].offset_set_tile_start + /* draw from bottom to top */ for(i = 0;i &amp;lt; (*dr).allocated_segments;i++) if((*dr).tile[segment].format != kCISType) GetPortBounds((*dr).RollBKG,&amp;amp;portVisRect); //= OffsetRect(&amp;amp;(*dr).redrawRect,(unsigned /* scale the source rectangle */ vtotal = ((*dr).redrawRect.bottom-(*dr).redrawRect.top) * htotal = ((*dr).redrawRect.right-(*dr).redrawRect.left) * if((*dr).redrawRect.top &amp;lt; 0) /* clamp right to window */ if((*dr).redrawRect.right &amp;gt; (*dr).rollWidth) (contRect.right-contRect.left)) /* window is scaled or split */ #if 0 // debug blanks PenPat(GetQDGlobalsBlack(&amp;amp;black));                                // frame user areas #endif if(contRect.right - lowerPane.right &amp;gt; 0) ClipRect(&amp;amp;lowerPane); /* this is the rect we draw into */ /*backscale modified rect */ (*dr).redrawRect.top = (*dr).redrawRect.bottom - (*dr).redrawRect.right = (*dr).redrawRect.left + if((*dr).RollBKG) if (LockPixels(thePix)) CopyBits((BitMap *) *thePix,GetPortBitMapForCopyBits(GetWindowPort(window)), else if(lines) else</body>
  </mail>
  <mail>
    <header>Re: CGColorSpace release from CIColorControls</header>
    <body>Hello everyone, Well, I pulled it all back to a Hello World implementation with just the image conversions, the filter and the output, and I couldn't replicate the problem. I'm clueless at this point. I think I'm going to file this one under &amp;quot;Do it again and hope it works next time&amp;quot; which itself is in my &amp;quot;Paper over the cracks&amp;quot; file. It's a school project which has to survive a 5 minute demo meeting, I should be able to find a way even if I can't plug the leak. Thanks guys, Chris</body>
  </mail>
  <mail>
    <header>Re: CGColorSpace release from CIColorControls</header>
    <body>Hi Frank, Yeah, it appears to be once an image. I'm not sure what you mean &amp;quot;where I see it leaked from&amp;quot;. Its being leaked by applying the CIColorControls filter to the CIImage, if I don't apply the filter (I've used an Affine Transform successfully), I don't have any problems.  The CGColorSpaces are all allocated when I first create the CIImage from the NSImage. Here's a trace of the retains and releases from OmniObjectMeter: +1 - allocated - when I call convertToCIImage from the NSImage category (I've emailed to this thread the line that I edited from Dan's code to do this) _CGColorSpaceRetain: +1 _CGImageCreate +1 _load_image_format +1 _CGCOlorTransformConvertNeedsCMS +1 _fe_tree_new_sampler +1 _fe_tree_new_image +1 _fe_tree_color_rgb_colorspace +1 _fe_tree_expand_color_1 -1 _CGColorTransformConvertNeedsCMS -1 _fe_tree_expand_color_1 -1 _imageFinalize -1 _image_buffer_finalize -1 _SetCustomCGColorSpace -1 _xform_release This leaves a count of +2 retains. I tried using Roland's new code using imageWithData (which was a great find!), and I also end up with +2 retains, the new retain elements added to that release fine, and I end up with the same leftover. If I can provide any more information, I will do my best to find it for you guys :) Thanks! Chris</body>
  </mail>
  <mail>
    <header>Re: CGColorSpace release from CIColorControls</header>
    <body>Hey Roland, I'm not the newbie to ask whether it's inferior, but it does seem to do the job, yes :)  It's a bit nicer than what Dan and I were doing, so I've put that in instead. Thanks! However, you need to get rid of the return autorelease, it looks like imageWithData autoreleases already, and you'lld crash when you get back again. I used ObjectAlloc, and it's not leaking the images if you bail the autorelease. However, I'm still seeing a CGColorSpace leak, I'm just half way through an email about that :D Cheers Chris</body>
  </mail>
  <mail>
    <header>Re: CGColorSpace release from CIColorControls</header>
    <body>- (CIImage *)convertToCIImage CIImage *returnImage = [CIImage imageWith On Mar 10, 2006, at 4:18 PM, Chris Lewis wrote:</body>
  </mail>
  <mail>
    <header>Re: CGColorSpace release from CIColorControls</header>
    <body>Hello Roland, I edited the line to read this instead: - (CIImage *)convertToCIImage CIImage *returnImage = [[CIImage alloc] initWithBitmapImageRep:[NSBitmapImageRep imageRepWith It seems to work: it doesn't leak when I am just doing the Affine Transform filter. Cheers Chris</body>
  </mail>
  <mail>
    <header>Re: CGColorSpace release from CIColorControls</header>
    <body>On Mar 10, 2006, at 12:34 PM, Chris Lewis wrote: How did you get Dan Wood's NSImage category to work? The line that says: Thanks, Roland - (CIImage *)toCIImage CIImage *im = [[[CIImage alloc] initWithBitmapImageRep:bitmapimagerep]</body>
  </mail>
  <mail>
    <header>Re: CGColorSpace release from CIColorControls</header>
    <body>Where do you see the colorspace being leaked from and is it one colorspace per drawing of an image?</body>
  </mail>
  <mail>
    <header>CGColorSpace release from CIColorControls</header>
    <body>Good evening everyone, I hope I find you well! I've been progressing in my understanding of Cocoa and reading up on Quartz with the Programming With Quartz book (which is quite lovely), and I seem to be getting the hang of retain/release counts and generally being able to find holes in my program when they crop up, but this one is stumping me. I get an NSImage which I convert to a CIImage using Dan Wood's code (). I then create a CIColorControls filter, do some adjustments, and get a new image out. I then convert the image back to an NSImage and place that into an NSView. I'll be optimising that to draw the CIImage directly rather than the conversion. I'm also making new filters each time, which I'll also fix up. I'm having a problem with  CGColorSpaces being leaked, which in turn leaks CFData, CGDataProviders and CMProfiles .Everything else is being freed up fine, my CIImages, NSImages, CIColors, and CIFilters are all being cleaned nicely, along with every other object in ObjectAlloc. - (void)updateCameraView : (NSNotification *)note CIImage *ciAdjustedImage = [ciCurrentImage [adjustedImage release]; // Cocoa will throw this away if nil - (CIImage *)adjustImageSaturation : (double)saturationValue brightness : (double)brightnessValue contrast : (double)contrastValue [f setValue:[NSNumber numberWithDouble:saturationValue] [f setValue:[NSNumber numberWithDouble:brightnessValue] [f setValue:[NSNumber numberWithDouble:contrastValue] I don't believe Dan Wood's code is wrong, displaying the mirrored CIImage (transformed using a CIAffineTransform filter) doesn't have the same problem (I guess because AffineTransform doesn't change the color space). If I don't run the mirrored image through the adjustment filter, I will not leak. If I don't place the image into the adjustedImageView, I won't leak either (not sure about that one). Has anyone seen this behaviour exhibited? Any ideas how I can free up these CGColorSpaces? I guess I am doing something silly here (aside from making new filters and converting to NSImages :D) Many thanks everyone, your help was invaluable in the past, and I sincerly appreciate you guys taking your own time to give me a hand. Cheers, Chris Lewis</body>
  </mail>
  <mail>
    <header>The Efficiency of Clipping</header>
    <body>In my application, I need to clip a particular shape out of the current context (draw everywhere but inside this shape).  To do that, I construct a compound path with a rectangle derived from CGContextGetClipBoundingBox and my shape.  (Then I EOClip). The results I get are just what I want but we do this frequently enough that it's consuming about 10% of our drawing time.  I'd like to knock this down a bit if I can. One thought that occurred to me was that I could probably generate a tighter &amp;quot;outer path&amp;quot; (the one currently created form CGContextGetClipBoundingBox) if that would help.  Before I get too crazy about it, however, I thought I would ask, is that likely to buy me anything?  Does the size of the clipping &amp;quot;region&amp;quot; affect the speed of the clipping operation? Also, in this case I could probably live with the fact that the edges of the clipping area are not antialiased so long as the drawing that is clipped is.  Is that possible to do and, if so, is it likely to improve my performance any? Scott</body>
  </mail>
  <mail>
    <header>CIImage from CVPixelBufferRef?</header>
    <body>Is there a function or other means to get a CIImage from a CVPixelBufferRef?</body>
  </mail>
  <mail>
    <header>Re: CIImage to char * data</header>
    <body>This definitely should work if everything is set up correctly (pixelformat etc.). I would first check your rendering by rendering into an onscreen context. Also just draw some OpenGL primitive into the context to see if the context is not rendering in general or is it just a problem with the CI rendering.</body>
  </mail>
  <mail>
    <header>Re: CIFilter Dashboard Widget?</header>
    <body>You need to log into ADC, you find it in the Downloads section under Developert Tools</body>
  </mail>
  <mail>
    <header>CIFilter Dashboard Widget?</header>
    <body>Does anyone know where I can get that dashboard widget that listed all the CIFilters? A quick Google search, or a look through the Dashboard downloads, doesn't show anything. Thanks, Colin</body>
  </mail>
  <mail>
    <header>Re: Drawing Performance issue</header>
    <body>On Mar 8, 2006, at 2:15 PM, Ricky Sharp wrote: The display is &amp;quot;millions&amp;quot; but I don't have the displays calibrated. Theoretically, calibration shouldn't be an issue because I am drawing using the display's color space.</body>
  </mail>
  <mail>
    <header>Re: Drawing Performance issue</header>
    <body>First, sorry if this is a duplicate.  web interface timed out just as I was sending the original. Anyhow... I googled on this an &amp;quot;CMMConvTask&amp;quot; shows up in several stack traces like this one: Thread 1: 0 libSystem.B.dylib 0x900251e7 semaphore_wait_signal_trap + 7 1 com.apple.ColorSync 0x91590b8f pthreadSemaphoreWait(t_pthreadSemaphore*) + 35 2 com.apple.ColorSync 0x915ab088 CMMConvTask(void*) + 60 3 libSystem.B.dylib 0x90024b47 _pthread_body + 84 So it's definitely a ColorSync routine. I don't know why the slowdown, but two questions come to mind: * Is your display calibrated? * What's the bit depth of the display? I'm assuming you have it set to millions? -- Rick Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Drawing Performance issue</header>
    <body>In profiling a drawing performance issue with sampler, I found that by far my application is spending most of it's time in a thread whose Thinking that this was something to do with Color Management (Because of the CMM in the name), I double checked my drawing code.  I even went so far as to always draw using the display color space just to see if that had an impact.  I didn't really see one. Can anyone confirm if this is a color matching routine?  If you can suggest why my application might spend 65% of it's time in this routine,I would be grateful. The operation I was testing was maximizing (and minimizing) the window repeatedly.</body>
  </mail>
  <mail>
    <header>Re: CIImage to char * data</header>
    <body>Thanks for this but I've added a glFlush abut I still cannot glread from a . I built a QC Renderer and I can perform glread on this no problem and the same type of filters run much quicker that in software so gpu is being used in this case, however I want to use CIFilters. The CIVideoDemoGL example works ok. However I notice that this and all other examples are making the [CIContext contextWithCGLContext from an NSOpenGLView. i.e. an onscreen view. But all offscreen code seems to use CGLCreateContext and CGLSetCurrentContext. It's seems like it's still performing software rendering??? Which could be the reason the glread returns with the buffer untouched.</body>
  </mail>
  <mail>
    <header>Re: ATSUI text rendering performance problem</header>
    <body>On Mar 7, 2006, at 9:10 AM, Teemu Ikonen wrote: Yes, calling flush is a Bad Idea‚Ñ¢ except under very carefully controlled circumstances.  In many cases it can cause your application to block for as much as 1/60th of a second.</body>
  </mail>
  <mail>
    <header>Re: ATSUI text rendering performance problem</header>
    <body>For interested, the problem was actually two fold, the design issue of unnecessary graphic context reuse and the stupid user problem part.. ATSUDrawText( .. ) CGContextFlush(ctx);  // DEBUG: remove me These together destroyed the performance on NSView draw, when I eliminated the flush the performance increased 10-fold. By eliminating the graphic context reuse I got even more speed though not so drastic anymore :)</body>
  </mail>
  <mail>
    <header>Re: ATSUI text rendering performance problem</header>
    <body>Ouch. A few things I would suggest changing: 1) Don't get the window's graphics context. Always use the graphics context of your view. This is either from [NSView lockFocus]; gc = [NSGraphicsContext currentContext], or simply [NSGraphics currentContext] if you're within a drawRect call. 2) If you're in a drawRect call, get the current context from 3) Don't store those graphics contexts. For all we know, they're set up at-render time and are specifically used for that drawing operation. This is most likely the cause of the problem you're seeing. The overhead for grabbing the current context each time through the drawRect method will be dwarfed by the actual drawing you do. I wouldn't be surprised if switching to the currentContext model clears up all of the problem's you're seeing. FWIW, that's what I've used in combination with ATSU, and it blazes* -Jon * Blazes as fast as ASTUI blazes ;) -- Jonathan Johnson REAL Software, Inc.</body>
  </mail>
  <mail>
    <header>ATSUI text rendering performance problem</header>
    <body>I've the strangest problem with ATSUI text rendering performance. At high level my code is constructed like this. - Cocoa application - Custom NSView in a window, overridden drawRect to draw content - NSView is auto resized by Cocoa on window resize. - If my data changes, I use setNeedsDisplay  to the NSView The drawRect is implemented like this - Clear the background with fillRect - Draw lines of text with ATSUDrawText (layouts are created and disposed for each text draw) The text draws are done using separate CGContextRef handle, that I've acquired using these calls NSGraphicsContext *gc = [[NSGraphicsContext I have the textContext in class variable, so I set it on first call to drawRect and then just reuse it. The problem in nutshell is, that when window manager calls drawRect after resize the text rendering is  fast. But when I use 'setNeedsDisplay' to get window manager to call drawRect, the ATSUDrawText is very, very slow in rendering the text. I can actually see as the text lines show up in the view one by one. When draws are done on resize, rendering happens like following - view is cleared on call to fillRect - text is rendered line by line in view When draws are done by call to setNeedsDisplay, rendering happens like following - view is not cleared on call to fillRect - text is rendered line by line in view over the old text (looks messy) - the view flashes and the correct view appears (just like after resizing) So my conclusion is that the window manager handles something (like graphic buffering) differently on resize as after call to setNeedsDisplay.</body>
  </mail>
  <mail>
    <header>Re: CIImage to char * data</header>
    <body>Thanks, that makes sense. So I want to use the OpenGL context to use the gpu, but I can't get glread to do anything. How does glread know where to get the pixels from? Does the CGLSetCurrentContext do this? One difference between my code and CIVideoDemoGL is that I use [CIImage imageWithBitmapData ... to specify the source of image data rather than [CIImage imageWithCVImageBuffer ... However I know that this bit works, because it's how I made the CIImage for the CGLContext. _ciContext = [CIContext contextWithCGLContext:cgl_ctx pixelFormat:_pixelFormat options:[NSDictionary dictionaryWithObject: [_ciContext drawImage:[_affineFilter valueForKey:@&amp;quot;outputImage&amp;quot;] // readback glReadPixels(0, 0, 1920, 1080, GL_BGRA, // cleanup</body>
  </mail>
  <mail>
    <header>Re: CIImage to char * data</header>
    <body>On Mar 6, 2006, at 9:55 AM, email@hidden wrote: You can't pass NULL to that routine (as the buffer space) so far as I know. The documentation doesn't say anything about that routine being able to allocate memory. I ran a test on my machine to see what happens if you do pass NULL as the memory pointer.  I got back NULL indicating that the context could not be created.</body>
  </mail>
  <mail>
    <header>Re: CIImage to char * data</header>
    <body>The difference is that the bitmap context is rendering purely in software not on GPU. When using the OpenGL context with glReadPixels you render on the GPU but there is the price of the read back.</body>
  </mail>
  <mail>
    <header>Re: CIImage to char * data</header>
    <body>Three questions on this :- I've seen CGBitmapContextCreate used with a null pointer, where is the memory in this case? I'm using a CGBitmapContext as suggested to draw Core Images. I've also added some additional Quartz drawing and it all works well. However I've noticed that the bitmap buffer is always up to date. Surely this means that every step of drawing is being copied from the gpu to ram. This doesn't seem efficient and my example runs quite slowly. Is it possible to delay the update of the bitmap buffer? Or otherwise speed this up. Or is the answer to use glRead? I've tried to use glread as follows, but glread returns immediately with no errors and the buffer isn't updated. (CGLPixelFormatAttribute)24, kCGLPFAAlphaSize, _ciContext = [CIContext contextWithCGLContext:cgl_ctx pixelFormat:_pixelFormat options:[NSDictionary dictionaryWithObject: [_ciContext drawImage:[_ciFilter valueForKey:@&amp;quot;outputImage&amp;quot;] // readback glReadPixels(0, 0, _dstWidth, _dstHeight, GL_BGRA,</body>
  </mail>
  <mail>
    <header>Re: CIImage to char * data</header>
    <body>There are 2 ways to do that: You can use a CGLContext as a base for your CIContext and use glReadPixels to read back the data. As an example you can look at the CIVideoDemoGL sample code that uses this mechanism to export the movie: CIVideoDemoGL/CIVideoDemoGL.html Or you can create a CGBitmapContext and create a CIContext from that using contextWithCGContext and draw into that.</body>
  </mail>
  <mail>
    <header>CIImage to char * data</header>
    <body>Hi all, I'm trying to get access to the pixel data of a CIImage.  My idea was to get a CGImageRef from the CIImage through [CIContext createCGImage: ...], then draw that CGImageRef to a CGBitmapContextRef, finally accessing the pixels through the bitmap.  This does not seem to be working as planned; while I don't seem to be getting any errors, the pixels are all 0 (r=0, g=0, etc) valued. Here is the code: CGColorSpaceRef CGImageAlphaInfo dst_alphainfo = has_alpha ? kCGImage unsigned char *bitmapData = malloc(sizeof(char) * 320 CGContextRef dst_contextref = CGBitmapContextCreate(b itmapData, 320, 240, bps, rowBytes, dst_colorspaceref, dst_alphainfo CGImageRef src_imageref = CGContextDrawImage( dst_contextref, theRect, src_imag position = bitmapData unsigned char unsigned char unsigned char unsigned char // do something with r g b</body>
  </mail>
  <mail>
    <header>Re: Shading anomalies</header>
    <body>On Mar 3, 2006, at 3:33 AM, Torsten Radtke wrote: I believe that somebody posted a variation on this problem to this list before.  Neverthless, you should write it up with examples and put it in bugreport.apple.com.</body>
  </mail>
  <mail>
    <header>Shading anomalies</header>
    <body>I discovered a problem with CGShadings on Tiger: If I draw a horizontal shading (i.e. the y-coordinates of start and end point are equal) and the red component of both the start and end points is equal to 1.0, the shading will be opaque (alpha values for both start and end point will be ignored). If I change the red component for one of the points to a value less than 1.0, or change the y-coordinate of one of the points, the shading draws as expected. Here is the code: - (void) drawRect: (NSRect) rect CGContextRef context = [[NSGraphicsContext currentContext] shadingFunction = CGFunctionCreate(NULL, 1, inputValueRange, 4, shading = CGShadingCreateAxial(colorSpace, CGPointMake (rect.origin.x, rect.size.height*0.5f), CGPointMake(rect.size.width, rect.size.height*0.5f), static void evaluateGradient(void *info, const float *in, float *out) Thanks for any help, Torsten</body>
  </mail>
  <mail>
    <header>Re: [newbie] drawing a Quartz animated view with transparency</header>
    <body>This will composite clear instead of setting it.  To replace the color in the destination, you'd want:</body>
  </mail>
  <mail>
    <header>Re: [newbie] drawing a Quartz animated view with transparency</header>
    <body>On Mar 1, 2006, at 6:53 AM, Renaud Boisjoly wrote: I'm afraid I can't help with that.  More likely than not the contents of that view are actually being drawn by OpenGL.  My guess would be that you have to set up something in Composer that would clear the background on each frame.</body>
  </mail>
  <mail>
    <header>Re: [newbie] drawing a Quartz animated view with transparency</header>
    <body>I've tried subclassing QCView and placing it in my subclass' drawrect method but it only gets called once when the original view is drawn, not during the animation... The view is properly transparent, my animation draws correctly with its alpha mask, it just doesn't erase itself as it animates. The same way it does if I disable Erase Background in Compositor...</body>
  </mail>
  <mail>
    <header>Re: Various problems with CoreVideo/CoreImage</header>
    <body>^^^ Oops. Sorry I wanted to say: &amp;quot;I'm not sure it is the problem&amp;quot;. Yves Schmid * GarageCube</body>
  </mail>
  <mail>
    <header>Re: Which color space?</header>
    <body>I was basing my &amp;quot;misconception&amp;quot; on the docs: Simon</body>
  </mail>
  <mail>
    <header>Re: Which color space?</header>
    <body>Simon</body>
  </mail>
  <mail>
    <header>Re: Which color space?</header>
    <body>Thank you for your response David. ¬†As I hinted at in my original email, I *have* looked at the docs/headers, but everything seems to have an alpha channel. ¬†I&amp;#39;d like to create a simple grayscale image that does *not* contain an alpha channel. Simon</body>
  </mail>
  <mail>
    <header>Which color space?</header>
    <body>¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† pixelsWide:(int)[inputImageRep pixelsWide] ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† samplesPerPixel:(int)1 ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† isPlanar:(BOOL)NO</body>
  </mail>
  <mail>
    <header>exported QT .qtz cannot be reedited in QC4</header>
    <body>Hello list, Although we used to be able to edit exported quicktime quartz's in the quartz composer, after updating to Snow Leopard and Quartz Composer 4 this versatile function works no more. Is there a work around, or am I missing something? Any help on this matter is greatly appreciated. thnx Terry</body>
  </mail>
  <mail>
    <header>Re: Core Image - Image unit question</header>
    <body>In general, you cannot pass any information from one pixel to another. So the usual algorithm for, say, a Floyd-Stienberg dither can't be done in Core Image. There may be some way to work around the limitation, though. For example, for each pixel in the output, sample the surrounding input pixels that would normally be 0 in the Floyd-Stienberg kernel, dither each one to figure out the error propogated from each one, add those values together, add them to the current pixel, and round it off as appropriate. It means you'll be sampling a bunch of times more than you would with the traditional algorithm, so it might not be very fast. And I can't verify that my method will give identical results. :-) But it's worth a try. Darrin -- Darrin Cardani - email@hidden President, Buena Software, Inc. Video, Image and Audio Processing Development</body>
  </mail>
  <mail>
    <header>Re: User Spaces vs. Generic Spaces</header>
    <body>On May 13, 2005, at 3:41 AM, Jerry wrote: I'll second that :) Thanks, -Jon -- Jonathan Johnson REAL Software, Inc.</body>
  </mail>
  <mail>
    <header>Re: Is there a way to generate an EPS file from a PDF file</header>
    <body>There is a simple way in Cocoa - NSView has method -dataWithEPSInsideRect:, which can be used as this: 1. Create a subclass of NSView, which draws your PDF using NSPDFImageRep in its drawRect: method, having view bounds same as PDF size Note that this may have some problems if you use color blends and transparency in PDF, but the output is quite ok in most cases. If your app is not Cocoa, you could write a simple app that does this transformation on files. It should take an hour or two for a Cocoa guy....</body>
  </mail>
  <mail>
    <header>Re: User Spaces vs. Generic Spaces</header>
    <body>On 12 May 2005, at 19:04, Haroon Sheikh wrote: Yes please.</body>
  </mail>
  <mail>
    <header>Core Image - Image unit question</header>
    <body>A fair while ago I wrote a program called Mac2Spec as part of an effort to learn Obj-C. I've recently revisited it and unsurprisingly it could be improved... The program basically applies a few filters to a perfectly good bitmap converting it into a brightly coloured mess suitable for display on the mighty Sinclair Spectrum. My question is can these filters be re-written in terms of Core Image? The reason I ask is that the simple examples I've seen so far are of the 'Apply this algorithm to each pixel in the image' variety. I want something a bit different: an ordered dither for example. To do an ordered dither I need to divide the image into squares (4x4 say); count how many pixels in a square are above a threshold; set pixels within the square depending on how many were above the threshold. Does this sound like the sort of thing that Core Image can do?  If so pointers as to how to start are welcome! Does Apple have some good documentation on the CIKernel language? All I found was a mention that it is similar to OpenGL Shading Language (and a plug for a book) but without the 3D bits. The stuff on the web that I've found on the subject concentrates on the 3D bits so that the world may have ever better robotic death spiders... James</body>
  </mail>
  <mail>
    <header>Re: cache or draw direct?</header>
    <body>In case this helps: 1.  With Quartz, you &amp;quot;draw into&amp;quot; 1 of 2 things: either: a)  CGContextRef this is like a window, or a PDF document, or similar. I think of it as a window. b)  CGBitmapContextRef this is like a gworld.  a bitmap buffer. 2.  In QD, you could &amp;quot;jump&amp;quot; between a window and a gworld using CopyBits.  Draw to the screen directly, and the CopyBits the bitmap over to a Gworld, or vice versa (usually vice versa, but not always). In Quartz, no more CopyBits, so you have to think ahead.  If you are every going to want those bits for anything, you had better write them into a CGBitmapContextRef first.  Once you draw to a CGContextRef (i.e. a window), it's gone. You can &amp;quot;CopyBits&amp;quot; in one direction:  CGBitmapContextRef --&amp;gt; CGContextRef. Here is the outline of the code, with gworld stuff (as I think about it) in [ ]. a)  CGBitmapContextCreate [ make the gworld ] b)  Do your drawing into that CGBitmapContextRef  [ set the port to the gworld ] c)  When you're ready to put that into a window (CGContextRef), get the &amp;quot;bits&amp;quot; in the CGBitmapContextRef via: d) Then make a CGImage using this provider e) Then draw this CGImageRef into your window (CGContextRef) [ (c)-(e) = CopyBits(gworld --&amp;gt; window),  CG style ] (note:  you can draw into a CGContextRef until you run out of electrons, but you won't see anything with your eyes until you do a CGContextFlush (unless you're playing with compositing windows and such)). 3.  Some Quartz drawing commands are deadly on speed.   Unless you're printing (or making a PDF), never stroke a path when you can just fill it.  StrokePath does all kinds of intersection tests on the path so the PDF stuff comes out perfect; but for screen drawing and refreshing on anything complicated, it will bring your app to its knees.   Figure out the &amp;quot;bounding path&amp;quot; for a given path, and fill it.   Silly, I know it sounds, but ... Similarly, cacheing to a CGBitmapContextRef (and either saving the CGBitmapContextRef or the CGImageRef you make from it) is essential for speed - if you needed to do this in QD for your app, you probably need to do the same in Quartz. Probably keeping a &amp;quot;core&amp;quot; template image in a CGBitmapContextRef, and then duplicating and then drawing into this duped CGBitmapContextRef any new tick marks etc. needed for a refresh would be the way to go. -Robert -- Robert R. Curtis, Ph.D. Managing Partner MathMonkeys, LLC Producers of LiveMath Software 26 Church Street, Suite 300 Harvard Square Cambridge, MA 02138 USA 617.497.2096 617.497.2116 FAX email@hidden AOL IM:  LiveMath ICQ:  258662981 Yahoo IM:  LiveMathRobert2004</body>
  </mail>
  <mail>
    <header>Re: IKImageView choppy?</header>
    <body>Question:  Have you profiled the code using Instruments to see what segment of code is being used during the 3 seconds?</body>
  </mail>
  <mail>
    <header>IKImageView choppy?</header>
    <body>the ImageKit is part of Quartz right? I have an IKImageView, and I'm putting CGImages (That I make out of NSImages) onto it. However, a normal 200DPI 8.5/11 page takes like 3 seconds to come up, appearing in rectangles about 2inches (screen) on a side at a time. This is really annoying. Is there a way around this? Alternatively, is there a way to double buffer the view? To have 2 IKImageViews and draw into one, and then display it? Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Precision issues with Gaussian Blur image unit resulting in	Gamma/darkening</header>
    <body>Hi. I'd like to confirm a possible bug I just found with Core Image (or, possibly Quartz Composers Core Image wrapper support) Create a composition with an image input, image out to Apples Gaussian Blur, to a billboard. Note the color of your image. Now, make the radius of the Gaussian Blur very low, something like, 0.0001, note the shift in gamma in your image, it becomes ~10% darker. A value of 0.001 has no effect, so this looks like a precision issue. Now, I know, &amp;quot;dont do that&amp;quot;, unfortunately, when using Quartz Composer (or well, even outside, but I have not tested in a host app), you can get into situations where an LFO, Interpolator, Timeline, or random will give you that value, thus rendering an incorrect image. Simple Patch demonstrating: I'll file a bug, but I thought it worth mentioning on the list. Apologies for the cross post, but I thought both 'teams' (?) might be interested, or have something to say. Thank you. Attachment:</body>
  </mail>
  <mail>
    <header>Re: Coordinate inconsistencies</header>
    <body>Thanks, that solved the problem. Andreas --</body>
  </mail>
  <mail>
    <header>Re: Coordinate inconsistencies</header>
    <body>Quartz draws its stroke evenly on both sides of the line. The normal expectation of a line drawn from 0,0 to 640,0 with a line width of 1 and 1:1 context:device pixel mapping is that you get a 1 pixel wide line at 50% opacity due to antialiasing... But since you turned antialiasing off, Quartz has to pick a side for the stroke to fall on. I would instead recommend you turn antialiasing off and present your lines such that the stroke will fall evenly on either side to produce your desired result. That typically means for a line width of 1 and 1:1 context:device pixel mapping that you offset your coordinates by 0.5. If you want something resolution independent, you can use the user to device transform function to calculate the actual offset needed. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: Coordinate inconsistencies</header>
    <body>Yes, of course. That's all set up correctly and I'm just leaving this out for briefness' sake. As I said, it all works fine when using y=1. I.e. this will draw to the *first* line of my 640x480 display with the color specified in CGContextSetRGBStrokeColor(): And this does nothing: My assumption is that the latter is attempting to draw at a y-position of -1 which is of course not visible. I did some further tests and it really seems that the y-position is one based. I.e. passing y=2 will draw on the *second* line from the top -- not the third as one would assume! The x-position is zero based, however. It's somewhat confusing. Antialiasing is turned off, btw. Andreas --</body>
  </mail>
  <mail>
    <header>Re: Coordinate inconsistencies</header>
    <body>Did you set the stroke color? Quartz keeps separate stroke and fill colors, and your stroke color might just be the same as the background color. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>CGPDFPageRef memory leak</header>
    <body>&lt;SPAN style="FONT-FAMILY: Helvetica; FONT-SIZE: medium" class=Apple-style-span&gt; I am new to &amp;nbsp;iphone application development. I found a query on "CGPDFPageRef memory leak" by&amp;nbsp;Carlo &lt;FONT class=Apple-style-span size=3 face="'Lucida Grande', Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 12px" class=Apple-style-span&gt;&lt;FONT class=Apple-style-span face=Helvetica&gt;&lt;SPAN style="FONT-SIZE: medium" class=Apple-style-span&gt; &lt;SPAN style="FONT-FAMILY: 'Lucida Grande', Geneva, Arial, Verdana, Helvetica, sans-serif; FONT-SIZE: 12px" class=Apple-style-span&gt; "Does anyone have an idea why this code would leak memory? The method is used to flip through the pages of the PDF document, but something along the line does not seem to be released properly. I narrowed the problem down to this chunk of code since the footprint of my application shoots up drastically whenever I pull a page that has graphics on it. It's been driving me crazy for the past several days, so any help would be greatly appreciated! -(&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;UIImage&amp;nbsp;*)pageAtIndex:(&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;NSInteger)pageNumber withWidth:(&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;CGFloat)width andHeight:(&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;CGFloat &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(180,20,161)"&gt;if((pageNumber&amp;gt;&lt;SPAN style="COLOR: rgb(57,0,213)"&gt;0) &amp;amp;&amp;amp; (pageNumber&amp;lt;=&lt;SPAN style="COLOR: rgb(85,129,134)"&gt;pageCount &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;CGFloat&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;scaleRatio;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt; // multiplier by which the PDF Page will be scaled &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;BR class=webkit-block-placeholder&gt; &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;UIGraphicsBeginImageContext&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;(CGSizeMake&lt;SPAN &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;CGContextRef&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;context =&amp;nbsp;UIGraphicsGetCurrentContext&lt;SPAN &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;CGPDFPageRef&amp;nbsp;page =&amp;nbsp;&lt;SPAN style="COLOR: rgb(63,11,128)"&gt;CGPDFDocumentGetPage(&lt;SPAN style="COLOR: rgb(85,129,134)"&gt;pdf &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;CGRect&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;pageRect =&amp;nbsp;CGPDFPageGetBoxRect&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;(page,&amp;nbsp;kCGPDFBleedBox&lt;SPAN &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;BR class=webkit-block-placeholder&gt; &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;//Figure out the orientation of the PDF page and set the scaleRatio accordingly &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(180,20,161)"&gt;if(pageRect.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;size.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;width/pageRect.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;size.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;height&amp;nbsp;&amp;lt;&amp;nbsp;&lt;SPAN style="COLOR: rgb(57,0,213)"&gt;1.0 &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;scaleRatio = height/pageRect.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;size.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;height &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt; &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(180,20,161)"&gt;else &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;scaleRatio = width/pageRect.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;size.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;width;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt; &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt; &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;BR class=webkit-block-placeholder&gt; &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;//Calculate the offset to center the image &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;CGFloat&amp;nbsp;xOffset =&amp;nbsp;&lt;SPAN style="COLOR: rgb(57,0,213)"&gt;0.0 &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;CGFloat &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(180,20,161)"&gt;if(pageRect.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;size.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;width &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;xOffset = (width/&lt;SPAN style="COLOR: rgb(57,0,213)"&gt;2)-(pageRect.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;size.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;width*scaleRatio/&lt;SPAN style="COLOR: rgb(57,0,213)"&gt;2 &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt; &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(180,20,161)"&gt;else &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;yOffset = height-((height/&lt;SPAN style="COLOR: rgb(57,0,213)"&gt;2)-(pageRect.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;size.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;height*scaleRatio/&lt;SPAN style="COLOR: rgb(57,0,213)"&gt;2));&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt; &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt; &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(63,11,128)"&gt;CGContextTranslateCTM(context, xOffset, &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(63,11,128)"&gt;CGContextScaleCTM(context,&amp;nbsp;&lt;SPAN style="COLOR: rgb(57,0,213)"&gt;1.0, -&lt;SPAN style="COLOR: rgb(57,0,213)"&gt;1.0 &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;CGContextSaveGState&lt;SPAN &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;CGAffineTransform&amp;nbsp;pdfTransform =&amp;nbsp;&lt;SPAN style="COLOR: rgb(63,11,128)"&gt;CGPDFPageGetDrawingTransform(page,&amp;nbsp;&lt;SPAN style="COLOR: rgb(63,11,128)"&gt;kCGPDFBleedBox,&amp;nbsp;&lt;SPAN style="COLOR: rgb(63,11,128)"&gt;CGRectMake(&lt;SPAN style="COLOR: rgb(57,0,213)"&gt;0,&amp;nbsp;&lt;SPAN style="COLOR: rgb(57,0,213)"&gt;0, pageRect.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;size.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;width, pageRect.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;size.&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;height),&amp;nbsp;&lt;SPAN style="COLOR: rgb(57,0,213)"&gt;0,&amp;nbsp;&lt;SPAN style="COLOR: rgb(116,73,45)"&gt;true &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;pdfTransform =&amp;nbsp;&lt;SPAN style="COLOR: rgb(63,11,128)"&gt;CGAffineTransformScale(pdfTransform, &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;BR class=webkit-block-placeholder&gt; &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(63,11,128)"&gt;CGContextConcatCTM(context, &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;CGContextDrawPDFPage&lt;SPAN &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;BR class=webkit-block-placeholder&gt; &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(112,49,168)"&gt;UIImage&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;*tempImage =&amp;nbsp;UIGraphicsGetImageFromCurrentImageContext&lt;SPAN &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;BR class=webkit-block-placeholder&gt; &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;CGContextRestoreGState&lt;SPAN &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;UIGraphicsEndPDFContext&lt;SPAN &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;UIGraphicsEndImageContext&lt;SPAN &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;BR class=webkit-block-placeholder&gt; &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(180,20,161)"&gt;return &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt; &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;return&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;nil&lt;SPAN &lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt; &lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt; &lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt;Carlo &lt;FONT class=Apple-style-span size=3 face="Menlo, Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt; &lt;FONT class=Apple-style-span size=3 face="Menlo, Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt; &lt;FONT class=Apple-style-span size=3 face="Menlo, Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt; &lt;FONT class=Apple-style-span size=3 face="Menlo, Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt; &lt;FONT class=Apple-style-span size=3 face="Menlo, Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt; &lt;FONT class=Apple-style-span size=3 face="Menlo, Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt;&lt;FONT class=Apple-style-span color=#3100fb&gt;I cannot replicate the issue i use the following code to get the &lt;FONT class=Apple-style-span size=3 face="Menlo, Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt; &lt;FONT class=Apple-style-span size=3 face="Menlo, Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt; &lt;FONT class=Apple-style-span size=3 face="Menlo, Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt; &lt;FONT class=Apple-style-span size=3 face="Menlo, Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt;" &lt;FONT class=Apple-style-span size=3 face="Menlo, Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt; &lt;SPAN style="COLOR: rgb(197,30,160)"&gt;@implementation - (&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;id &amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;if&amp;nbsp;(&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;self&amp;nbsp;= [&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;super&amp;nbsp;&lt;SPAN style="COLOR: rgb(68,10,127)"&gt;init &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;CFURLRef&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;pdfURL =&amp;nbsp;CFBundleCopyResourceURL&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;(CFBundleGetMainBundle&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;(),&amp;nbsp;&lt;SPAN style="COLOR: rgb(124,74,45)"&gt;CFSTR&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;(&lt;SPAN style="COLOR: rgb(219,51,34)"&gt;"Android-The-Developers-Guide.pdf"&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;),&amp;nbsp;&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;NULL&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;,&amp;nbsp;&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;NULL&lt;SPAN &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(70,128,135)"&gt;pdf&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;=&amp;nbsp;CGPDFDocumentCreateWithURL&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;((&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;CFURLRef&lt;SPAN &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(68,10,127)"&gt;CFRelease &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;return&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;self&lt;SPAN - (&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;void &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;CGPDFDocumentRelease&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;(&lt;SPAN style="COLOR: rgb(70,128,135)"&gt;pdf&lt;SPAN &amp;nbsp; &amp;nbsp; [&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;super&amp;nbsp;&lt;SPAN style="COLOR: rgb(68,10,127)"&gt;dealloc - (&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;void) displayPageNumber:(&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;NSUInteger &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;NSUInteger&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;numberOfPages =&amp;nbsp;CGPDFDocumentGetNumberOfPages&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;(&lt;SPAN style="COLOR: rgb(70,128,135)"&gt;pdf&lt;SPAN &amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;NSString&amp;nbsp;*pageNumberString = [&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;NSString&amp;nbsp;&lt;SPAN style="COLOR: rgb(68,10,127)"&gt;stringWithFormat:&lt;SPAN style="COLOR: rgb(219,51,34)"&gt;@"Page %u of %u", pageNumber, &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;if&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;(&lt;SPAN style="COLOR: rgb(70,128,135)"&gt;leavesView&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;.&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;mode&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;==&amp;nbsp;LeavesViewModeFacingPages&lt;SPAN &amp;nbsp; pageNumberString = [&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;NSString&amp;nbsp;&lt;SPAN style="COLOR: rgb(68,10,127)"&gt;stringWithFormat:&lt;SPAN style="COLOR: rgb(219,51,34)"&gt;@"Page %u of %u", pageNumber-&lt;SPAN style="COLOR: rgb(58,0,213)"&gt;1 style="COLOR: rgb(197,30,160)"&gt;if&amp;nbsp;(pageNumber &amp;gt;&amp;nbsp;&lt;SPAN style="COLOR: rgb(58,0,213)"&gt;1 &amp;nbsp; pageNumberString = [&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;NSString&amp;nbsp;&lt;SPAN style="COLOR: rgb(68,10,127)"&gt;stringWithFormat:&lt;SPAN style="COLOR: rgb(219,51,34)"&gt;@"Pages %u-%u of %u", pageNumber -&amp;nbsp;, pageNumber, &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;self.&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;navigationItem.&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;title &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;BR class=webkit-block-placeholder&gt; - (&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;void) willRotateToInterfaceOrientation:(&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;UIInterfaceOrientation)toInterfaceOrientation duration:(&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;NSTimeInterval &amp;nbsp; &amp;nbsp; [&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;super&amp;nbsp;&lt;SPAN style="COLOR: rgb(68,10,127)"&gt;willRotateToInterfaceOrientation:toInterfaceOrientation&amp;nbsp;&lt;SPAN style="COLOR: rgb(68,10,127)"&gt;duration &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp; &amp;nbsp; [&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;self&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;&lt;SPAN style="COLOR: rgb(42,88,93)"&gt;displayPageNumber&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;:leavesView&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;.currentPageIndex&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;+&amp;nbsp;&lt;SPAN style="COLOR: rgb(58,0,213)"&gt;1&lt;SPAN #pragma mark&amp;nbsp; LeavesViewDelegate methods - (&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;void) leavesView:(&lt;SPAN style="COLOR: rgb(70,128,135)"&gt;LeavesView&amp;nbsp;*)&lt;SPAN style="COLOR: rgb(70,128,135)"&gt;leavesView&amp;nbsp;willTurnToPageAtIndex:(&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;NSUInteger &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;[&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;self&amp;nbsp;&lt;SPAN style="COLOR: rgb(42,88,93)"&gt;displayPageNumber:pageIndex +&amp;nbsp;&lt;SPAN style="COLOR: rgb(58,0,213)"&gt;1 #pragma mark LeavesViewDataSource methods - (&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;NSUInteger) numberOfPagesInLeavesView:(&lt;SPAN style="COLOR: rgb(70,128,135)"&gt;LeavesView*)&lt;SPAN style="COLOR: rgb(70,128,135)"&gt;leavesView &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;return&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;CGPDFDocumentGetNumberOfPages&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;(&lt;SPAN style="COLOR: rgb(70,128,135)"&gt;pdf&lt;SPAN - (&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;void) renderPageAtIndex:(&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;NSUInteger)index inContext:(&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;CGContextRef &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;CGPDFPageRef&amp;nbsp;page =&amp;nbsp;&lt;SPAN style="COLOR: rgb(68,10,127)"&gt;CGPDFDocumentGetPage(&lt;SPAN style="COLOR: rgb(70,128,135)"&gt;pdf, index +&amp;nbsp;&lt;SPAN style="COLOR: rgb(58,0,213)"&gt;89 &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(120,49,167)"&gt;CGAffineTransform&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;transform =&amp;nbsp;&lt;SPAN style="COLOR: rgb(42,88,93)"&gt;aspectFit&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;(CGPDFPageGetBoxRect&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;(page,&amp;nbsp;kCGPDFMediaBox&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;), &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;CGContextGetClipBoundingBox&lt;SPAN &lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(68,10,127)"&gt;CGContextConcatCTM &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;CGContextDrawPDFPage&lt;SPAN #pragma mark UIViewController - (&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;void &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;super&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;viewDidLoad&lt;SPAN &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;leavesView&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;.backgroundRendering&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;=&amp;nbsp;&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;YES&lt;SPAN &lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&lt;SPAN style="WHITE-SPACE: pre" class=Apple-tab-span&gt;&lt;SPAN style="COLOR: rgb(197,30,160)"&gt;self&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;&amp;nbsp;displayPageNumber&lt;SPAN style="COLOR: rgb(0,0,0)"&gt;:&lt;SPAN style="COLOR: rgb(58,0,213)"&gt;1&lt;SPAN &lt;FONT class=Apple-style-span color=#c51ea0&gt; &lt;FONT class=Apple-style-span size=3 face="Menlo, Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt;&lt;FONT class=Apple-style-span color=#3100fb&gt;" &lt;FONT class=Apple-style-span size=3 face="Menlo, Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt;&lt;FONT class=Apple-style-span color=#3100fb&gt;It doesn't show memory leaks accept for the "Core Graphics" which is default .&amp;nbsp;I am able to get both the views "single page view in portrait mode" &amp;amp; " double page view in landscape mode".&amp;nbsp;Is my approach to get &lt;FONT class=Apple-style-span size=3 face="Menlo, Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt;&lt;FONT class=Apple-style-span color=#3100fb&gt; &lt;FONT class=Apple-style-span size=3 face="Menlo, Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt;&lt;FONT class=Apple-style-span color=#3100fb&gt;Regards' &lt;FONT class=Apple-style-span size=3 face="Menlo, Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt;&lt;FONT class=Apple-style-span color=#3100fb&gt;Mreetyunjaya &lt;FONT class=Apple-style-span color=#3100fb size=3 face="Menlo, Geneva, Arial, Verdana, Helvetica, sans-serif"&gt;&lt;SPAN style="FONT-SIZE: 11px" class=Apple-style-span&gt;</body>
  </mail>
  <mail>
    <header>Coordinate inconsistencies</header>
    <body>Hi, I noticed some inconsistencies with coordinates in Quartz. My application (Carbon) draws in response to the kEventControlDraw message. The CGContext is passed as a parameter to this message. Now I want to draw a rectangle of size 640*1 in the top left corner of my window. The following is working fine: But using this line based approach doesn't work although the coordinates are exactly the same: This doesn't work! Nothing gets drawn. If I set &amp;quot;y&amp;quot; to 1, however, the line is correctly drawn (the look is now the same as the call the CGContextFillRect() above although this used a &amp;quot;y&amp;quot; of 0 and not 1!!). Could it be that the path offsets are one based while CGContextFillRect() is zero based? I.e. when passing a &amp;quot;y&amp;quot; of 0 to the path drawing function it will assume an offset of -1 on the y-axis? Tks, Andreas --</body>
  </mail>
  <mail>
    <header>Re: PNG gamma paranoia</header>
    <body>Hi Thomas Unfortunately not, because my application is skinnable, so the user could still feed his own PNGs to my app. But as I said, a solution would be to simply use libpng directly. The only disadvantage of this solution is that my app's size grows by about 300kb or so... Greets, Andreas --</body>
  </mail>
  <mail>
    <header>Re: PNG gamma paranoia</header>
    <body>Hi Andreas, One possible solution could be to remove the gamma chunk from your PNG images, if that's an option for you. One link I found is they suggest using Pngcrush. Regards Thomas</body>
  </mail>
  <mail>
    <header>Re: PNG gamma paranoia</header>
    <body>Addendum: It really must be the PNG gamma chunk which is causing the problems because it also doesn't happen when using JPEG images or BMP images. It's really only with PNGs. So I'd need a way of forbidding CGImage to do any gamma corrections. Here's some more info on the &amp;quot;Sad Story of PNG Gamma Correction&amp;quot;: The article's about combining PNG images and CSS colored elements in a web browser and getting different looks because the PNGs get gamma correction applied while the CSS elements stay untouched. That's pretty much the same issue I'm currently experiencing. Andreas --</body>
  </mail>
  <mail>
    <header>Re: PNG gamma paranoia</header>
    <body>Colorspace shouldn't be the problem because it's also happening when I use the QuickDraw API which doesn't work with colorspaces at all (my application can use either Quartz or QuickDraw or its drawing depending on the respective setting in a config file). If Quartz is enabled, I'm using the colorspace obtained like this: Maybe the problem is related to the way I'm dealing with the image data. Because of the multi-platform nature of my application, I'm storing all images as 32-bit raw ARGB pixel arrays. On Mac, I'm extracting the raw pixels from a CGImage using CGDataProviderCopyData(). and then delete the CGImage. Now, every time an image needs to be drawn, my app will create an appropriate container for it on the fly (either a GWorld for QuickDraw mode or a CGImage for Quartz mode). When the image has been drawn, the container is immediately deleted. Andreas --</body>
  </mail>
  <mail>
    <header>Re: glGetString() crashes</header>
    <body>Fair enough, although a non-crashing NULL result would be nicer (or better, &amp;quot;&amp;lt;multiple--no context specified&amp;gt;&amp;quot;. That might allow GL_VERSION to be returned without a context.</body>
  </mail>
  <mail>
    <header>Re: PNG gamma paranoia</header>
    <body>2010/7/28 Andreas Falkenhahn &amp;lt;email@hidden&amp;gt;: Which RGB colorspace is this color coming from? What's the colorspace of the window in which you're doing the drawing? They're probably in different colorspaces, and ColorSync is doing what it's been doing for over a decade and a half. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>PNG gamma paranoia</header>
    <body>Hi, is there a way to tell the CGImage API to ignore any possible gamma values in a PNG? What happens here is this: In my window I'm drawing the background manually using CGContextFillRect(). Let's say I'm filling the whole background of my window using a static color of #939393 (RGB). Now on top of this grey background I'll draw some PNG pictures. They do not have an alpha channel. Instead, they have the same background filling color as my window. I.e. in this case, #939393. And this leads to the problem! Because CGImage seems to apply some gamma correction to the image the look is radically different now! There's a great difference between the background color as drawn by the CGContextFillRect() function and the background color inside my PNG images as drawn by HIViewDragCGImage() although they are both #939393. I guess that the difference stems from the fact that the PNG image gets gamma &amp;quot;corrected&amp;quot; and the rectangle drawn by CGContextFillRect() obviously doesn't. I did a quick test and loaded the PNG using libpng directly instead of the CGImage API. And indeed, when using libpng the image looks just fine because gamma correction doesn't seem to get applied automatically. However, as soon as I use CGImage, there's an unacceptable visual glitch. Is there any way to get around this or should I just stick to libpng directly for this purpose? Tks, Andreas --</body>
  </mail>
  <mail>
    <header>Re: glGetString() crashes</header>
    <body>You're welcome.  It occurs to me that I can make this make more sense: Suppose your Mac has 2 video cards, made by different manufacturers. Then there are 2 possible results from glGetString(GL_VENDOR).  Having an OpenGL context specifies which card you're talking to. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: glGetString() crashes</header>
    <body>Huh. Okay, thanks! -- Rick</body>
  </mail>
  <mail>
    <header>Re: glGetString() crashes</header>
    <body>-- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>glGetString() crashes</header>
    <body>I have a Cocoa app that I'm working on, and it uses a bit of OpenGL (in a Core Video application). Out of curiosity, I'm trying to see what version of OpenGL I have, so I called glGetString(GL_VENDOR). This crashes with EXEC_BAD_ACCESS. Can I not call this? -- Rick</body>
  </mail>
  <mail>
    <header>Re: Altering colors</header>
    <body>This is iOS, which I think does not have Core Image. The actual gradient generation is easy, once I have the color pairs, and I'm doing that already. I was just looking for a way to reduce the number of explicit colors that must be specified. At this point, I've already implemented the explicit specification, so the need to generate colors is diminished. Thanks for pointing me to CIColors, though. -- Rick</body>
  </mail>
  <mail>
    <header>Re: Altering colors</header>
    <body>Rick, Do you have the option to work with a CIColor object as opposed to a CGColorRef?  If so, you can use the CIConstantColorGenerator filter to generate your color and process  the brightness and  saturation into a final form CIImage. Additionally, if you need to generate some gradients, you can use the CILinearGradient with your input color pairs. regards, douglas</body>
  </mail>
  <mail>
    <header>Re: CGCompleteDisplayConfiguration fails 0x10000003 in daemon after	logout/login</header>
    <body>On Mon, Jul 19, 2010 at 2:29 AM, Rodney Brooks Read the following carefully...</body>
  </mail>
  <mail>
    <header>Dock-like application behavior</header>
    <body>How to make a window not to activate its process after click on that window? (for example, like Dock does that - after you click on the dock, your currently active process will remain active, while dock receives mouse event and performs some actions). Is it possible to do, if my application is not a system agent, like dock? Thanks</body>
  </mail>
  <mail>
    <header>CGWindowListCreateImage blocks?</header>
    <body>I have noticed that CGWindowListCreateImage will sometimes block until the mouse is moved over my apps window or there is other user activity (I don't quite know what's triggering it to return yet besides the mouse). Is this normal behavior or is there something abnormal about my app that could cause this? The only window I'm using is Carbon and transparent if this has anything to do with it but I can't think of anything else unusual that could cause this unless this is intentional behavior. Thanks for any ideas. Regards, Josef</body>
  </mail>
  <mail>
    <header>Re: Altering colors</header>
    <body>You can convert a color to an HSV color space, extract its components, tweak the Saturation or Value components, and create a new color from the tweaked components. This works pretty well in practice. The exact results you get will depend on which HSV colorspace you do the tweaking in‚Äî just as with RGB colorspaces, there are many‚Äî so you may have to do some experimentation to get the results your designers are after. (The CG gradients are always interpolated in a particular RGB colorspace, though, IIRC.) There are many possible operations for any given pair of colors. :) What I would probably do is convert each color pair the graphic designers have chosen to HSV, list them in pairs, and eyeball a relationship between them. A linear transform (add/subtract a small offset, and multiply by a factor a little larger or smaller than 1.0) is probably good enough.</body>
  </mail>
  <mail>
    <header>Altering colors</header>
    <body>In an app I'm currently working on, our designers have created a number of gradients based on a basic source color (i.e. a shade of blue or yellow). They've specified the start and end color of each gradient, making for a lot of colors to parameterize in the app. It occurs to me that I ought to be able to get close to their colors by using a single color, and then applying a series of mathematical transforms on it. Are there operations on CGColorRef to make it more or less saturated, and more or less bright? Is there a way to take two colors and determine the operation that would convert one to the other? Thanks! -- Rick</body>
  </mail>
  <mail>
    <header>multiplexer to url</header>
    <body>Attachment:</body>
  </mail>
  <mail>
    <header>[ANN] PFiddlesoft Frameworks new versions</header>
    <body>&amp;gt;. PFAssistive Framework 3.1.1 wraps Apple's Accessibility API, and PFEventTaps Framework 1.3.1 wraps Apple's Quartz Event Taps API. They extend and enhance the APIs, for example, by providing delegate methods to receive and act on accessibility notifications and Quartz events from any and all running applications. Mac OS X 10.5 Leopard or newer, universal binaries, reference counted memory management only. These updates support the AXMakeProcessTrusted() function in Apple's Accessibility API. An assistive application running as a trusted process can use all of the facilities of the Accessibility and Quartz Event Taps APIs without requiring the user to turn on the &amp;quot;Enable access for assistive devices&amp;quot; setting in the Universal Access pane of System Preferences. Authentication is of course required in order to make an assistive application run as a trusted process, but only once. To run as a trusted process, an assistive application cannot use embedded frameworks, or any framework with an INSTALL_NAME setting that includes @executable_path or @rpath. This version of the PFiddlesoft Frameworks complies with this undocumented Apple requirement, allowing you to make your assistive application run as a trusted process if you install the PFiddlesoft Frameworks as shared frameworks in /Library/Frameworks. Assistive applications that do not take advantage of the trusted process feature can still embed the frameworks in the application package by including a script build phase that uses the install_name_tool command line tool to insert the @executable_path setting at build time. If you don't know what I'm talking about, read the last chapter of new version 1.2 of our Assistive Application Programming Guide. Although the Guide is geared to the PFiddlesoft Frameworks, it contains valuable information not available elsewhere regarding the use of the Accessibility and Quartz Event Taps APIs in assistive applications. Download it at the same URL: &amp;lt;&amp;gt;. -- Bill Cheeseman - email@hidden</body>
  </mail>
  <mail>
    <header>Confusing behavior with CIContext and color spaces.</header>
    <body>I'm finding some confusing behavior when drawing a CIImage to the system supplied CIContext, and then switching to a custom CIContext, and then back.  Here's the steps I'm taking: * Take a CIImage, pass it though a custom filter, and it produces image A. * Toggle a flag, take the same image, pass it through the same filter, draw it using a custom CIContext with a specific color space (kCGColorSpaceGenericRGB), and it produces image B. * Toggle the flag again, so it goes back to the original CIContext, and it produces image B. * Scratch head.  Shouldn't it be producing image A? I'm wondering if maybe the context I'm setting up somehow becomes cached, or does something to the system supplied context to use the new color space?  I'm creating the new context like so: Can anyone enlighten me as to what is going on? For the curious, here's the whole file: And the test project: thanks, -gus -- August 'Gus' Mueller Flying Meat Inc.</body>
  </mail>
  <mail>
    <header>CGCompleteDisplayConfiguration fails 0x10000003 in daemon after 	logout/login</header>
    <body>We have a client-server based application, client being a console based app and server being simple cabon application. - The server runs as a daemon in background. - The client runs as standalone application and sends commands to server for processing. int changeDisplayMode(int¬† width, int height, int depth) ¬† ¬† ¬† ¬†err_beginDC =¬† CGBeginDisplayConfiguration(&amp;amp;config);¬† ¬†err_completeDC = CGCompleteDisplayConfiguration (config,kCGConfigurePermanently );¬†//===&amp;gt; FAILs after logout/login ¬† ¬†return err_completeDC ;¬† main() ¬†¬†daemonize();¬†// make daemon ¬†establish_socket(); // socket communication ¬†process_command(); // this internally calls changeDisplayMode The server runs fine and is able to change resolution as desired. But if we log out and log in, the &amp;quot;CGCompleteDisplayConfiguration&amp;quot; API fails it gives following error: 0x10000003 Please share - Are we missing something in the code? If yes, please suggest - It seems like there something has been invalidated after log-out and login. Is it so? Please share your thoughts Thanks, Rodney</body>
  </mail>
  <mail>
    <header>CIImage access into pixel data</header>
    <body>Right now I am working on app that operates on black&amp;amp;white image. So I am passing NSImage to my method and inside I am making CIImage conversion from NSImage. That is no problem. But i cannot find solution how i can get access into each pixel. It is important to do so because i need to know if that pixel is black or whit (and it&amp;#39;s closest neighbourhood too) Best regards, Wojtek</body>
  </mail>
  <mail>
    <header>Re: Request</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Request</header>
    <body>Attachment:</body>
  </mail>
  <mail>
    <header>Re: about pdf content parsing (Chinese or Korea or Japanese)</header>
    <body>dear david.duncan, thanks for your remind, i will modify my code and¬†read the Memory Management guides.</body>
  </mail>
  <mail>
    <header>CGPDF... and leaks. Is there a workaround?</header>
    <body>Hello, I'm writing an app on iPad that lets the user navigate through a &amp;gt;700 page pdf. When he's on some page I preload the next and previous pages as UIImage (using CGContextDrawPDFPage()) so navigation is faster. Everything is fine at first, but after a few tens of pages the memory gets full and I can preload fewer and fewer images until I can't even create the UIImage for the current page. The common explanation here is that the pdf library builds up some sort of cache which can be purged by releasing the CGPDFDocumentRef.  Unfortunately this doesn't really work. ObjectAlloc shows me that I still have thousands of small memory blocks hanging around between 8 bytes and 32 kilobytes that somehow were created in the CGPDF... methods. For example, the 96 byte blocks are from here: 18 CoreGraphics CGContextDrawPDFPage 17 CoreGraphics CGPDFDrawingContextDraw 16 CoreGraphics CGPDFScannerScan 15 CoreGraphics read_objects 14 CoreGraphics handle_xname 13 CoreGraphics op_TJ 12 CoreGraphics CGPDFDrawingContextDrawText 11 CoreGraphics CGPDFTextLayoutDraw 10 CoreGraphics draw_glyphs 9 CoreGraphics draw_glyphs 8 libRIP.A.dylib ripc_DrawGlyphs 7 libRIP.A.dylib ripc_RenderGlyphs 6 CoreGraphics CGGlyphLockLockGlyphBitmaps 5 CoreGraphics create_missing_bitmaps 4 CoreGraphics CGFontCreateGlyphBitmaps 3 CoreGraphics CGFontCreateGlyphBitmaps8 2 CoreGraphics CGFontGetGlyphPaths 1 CoreGraphics glyph_path_end8 0 CoreGraphics CGGlyphBitmapCreate and for me there are over 3000 of them alive. More than 600 1kb blocks are from CGPDFDictionaryCreateWithObjects (called by CGPDFDocumentGetPage) and CGFontCacheSetValue (CGContextDrawPDFPage). And so on. The customer is very unhappy and me too. What can I do? There must be some sort of workaround because there are pdf readers that seem to work better... Thanks and regards, Sebastian Mecklenburg</body>
  </mail>
  <mail>
    <header>Re: about pdf content parsing (Chinese or Korea or Japanese)</header>
    <body>Hi Erwin, I've never used a third-party library for this, so I can't really make any specific recommendations. Others on this list may be able to chime in? If you decide to write your own code, I think you'll get the best bang for your buck by searching for &amp;quot;ToUnicode&amp;quot; in the Adobe PDF Reference Guide. But converting to unicode is only the start of it (and many fonts don't have conversion dictionaries). PDF is a layout format, and text glyphs may be painted in any order, and spaces may be encoded by simply changing the current text matrix, so you have to use heuristics to determine how to join together those snippets of text... Best wishes, Hamish</body>
  </mail>
  <mail>
    <header>Re: about pdf content parsing (Chinese or Korea or Japanese)</header>
    <body>How are you converting the arguments of the Tj/TJ operators into text? If you&amp;#39;re treating the character codes as ASCII, that&amp;#39;ll be why it&amp;#39;s working for some English text but not much else. It&amp;#39;s very difficult, in the general case, to extract text from PDFs, because they don&amp;#39;t necessarily contain structured elements. Have you looked into third-party libraries for doing so?</body>
  </mail>
  <mail>
    <header>Re: about pdf content parsing (Chinese or Korea or Japanese)</header>
    <body>How are you converting the arguments of the Tj/TJ operators into text? If you're treating the character codes as ASCII, that'll be why it's working for some English text but not much else. It's very difficult, in the general case, to extract text from PDFs, because they don't necessarily contain structured elements. Have you looked into third-party libraries for doing so? H</body>
  </mail>
  <mail>
    <header>about pdf content parsing (Chinese or Korea or Japanese)</header>
    <body>Erwin</body>
  </mail>
  <mail>
    <header>Re: Hit testing a bezier curve</header>
    <body>Neither NSBezierPath nor CGPathRef give you a way to get at a &amp;quot;flattened&amp;quot; path.  Primarily because the amount of the flattening is determined at runtime based on the context into which the path is being drawn. You can easily flatten the path yourself, though.  A google search for &amp;quot;De Casteljau's Algorithm&amp;quot; should turn up more than enough information. Scott</body>
  </mail>
  <mail>
    <header>Re: CGLayer reuse problem</header>
    <body>What I forgot: this is all under 10.6.4.</body>
  </mail>
  <mail>
    <header>Re: italics and bold text in quartz</header>
    <body>Generally the preferred approach is to use the styled font rather than the plain variation (e.g., Helvetica-Bold vs. Helvetica).  ATSUI will synthesize these styles, much the way QuickDraw does.  It probably uses the techniques you mentioned earlier (i.e., slant the transformation for italic and/or stroke the characters for bold).  You can do the same thing, but I don't know of any sample code that does it. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Re: italics and bold text in quartz</header>
    <body>Well, to draw a text with quartz I normally use a sequence of commands like that CGContextSelectFont( ... ) CGContextShowTextAtPoint( ... ) but I only get plain styled text. Is there around any working (simple) example that allows to get bold/italic (eventually using ATSU if it's really not possible to avoid it) ? thank for any hints. stefano</body>
  </mail>
  <mail>
    <header>Re: italics and bold text in quartz</header>
    <body>Well you can always simulate italics with a 14 degree skew.  As for bold, well, that's a different beast entirely. But are you saying you want to use the low-level Quartz APIs for drawing text and not ATSUI? That seems a bit odd, I thought there were all kinds of limitations on doing this (including lack of unicode support). Bryan</body>
  </mail>
  <mail>
    <header>italics and bold text in quartz</header>
    <body>Hi, is there any simple way to draw a text in quartz setting the italic/bold style without using ATSU? for example, stroking the text and slanting is feasible just using quartz? any example? stefano</body>
  </mail>
  <mail>
    <header>Clipping Problem...</header>
    <body>I am having problems when I attempt to draw clipped images. When I draw the image without a clipping path it draws exactly where I want it to.  I do this by first concating a matrix onto the CGContextRef's matrix that has the flip and any offsets/rotations built into it.  I then draw the image using CGContextDrawImage.  This places the image exactly where I need it. However when I go to draw the image with a clipping path I first clip the context to the correct path and then use the same code I use to draw without a clipping path by concating the matrix and calling CGContextDrawImage.  This also works fine as long as the clipping path's top is greater then or equal to the top of the image (bounds of clipping and the top of the image draws at 20.0 instead of 10.0 like it should have and would have if the clipping path had been higher. The code I use goes something like this: if (IsClipped() == true) //I have a matrix class that does all my matrix work for me (UMatrix) CGRect                         bounds;             //Bounds of the image matrix.Flip(false, true, Point(72.0, 72.0)); //Flip in the vertical around the center point of the bounds UMatrix                         offsetMatrix;      //Whatever offsets have been made to the object matrix                           += offsetMatrix; //Concat on offsets and any other transformations to the matrix CGContextConcatCTM(cgContext, matrix);          //Add the matrix to the context's matrix As I said earlier this works fine if IsClipped is false or if the bounds of the path that is used to clip is higher up on the context then the image.  If the top of the image is higher then the clip, the image gets offset to the top of the clip when drawing. Is there something wrong with what I am doing, or is this the way it is suppose to work (and I need to be doing some type of offset when drawing the image), or is this some type of bug? Thanks Andy Brace Dex Media</body>
  </mail>
  <mail>
    <header>Re: Drawing in Offscreen Buffer - ColorSync Headache</header>
    <body>You are encountering an interesting issue with device color spaces and PDF documents. When Quartz 2D renders PDFs that contain device RGB colors to the display it matches them to GenericRGB (a fixed colorspace). This is so that all PDF documents render the same on all displays and don't look different on one system then they do on another. In your case you are rendering a rectangle in device RGB colorspace and a PDF document that contains device RGB data. Unfortunately using DeviceRGB is much more complicated than it appears. Given that DeviceRGB is device dependant, it can sometimes be un-matched to a destination and some times be interpreted as GenericRGB. When you use the display's colorspace for the bitmap context, the rectangle is unmatched when drawn to the window, but the PDF is matched to GenericRGB for consistency. When you rendering it a bitmap context, unfortunately the rectangle is matched as GenericRGB (this is a bug in Quartz), and the PDF is also matched as GenericRGB. That's why you see a difference. A solution is to tag the colorspace you use for drawing the rectangle and not use device colors. For example you could use the main display's colorspace and not use SetRGBFillColor which uses DeviceRGB. Or you can use GenericRGB (see code sample below). Either non-deviceRGB colorspaces will work in your case. When you use DeviceRGB as the colorspace of the bitmap context, there is no notion of what DeviceRGB means on a destination rendering context. So we use GenericRGB as the destination colorspace. In this case, the rectangle is not matched to the window or bitmap context, but the PDF is matched as genericRGB to the display, but left unmatched when drawn to the bitmap context because PDF's are always matched as GenericRGB. Sounds confusing, right? Moral of the story, always use tagged colors and not rely on device RGB if possible. You'll face less problems. DeviceRGB is really for experts :-) I can understand that this is not straight forward for most developers and we are working on improving the offscreen caching problem. Most people just want to cache their drawing offscreen and draw it to the window later as fast as possible without worrying about colorspaces. All I can say is we are working on a solution, and you will have to resort to the explicit tagging solution I mention above. haroon P.S. there is no convenient way to get the GenericRGB colorspace that is used when matching device colors in PDF documents. But you can use the following sample code to get at it: static CGColorSpaceRef getGenericRGBColorSpace () if (genericRGBProfile == (CMProfileRef) 0) strcpy (loc.u.pathLoc.path, if (CMOpenProfile(&amp;amp;genericRGBProfile, &amp;amp;loc) != noErr) genericRGB = CGColorSpaceCreateWithPlatformColorSpace(</body>
  </mail>
  <mail>
    <header>Re: Quartz counterpart to NSCopyBits?</header>
    <body>Although my original question is still valid, I may have gotten closer to (one incarnation of) the problem. It seems that when caching a subset of the display (using NSWindow's cacheImageInRect: method), all previous rectangle &amp;quot;moves&amp;quot; are no longer optimized (i.e. they will then flash in Quartz Debug, and I can also measure the impact, in that drawing takes longer, proportional to the number of pixels &amp;quot;moved&amp;quot;). Unfortunately this does not describe all the problems I have encountered, because some actions may disable &amp;quot;optimization&amp;quot; for all further renderings (although I have not isolated these actions) -- and there is also the very weird case of drawing highlighted text with ATSUI, which cause the entire view port to be flushed in the future (i.e. sticky), and if the view port is resized, the original size of the view port will still flash in Quartz Debug (i.e. now only a subset of the entire view port).</body>
  </mail>
  <mail>
    <header>Quartz counterpart to NSCopyBits?</header>
    <body>Cocoa has an NSCopyBits() function which copies a rectangle to a new position. Is there any similar functionality in Quartz? I am asking because I use the above function to &amp;quot;scroll&amp;quot; contents in my window (instead of redraw) -- when I use Quartz Debug and enable &amp;quot;Flash Screen Updates&amp;quot;, the portions I &amp;quot;move&amp;quot; do not flash, only the portions where I actually draw. Similar can be seen in all scrollable gadgets (like table and text views). My problem is that sometimes it *will* flash the &amp;quot;moved&amp;quot; rectangle, and at other times it will even flash the entire clipping rectangle. Of course it does not just happen at random, but I have a very hard time debugging this stuff, some seems to be related to setting the frame notification, other reason for it involves ATSUI.. So I figured if there was a lower level API, I might have an easier time debugging this stuff -- IIRC somewhere I read that NSCopyBits() *tries* to optimize copying by only moving the content on the screen (instead of doing it in offscreen buffer and flushing), so perhaps I do something which makes the system think that this optimization can no longer be used...</body>
  </mail>
  <mail>
    <header>Re: Drawing in Offscreen Buffer - ColorSync Headache</header>
    <body>Am 10.03.2004 um 00:22 schrieb Haroon Sheikh: I'm doing that, and the colors match for the PDF, but not for the background. If, on the other hand, I'm using DeviceRGB color space, the colors match for the background, but not for the PDF. For me the color space of the image doesn't appear to have any effect at all. I made a demo application to show the problem. I haven't filed a bug yet, since I'm sure this is my problem somewhere. Since the display obviously depends on the color sync profile I made a screenshot how this looks on my screen. What am I doing wrong here? Manfred</body>
  </mail>
  <mail>
    <header>fast cross fade effect between 2 full screen images</header>
    <body>I am trying to create a cross fade effect between 2 full screen images. I've tried using OpenGL bitmaps, glBlendColorEXT() is fast but not available on all drivers, I've also tried with -[NSImage dissolveToPoint...] but it is too slow on a low end machine such as G3 400Mhz. I could use textures but i don't think they'll always be able to accomodate the size of my hires images resulting in pixelization... Any ideas on how to implement this ? I know it is possible since iPhoto4 does it fast/smooth enough.</body>
  </mail>
  <mail>
    <header>Drawing in Offscreen Buffer - ColorSync Headache</header>
    <body>CGContextFillRect()) and on top of that I draw a PDF (CGContextDrawPDFPage()). At times I need to draw this into an offscreen buffer first (CGBitmapContextCreate(), CGImageCreate(), CGContextDrawImage()) for performance reasons. So far so good. Then comes ColorSync, which does some color changes. Obviously the color must mach exactly whether I draw directly or via the offscreen buffer. To get the colors of the PDF matching to that of the screen, I musst create the bitmap with the color sync profile of the main screen. But then the background color does not match. To get the color of the background match, I have to create the bitmap with a DeviceRGB color space. But then the colors of the PDF don't match any more. The image I create from the bitmap also takes a colorspace parameter, but this does not have any apparent effect. Neither has the rendering intent parameter (whatever that might be). How can I draw into an offscreen buffer so that the colors match exactly those when drawn directly? Thanks Manfred</body>
  </mail>
  <mail>
    <header>conversion functions?</header>
    <body>hiya, ...I seem to remember that there are some &amp;quot;convenience&amp;quot; functions for converting between quickdraw rect's and CGRect's, but I've come up empty handed when searching thru Xcode's documentation facilities, and same with google...was I just dreaming about this? thanx, jamie</body>
  </mail>
  <mail>
    <header>Re: ATSUHighlightText permanently sets the refresh rect</header>
    <body>Sorry, I should have added that my view is inside an NSScrollView, and thus it is when scrolling that the NSClipView copies the contents and asks my view to redraw the new portion, that the entire view flashes. Copy on scroll is enabled, and as said, it does not flash if I don't make the initial call to ATSUHighlightText. I do not know if this is an actual problem (slowing down refresh/scrolling), or if it is Quartz Debug which gets it wrong/decides to flash rectangles which are not redrawn but moved (which it normally doesn't flash, and doesn't do either, if I refrain from using ATSUHighlightText).</body>
  </mail>
  <mail>
    <header>ATSUHighlightText permanently sets the refresh rect</header>
    <body>If I make a single call to ATSUHighlightText(), then all following redraws will flush the entire clipping rectangle that was active when the call was made to ATSUHighlightText. To clarify, I have implemented drawRect in a Cocoa NSView subclass like this: - (void)drawRect:(NSRect)rect if(!didCall) ATSUHighlightText(layout, 0, 0, So only the first time that drawRect is called, will it draw the highlighted text. All further calls are no-op's. I use Quartz Debug and enable &amp;quot;Flash screen updates (yellow)&amp;quot;. Each time my view is sent a drawRect:, the entire view flashes, no matter how small the rectangle (provided) is (I do return YES in isOpaque). If I comment the ATSUI line, the flashing is gone. If I resize my view to e.g. twice the size and again send it a redraw with a small rect, then the flashing rectangle has the size of the *original* view. So somehow it seems that the ATSUI command will mark the entire clip rect dirty, and in a way that will make Quartz think that it is never flushed. What actually happens here? is ATSUI buggy? is there a way I can inspect the dirty regions of Quartz for better debugging this problem?</body>
  </mail>
  <mail>
    <header>Re: CGContextShowGlyphs -- not for drawing strings?</header>
    <body>Thanks -- I already did try the Carbon list but w/o luck. So I really appreciate the help I am getting here! Turns out this is redundant anyway, as it would seem ATSUI already does clip text to the current clipping rectangle -- what was slowing down text display was the layout and not drawing (even though this was actually just 7 bit text in a fixed width font, seems ATSUI makes no short cuts). Ahh, yes -- no need to mess with the font transformation matrix when I can make Quartz do the same w/o the associated problems, thanks! Okay, I'd hoped the backing store was obsolete after the layout, since the layout deals with glyphs, not unicode character points -- but it would seem I am wrong ;) although there is (as mentioned) the GetGlyphInfo, which obtains only the glyph info, which can later be drawn with DrawGlyphInfo (so here the backing store really ought to be unused), but I did not try this function because it said it was &amp;quot;not recommended&amp;quot;, so perhaps it does not draw the glyphs in the same way as the DrawTextLayout when complex layouts are used. For the records: the problem I mentioned about skipping tab-stops, it would seem that this is more of a feature of ATSUI than precision problems, as the problem is always reproducible when a tab starts exactly at a tab-stop position. Not a feature I have seen elsewhere or that I find useful, but seeing how it's reproducible, I would assume it's by-design.</body>
  </mail>
  <mail>
    <header>Re: PDF Marked Contexts</header>
    <body>No current way to do this. Please file an enhancement request with more details of when you'd like to insert the marked-context haroon [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: CGContextShowGlyphs -- not for drawing strings?</header>
    <body>There is probably more ATSUI knowledge on the Carbon development list, but I'll give it a shot. Yes. I don't know. I'd suggest keeping a matrix with your ATSUI layout object to flip your transformation back to the default orientation by concatenating it with the context's CTM before drawing with ATSUI.  You could also do your translation (mentioned above) here. You're responsible for maintaining the layout's backing store so you need to keep it around as long as it's in use by the layout.  That's another thing you have to carry around with your layout objects. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Re: CGContextShowGlyphs -- not for drawing strings?</header>
    <body>Okay, thanks! From the little I know about OpenType specs and Unicode, this is definitely not a path I wish to follow ;) so I am using ATSUI, unfortunately this has a few problems, it's probably off topic for this list, but I'll state them nonetheless, hoping that someone can help me: ATSUDrawText uses the Fixed type limited to +/- 32K, should I use a Quartz translation? there is a convenience constant (kATSUUseGrafPortPenLoc), but it does not seem to use the Quartz text position. I draw in a fixed width font and place tab stops for every 3rd character (all calculated with the Fixed type), ATSUI will sometimes skip a tab -- what I think happens is that it has the text position 0.1 pixels before next tab stop or similar, and thus the tab is 0.1 pixel instead of 3 times the fixed glyph width. To minimize redraw I use ATSUDirectGetLayoutDataArrayPtrFromTextLayout and only draw the visible glyphs, but it crashes for zero length layouts (i.e. empty lines) -- is there anyway I can query a layout for length, before calling the above? I render into a flipped context, and thus I provide a text transformation matrix to counter this (i.e. with -1 instead of 1 for d), but it screws up other parts of ATSUI, e.g. underline position is wrong, synthetic italic is slanted to the left (instead of right) and ascend/descend for a line layout returns 0. Finally, I do not have my text as UniChar*, but use ATSUCreateTextLayoutWithTextPtr (so I need to create a temporary buffer), but if I dispose this buffer before I draw the layout, it will misbehave (even though glyph translation has been done) -- anyway around that? In theory I could use ATSUGetGlyphInfo on the layout and later draw these (disposing the layout, as the glyph info is a copy) with ATSUDrawGlyphInfo, but the latter function is &amp;quot;not recommended&amp;quot;. Kind Regards Allan</body>
  </mail>
  <mail>
    <header>Re: CGContextShowGlyphs -- not for drawing strings?</header>
    <body>Correct. Quartz contains no text layout facilities at all. Either you do it yourself using the information in TrueType tables and a lot of Unicode knowledge and algorithms (be prepared to write substantial amounts of code) or you use ATSUI. Best regards, Kai -- RagTime GmbH                          Tel: [49] (2103) 9657-0 Itterpark 5                           Fax: [49] (2103) 9657-96 D-40724 Hilden Deutschland</body>
  </mail>
  <mail>
    <header>Re: Custom image formats</header>
    <body>David Duncan: No, it's a specialised custom format. Using an import/export component able to work with Preview and well-behavied third-party applications struck me as significantly more elegant (as well as simpler) than a task-specific converter application. The idea that a standard image import/export infrastructure would not support such components did not even occur to me. I would in fact go so far as to say that without this, ImageIO is severely broken (and, for all its warts, QuickTime's image compression API gets it right-ish). In fact, providing an ImageIO component for any custom image formats ought to be encouraged in the same way that a Spotlight indexer for custom file formats is, and for essentially the same reason. (In an ideal world, of course, people would stick to well-understood formats, and such formats would be appropriate for every use. I, for one, have yet to come across a gateway to an ideal world.) -- Jens Ayton Sed quis custodiet ipsos custodes?</body>
  </mail>
  <mail>
    <header>Re: Custom image formats</header>
    <body>Nope. Nope. Not that I know of. Is there a particular format that you want to evangelize that ImageIO should support? -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Custom image formats</header>
    <body>For a while now, I've been aware of ImageIO as the reccomended way to move image data in and out of applications. I have assumed as a matter of course that there would be a way to write import and export modules for this system, but now that I need to do so, there appears not to be. * Is there such a thing as an ImageIO import/export plug-in? * If not, is there some other format I should use? Will ImageIO use QuickTime graphics importers? * If not, is there a way to write importers that Preview will use? Will Preview use QuickTime graphics importers? -- Jens Ayton Sed quis custodiet ipsos custodes?</body>
  </mail>
  <mail>
    <header>Re: QD -&amp;gt; Quartz = poor performance</header>
    <body>CG tries it's best to give you as much performance as possible, but as of Tiger I don't believe any of the Q2DE features are enabled by default, and I don't know of any way to enable any of them yourself (even for just your application). If you really want hardware stretching, then your going to have to do the final stretch-blit via OpenGL at this time. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: QD -&amp;gt; Quartz = poor performance</header>
    <body>Use OpenGL for this. -:sigma.SB</body>
  </mail>
  <mail>
    <header>Re: QD -&amp;gt; Quartz = poor performance</header>
    <body>the user can go full screen on any monitor.  I'm stress testing on my SGI MultiLink monitor, but it could also be HD sized (1920 x 1080). the typical case of course is 720 x 480 i was super duper hoping that by using a CGLayerRef, of size 320 x 240, that the stretch-blit would happen in hardware. i was under the impression that you can get Q2DE behavior even now by using CGLayerRef's?</body>
  </mail>
  <mail>
    <header>Re: QD -&amp;gt; Quartz = poor performance</header>
    <body>Tried a 1280x960 or 1600x1200 window? Seems that 1046 dimension might be putting you on a lesser performing stretch path. Either way your pushing a lot of pixels, and on a 1Ghz PPC you might simply be pushing bus limits (since this sounds like a G4). If you are, the reduced FPS under Quartz might be due to additional memory operations that Quartz can't avoid but QD can. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: QD -&amp;gt; Quartz = poor performance</header>
    <body>i originally said i was invalidating at 75 hz, which was incorrect, i was invalidating at 30 hz Okay.  I've reworked my app to be a bit more modern.  Now, when using CoreGraphics: * no more quickdraw at all * it's now a composited window * uses a &amp;quot;user item&amp;quot; control as the HIView in which to draw * uses carbon events, responds to kEventControlDraw event (no more updateEvent) * extracts contextRef from the event (no longer constructing a QD context from the port) * graphics preparation happens on a timer at 30 hz, then calls HIViewSetNeedsDisplay * no more flushing at all, only responding to draw events, no work done during that except to blit what was already prepared * now creating CGImageRef directly, no longer constructing a QD context around the bits then blitting that into an image ref * now designating the original (QD) bitmap image as having premultiplied alpha, i heard this makes things faster.  no visual difference. when NOT using CoreGraphics it does minimal quickdraw (i think the only calls are setport and copybits) Window size set to 1600 x 1046, always stretch-blitting from a bitmap of size 320 x 240 With Quartz 2D Extreme:	I get 30 fps, with or without beam sync, but no sweet high quality interpolation.  CPU is at about 50% (1GHz solo ppc) QD, No Beam Sync: 24 fps QD, With Beam Sync: 17 fps Interpolation Qual: None: Quartz: No beam sync: 13 fps Quartz: With beam sync: 9 fps Interpolation Qual: High: Quartz: No beam sync: 11 fps Quartz: With beam sync: 8 fps i have tried making a CGLayerRef, but that creates a performance penalty, even with Q2DE so okay yes turning off the beam sync does give me slightly better frame rates.  but i've followed all the guidelines from here: Drawing/Articles/FlushingContent.html so hmm.  i'm at a loss now.  even with beam syncing turned off, it's still spending the vast majority of time in the CGContextDrawImage call. i've been looking at this a lot.  i've read all the doc.  i've spent time with Shark.  i've googled.  i've done my homework.  (yes i did reply to one before i had any time to do anything, i was just wanting to reply to the Q's asked and keep the thread going). so i'm not sure if anything can be done now.  any ideas?</body>
  </mail>
  <mail>
    <header>CIImage CILanczosScaleTransform taking 15-20 secs?</header>
    <body>I'm new to Cocoa Dev and Quartz/Core Image.  I'm writing an iphoto plugin that will resize images and upload to my website.  Everything is working just fine, except that it takes FOREVER to resize the images.  Before this I'd use an apple script that would use Image Events to resize the images.  That was much faster. Right now, its takes ~15-20 sec to resize a 3072 x 2040 image.  This seems a tad excessive no? If I take out the then everything everything is really fast...Am I not using a proper context?  Am I not getting GPU acceleration? I have a 12 inch powerbook with the Nvidia 5200.  Using the 'Reducer' Example, performance seems much faster, not to mention Core Image funhouse is also way faster. Below is the code extracted from my plugin into a command line utilitity. Its not meant to be clean code by any means.  Any thoughts? int main(int argc, char *argv[]) CIImage *original = [[CIImage alloc] initWithContentsOfURL:[NSURL CIImage *resizeCIImage(CIImage* in_image, SInt32 in_maxDimension) if([in_image extent].size.height &amp;gt; [in_image extent].size.width) else [clampFilter setValue:[NSAffineTransform transform] CIFilter *scaleFilter = [CIFilter [scaleFilter setValue:[NSNumber numberWithFloat:yscale] [scaleFilter setValue:[clampFilter valueForKey:@&amp;quot;outputImage&amp;quot;] CIVector *cropRect =[CIVector vectorWithX:0.0 Y:0.0 Z: finalW W: [cropFilter setValue: [scaleFilter valueForKey:@&amp;quot;outputImage&amp;quot;] void saveCIImageToTmp(CIImage* in_image, NSString* in_filePrefix) // Create a new NSBitmapImageRep. NSLog(@&amp;quot;Creating Bitmap to draw to: Height: %f Width: %f&amp;quot;, NSBitmapImageRep *theBitMapToBeSaved = [[NSBitmapImageRep alloc] initWithBitmapDataPlanes:NULL pixelsWide:[in_image extent].size.width pixelsHigh:[in_image extent].size.height bitsPerSample:8 samplesPerPixel:4 hasAlpha:YES isPlanar:NO colorSpaceName:NSDeviceRGBColorSpace // Create an NSGraphicsContext that draws into the NSBitmapImageRep. // (This capability is new in Tiger.) NSGraphicsContext *nsContext = [NSGraphicsContext // Save the previous graphics context and state, and make our bitmap // context current. // Get a CIContext from the NSGraphicsContext, and use it to draw the // CIImage into the NSBitmapImageRep. [[nsContext CIContext] drawImage:in_image // Restore the previous graphics context and state. // Save to Temp File NSDictionary *theFileSaveDictionary = [NSDictionary dictionaryWithObjectsAndKeys:[NSNumber numberWithFloat: [[theBitMapToBeSaved representationUsingType:NSJPEGFileType properties:theFileSaveDictionary] writeToFile: [tempPath stringByAppendingFormat:@&amp;quot;%@.JPG&amp;quot;, in_filePrefix]</body>
  </mail>
  <mail>
    <header>Re: QD -&amp;gt; Quartz = poor performance</header>
    <body>There is a performance hit to using the Begin/End calls instead of using Compositing mode, but it shouldn't be a 3X difference. Using Compositing mode, Carbon events and the Standard handler would likely simplify your code otherwise, which may be a design win if not a performance one. I don't believe that to be the case. From what I'm told most video cards use at most 7 levels of anti-aliasing (via OpenGL), whereas Quartz does 255 levels. As such, unless your scaling your images as they draw, the only anti-aliasing your typically getting is on the edges of the image (you may get anti-aliasing internally to the image, but most of the time you want to avoid those cases rathe than encourage them when you aren't scaling the image as well). That said, in my experiments, turning anti-aliasing on/off was a small gain if at all. It's more interesting when you get to more complicated drawing (like that of a PDF). -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Core Image, Crop, and DPI</header>
    <body>As a follow-up, this only appears when using the CICrop filter through a Quartz Composer composition.  Building the filter in code does not result in the same problem.</body>
  </mail>
  <mail>
    <header>Re: QD -&amp;gt; Quartz = poor performance</header>
    <body>okay the conversation got complicated so I moved some to my wiki, click here to catch up: then: my timer, when drawing CDG runs at 72 fps.  i should be allowed to invalidate 72 times/second without penalty right?  (72 fps is the &amp;quot;framerate&amp;quot; of CDG's) if there's enough time then i'll get at most 60 when drawing a QuickTime movie, the invalidate timer runs at frame completion, which varies according to the movie's framerate. when my timer fires, no.  i just take the window and make a port context out of it and use that.  when I get an UpdateEvent (yes, still using WNE) then i do beginupdate/endupdate, is that a QD thing?  would it be better to make it an HIView type window?  should it be a compositing window?  with &amp;quot;use standard handler&amp;quot; and use carbonevents? i was hoping to keep the anti-aliasing.  i was under the impression it was done in hardware, during the stretchblit to the screen?</body>
  </mail>
  <mail>
    <header>Core Image, Crop, and DPI</header>
    <body>I'm having a bit of trouble figuring out what exactly Core Image is doing with the values fed into a crop filter.  I am unable to find anything in the documentation to hint at what might be going on here. (Some numbers are measured by hand so not necessarily completely precise.) I have a TIFF image that is 3500x2800 pixels at 350 dpi.  The crop coordinates I am working with are (in pixel coordinates) 1880, 1591 -- 170 x 170.  The corresponding coordinates in Core Image are 1100, 930 -- 100 x 100. Why is this?  Those values do not appear to be pixels or points.  The effective DPI if they are indeed points is 205.88; however, this value changes depending on the dimensions and DPI of the source image so I'm inclined to think it's not points. Coincidentally, the same filter on a 72 dpi image works as expected.</body>
  </mail>
  <mail>
    <header>Re: QD -&amp;gt; Quartz = poor performance</header>
    <body>wrote: Any chance there's an explicit flush (CGContextFlush, etc.) in the loop? CG drawing calls block if a flush is in progress, so you can see that if the drawing loop is theoretically capable of 30fps (or even 1000fps), but there's an explicit flush in the loop, drawing can be throttled to much less than 30fps. Shark will show that most your time is being spent inside drawing APIs, making them look &amp;quot;slow&amp;quot;, when actually they are sitting there waiting to unblock. Quartz Debug's Frame Meter and Beam Sync tools can help locate (or rule out) the problem.</body>
  </mail>
  <mail>
    <header>Re: QD -&amp;gt; Quartz = poor performance</header>
    <body>That's definitely strange, and I'm at a loss to explain that right this moment... I do have an app that is drawing a sample image that is rotating about it's center and I'm getting better than 30 fps. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: QD -&amp;gt; Quartz = poor performance</header>
    <body>okay i answered my own question.  the answer is no.  i also found out that there is no time spent in the calls to create the CGImage, all the time is spent in the call to draw it.  (ie: even if i ONLY call CGContextDrawImage() on each frame, using a pre-created image, i still get 9 fps.</body>
  </mail>
  <mail>
    <header>Re: QD -&amp;gt; Quartz = poor performance</header>
    <body>can i keep that imageRef around forever, and whenever I CGContextDrawImage it gets the bits from the provider??</body>
  </mail>
  <mail>
    <header>Re: QD -&amp;gt; Quartz = poor performance</header>
    <body>ScCFReleaser&amp;lt;CGColorSpaceRef&amp;gt;	colorSpace ScCFReleaser&amp;lt;CGDataProviderRef&amp;gt;	provider (CGDataProviderCreateWithData( i_srcP-&amp;gt;i_imageRef = CGImageCreate( i_srcP-&amp;gt;width(), i_srcP-&amp;gt;height(), 8, 32, i_srcP-&amp;gt;i_rowBytesL, colorSpace, kCGBitmapByteOrder32Host | kCGImageAlphaNoneSkipFirst, provider, NULL, false, CGContextDrawImage( sigh.  sadness</body>
  </mail>
  <mail>
    <header>Re: QD -&amp;gt; Quartz = poor performance</header>
    <body>well, in fact i must do bitwise xor.  in super precise fact i'm working with 16 color pictures, yes that's 4 bit color, really and truly and using a CLUT.   you just plain can't do that in CV without lots and lots of custom code, whereas QD &amp;quot;just works&amp;quot; with practically no extra code the CD+G spec is written this way, no way to get around that requirement. i then copybits blit that into a 32bit offscreen, then blit THAT into the CGLayer than it is to call: QDBeginCGContext() CGBitmapContextCreateImage() QDEndCGContext() well, i suppose that makes sense, 1 call not three eh?  so i'll try skipping the layer part and blit the cgimage directly.</body>
  </mail>
  <mail>
    <header>Re: QD -&amp;gt; Quartz = poor performance</header>
    <body>I went and did a little looking, and if you don't need ScrollRect (which I suppose depends more on what your doing, but I suspect it to be possible...) there is a way to do XOR (or more appropriately, color inversion) in CoreGraphics: CGContextFillRect(context, fillArea); // fillArea=whatever area you want to invert. (You can replace the CGContextFillRect with any CG drawing method and get custom inversions) cocoa/2006/1/24/155151 ). If you already have bits you don't want to modify, then just create a CGImage from the bits and draw it. No need to go through a bitmap context unless you want to do Quartz drawing as well. CGLayers have a number of uses that include drawing the same content multiple times. However, if you already have the data in hand, and don't need to actually do anything but transfer it to screen, then creating a CGImage will often be better, as CG is able to apply similar optimizations to CGImages as are done for CGLayers for drawing. I'm totally unfamiliar with how you get your data from the CD, however I would think that it would be possible to do much of what you want with CGImages and Affine transforms. If this is the case, then yes, CGLayers might be the way to go, as at that point, you'd do all your compositing in a CGLayer (instead of into the GWorld) and then composite that elsewhere. The exact nature of your graphics will really determine how you do your final drawing and what optimizations we can do to it. I'm not ready to declare that, and if it were the case I would say to file a performance bug on whatever you find is too slow. Worst we say is that there is nothing we can do :). -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: QD -&amp;gt; Quartz = poor performance</header>
    <body>well, no, i mean a QT context, around my bits that live in the GWorld port sure that's fine i expect i'll have to copy bits at least once, i'm trying to avoid copying twice however but i want to draw the layer more than once.  they say use a layer if you plan to blit it more than once?  right? i'll give more info: this is a karaoke application (insert snickers here).  so i get some bits from the CD, update my GWorld (xor, scroll, set bits whatever), then i just want to take the GWorld and blit it to one or more places on the screen (different windows, panes within a window etc).  since i blit it to potentially more than one place, i thought a layer would be the right way to go.  but to get it to the layer in the first place seems to involve two copy operations.  does it make sense that i get a slower framerate? i have no quartz drawing to do before or after i've got the bits, i JUST need to blit them as fast as I can.  QD seems to be the speed winner here?</body>
  </mail>
  <mail>
    <header>Re: QD -&amp;gt; Quartz = poor performance</header>
    <body>ScrollRect() etc? Do you mean Quickdraw APIs or your own? Do you mean CGBitmapContext? CGBitmapContextCreateImage() won't copy the bits unless you write to the bitmap (it implements Copy-on-Write semantics). Creating a layer will require some form of allocation. The nature of that allocation depends on the specific type of context your actually dealing with, but it is not free. This doesn't really make much sense, as it will do more work than is necessary. This is especially true as you are just taking the context image and drawing it again - you can draw the CGImageRef you got previously instead. CGLayers are valuable when you just need to do miscellaneous drawing that is associated with a context but multiple times. So for example, if you had a complex drawing operation (such as a composition of multiple images) that you wanted to draw to multiple places in a context, then using a CGLayer is extremely valuable. But in your case, you need to work with the actual bits themselves. In that case, do your Quartz drawing to the CGBitmapContext, then your custom work on the backing store for that bitmap context. When you've finalized the bits, obtain a CGImageRef from the bitmap context and use that to draw to your final target. If you need to further manipulate the bits from the bitmap context, release the image first so as to avoid the CoW operation. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>QD -&amp;gt; Quartz = poor performance</header>
    <body>when i'm using QD, i can get 30 fps.  when i go with Quartz, i bog down to 15-20 fps. i must be doing some thing wrong. first, i have a bitmap, that is my frame.  it must be a bitmap, because i must be able to see the bits (for xor mode) and must be able to scroll the bits , eg: scrollrect etc. and yes i do actually need xor in the old-skool, i just copybits the pix to it's destination(s) (there may be more than one destination) create a qd context around the pixels -&amp;gt; fast, no bits copied then use CGBitmapContextCreateImage(), to get a CGImageRef -&amp;gt; slow, bits must be copied?? then CGLayerCreateWithContext() -&amp;gt; fast, no bits yet then CGContextDrawImage() -&amp;gt; slow, bits get copied again and then finally loop per destination: CGContextDrawLayerInRect() -&amp;gt; move the bits yet again the last i presume is going to be fast, cuz it should be in hardware? but maybe not? is there a way to create a layer based on the original bits to skip one bit copy?</body>
  </mail>
  <mail>
    <header>Re: Memory Leak with CIContext + NSGraphicsContext??</header>
    <body>I've seen it too and I've seen a few others posts about it as well. My solution to the problem was to create a single CIContext along with it's offscreen bitmap and use that same context every time.</body>
  </mail>
  <mail>
    <header>Memory Leak with CIContext + NSGraphicsContext??</header>
    <body>I'm trying to render a CIImage into a bitmap. There's a fair amount of sample code on the web and on Apple's web site that looks something follows this form: 0) Start with a CIImage as input 1) Create a bitmap image (e.g. a NSBitmapImageRep) 2) Create an NSGraphicsContext using the NSBitmapImageRep 3) Save the graphics state and set the new NSGraphicsContext as the current context 4) Get a CIContext from the new NSGraphicsContext using the CIContext method 5) Draw the CIImage using the CIContext drawImage method 6) Restore the graphics state 7) Release the bitmap image I'm trying this and getting a massive memory leak. This bug can be demonstrated in the Apple sample application &amp;quot;Reducer&amp;quot;. (More on that below.) I've tried surrounding thie above code with an autorelease pool, but the NSGraphicsContext retain count is set at 3 after releasing the pool -- that doesn't seem like it should be, no? As an alternative, I tried using a CGBitmapContext to hold the bitmap, and then used code like this surrounded by an autorelease pool CIContext* ciContext = [CIContext I'm getting indeterminate results -- the first time I use it, the memory leak does not seem to occur. But upon a second trial (within the same run of the program) it does seem to occur. I am still researching this workaround. In any case, is there a way to get get my CIImage converted to a bitmap without introducing this leak??? Best Wishes, Mark P.S. To see an example of the problem, look at the Apple sample application &amp;quot;Reducer&amp;quot;, specifically the &amp;quot;outputBitmapImageRep&amp;quot; routine in the file &amp;quot;ImageReducer.m&amp;quot;. Try this. 1) Compile the &amp;quot;Reducer&amp;quot; sample code from Apple's developer site. 2) Run it. 3) Move the image adjustment sliders around rapidly and watch the virtual memory allocation grow and grow. 4) Release the mouse button and wait --- the memory is never released. ciContext drawImage:ciImage atPoint:imageDestinationRect.origin On both my PowerBook G4 and my MacBookPro, this test exhibits a serious (i.e., large) memory leak. -- ======================================================================= Mark Coniglio             |  email@hidden TroikaTronix              | Isadora¬Æ                  | |  (interactive media processing environment) =======================================================================</body>
  </mail>
  <mail>
    <header>Re: Rendering fonts without activating them</header>
    <body>&amp;gt;What you might try is activating the font in your local context with &amp;gt;the kATSOptionFlagsDoNotNotify option.  That might prevent the font &amp;gt;from shing up in the font panel.  Then, after drawing, you can use &amp;gt;ATSFontDeactivate (again with kATSOptionFlagsDoNotNotify) to remove &amp;gt;the font. Thank you Scott. I'm afraid that activating and deactivating fonts would be a bit slow. However I'll give it a try. - alessandro.</body>
  </mail>
  <mail>
    <header>Re: Rendering fonts without activating them</header>
    <body>Attachment:</body>
  </mail>
  <mail>
    <header>Re: Is it an NSCachedImageRep or Is it an NSBitmapImageRep?</header>
    <body>NSImage is designed to hold onto various representation of the same image so one can quickly redraw a scaled/color corrected image for example for display to the screen while also holding onto the full sized uncorrected representation. The cached image you are getting back is likely in support of the image view drawing a scaled version of your image. Yeah this can be a little confusing at first... This covered in the following concept document. So you can do a few things... walk the array of references you get back from the image looking for the one you want using isKindOfClass: [1] to match on NSBitmapImageRep, walk the array of reference looking for one that supports the method you need using respondsToSelector: [2] or get a representation based on target device needs using bestRepresentationForDevice [3]. Other ways??? Of course if you are loading the image from a file to begin with (not accepting dropped images, etc. on you image view) why not simply create an NSBitmapImageRep yourself with the file data and the use that to create the images you need. Also note this isn't really a Quartz related question it is more of a Cocoa question (I know it was suggested that you message this list but...). Various online research exist to help look up answers to common issues like this for example (many can be found by starting with google)... -Shawn [1] ObjC_classic/Protocols/NSObject.html#//apple_ref/doc/uid/20000052/ [2] ObjC_classic/Protocols/NSObject.html#//apple_ref/doc/uid/20000052/ [3] ApplicationKit/ObjC_classic/Classes/NSImage.html#//apple_ref/doc/uid/</body>
  </mail>
  <mail>
    <header>Is it an NSCachedImageRep or Is it an NSBitmapImageRep?</header>
    <body>I have a NSImageView called leftImage.  that was created in the Interface builder: At some point in my code I need access to this image: Later on in the code I need access to the pixel data: Sometime this seems to be a different class then I was expecting and the methods I was using no longer work. It seems that sometime this is a NSBitmapImageRep and othertimes (depending on the file that was loaded with InitByRef) it is a NSCachedImageRep 1)  How do I know at run time if I'm going to get the NSBitmapImageRep or the NSCachedImageRep? If I have the NSBitmapImageRep getting access to the pixel data is easy: However this method fails if I ended up with NSCachedImageRep. 2) So assuming I'm able to figure out what class was actually given to me and it is the NSCachedImageRep,  how do I go about getting access to the bitmapData as I did for the NSBitmapImageRep? Cheers! Brian</body>
  </mail>
  <mail>
    <header>Re: Drawing icons fast : CGImageRef or IconRef ?</header>
    <body>Whoops, you're right - I didn't have the list sorted by name. :) Jason [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: A question about [bitmapImageRep bitmapData]</header>
    <body>It seems obvious to me that it is a NSCachedImageRep in some cases and NSBitmapImageRep in other cases... What I don't know is how one should determine the representation and act accordingly... Getting the bitmapData works form NSBitmapImageRep but I am not sure how to get it if it is a CachedImageRep.</body>
  </mail>
  <mail>
    <header>Re: Drawing icons fast : CGImageRef or IconRef ?</header>
    <body>As a follow up I Sharked (hey I can create a new verb if I want) iPhoto while it does it live resizing of image thumb nails (moving the size slider in the lower right corner). It looks like it does the following calls... CGContextSaveGState various CTM transforms CGContextSetInterpolationQuality CGContextDrawImage CGContextRestoreGState -Shawn</body>
  </mail>
  <mail>
    <header>Re: Drawing icons fast : CGImageRef or IconRef ?</header>
    <body>I agree.  The original poster had also posted a message in Carbon-development indicating that both PlotIconRefInContext and the CGImageDraw route were both unfavorably slow. That leaves the OpenGL route and there are two ways I can think of to use OpenGL in this case. The simplest is the fact that, in Panther anyway, you can create a CGContext that draws on an OpenGL context.  To do that you have to first create the OpenGL context (left as an exercise to the reader... see AGL or NSOpenGL&amp;lt;blah&amp;gt;).  Then you can call CGGLContextCreate to create the CGContext on that OpenGL context.  From there you would just use CGContextDrawImage.  What's not clear to me is, if you take this route, will the CGGLContext in Panther be clever enough to optimize the repeated drawings of the same CGImage. The longer and more involved route is to take the CGBitmapContext that you mentioned before and use it's pixel map to create an OpenGL texture.  Using various features of OpenGL, you can actually ask OpenGL to download your texture (the icon in this case) down to VRAM.  From that point it's a simple matter of just asking OpenGL to draw a rectangle with your texture at whatever size you want your icon to show up.  If the Dock is using a technique that is available through the public API set then I suspect that this is what it may be doing.  The Technique is not that complicated, but it is a bit involved and requires you to get your feet wet in OpenGL. As Haroon posted here a couple of days ago, the CGLayer object in Tiger should make that whole OpenGL path a whole lot more accessible to Quartz developers. -- Macintosh Software Engineering Consulting Services [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: Drawing icons fast : CGImageRef or IconRef ?</header>
    <body>Am 30.07.2004 um 21:02 schrieb Scott Thompson: You could plot the icon into a 128 x 128 bitmap context, make that a CGImage and then draw the CGImage at the desired size. Or, if that's not fast enough, you could probably also use OpenGL to draw the bitmap context, but I wouldn't know how to do that. Manfred</body>
  </mail>
  <mail>
    <header>Re: Drawing icons fast : CGImageRef or IconRef ?</header>
    <body>Well how is it slower? How are you scaling and drawing things? Note you converted the images to png which is can be a lossy process and it likely why your icons now don't look as good. You likely don't need to go the png route but just have raw bitmaps used to create a CGImageRef. You can use OpenGL to do this but I believe if you keep and reuse your CGImageRefs for your icons around and simply draw them to the context scaled as needed your performance should be good. You can likely turn down the quality of the scaling while live resizing is taking place (I believe iPhoto does this for example) and turn it back up when scaling is done. Someday you may end up using GPU / OpenGL without even having to know how. -Shawn</body>
  </mail>
  <mail>
    <header>A question about [bitmapImageRep bitmapData]</header>
    <body>2004-07-30 13:26:53.628 OpticFlow[11258] bitmaptImageRep ColorSpace=NSCalibratedWhiteColorSpace BPS=8 Pixels=93x87 Alpha=NO 2004-07-30 13:26:53.628 OpticFlow[11258] *** -[NSCachedImageRep bitmapData]: selector not recognized Why is it that bitmapData is not recognized for this image yet it is for this image? 2004-07-30 13:28:12.526 OpticFlow[11258] bitmaptImageRep ColorSpace=NSCalibratedRGBColorSpace BPS=8 Pixels=3072x2048 What have I misunderstood? I'm trying to get the address of the images pixels... Seems to work for some images but not for others.... Must I put some tests in the code to determine an images type before I can get the pointer to the data? Thanks, Brian.</body>
  </mail>
  <mail>
    <header>Re: Drawing icons fast : CGImageRef or IconRef ?</header>
    <body>I see the same thing here. I suspect you're right.  I suspect that under the covers Apple is simply using the same scaling technology that they use for things like the Genie effect.  Using that technology one would only have to draw the content of the window once and could rely on the window system and the Quartz Compositor to scale the window's contents from that point onward. What would be interesting to know, however, is how to reproduce that setup through the public interfaces. Currently, through both Carbon and Cocoa, when a window is resized, the application must take steps to see that the contents of the window are redrawn _before_ the Quartz Compositor (be it the &amp;quot;Extreme&amp;quot; compositor or the software variety) can come into play.  That means that an application would have to have a fast way to redraw the contents of the window and if PlotIconRefInContext is &amp;quot;slow&amp;quot; then the application will have to take another route. -- Macintosh Software Engineering Consulting Services [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: Drawing icons fast : CGImageRef or IconRef ?</header>
    <body>Mine shows about twenty six 128x128, two 1x1, one 54x19,  and one correctly? I have exactly twenty six items in my dock and it background is exactly 1494x67 when shown. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Drawing icons fast : CGImageRef or IconRef ?</header>
    <body>Am 30.07.2004 um 19:09 schrieb Jason Harris: My Dock currently owns 24 windows and selecting them in Quartz Debug makes individual icons blink, so each icon is its own window. They are 128x128 pixels large, so on Quartz Extreme enabled machines OpenGL would handle the scaling during the magnification. Drawing the icons the Dock probably doesn't have to do very often, so even if PlotIconRefInContext is slowish it shouldn't be a problem for the Dock. Manfred</body>
  </mail>
  <mail>
    <header>Re: Make a new NSImage</header>
    <body>You've pretty much got it -[NSBitmapImageRep initWithBitmapDataPlanes:pixelsWide:pixelsHigh:bitsPerSample: samplesPerPixel:hasAlpha:isPlanar:colorSpaceName:bytesPerRow: bitsPerPixel:] -[NSImage initWithSize:] -[NSImage addRepresentation:] -[NSImageView setImage:] The image width you'll pass to NSBitmapImageRep is an integer, corresponding to the bytesPerRow of your bitmapped data. Jason Harris Lead Developer Geekspiff [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: Make a new NSImage</header>
    <body>How isn't what you outlined working for you, what is tripping you up? You basically have the exact outline of what one likely needs to do. Have you looked at the Tableau example code [1]? It basically does a conversion from a binary blob of image data into a out of NSBitmapImageReps. Look in the FilterTest.m file at the various copyResultToNSBitmapImageRep: and loadImageFromNSBitmapImageRep: methods -Shawn [1] /Developer/Examples/Accelerate/vImage/Tableau</body>
  </mail>
  <mail>
    <header>Re: Drawing icons fast : CGImageRef or IconRef ?</header>
    <body>The window list in Quartz Debug on my machine shows the Dock only &amp;quot;owns&amp;quot; two windows.  I've got more than two icons in my dock. :) Jason Harris Lead Developer Geekspiff [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Make a new NSImage</header>
    <body>I'm trying to create an NSImage to show in an NSImageView. I want to be able to set the: width, height, number of bits per pixel, number of samples per pixel and I have a pointer to an unsigned char * img[x*y*samplesPerPixel] (The pixels are organized r,g,b,r,g,b etc...)  1 plane? I don't know if the image width must be rounded in cocoa... I gather I need to somehow create a bitmapImageRep and and NSImage and that I add the representation to the NSImage and then I add the NSImage to the NSImageView... Help please. TIA Brian.</body>
  </mail>
  <mail>
    <header>Re: Drawing icons fast : CGImageRef or IconRef ?</header>
    <body>I think you're making an over-generalization. All you can determine from using QuartzDebugger is that the compositing of the dock windows might be handled through Quartz Extreme (on Quartz Extreme enabled systems).  How the _contents_ of those windows is drawn (i.e. the actual icons themselves), is still open to speculation.  It could very well be the case that the dock icons themselves are drawn into their windows using PlotIconRefInContext. However, given that the original poster was having performance problems with PlotIconRefInContext, I'll agree that it does not seem very likely :-) -- Macintosh Software Engineering Consulting Services [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: Drawing icons fast : CGImageRef or IconRef ?</header>
    <body>If you play around with Quartz Debugger you'll see that each Dock icon lies in its own window.  That way they could be handled by Quartz Extreme.  So yes, it uses OpenGL to achieve that.</body>
  </mail>
  <mail>
    <header>Drawing icons fast : CGImageRef or IconRef ?</header>
    <body>Hi all, In my project, I need to display file icons at many sizes and the user can live-resize all of them (there can be 50 of them onscreen), so I'm looking for the best and fastest way to draw the icons. Right now I use IconRefs, and draw the icons in my CGContextRef using PlotIconRefInContext. However, while profiling my app, I noticed that PlotIconRefInContext calls private APIs to draw CGImages... so I was wondering if drawing CGImageRefs myself wouldn't be faster. So I converted all my .icns files into .png ones, modified my code to handle CGImageRefs.. The result is odd : it's slower, and the icons don't look as anti-aliased as with IconRefs. All in all, if I compare the speed of my app with the speed of the Dock at resizing the icons, the Dock is blazing fast. Some people told me it uses OpenGL to achive that. Is it true ? Is it what I should try to implement ? Can anybody enlighten me please ? Thanks, Jirome</body>
  </mail>
  <mail>
    <header>Re: Obtaining CGImageRef from an NSImage</header>
    <body>(snip) Yep, it's premultiplied RGBA, not ARGB.  Sorry for the confusion. I had some test code that was going from a grayscale + alpha NSImage to a CGImageRef.  The CGImageRef creation was failing if I used a grayscale colorspace and alpha.  I wound up converting it to RGBA instead.  This code hasn't hit production (or even real testing) yet, so I may have screwed things up, but that was my experience. I'll be looking at this some more in the next few weeks, so if I find anything that doesn't correlate with what you've said, I'll post back. Jason Harris Lead Developer Geekspiff</body>
  </mail>
  <mail>
    <header>Re: Obtaining CGImageRef from an NSImage</header>
    <body>This isn't quite accurate.  NSImage always draws using CG, so anything it supports CG supports (albeit in some cases not via public API).  It is true that CGImageRef works with bitmap image data, whereas NSImage can also work with PDF &amp;amp; PS/EPS, so in that sense they aren't equivalent. The most straightforward conversion path to a CGImageRef is from an NSBitmapImageRep.  If the data is interleaved, you can use kCGImageAlphaPremultipliedLast or kCGImageAlphaNone, depending on whether or not the &amp;quot;hasAlpha&amp;quot; method returns true or false.  (IIRC, NSBitmapImageReps with alpha are always premultiplied in Panther and earlier.)  There's no restriction on the colorspace for the image data gray, RGB and CMYK should all work, with or without alpha.  (If not, that's a bug.) If the data is planar, you have to create a custom data provider and do some more work.  Fortunately, those types of images are relatively infrequent. I believe NSBitmapImageReps use premultiplied RGBA in all cases in Panther and before.  Also, CGImageRefs support grayscale images with alpha, so you don't need to convert it to RGBA.  (If that's not working for you, it's a bug and it would be helpful to have a test case which demonstrates this.) Derek</body>
  </mail>
  <mail>
    <header>Re: Obtaining CGImageRef from an NSImage</header>
    <body>Very good info to know.  Thanks. initWithFocusedViewRect seems to be a good approach for me.  I do plan to use that API for some other work I need to do. Thanks for the heads-up on this one. I'll definitey be filing a couple of enhancement requests for such APIs. Thanks again, Rick Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: Obtaining CGImageRef from an NSImage</header>
    <body>This is slightly tricky because NSImage supports a richer set of formats that CGImageRef does.  Basically, make sure you've got your NSImage bitmap data in interleaved form (as opposed to planar).  Make sure that if there's an alpha channel, the image is premultiplied RGB (not grayscale or CMYK). Then, get an NSBitmapImageRep from the image (either by enumerating through the image's representations or by using initWithFocusedViewRect:).  You'll pass the pointer obtained by bitmapData to CGDataProviderCreateWithData. Note that if you need to use initWithFocusedViewRect and your image is narrower than 17 pixels, you'll need to create a new empty image, call setCachedSeparately:YES on it, lock focus on it, draw your image, and then initWithFocusedViewRect.   This works around a CTM bug in AppKit. IIRC, if alpha is present in an NSBitmapImageRep, it's stored as premultiplied ARGB.  You'll use this fact to figure out your alphaInfo - either p-m ARGB or none.  If the image has alpha and the colorspace is grayscale, you'll have to convert it to premultiplied ARGB.  NSImage supports that format, but CGImageRef doesn't. Hope this helps!  It would be nice if there was an API for this. Jason Harris Lead Developer Geekspiff</body>
  </mail>
  <mail>
    <header>Obtaining CGImageRef from an NSImage</header>
    <body>I have the need to obtain a CGImageRef from an NSImage.  I first looked to see if this fell under the toll-free bridge concept, but nothing turned up. At first, I thought the following would work: NSImage* someImage = ... NSImageRep* anImageRep = [someImage bestRepresentationForDevice:nil]; // use current device CGImageRef aCGImage = CGImageCreate ( [anImageRep pixelsWide], [anImageRep pixelsHigh], [anImageRep bitsPerSample],  // Is this correct? someBitsPerPixel, 0, // I think this means system computes this for me someColorSpace, someAlphaInfo, aDataProvider, NULL, /* no decode array */ false, /* do not interpolate */ But I see no method in obtaining the data from the NSImageRep.  Furthermore, this seems problematic to me as the &amp;quot;best representation&amp;quot; could be anything: bitmap, pdf, etc. Thoughts? Thanks, Rick Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: CarbonIT-Carbon, HIView, CoreImage, CILayer, and CIContext</header>
    <body>cross posting to quartz-dev also. The reason is that the CGLayer is based on the characteristics of the representation for that context. It could be a buffer in vram, it could be a bitmap context, or it could be PDF layer or an internal display list. You are guaranteed that it will be the best representation for the context it was created on. Now you can draw that layer ref to a different context and in that case it may not be fastest or appropriate representation (i.e. it could be rasterized or it has to be pulled off of vram to be rendered into different context). So being able to draw a CGLayerRef to a different context will work and is supposed to work. Not necessarily, they should all work as they should fall back to the altivec implementation in that case. Not sure why it isn't. At the moment you should be able to cache the CGLayerRef and re-use it on a new CGContext. While you can't rely on caching a CGContext or CIContext for a HIView, the CGLayerRef content are fixed and as long as you hold onto it, it will image with the cached content on whatever context you later decide to draw it on. In practice the transformations and clips associated with a CGContext in a HIView do not apply to CGLayerRefs (as they are effectively a new CGContext with their own gstate). BTW, in the WWDC bits, the CGLayerRefs were implemented as a BitmapContext so you won't get acceleration just yet. This will be fixed in  a future seed. Yes this is supposed to work as part of the definition of CGLayerRefs, with the caveat that depending on the source context against which the layer ref was built against, it may not be the best representation for the destination context and as such there might be further color matching involved. Or the LayerRef might be a bitmap even though the destination is PDF. A CGLayerRef can be one of many things. It is the best representation for the Context you create it for. So it could be an offscreen vram buffer, a DRAM buffer, or a PDF document. This should work as we will try and get it to the other context in as fast as possible. We may have to create a new texture that is a copy of the original. It may have to be read back from vram and uploaded again, etc. Color matching is one aspect. Another is making sure we know what type of backing context the CGLayerRef represents. i.e. vram surface, dram buffer, pdf display list, etc. Yes Yes in general it should be fine with respect to reusing the CGLayer across any window. The only caveat is that a window could be 16 bit or 32 bit depending on the display mode. So depending on how the application and the toolkits work, one could conceive of  a scenario where one could have both 16 bit and 32 bit windows in the same application. For example, on a mode change, I believe, Cocoa does not promote/demote all windows to 16/32 bits. Even in this case the CGLayer will work, but it may no longer be the best representation. Thanks for asking clarification on a new feature that might not be clear given we don't have documentation yet. haroon P.S. I may not respond to further emails as I'll be on vacation soon. [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>creating CGContextRef for CW View</header>
    <body>How do I create CGContextRef for View in code warriro. I am using CW 9.2 mach-o project. currently, I am using I think it sets up with window port. Therefore the scroller in the view does not response. It also gives me problem in adjusting co-ordinate system. What I am missing here? Thanks in advance. Wahidun Chowdhury SNX Software 90 Park Ave NY, NY 10016</body>
  </mail>
  <mail>
    <header>Re: PDF and CMYK color space</header>
    <body>When you create a color, you specify a color space for that color.  We use that color space in the PDF file (with the exception that device color spaces are handled specially).  If you use a CMYK color space for a color, then the output PDF will contain that CMYK color space.  You might want to look at the CGColor API and use it instead of There may be ATSUI API to specify a CGColorRef directly, but I'm not sure about that. For PDF output, ColorSync typically  doesn't come into play except when converting colors (from one color space to another color space, for example).  Why do you want to disable ColorSync in this case? Derek</body>
  </mail>
  <mail>
    <header>Re: PDF and CMYK color space</header>
    <body>Are you specifying the color for the text within the style runs using kATSUColorTag? When we had this problem, we had to forgo specifying a color in the ATSUI style.  Instead we used the color settings in the CGContext. To do that you have to specify the CGContextRef in your layout controls (using).  Then when you have no colors in in your text style, then ATSUI will use the current drawing color (and text mode) of the CGContext.  Unfortunately this makes it more difficult to draw text that contains multiple runs of color text. Each individual color will retain it's identity within the PDF file. Colors that you set using CMYK values will remain CMYK and colors that you put in the context as RGB will remain RGB. As I said above, you can continue to use ATSUI by using the CGContext's graphics state to control the text color rather than using the RGB colors through ATSUI styles. I'm still struggling with the proper way to use color spaces in Core Graphics myself so take this advice with a grain of salt.  My understanding from discussions at WWDC was that in systems prior to Tiger, the proper way to specify colors in such a way that they do not go through Color Sync is to specify them in the &amp;quot;Device&amp;quot; color space. That means that if you use CGContextSetCMYKFillColor you will be specifying a color in the  color space of the output device. An ATSUI text color would be dealt with by the system as being specified in the Device RGB color space.  That means that the device would be responsible for translating the RGB color to CMYK. -- Macintosh Software Engineering Consulting Services [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>PDF and CMYK color space</header>
    <body>Hi, We're using CG and ATSUI to create PDFs via CGPDFContextCreateWithURL. We're specifying color using CMYK where possible (using CGContextSetCMYKFillColor etc), but the ATSUI API seems to require color to be specified using RGB. Since our customers generally know what CMYK values they want to use in their publication and for the device they use, we would like the PDF we generate to use a CMYK color space and the values entered by the user without any interference by Quartz or ColorSync. Right now, it looks like our PDFs are generated with a RGB color space and after looking at the available documentation on creating PDFs, CGColor, CGColorSpace and ColorSync it's not clear how we could control that, and what will happen to colors specified using RGB (in the case of the ATSUI text). So before we start experimenting and trying to figure this out ourselves, I thought I'd ask for some pointers here. Is there any way to control what color space the PDF will use when created via CGPDFContextCreateWithURL? ie. If we specify all colors using a CMYK color space will the generated PDF use a CMYK color space? If yes, is there a way to continue using ATSUI? It looks like generating the PDF using the printing API would offer more control, but can we effectively disable ColorSync somehow? In that case, how would the ATSUI text color be dealt with by the system? Any pointers appreciated. Thanks, -Chris -- Chris White Codeworks Software Ltd</body>
  </mail>
  <mail>
    <header>Re: CG Help!!!</header>
    <body>You only need a single CGContext. Think of the CGContext like a QuickDraw Port or a GDI HDC. You need to get a CGContext for the window. For each image you need to create a CGImageRef. Then you need to draw the images into the context using CGContextDrawImage. I highly recommend you read the Quartz 2D Documentation : drawingwithquartz2d/index.html Better yet you can use QuickTime to return a CGImageRef for you using the function : GraphicsImportCreateCGImage If you use QuickTime and the alpha channel is already specified in the image, it will create a CGImageRef appropriately. If you choose not to use the GraphicsImportCreateCGImage function (for backward compatibility as it was introduced in Panther), then you need to find out from QuickTime what the alpha/color information is for an image. Check out the following sample code that converts from QuickTime to CG. I'd still recommend the GraphicsImportCreateCGImage function. If instead you want to further apply a constant alpha value to the entire image, then you can set the alpha into the CGContext : CGContextSetAlpha No, you should use affine transforms in CG. Read about them here (if the link is on two separate lines, copy and paste it into your browser): drawingwithquartz2d/dq_affine/chapter_6_section_2.html#//apple_ref/doc/ uid/TP30001066-CH204-CJBBHBEF You can save the GState before applying the transformation and then restore the GState before working on the second image. haroon</body>
  </mail>
  <mail>
    <header>Re: CG Help!!!</header>
    <body>You should only need one CGContext.  Quartz uses a painters algorithm for drawing so simply draw one image, and then the other. Not really.  For PNG and JPEG images you can go directly through Quartz with CGImageCreateWithJPEGDataProvider and CGImageCreateWithPNGDataProvider.  But QuickTime should give you a broader reach in terms of image formats for the time being. Hmm... that's an interesting question. I don't know that alpha channel makes much sense in a CMYK environment. Even if Quartz allowed you to change the alpha channel value of a CYMK image, the printer or imagesetter is not likely to support it.  You probably would have to use muted CMYK values and set them to overprint, but I don't really have any idea how you could tell Quartz that you want overprinting. -- Macintosh Software Engineering Consulting Services [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>CG Help!!!</header>
    <body>HI everyone, I am trying to open 2 graphics on top of each other. One is semi transparent giving it a water mark look. Will there be one CGContext or two. I used quick time function to open images to give me GWorld, because this way I can open many image format. Is it a bad idea for any reason? How do you set alpha for CMYK color space. Is it always Alphanone? Since I may want to rotate one graphic on another do I need clipping? PS: I am using mach-o in Code warrior 9.2. Thanks Wahidun Chowdhury</body>
  </mail>
  <mail>
    <header>Re: different CTM strategies?</header>
    <body>The two technique will yield identical transformation matricies.  The only &amp;quot;performance gotcha&amp;quot; I can think of is that the code in the #else clause will perform additional matrix math that code in the other clause won't have to do (the matricies have already been pre-multiplied). If you were calling this code many, many times (say in a tight loop) then the additional multiplications might be an issue.  As this code looks to be used to setup a drawing context into which there is likely a lot more drawing to be done, the calculations involved will be pretty insignificant compared to the computations needed to do the actual drawing. In the grand scheme of things it doesn't look like the overhead of the additional math will make that much difference. Scott [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: ClipCGContextToRegion problem</header>
    <body>;-) ...in the grand tradition of replying to your own post:  we managed to fix the problem!  Turns out that we needed to intersect whatever clip region used with the window's visRgn before setting the CG clip or synching, as follows: ...but I'm still unclear as to whether the CGContextSaveGState()/CGContextRestoreGState() calls are even necessary when dealing with QDBeginCGContext()/QDEndCGContext()? thanx, jamie</body>
  </mail>
  <mail>
    <header>different CTM strategies?</header>
    <body>...is there any considerable difference (ie. performance or other possible gotcha's) between the following CTM's?: #if 1 coordsTransform = CGAffineTransformMake(1.0f, 0.0f, 0.0f, -1.0f, 0.0f, #else CGContextTranslateCTM( outContext, 0, (float)(boundsRect.bottom - #endif thanks, jamie</body>
  </mail>
  <mail>
    <header>Problem CalcCMask w pixmap from CGImage</header>
    <body>reetings. (Still) working on code to support hit testing of strokes and fill areas encoded as SVG directives. The problem involves calls for CalcCMask using coerced pixmap data obtained from a bitmap context. The algorithm - Allocate space in a buffer for pixels to be drawn Create an RGB colorspace and RGB bitmap context Draw the test path into that context, using pre-defined colors for stroke and fill Build a PixMap data structure off the pixmap data in the bitmap Call CalcCMask to build a bitmap mask off the pre-determined colors used for stroke and fill Build a region off the bitmap mask for hit testing I think that if the fill color is set to 100% blue, and the seedColor.blue is set to 255, I will get 1's wherever the source image is 100% blue as color (or areas completely enclosed by blue pixels). No matter what I do, the CalcCMask returns a bitmap filled with 1's. No changes to the seed color, or changes to the shape drawing into the pixmap seems to have any affect on this behavior. I have verified that the pixmap data is correct (I kludge an image display that properly shows the stenciled blue and red areas, and auditing of the bounds rects show nothing is amiss - I also dump the resulting bitmap to verify the changing of the bitmap data as a function of the CalcCMask). For simplicity, the source and dest rectangles are all set to the same quickdraw coordinates, origin at 0,0, with dimensions derived from the quartz pixmap bounds. I have enclosed a snip of the code that is suspect. Somewhere, I am not building the PixMap correctly, do not understand what CalcCMask does, or not using CalcCMask correctly. The real suspect code is flanked by &amp;quot;^^^^^^^^^^^^^^^^^^&amp;quot; Any help is appreciated. pathBuffer holds the pixels that were written by the CG DrawPath directive. pathRect is a CG rectangle containing the path on the canvas being displayed bitBuffer - holds the mask bits Begin code // The pathRect represents the CGRect bounding the path // as drawn on the screen in user coordinates. We want to // draw that same path into a pixmap, but we have to move // the coordinates so that the lower left (0,0) of the pixmap // lines up with the lower left of the path as drawn. ::CGContextTranslateCTM(maskContext,(0-pathRect.origin.x), printf(&amp;quot;KCF final pixmap matrix is = %f %f %f %f %f %f\n&amp;quot;, printf(&amp;quot;Path bounding box x y w h = %f %f %f %f \n&amp;quot;, pathRect.origin.x, pathRect.origin.y, // // The following colors will be used for calc mask // Stroke is RED // Fill is BLUE // snip.... stuff setting up drawing pen, etc... // now draw the target path into our mask context // printf(&amp;quot;Mask Pixmap bbox x y w h = %f %f %f %f \n&amp;quot;, pixmapRect.origin.x, pixmapRect.origin.y, // KLUDGE SNIPPET TO SHOW RED/BLUE strokes and fills // // !!!!!!!!!!!!!!!!!!! sample code to display stuff CGDataProviderRef theProvider = CGDataProviderCreateWithData(NULL,pathBuffer, CGImageRef thePixmapImage = ::CGImageCreate(pixmapWidth,pixmapHeight, 8,32,(pixmapWidth*4),maskColorspace,kCGImageAlphaNoneSkipFirst, // !!!!!!!!!!!!!!!!! sample code to display stuff // calculate bitMap row width in BYTES // Now build a 1 bit bitmap to be used to create our region PixMap pathPixmap;   // used as input to CalcMask BitMap maskBitmap;     // used as output (one bit mask) // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ // // For simplicty we set all our quickdraw coordinate relative // to coordinate 0,0, then offset the region // pathPixmap.rowBytes = (pixmapWidth*4 + 0x8000); // flag bit for pixmap // // Build space to receive the mask bitmap // Build the mask CalcCMask((BitMap *)&amp;amp;pathPixmap,&amp;amp;maskBitmap, // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ // (pathX, pathY calculated elsewhere.... printf(&amp;quot; rgn after offset l t r b = ... %d %d %d %d\n&amp;quot;,</body>
  </mail>
  <mail>
    <header>Re: Preventing Window Selection</header>
    <body>This probably won't help, but Microsoft Office X apps (especially Excel, at least on our machines) seem to do this..  If you click on the menubar, the window will come to the front, but if you click in the content region, it won't.  But the app will activate in either case, which makes things quite annoying..  Of course, isn't Office X Carbon? Jim</body>
  </mail>
  <mail>
    <header>ClipCGContextToRegion problem</header>
    <body>...In trying to move Tk into the quartz method of drawing, we've created a call that passes in a QD port and sets it up as a CGContext...this has been working fine for most things, but on closer inspection it seems that lower right corner grow box on the window is not always drawing correctly... We've noticed that in the following code, if we comment out the section to do with ClipCGContextToRegion(), the grow box works fine (but of course there are other drawing problems!)...in Quickdraw.h it mentions that &amp;quot;In order to produce the expected *    outcome in ClipCGContextToRegion, this function needs to reset *    any CGContext clipPath, before setting it to the converted *    region. Consequently, the previous clipPath in the CGContext is *    lost, and cannot be restored in a *    CGContextSaveGState/CGContextRestoreGState bracket around the *    ClipCGContextToRegion call.&amp;quot; Does ClipCGContextToRegion() take care of the CGContext clipPath reset, or should this be done explicitly (and how?): coordsTransform = CGAffineTransformMake(1.0f, 0.0f, 0.0f, -1.0f, 0.0f, /* Now offset the CTM to the subwindow offset */ CGContextTranslateCTM(outContext, (float)macWin-&amp;gt;xOff, ...does anything look wildly outta place here?  Perhaps there's a better way to transform the coordinate space? thanks, jamie</body>
  </mail>
  <mail>
    <header>Re: Rendering to PDF</header>
    <body>here's a cleaned-up version (I hope). It appears that others have already diagnosed the original bug here: the return value of &amp;quot;fwrite&amp;quot; in your code isn't what you expect.  I wanted to follow up to Dave's mail, however: -- It's not necessary to call CGContextFlush for a PDF context, it's principally only for window contexts when a client wants to explicitly force what's been drawn so far to the screen. CGContextEndPage in a PDF context will appear in the output PDF file. After CGContextEndPage, the page is effectively complete (that is, you can't add anything else to it, but it's possible that not all of the resources associated with the page have been written to the PDF file). -- You don't need to retain the data consumer since the PDF context will retain it.  However, you should make sure that you don't inadvertently discard, deallocate or destroy any of the state needed for output (for example, the output FILE *) until after the data consumer is deallocated.  The poorly-named &amp;quot;releaseConsumer&amp;quot; callback function is the place to put all of your &amp;quot;shutdown&amp;quot; code. -- Due to an unfortunate...well, bug isn't quite right---let's call it a misfeature---in Panther and earlier, the final parts of the PDF file will not be written until the CGPDFContext is deallocated.  You should make sure to always call CGContextRelease (or CFRelease) on a PDF context or you will likely end up with an invalid PDF file. Derek</body>
  </mail>
  <mail>
    <header>Re: Rendering to PDF</header>
    <body>It appears that others have already diagnosed the original bug here: =20 the return value of &amp;quot;fwrite&amp;quot; in your code isn't what you expect.  I=20 wanted to follow up to Dave's mail, however: =95 It's not necessary to call CGContextFlush for a PDF context, = although=20 it does no harm.  CGContextFlush is a no-op for most contexts; it's=20 principally only for window contexts when a client wants to explicitly=20= force what's been drawn so far to the screen. =95=A0Only the drawing operations between CGContextBeginPage &amp;amp;=20 CGContextEndPage in a PDF context will appear in the output PDF file. =20= After CGContextEndPage, the page is effectively complete (that is, you=20= can't add anything else to it, but it's possible that not all of the=20 resources associated with the page have been written to the PDF file). =95 You don't need to retain the data consumer since the PDF context = will=20 retain it.  However, you should make sure that you don't inadvertently=20= discard, deallocate or destroy any of the state needed for output (for=20= example, the output FILE *) until after the data consumer is=20 deallocated.  The poorly-named &amp;quot;releaseConsumer&amp;quot; callback function is=20 the place to put all of your &amp;quot;shutdown&amp;quot; code. =95=A0Due to an unfortunate...well, bug isn't quite right=A0=97 let's = call it a=20 misfeature=A0=97 in Panther and earlier, the final parts of the PDF file=20= will not be written until the CGPDFContext is deallocated.  You should=20= make sure to always call CGContextRelease (or CFRelease) on a PDF=20 context or you will likely end up with an invalid PDF file. Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>CGInterpolationQuality with printing</header>
    <body>it seems the CGInterpolationQuality set via CGContextSetInterpolationQuality() is ignored during printing.  is there any way to get this to work short of manually resampling my images?  here's an example: void ReleaseData(void* theInfo, const void* theData, size_t theSize) CGImageRef CreateImage(size_t theWidth, size_t theHeight) for(size_t h = 0; h &amp;lt; theHeight; h++) for(size_t w = 0; w &amp;lt; theWidth; w++) ((unsigned int*) theData)[theWidth * h + w] = (rand() % 2 &amp;gt; 0 ? CGDataProviderRef theProvider = CGDataProviderCreateWithData(NULL, CGImageRef theImage = CGImageCreate(theWidth, theHeight, 8, 32, theWidth*4, theColorSpace, kCGImageAlphaFirst, theProvider, void DrawImages(CGContextRef theContext, CGRect theBounds) void DoPrint() PMSessionPrintDialog(thePrintSession, thePrintSettings, thePageFormat, if(print) PMSessionBeginDocument(thePrintSession, thePrintSettings, PMSessionGetGraphicsContext(thePrintSession, CGRect cgBounds = CGRectMake(qdBounds.left, qdBounds.top, int main(int argc, char* argv[]) thanks. rob raguet-schofield (rob ra gA skO fEld)</body>
  </mail>
  <mail>
    <header>Re: Quartz Extreme with an iMac</header>
    <body>I'm running on Panther and not Tiger.  I'm aware of the NDA regarding Tiger.  I do have Xcode 1.5 installed, which is also under NDA, but I'm not sure if the Quartz Debug that I have came from Xcode 1.5 or Xcode 1.2.  My Quartz Debug program is 1.3 (v10). Haroon contacted me directly so I'll pursue the issue with him and file a bug if necessary. Thanks to all for the links to the different sets of documentation available regarding hardware. Mike</body>
  </mail>
  <mail>
    <header>Re: Quartz Extreme with an iMac</header>
    <body>Looks down at the bottom of this page... ...and you can get the full list of hardware... -Shawn</body>
  </mail>
  <mail>
    <header>Re: Quartz Extreme with an iMac</header>
    <body>This, fortunately, doesn't have anything to do with Tiger.  The version of QuartzDebug that in installed by Xcode 1.2 (and probably even back to 1.0), includes a &amp;quot;Disable/Enable Quartz Extreme&amp;quot; menu item from its &amp;quot;Tools&amp;quot; menu. On Apple's public site on Mac OS X, it does mention that the GeForce4MX card should allow QE.  On another page, it also mentions the requirement of AGP 2x.  The current round of iMacs (base model at 1GHz G4) are all AGP 4x.  One would only assume that its predecessor (800 MHz G4) would at least be AGP 2x. I know Apple maintains tech notes on various hardware models they've shipped, but it appears that only the latest revisions are represented: Rick Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: Quartz Extreme with an iMac</header>
    <body>If your are asking about Tiger... don't. It is under NDA and this is a public list. -Shawn</body>
  </mail>
  <mail>
    <header>Quartz Extreme with an iMac</header>
    <body>At WWDC they talked about using Quartz Debug for testing Quartz issues. They also said that Quartz Debug could be used to turn Quartz Extreme on and off.  On my computer the Quartz Extreme option shows as dimmed meaning that I can't select it. I did some searching on the web and I'm trying to figure out why my computer would not support Quartz Extreme. I have an 800 MHz G4 iMac with 512MB of memory.  It has a NVidia Geforce4 MX graphics card with 32MB of memory.  Per the information I found Geforce4 MX cards should be good enough to support Quartz Extreme. Am I doing something wrong or does none of the iMacs support Quartz Extreme regardless of what graphics we have? Thanks, Mike Lazear</body>
  </mail>
  <mail>
    <header>Re: Rendering to PDF</header>
    <body>for the size of each, e.g.: This avoids the extra calculations and is probably what you want... -- ______________________________________________________________________ Michael Sweet, Easy Software Products           mike at easysw dot com Printing Software for UNIX</body>
  </mail>
  <mail>
    <header>Re: Rendering to PDF</header>
    <body>fwrite() returns a number of objects and not a number of bytes.  you probably want something like: quartz is probably choking because you are returning a short number of bytes off the bat, and not returning the correct number of bytes. nibs</body>
  </mail>
  <mail>
    <header>Re: Quartz &amp;amp; Performance Problems</header>
    <body>If you only draw the audio samples that could possible affect how the waveform is viewed given your zoom and clipping setup, I don't think you'd loose any accuracy and the display would still be fast. I'd suggest: Let X be the number of audio samples you are trying to display. This isn't your full waveform length, just what is going to be displayed onscreen at the time. Let N be the number of pixels wide of the view. You should be able to just sample the audio samples at: i * X / (2*N) where i is 0 to N. And use these samples to build your waveform view. You'll need to interpolate of course. This way you should only be drawing what the screen is actually capable of displaying. Plus some adjustment to account for my vague understanding of nyquest theorem applied to drawing.  I don't see how you'd loose accuracy this way. Unless I'm missing something of course ;-) -Corey O'Connor On Thu, 22 Jul 2004 11:59:24 -0300, Carlos Eduardo Mello</body>
  </mail>
  <mail>
    <header>Bevel button drawing with QD?</header>
    <body>We have some bevel buttons created with CreateBevelButtonControl. They have IconRefs installed with SetBevelButtonContentInfo. If drawing of those controls happens between calls to QDBeginCGContext and QDEndCGContext, I'm seeing the warning that &amp;quot;Ignoring Quickdraw drawing between QDBeginCGContext and QDEndCGContext&amp;quot;. So, are bevel buttons still drawing something with QD? If so, what and why? _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: SIGPIPE with dataWithPDFInsideRect</header>
    <body>I'm assuming you are in a gdb session with the application when this happens. This is due to the way the CUPS printing system generates signals. You need to tell gdb to ignore these signals by executing: handle SIGPIPE nostop noprint pass in gdb prior to debugging. I have this in my .gdbinit file in my home directory and that solves the problem; it then applies to all debugging sessions. If this happens outside of gdb please let us know. David</body>
  </mail>
  <mail>
    <header>Re: CGImage Questions</header>
    <body>It sounds like Jason is asking about grayscale images, not bitmap contexts.  Is that correct?  If so, what's the form of the grayscale + alpha images you're working with? Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: API for GIF files?</header>
    <body>If you don't mind the inherent nastiness, you can use Quicktime to read the file and get a CGImage.  Apple's got some sample code for doing this with TIFFs - I think you just change a constant to do it for GIF. Jason</body>
  </mail>
  <mail>
    <header>SIGPIPE with dataWithPDFInsideRect</header>
    <body>Sorry to cross post, but I'm not sure if this is properly a cocoa or a quartz problem: I'm using  dataWithPDFInsideRect to get a PDF representation of some graphics in preparation to do a drag and drop: Every once in a while (when the application has been sitting out of focus for a bit ?) it crashes with a SIGPIPE (stack trace below). Nothing appears amiss with the views, and this has happened when the graphics to be rendered are as simple as a single rectangle. It is not repeatable - doing the same thing with the same file will work the next 100 times. I understand that it is probably using the same code to prepare the pdf as it does when making a spool file, but why is it trying to contact a printer ? Why is it trying to connect to anything at all? (My default printer is reached over an Airport connection with an iMac that is probably asleep unless I go in the other room to wake it up if I want to print, but what has this to do with the price of tea in China...) Anyone have any ideas ? Thanks, Bob #0  0x90013508 in sendto () #1  0x93a23fd4 in httpPrintf () #2  0x93a24a30 in httpGetLength () #3  0x93a2d29c in cupsDoFileRequest () #4  0x952460e4 in CPLPrinterIsRemote () #5  0x95276644 in OpaquePMPrinter::IsPrinterDirectConnect() () #6  0x952465bc in PMPrinterIsFavorite () #7  0x95246554 in PMPrinterIsFavorite () #8  0x9523c4a8 in PMCreateSession () #9  0x9523c3ac in PMCreateSession () #10 0x9523c2e0 in PMCreateSession () #11 0x9523c1dc in PMCreateSession () #12 0x9523c104 in PMCreateSession () #13 0x931e3504 in -[NSPrintInfo(NSPrivate) _printSessionForGetting] () #14 0x931e92e0 in -[NSPrintInfo(NSPrivate) _printSettingsForGetting] () #15 0x93179134 in -[NSPrintInfo(NSManagedAttributes) _objectForAttributeKey:] () #16 0x93278dec in -[NSConcretePrintOperation runOperation] () #17 0x93352f24 in -[NSView(NSPrinting) dataWithPDFInsideRect:] () #18 0x00034e50 in -[ZGDisplayList dataWithPDFForDisplayListInView:] (self=0x9a03da0, _cmd=0xf4d90,</body>
  </mail>
  <mail>
    <header>Re: API for GIF files?</header>
    <body>Thanks to both of you. Dean</body>
  </mail>
  <mail>
    <header>Re: API for GIF files?</header>
    <body>In you're in Cocoa, NSBitmapImageRep can read/write GIF files. Ali Begin forwarded message:</body>
  </mail>
  <mail>
    <header>Re: CGImage Questions</header>
    <body>Correct, CG doesn't support a Bitmap Context with grayscale + alpha. It supports the grayscale and alpha only context separately. Don't know if there is a way to convert a NSImage to a CGImageRef. haroon</body>
  </mail>
  <mail>
    <header>Re: Need ColorSpace Advice</header>
    <body>There are many choices for an internal working space, but make sure it is not DeviceRGB. You can use ColorSync to load an explicit profile like GenericRGB (the mac standard profile) or even choose to use sRGB, but stick to a calibrated profile. Similar on export you should export with the color space of your internal representation. This way your image should be displayed consistently on a different computer. This is the same model that most photo applications work in. They let the user override what the working space is also. haroon</body>
  </mail>
  <mail>
    <header>Re: API for GIF files?</header>
    <body>I don't think that there is a direct API. Use QuickTime to do the importing for you. Here's sample code:</body>
  </mail>
  <mail>
    <header>CGPDFDocumentRelease Crashes Consistently</header>
    <body>My app creates a PDF file, that it then renders to a GWorld, then deletes. (This is just one path things may take, so don't worry about why I'm doing it this way.) Creating the PDF and rendering it to the GWorld all works fine. The problem is that after rendering to the GWorld (via CGContextDrawPDFDocument to a context created for the GWorld), I try to release the PDF document via CGPDFDocumentRelease. When I do this I get a consistent crash: a EXC_BAD_ACCESS inside of CFRelease. If I don't do the CGPDFDocumentRelease then I don't get the crash, but when I go to delete the PDF file, I get a file busy error. Is there something non-obvious about CGPDFDocumentRelease? The docs clearly say I that I'm responsible for doing this after creating the PDF document ref via CGPDFDocumentCreateWithURL. -- Bob ----- Bob Currier                                  email@hidden Iron Chef Tater Tot                          voice: +1 949 493-3444 fax:   +1 949 203-2108 web:   www.synthetic-ap.com</body>
  </mail>
  <mail>
    <header>API for GIF files?</header>
    <body>I've been unable to find a method for creating a bitmap image from a GIF file.  I assume my search technique is at fault--Preview, for example, readily reads and displays such a file.  Can you point me in the right direction, or must I find a method for converting such files to JPEG?</body>
  </mail>
  <mail>
    <header>CGImage Questions</header>
    <body>Hi again, List, I was surprised to see that CoreGraphics doesn't support grayscale colorspace images with alpha.  Did I miss something? Also, is there any good way of converting an NSImage to a CGImageRef? I see that there's a function in WebKit that'll do it, but I'd like to avoid linking with WebKit if possible. Doing a manual conversion is tricky because of calibrated colorspaces and because NSImage _does_ support grayscale + alpha. Thanks! Jason Harris Lead Developer Geekspiff [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Need ColorSpace Advice</header>
    <body>Hi List, I'm working on an image compositing library that implements the Porter-Duff compositing modes and the compositing modes in the SVG spec for CGImages.  Everything's looking good, but I could use some advice regarding colorspaces. Here's a flowchart: externally created image | \ / my internal representation, using the original colorspace/profile | \ / some operation performed, I convert to either DeviceGray or DeviceRGB, and kCGImageAlphaNone, kCGImageAlphaNoneSkipFirst, or kCGImageAlphaPremultipliedFirst, as needed | \ / compositing with other images | \ / export to external representation So, my question is this:  What colorspace should I be using for my internal representation, and what colorspace and/or profile should I use when I export?  The exported image will be used on a different computer than the one that the compositing occurred on, so I want to maintain generality and color-correctness. Thanks in advance! Jason Harris Lead Developer Geekspiff [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: drawing to a CGContext during printing</header>
    <body>There are two important points of interest that are most likely the relevant places to examine in your code: This call needs to specifically say you want to use a CoreGraphics context. Since you are calling this function I assume you are already doing so. The right way to call this is: CFArrayRef  graphicsContextsArray = CFArrayCreate(NULL, (const PMSessionSetDocumentFormatGeneration(printSession, This MUST precede your call to PMSessionBeginDocument or PMSessionBeginDocumentNoDialog. From your text here it sounds like you do this. This will definitely NOT work if you call PMSessionSetDocumentFormatGeneration AFTER PMSessionBeginDocument or PMSessionBeginDocumentNoDialog. This should only be necessary if you are using a destination other than the printer, i.e. a file. This should look something like: status = PMSessionGetGraphicsContext(printSession, It is possible you are doing everything correctly but your drawing isn't in the imageable area of the page. This could happen if, for example, you are assuming a coordinate system other than the default Quartz coordinate system where the Quartz origin is at the lower left corner of the sheet and the y axis goes up the page. One way to check this kind of thing would be to make your destination be a PDF document on disk and then look at the PDF document you create. If you don't spot something wrong based on the information I've supplied above, it would be useful to know what you are seeing in the console during the execution of your tool/application. David</body>
  </mail>
  <mail>
    <header>(no subject)</header>
    <body>I'm trying to use Quartz 2D to print directly to the printer, and have been unable to work out how to get a printing context from the print manager.  From the CarbonPrintingManager_ref, I've deduced that the sequence is something like this:  (appropriate error checking deleted to save space) PMCreateSession PMCreatePageFormat PMSessionDefaultPageFormat PMCreatePrintSettings PMSessionDefaultPrintSettings (obscure stuff) PMSessionSetDocumetnFormatGeneration PMSessionSetDestination PMSessionBeginDocumentNoDialog PMSessionBeginPageNoDialog PMSessionGetGraphicsContext core graphics directives using context PMSessionEndPageNoDialog PMSessionEndDocumentNoDialog etc. So far, when I run this, the core graphics directives don't seem to be having any effect--the printer spits out a blank page.  I assume that's because I haven't successfully acquired a valid graphics context, because when I use the same core graphics directives with a PDF graphics context, they show up. Can anybody direct me to sample code?  The only one I can find uses mixed QuickDraw and core graphics. Dean</body>
  </mail>
  <mail>
    <header>Re: why is quartz still interpolating ??</header>
    <body>Thanks !! Doing this with a 1 bit mask, I get no extraneous colors. Saved from an ugly workaround. .......Bob</body>
  </mail>
  <mail>
    <header>Re: why is quartz still interpolating ??</header>
    <body>Try : CGImageMaskCreate( with shouldInterpolate to YES ) then CGContextSetInterpolationQuality(  gcref, This is a known bug. haroon [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>why is quartz still interpolating ??</header>
    <body>I draw a bunch of single color bitmaps. Since the color may change, I drop down into core graphics and use image masks. My problem is that I can't get Quartz to stop interpolating - no matter what I do I wind up with a few pixels that are neither the color that I'm drawing in nor the background color. The image mask is created with the last argument of CGCreateImageMask (&amp;quot;shouldInterpolate&amp;quot;) set to NO. I also call : This cuts down on the number of interpolated pixels but doesn't eliminate them. Why do I care ?  In some cases I'm essentially making a false color image. The colors encode something and I examine the pixels produced after rendering to try and extract some information. It is extremely inconvenient to have colors that I didn't ask for. This seems a bit unreasonable to me - InterpolateNone should be the easiest thing to do - either the center of the destination pixel, transformed back to the source image is inside a colored pixel or not. Does anyone know a way to force this to behave? - I have a workaround, but it is ugly. .....Bob Clair</body>
  </mail>
  <mail>
    <header>Re: PDF rendering problem</header>
    <body>I'm using NSImage as the code is used for rendering images from many sources including icons loaded from the bundle resources. However, I know that there will only every be one image rep in any given NSImage in the system so can get it and draw as you suggest for all and see what happens. Mark</body>
  </mail>
  <mail>
    <header>Re: PDF rendering problem</header>
    <body>Why are you adding it to an NSImage?  Use the NSPDFImageRep directly and tell it to -draw itself.  This should give you much more predictable results.  NSImage is for drawing icons and other static resources. Using the NSPDFImageRep directly instead of via NSImage also tends to be measurably and noticeably faster, in my experience. Cheers, Marcel -- Marcel Weiher				Metaobject Software Technologies email@hidden		www.metaobject.com Metaprogramming for the Graphic Arts.   HOM, IDEAs, MetaAd etc. 1d480c25f397c4786386135f8e8938e4</body>
  </mail>
  <mail>
    <header>PDF rendering problem</header>
    <body>I'm working on porting a Windows application to OS X and I've come across an issue with PDF rendering. Firstly I'm working Cocoa and want to render a PDF that has been loaded with NSPDFImageRep and added to an NSImage instance to an NSView. The NSVIew is flipped and I want to re-render the PDF each time its drawn as the code supports zooming and a cached bitmap copy will only have the resolution of the first rendering. So I've set the flags so that caching is off and issue a recache before each draw. Having done this issuing a drawInRect with NSCompositeSourceOver results in the PDF being re-rendered to the screen. For some reason NSCompositeCopy does not cause a re-render but a cached copy to be still used - although this is a side issue. The real issue is that if I'm requesting a portion of the PDF to be rendered rather than the whole page and either the NSPDFImageRep is set as flipped or I flip during the drawInRect call by using -ve height the results are offset in the y axis from the location set by the destination rect. If flipping is not attempted I get the correct location but the image inverted. If caching is off it works correctly too, no doubt as no flip is attempted when actually rendering the PDF to a raster image. I can see a simple workaround by creating my own temp raster image from the PDF without flipping and then flip as I copy this to the output view but I can't figure out why the direct rendering doesn't appear to work correctly. If I enable caching then with the same source and dest rectangles it works as you would expect, with caching disabled so that the PDF is rendered directly to the view with flipping it ends up in the wrong location. Is there a bug in the PDF renderer when flipping is involved? Thanks Mark</body>
  </mail>
  <mail>
    <header>Re: PDF generation</header>
    <body>Call CGContextRelease at the end of the function.  Make sure that all=20 drawing code appears within the BeginPage/EndPage calls=A0=97=A0in your = case,=20 move the CGContextSetRGBFillColor and CGContextSetTextDrawingMode. Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: PDF generation</header>
    <body>Regarding: The obvious thing that is missing is that you aren't calling CGContextRelease when you are done with the context. There may be other issues in your code but that is the first thing I'd try. David</body>
  </mail>
  <mail>
    <header>Re: PDF generation</header>
    <body>CGContextFlush(context) ??? -Shawn</body>
  </mail>
  <mail>
    <header>PDF generation</header>
    <body>I'm trying to write a simple application to generate pdf output.  I copied the CreatePDFContext example from &amp;quot;drawingwithquartz2d&amp;quot; which I got from the Apple Developer's website, then tried the following sample code: int main() CGContextSelectFont(context,&amp;quot;Times New Roman&amp;quot;, CGContextShowTextAtPoint(context,18.0, 762.0, It does generate a pdf file, but nothing is in it.  Am I overlooking something? Dean Ritchie</body>
  </mail>
  <mail>
    <header>Re: Paths and hit testing, again.</header>
    <body>This seems to come up regularly.  Perhaps someone in DTS could create a sample app? Here's what I do.  Note that I use a left-hand coordinate system (i.e., the y axis increases from top to bottom).  Otherwise you'd change the CTM scale and translation parameters. Regards, Nick Nallick During initialization: * Create a gray 1x1 bitmap context using a static byte as the bitmap. * Set the context's fill and stroke to opaque black. sHitTestContext = ::CGBitmapContextCreate(&amp;amp;sHitTestBitmap, 1, 1, 8, 1, To hit test: * Clear the test bitmap. * Translate the CTM of the test context to the desired point. * Fill and/or stroke the path (after setting the correct stroke parameters) in the test context. * Flush the test context. * Test the bitmap for a non-zero value. // construct path in sHitTestContext if (inDrawingMode &amp;gt;= kCGPathStroke) // set the desired line dash, join, cap, and width in sHitTestContext</body>
  </mail>
  <mail>
    <header>Re: Drawing blurry stuff</header>
    <body>Na, the edges were just blurry. ;-) -- Enjoy, George Warner, Schizophrenic Optimization Scientists Apple Developer Technical Support (DTS)</body>
  </mail>
  <mail>
    <header>Paths and hit testing, again.</header>
    <body>I have recently inherited a project that is converting quickdraw to quartz. The old code used the region functions (PtInRgn) to do hit testing. I am trying to figure out how to do that in Quartz. I have seen some posts that mentioned creating a 1 pixel context, and then testing the results of a drawing operation. Could someone post a bit of pseudo-code that can help better describe this technique? Alternatively, are there any other suggestions as to how to do hit testing on strokes and fill ares bounded by Quartz drawn bezier curves? Thanks</body>
  </mail>
  <mail>
    <header>Re: Drawing blurry stuff</header>
    <body>Yeah, I suppose it was.  :-) Into a CG context, either PDF or bitmap.  No OpenGL, just CG/AppKit. I'm rendering NSBezierPaths with shadows, and outputting them on-screen and into PDF files.  The shadows are fine on-screen and inside Preview, but they don't turn up in Adobe Acrobat Reader or Adobe Illustrator.  I can easily create my own shadows without the blur effect to ensure that they do turn up in Acrobat/Illustrator, but it would be nice to have the blur. From what Haroon just said it sounds like my only option for a blurry shadow is to use the CG shadow stuff. As I see it I have two options.  Either I roll my own shadow and live without the blur, or I use CG with bitmaps to get a blurry shadow with the compromises that involves. Steve</body>
  </mail>
  <mail>
    <header>Re: Drawing blurry stuff</header>
    <body>Steve, that's a pretty vague question. Where do you want to draw it? Into a CGContext? Are you using OpenGL and CG, or just CG? Is your image already blurred (with an alpha channel), or are you trying to blur an opaque object? Where does the image come from (an image file, an OpenGL render, QuickDraw, etc.)? You may be able to blur it before making an ImageRef for it. Dave</body>
  </mail>
  <mail>
    <header>Re: Drawing blurry stuff</header>
    <body>No other gaussian blur support in CorGraphics other than the shadow effect. haroon</body>
  </mail>
  <mail>
    <header>Drawing blurry stuff</header>
    <body>Just a quickie... Is there a way to draw stuff with blurred edges, like the shadow effect does? Or is this just a shadow thing? Steve</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics Resising window</header>
    <body>The &amp;quot;recommended&amp;quot; method of doing this is to use the Accessibility API's. -- Enjoy, George Warner, Schizophrenic Optimization Scientists Apple Developer Technical Support (DTS)</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics Resising window</header>
    <body>Selon Shawn Erickson &amp;lt;email@hidden&amp;gt;: between moving or resizing window &amp;quot;on the fly&amp;quot; with CoreGraphics. I 've seached and I found that I can use AppleEvent and mach_inject in order to do that and for this moment the moving run correctly. -- Vincent CUISSARD EPITA</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics Resising window</header>
    <body>What app are you working on? I want to make sure I avoid it. Bryan</body>
  </mail>
  <mail>
    <header>Getting PDF and/or EPS data</header>
    <body>Hi gang, So I have a view into which I'm drawing a load of stuff, and I'd like to be able to save it and also allow the graphics to be copied.  Having consulted the docs I found that NSView has some calls which would help with this, namely dataWithPDFInsideRect, dataWithEPSInsideRect, writePDFInsideRect:toPasteboard:, and writeEPSInsideRect:toPasteboard: Right now I can save out PDF and EPS files using the above calls, however neither fully matches the output I see in my window.  I use NSImage with an NSCustomImageRep in my view to render some bezier paths, making use of the compositing routines for masking purposes. I'm drawing and filling some things with transparent colours, and I'm also using NSShadow to get a shadow drawn. When I try to view my PDF inside Preview the compositing operations I have used are not obeyed.  The EPS version does seem to obey the compositing, so my transparent shapes get drawn correctly, but the shadow is missing.  In both cases composited elements seem to get rendered as bitmaps. Obviously what I really want is output that's the same as my window display but in EPS or PDF format, with no bitmapping of my composited elements. The only relevant hint I found for potentially sorting out the compositing problem was using the NSImage setDataRetained call, but this does not seem to have had an effect. Additionally using the writePDF... or writeEPS... calls giving the pasteboard as [NSPasteboard generalPasteboard] also doesn't seem to result in anything new being put on the clipboard at all.  My copy code is definitely being called, and the rect seems right... Does anybody have any hints as to how I might be able to sort this lot out or ideas as to what I might be doing wrong? Cheers, Steve</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics Resising window</header>
    <body>So in other words communicate with the application to get it to resize and move the windows it &amp;quot;owns&amp;quot;. AppleEvent/AppleScipt may be an avenue to achieve this goal. It is not clear to me exactly what type of product this will be so I am at a loss for better suggestions. -Shawn</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics Resising window</header>
    <body>CoreGraphics enforces that constraint, not Cocoa.  This restriction is deliberate.  If your process changes the size of a window being drawn by another process, the drawing coordinates may change in some environments.  For buffered windows, the buffer dimensions will change. This may produce incorrect behavior in the application (crash, corrupt drawing, etc.) Don't do that. Mike Paquette</body>
  </mail>
  <mail>
    <header>A quick compositing question...</header>
    <body>Hi gang, I'm curious about something...  This morning I ran up my test app that did compositing and chose &amp;quot;Print&amp;quot; from the file menu, and then &amp;quot;Preview&amp;quot;.  Sure enough a rendering of my window popped up inside Preview, however the graphic within my window wasn't drawn correctly. My mask lines weren't being cut out of the preview but actually drawn. Having checked the docs I found the following comment: When printing, the compositing methods do not composite, but attempt to render the same image on the page that compositing would render on the screen, choosing the best available representation for the printer. The op argument is ignored. Now in my real app I'd like to be able to print graphics that match up with what I actually see on screen but I'm worried that I'll get output like I'm seeing in my test app. Any advice as to how I might avoid this? Steve</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics Resising window</header>
    <body>Of course, but the problem is that i'm writing a program which handles move and resize of any window in real time (like in some Unix window manager as Fluxbox). So I think I can't use Cocoa functions because I 'm not the owner of the window. -- Vincent CUISSARD Fingerprint: 77FB 6956 4860 8439 CE9E  45BF 0571 8475 F679 00DE [demime 0.98b removed an attachment of type application/pgp-signature which had a name of PGP.sig]</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics Resising window</header>
    <body>DO NOT USE the CGSMoveWindow or any CGS* functions. These are private functions to CoreGraphics. We reserve the right to change these in any Mac OS X release or update and therefore will break your application. Maybe it's time to mangle the names of the private functions so they are harder to find :-) You should use the Carbon or Cocoa equivalent versions of the function. If you need any windowing functionality that is missing at the Carbon / Cocoa level, file an enhancement request at , but do not use any private functions. You do so at your own risk. Haroon Sheikh Manager, Graphics Software, Apple Computer, Inc.</body>
  </mail>
  <mail>
    <header>Re: Using QDPictDrawToCGContext</header>
    <body>The CGDrawPicture sample application demonstrates the QDPict routines. It uses the data fork of a file as a data source. If instead of using QDPictCreateWithURL as that sample does, you instead create a CGDataProvider and use QDPictCreateWithProvider. The simplest thing is to load the PICT resource in memory, lock it, and make a data provider with CGDataProviderCreateWithData. The release proc you pass to CGDataProviderCreateWithData is called when the data provider is no longer required so you should have your release proc unlock the resource and releases it from memory. Note that the QDPict routines are prepared to deal with picture data that begins at the first byte of the data your data provider supplies as well as picture data that begins after 512 bytes of data, i.e. data fork pictures. The sample code for CGDrawPicture is available at: Hope this helps, David</body>
  </mail>
  <mail>
    <header>Re: Using QDPictDrawToCGContext</header>
    <body>Try this : haroon</body>
  </mail>
  <mail>
    <header>CoreGraphics Resising window</header>
    <body>Hi, i try to do some stuff with the window and CoreGraphics and i have a little problem. I can move a window with the &amp;quot;CGSMoveWindow&amp;quot; function but i don't found the function which allow me to resize the window. Does anyone know how i can do that with CoreGraphics ? -- Vincent CUISSARD Fingerprint: 77FB 6956 4860 8439 CE9E  45BF 0571 8475 F679 00DE</body>
  </mail>
  <mail>
    <header>Using QDPictDrawToCGContext</header>
    <body>Is there any sample code that will take a QD Pict from resources and draw it into a CGContext? -Mike</body>
  </mail>
  <mail>
    <header>Re: Stroke or fill in clear?</header>
    <body>bugreport.apple.com -- top-posting: It's just a bad idea.</body>
  </mail>
  <mail>
    <header>Re: Stroke or fill in clear?</header>
    <body>Hi Dietmar, Sounds cool to me. Personally I think there should be as much equality between the CG and Cocoa APIs as is practical.  I've found a few disconnects so far which seem to mean that if I wish to get all of the features I desire I must use a combination of the two. I'd be happy to, since I agree that this would be useful.  Just point me to a suitable URL - I can't see where this is right now. However if this requires anything more than an &amp;quot;online&amp;quot; ADC membership I can't help out.  Once I finish writing the app I'm working on I might just be able to afford a better membership... compositing is good enough. BTW thanks for also pointing me towards the lockFocus call. Regards, Steve</body>
  </mail>
  <mail>
    <header>Re: Stroke or fill in clear?</header>
    <body>indeed this was my first solution (minus the clip part).  However if you have other graphics beneath the path you are trying to stroke and fill using transparent colours then the opaque white stroke is a problem, since it is very visible.  Also if you put a shadow on this then the opaque white will create a harder shadow. The real solution is, as had been suggested before, to use the compositing operations provided by NSImage.  I've now written some test code and can confirm that this technique works just fine.  The downside is that it's considerably more complex than the above. Regards, Steve</body>
  </mail>
  <mail>
    <header>Re: Test For Quartz Extreme</header>
    <body>Quartz_Services_Ref/qsref_main/function_group_4.html#//apple_ref/c/ Seth Willits ------------------------------------------------------------------------ --- President and Head Developer of Freak Software - REALbasic Guru at ResExcellence - Webmaster for REALbasic Game Central - -- Voltaire ------------------------------------------------------------------------ ---</body>
  </mail>
  <mail>
    <header>Re: Need advice on implementing CAD program</header>
    <body>On Jan 25, 2006, at 11:10 AM, Nick Nallick wrote: Most certainly, if I were developing a CAD system where the model denotes real-world dimensions, the model would store information in double-precision types. But, it's very likely that the imaging pipeline would remain in floating point, though. You only need the double precision for writing labels, and doing math (like geometry calculations). However, in this case, it's more schematic in nature, so as long as the drawn image &amp;quot;looks&amp;quot; correct (to the casual human observer), then it's sufficient. -- Rick</body>
  </mail>
  <mail>
    <header>Re: Need advice on implementing CAD program</header>
    <body>On Jan 25, 2006, at 11:46 AM, Rick Mann wrote: You may be running into the limits of single precision floating point.  I think it's pretty common for CAD systems to use double precision.  In that case you'd probably want to leave the CG context transformation alone and do all of your own transformations, then convert to single precision values to give to Quartz. Nick</body>
  </mail>
  <mail>
    <header>Re: Need advice on implementing CAD program</header>
    <body>On Jan 25, 2006, at 6:41 AM, Nick Nallick wrote: A long time ago, the first time I attempted this, this is exactly what I did. It worked well, until you got into extremes of zoom, at which point the line width was so wide that it seemed to lose its mind. But, it may work for what I need, and I think that will be the simplest approach. -- Rick</body>
  </mail>
  <mail>
    <header>Re: Need advice on implementing CAD program</header>
    <body>On Jan 24, 2006, at 10:26 PM, Rick Mann wrote: There are two approaches you can take to zooming your drawing.  You can either scale the CG context and adjust the size of things you're drawing, or you can leave the context alone and scale everything you draw. If it were me, I'd probably scale the context, pass the zoom scale into my drawing function, and use it to adjust the stroke width I'm using to draw.  For example, if you want your lines to be 1 pixel thick, set the stroke width to 1.0/zoomFactor. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Need advice on implementing CAD program</header>
    <body>I'm working on a 2-D CAD program. I'm using Quartz to do the drawing (of course), but I have this principal requirement: I want the user to be able to zoom in and out on portions of the drawing without the line thickness changing. Does this make sense? To this end, I implemented my own &amp;quot;canvas&amp;quot; on top of Quartz. My canvas has an AffineTransform it uses to maintain its own transformations, and a set of drawing primitives that more-or-less match those of CGContext. The point(s) passed to each is transformed, and then passed on to Quartz in the corresponding call. Here's the problem I'm running into: I want text to scale. To do this, when the scale is adjusted in my Canvas object, I also call through to Quartz and adjust the text scale. But when I measure text (to allow me to right-justify a label on the drawing), the measurement is affected by the current text transform. The effect is that labels shift based on the current display zoom setting. So, what suggestions do the Quartz experts have for me? Ideally, I could just tell quartz that line widths are specified in device (screen) coordinates, but to my knowledge there's no way to do that. -- Rick</body>
  </mail>
  <mail>
    <header>Re: PDFView stuck with password view up?</header>
    <body>rdar://4419062 I'm still interesting in other's take on this. I have a workaround, but I don't want to (yet) poison the collective knowledge with it and stifle a better idea... Thanks, Jim</body>
  </mail>
  <mail>
    <header>PDFView stuck with password view up?</header>
    <body>I have a PDF View which I re-use to display content for the life of the window (changing the content with -setDocument: as necessary.) If I display an encrypted and fully protected PDF in the view, I get the red password view. If I then setDocument: with an unprotected PDF, the password view is stuck. Thanks, Jim</body>
  </mail>
  <mail>
    <header>Writing out uncompressed PDF</header>
    <body>The PDF metafiles created by Quartz are usually flate encoded.  This is a good thing in general, but occasionally I'd like to be able to write out unencoded PDF files.  Is there a way to instruct the PDF context that it should not use flate encoding to obfuscate it's information?</body>
  </mail>
  <mail>
    <header>Re: Resource use of CG Fonts</header>
    <body>Thanks a lot Derek, this is what I wanted to know. Kai -- RagTime GmbH                          Tel: [49] (2103) 9657-0 Neustra√üe 69                          Fax: [49] (2103) 9657-96 D-40721 Hilden Deutschland</body>
  </mail>
  <mail>
    <header>Re: Resource use of CG Fonts</header>
    <body>I think your approach is exactly the right thing to do.  You don't really need to worry about resource use ‚Äî CGFonts are themselves lightweight, and we manage the resources they use internally to keep them low without sacrificing performance.  I'd create a CGFont once and hang on to it until the program exits.</body>
  </mail>
  <mail>
    <header>Resource use of CG Fonts</header>
    <body>I am creating CGFonts using CGFontCreateWithPlatformFont for all our text drawing. Initially, I recreate the CGFont each time to draw a piece of text. This turned out to be bad at least for the PDF conversion done by Quartz (as accessible via the print dialog): the resulting PDFs contain a separate embedded font subset for each time CGFontCreateWithPlatformFont is called. This can result in ridiculously large and slow PDFs. Now I am caching the CGFonts as I create them, using a mapping against ATSFontRefs. Works beautifully, but do I need to be concerned about resource use? Like restricting the number of fonts in the cache to some upper number? Or is it best (certainly it is easiest) just to keep any CGFont around once it is created until the program exists? Note: I have a notification for font list changes installed with ATSFontNotificationSubscribe and handle this correctly, dropping fonts from the cache which are no longer available. Thanks, Kai RagTime GmbH                          Tel: [49] (2103) 9657-0 Neustra√üe 69                          Fax: [49] (2103) 9657-96 D-40721 Hilden Deutschland</body>
  </mail>
  <mail>
    <header>Re: Question about CGColorSpaceCreateDeviceRGB()</header>
    <body>On Jan 10, 2006, at 8:04 AM, Stephen Chu wrote: Sure.  The essence of the problem is that device color spaces are, by definition, not device independent color spaces. :-) When you ask the system for a Device RGB color space, you're saying &amp;quot;I want to use (1, 0, 0)&amp;quot; as pure red regardless of what device I'm drawing on.  The trouble is that the color that (1, 0, 0) (i.e. turn on the red phosphor at it's highest intensity) represents on the monitor is very different from what (1, 0, 0) means on your printer (i.e. mix some combination of magenta and yellow to make &amp;quot;pure&amp;quot; red). This is equally true between any two monitors (say yours and mine). As a result, if you get your graphic just the way you want it to be on your computer using Device RGB, it won't look &amp;quot;right&amp;quot; on my computer.  The effect would be that device RGB adds a &amp;quot;random&amp;quot; error component to each color in your graphic. This lack of predictability is rarely a Good Thing‚Ñ¢. If you don't know what particular device your drawing is destined for, there are advantages to using a device-independent color space. Apple provides &amp;quot;generic&amp;quot; color spaces for this purpose. Generic RGB is a well defined space with predictable characteristics.  It also maps rather well to most Macintosh monitors and many printers. When you create a drawing in an offscreen bitmap that might be copied to the screen or to a printer, Generic RGB (or one of the other generic spaces depending on your color needs) may be a good choice. If you are drawing into an offscreen bitmap to cache drawings that you are going to copy to the screen, you should really give the offscreen bitmap the same color space as the monitor so the computer doesn't have to do color matching on every blit (even better use CGLayer which handles many of these details and more for you). IIRC the situation was complicated under systems prior to Tiger because sometimes when you asked for device RGB, you actually got a generic color space instead.  I don't remember the details, unfortunately, but it had something to do with offscreen bitmap contexts and/or images.  That's why I asked the OP if he was drawing into an offscreen context. :-) The API is there, of course, because it's not always a bad thing.  If you are trying to create color calibration hardware then you want to be able to ask the monitor to display the color it thinks is &amp;quot;pure red&amp;quot; so that your calibration instrument can read it.   It is also my understanding that &amp;quot;fixing&amp;quot; the problem with offscreen contexts in Tiger has created additional problems for folks that really do want device RGB. Unfortunately the generic RGB spaces were not as easy to get ahold of in versions of Mac OS X prior to 10.4 and, as a result, CGColorSpaceCreateDevcieRGB is rather prolific in spite of it's issues.  More information can be found in the QA1396 Scott _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Question about CGColorSpaceCreateDeviceRGB()</header>
    <body>On Jan 9, 2006, at 8:52 PM, Scott Thompson wrote: Can you explain more on why it's evil? And why is the API there if it's such a bad thing? --</body>
  </mail>
  <mail>
    <header>Re: Question about CGColorSpaceCreateDeviceRGB()</header>
    <body>On Jan 9, 2006, at 6:28 PM, Chas Spillar wrote: This code is Evil‚Ñ¢ unless it is trying to do some kind of color calibration of a graphics device. (or other very specialized tasks) A bit more about what you are drawing and why might be helpful. Are you drawing your graphics onto an offscreen bitmap or something? If so, what happens to the contents of that offscreen? If you don't want color matching then you should use the same colorspace throughout your pipeline and that color space should match the color space of the final destination.</body>
  </mail>
  <mail>
    <header>Question about CGColorSpaceCreateDeviceRGB()</header>
    <body>Hello, I have some code which I did not write which uses the call: CGColorSpaceCreateDeviceRGB(). Since this software is a server which isn't meant to be rendered on a Mac, I believe the intent was that no color matching be done.  I think this is what this call used to do. This works great on 10.3, but in 10.4 the behavior of this call was completely changed. What should be the equivalent call to get the same behavior on 10.4 as this used to give on 10.3?  I have tried CGColorSpaceCreateWithName(kCGColorSpaceGenericRGB) but I appear to be getting color matching and it appears to be slower. Thank you for your help, Chas.</body>
  </mail>
  <mail>
    <header>ANN: Quartz 2D for Mac OS X Programmers</header>
    <body>will be published soon by Addison-Wesley Professional.  The book introduces many of the differences in the imaging model between Quartz and pixel-based libraries like QuickDraw and GDI.  It introduces the basic drawing features of the library and discusses some simple drawing techniques.  As an added bonus, the book includes a basic introduction to Core Image. I imagine the book is too &amp;quot;basic&amp;quot; for many of the denizens of this list, but if you have a friend that has been struggling to understand the Quartz 2D imaging model and make it work for them, this text may serve as a gentle introduction. Not much information is available about the book yet as it is still in-process, but you can watch Amazon for more information as it becomes available:</body>
  </mail>
  <mail>
    <header>Re: CISampler and CIImage</header>
    <body>The answer to both questions is NO. You cannot access the data of a CISampler outside a kernel and you cannot access the sampler matrix.</body>
  </mail>
  <mail>
    <header>Re: Re: Clipping</header>
    <body>I changed my code, temporarily, to use CGRectUnion to merge all my clip rects into one rectangle, and that makes it all draw correctly, but it's not exactly an optimal solution. -- Eric Shepherd email@hidden</body>
  </mail>
  <mail>
    <header>CISampler and CIImage</header>
    <body>Hi all, I have a question about CIImage and CISampler usage. There was, already, a question about accessing pixel data of a CIImage object on this list. And, as stated, a CIImage object is just a recipe to build an image and we  cannot access the data directly. Even if I think that at least this recipe have to contain some sample of pixel color. Maybe those sample pixel are stored in a way that we cannot use it directly, aren't they ? On the other hand, with have CISampler object. The CI programing guide says that it's an object that retrieve the data of the image which it refers. Ahh! So I think, that we can use it to acces the data of our CIImage object. If I understand well, we can give to a CISampler object a location (x,y) on the working space and it gives back to us the RGB color (or grey level) of this location. But, I coudn't find anymore information about this unusual usage of CISampler. All that we have in the programing guid is just pass a CISampler object as input argument of a kernel routine. And I think that in this case the CISampler is the same as a sampler2D in GLSL. So, if we cannot access pixel data directly from CIImage object, can we use CISampler instead and not in a kernel routine ? Something like Other question is about the information stored in CISampler. I'd like to retrieve the value of the affine transformation. I tryed something like this without success NSArray *affine_matrix = [my_sampler Does it make any sense or just a stupidity and there's no way to do something like this ? Jaonary</body>
  </mail>
  <mail>
    <header>Re: Clipping</header>
    <body>I'm already taking that into account.  The clip rects are almost but not quite in the right place in my 720x540 window, being offset vertically. What's odd is it looks almost like there are periodic bands in the window, where as an object moves across one of these bands, the clip rects are split in half and wrapped around so that the top of the rect is below the bottom. But the clip rects themselves are perfectly correct, with the top left corner being in the right place, and the width and height correct. I assume that CGContextClipToRects doesn't require that the rects intersect one another, right? -- Eric Shepherd email@hidden</body>
  </mail>
  <mail>
    <header>Re: Clipping</header>
    <body>Re: On Jan 3, 2006, at 2:23 PM, Eric Shepherd wrote: I can't say whether you are running into a coordinate system issue, but here is a suggestion that might help. One way to debug coordinate system issues is to draw something known rather than what you are trying to draw. For example, what happens if you replace your call to CGContextClipToRects instead with a call to CGContextFillRects with the fill color set to something obvious for you. These two functions take the same arguments so this should be easy to do with the same set of rectangles. This should give you a good idea whether you are having some coordinate system confusion or whether there is something deeper going on. There are some additional ideas on how to debug coordinate system problems in Chapter 17 Performance and Debugging of the book &amp;quot;Programming With Quartz&amp;quot;. This is a book that Apple has published through Morgan Kaufmann Publishers. The publisher has made that chapter available as a sample chapter for download. Information about the book and a link to the sample chapter are available at: Hope this helps, David</body>
  </mail>
  <mail>
    <header>Re: Clipping</header>
    <body>Your coordinate system might be flipped vertically.  Would that be a reasonable explanation for what you're seeing? Quartz puts the origin at the bottom-left, while Quickdraw put it at the top-left. On Jan 3, 2006, at 3:23 PM, Eric Shepherd wrote: Attachment:</body>
  </mail>
  <mail>
    <header>Re: Clipping</header>
    <body>And getting the height out of the titleRect, but while that made things better, it wasn't quite right, either. -- Eric Shepherd email@hidden</body>
  </mail>
  <mail>
    <header>Re: Clipping</header>
    <body>Could it be your coordinate system (window vs. view) - the offset sounds suspiciously like the title bar. Just a wild guess... Frank</body>
  </mail>
  <mail>
    <header>Clipping</header>
    <body>I have code that needs to clip to a set of dirty rects, and am trying to use CGContextClipToRects to do it, but the result is... weird. The resulting clipping rectangles seem to be in the wrong place by somewhere around 25-30 pixels vertically. I've checked and rechecked my math, and it all looks right to me. Does anyone know of any quirks to this that I'm missing? -- Eric Shepherd email@hidden</body>
  </mail>
  <mail>
    <header>Re: OT: White Balance</header>
    <body>Jeff Schindler: No, but... Some alternatives to try: Hue rotation I/Q shift or stretch/compress U/V shift or stretch/compress -- Jens Ayton</body>
  </mail>
  <mail>
    <header>OT: White Balance</header>
    <body>Probably not quite the right list (there is no CoreImage list), so apologies for the OT subject. Can anyone here point to a good source for a white (gray) balance algorithm (web site, book, programmer, etc)? I've scoured the net and have found many generic descriptions of white balance, but nothing about how it's actually performed digitally. We're implementing a manual white balance, in which the user picks a color that *should* be gray in the picture and any color cast is removed. The general concept is that the picked color is converted to neutral, and all the other pixels in the image are shifted respectively, all in RGB space. But this doesn't seem to match what many of the professional digital photography tools do out there. Any help is much appreciated!</body>
  </mail>
  <mail>
    <header>cgshading bug? - alpha and white gradients</header>
    <body>static void myCalculateShadingValues (void *info, const float *in, float *out) for (k = 0; k &amp;lt; components -1; k++) *out++ = 1;  // white *out++ = v; // gradient on opacity in the one from the example in the quartz 2d programming guide shows an issue with transparent whites. the semicircle on my ibook is fully opaque white rather than translucent. (note, the example in question draws a white background, i have replaced this with a black background to demonstrate the issue) changing the &amp;quot;// white&amp;quot; line to 0.9 fixes this rather strange issue. any ideas for workarounds or what on earth i'm doing wrong? or is this really a bug? thanks in advance, Alex</body>
  </mail>
  <mail>
    <header>CIVideoDemoGL, OpenGL, DrawImage, drawAtPoint_2</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CIVideoDemoGL, OpenGL, DrawImage, drawAtPoint</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGPDFPageGetDrawingTransform refuses to upscale</header>
    <body>On Mar 31, 2005, at 1:40 PM, David Niemeijer wrote: That's going to depend somewhat on what you mean by &amp;quot;properly&amp;quot;.  To be honest I couldn't give you a correct answer without coding it and perhaps fiddling with the transforms until it came out right.  Even with quite a bit of experience at this I find that I still sometimes need some trial and error when I start concatenating several transforms together.  It can be hard to get your head all the way around this stuff. One trick I use to simplify the problem is to put the CTM origin where I want it and then pass the drawing function a rectangle with a zero freedom from the equation making the results easier to predict.</body>
  </mail>
  <mail>
    <header>Re: CGPDFPageGetDrawingTransform refuses to upscale</header>
    <body>That makes sense, but the rectangle into which I want to draw, itself has a non-zero origin and then we get the transform that CGPDFPageGetDrawingTransform does. I wish there was some straight forward example on how to achieve proper translation and scaling when going from one rectangle with its coordinates to another rectangle with its own coordinates. For example, the art box of the PDF looks like this: x 258.215 y 371.517 width 82.9009 height 104.702 x 4.9 y 12.375 width 88.2 height 129.25 What transforms would one use to properly scale and translate to get the PDF art to draw into that rectangle? I am already using this code to make sure the image does not appear upside down: david.</body>
  </mail>
  <mail>
    <header>Re: CGPDFPageGetDrawingTransform refuses to upscale</header>
    <body>On Mar 31, 2005, at 12:17 PM, David Niemeijer wrote: A scale is always relative to the current origin.  The easiest way to get the scale you expect is to first translate the origin so that it's at the point on your PDF that you expect to remain unchanged by the scale (i.e., generally either the upper-left or lower-left corner).  If you scale and then translate it can be more confusing because you're actually translating in the scaled coordinate system. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Re: CGPDFPageGetDrawingTransform refuses to upscale</header>
    <body>And, probably those hoops are simple for you as you must be using transformations all the time, but I have been puzzling on this now for some time and cannot seem to figure out how to do this correctly. I can scale easily, but then my image also needs to be translated to keep it within my rectangle (if I just scale, it walks out of my rect as I make the rect bigger and bigger). What would be the correct way to do the upscaling taking into account the image's art box and the rect I want to display it in? david.</body>
  </mail>
  <mail>
    <header>RE: CGPDFPageGetDrawingTransform refuses to upscale</header>
    <body>It sure does, but: For applications running in Mac OS X version 10.3 and later, it is recommended that you replace this function with CGContextDrawPDFPage. CGContextDrawPDFPage, in combination with CGPDFPageGetDrawingTransform also gives more control over what is drawn, the whole page, or like I want just the actual art work. david.</body>
  </mail>
  <mail>
    <header>Re: CGPDFPageGetDrawingTransform refuses to upscale</header>
    <body>This was, as sometimes happens, an historical decision that &amp;quot;seemed right at the time.&amp;quot;  This function was originally used for easy layout in N-up printing.  In that case, it seemed better to avoid scaling up a PDF file when it was smaller than the page rect ‚Äî instead, we kept the size unchanged and centered the PDF in the page rect.  Unfortunately, this particular behavior now makes it necessary to jump through some hoops if you want uniform scaling both up and down. We'll look into providing a solution for this, but in the meantime I'm afraid we're stuck with what's there.</body>
  </mail>
  <mail>
    <header>Re: Printing problem with patterns under images with alpha</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>RE: CGPDFPageGetDrawingTransform refuses to upscale</header>
    <body>The function CGContextDrawPDFDocument scales up nicely. -----Original Message----- From: quartz-dev-bounces+ejkerr=email@hidden [] On Behalf Of Derek Clegg Sent: Thursday, March 31, 2005 12:28 PM To: David Niemeijer Cc: email@hidden Subject: Re: CGPDFPageGetDrawingTransform refuses to upscale For good or for ill, what you are seeing is the correct behavior. For upscaling, the &amp;quot;easiest&amp;quot; thing to do is to scale the context yourself before using the drawing transform. Derek</body>
  </mail>
  <mail>
    <header>Re: what does shared window mean in QuartzDebug?</header>
    <body>Hi all: When using QuartzDebug, there is a field "shared". And it is explained in document as "Whether the window is shared or not. Shared windows can be manipulated by multiple applications. Non-shared windows are only modifiable by the application specified in the Application column." Does it mean that windows can be manipulated by other applications? How to do it? I am a confused for windowref is a pointer. Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGPDFPageGetDrawingTransform refuses to upscale</header>
    <body>That surprises me. So basically, I should first look at my rectangle versus the artBox and if my rect is bigger I should manually scale the context to make up for the difference and then proceed as before? And, if my rect is smaller I should do what I have always done. Why was this chosen as the correct behavior, it seems to make life more complex for developers? david.</body>
  </mail>
  <mail>
    <header>Re: CGPDFPageGetDrawingTransform refuses to upscale</header>
    <body>For good or for ill, what you are seeing is the correct behavior. For upscaling, the &amp;quot;easiest&amp;quot; thing to do is to scale the context yourself before using the drawing transform.</body>
  </mail>
  <mail>
    <header>Printing problem with patterns under images with alpha</header>
    <body>I have noticed a problem in Quartz when printing on postscript printers that doesn't appear in the preview, or on non-postscript printers. I draw a rectangle with a pattern fill, and then draw an image with an alpha channel on top.  In the areas where the image overlaps the rectangle, the pattern in the rectangle is blanked out.  Things drawn with a solid color are clearly visible under the image but anything drawn with a pattern is blanked out. My pattern is simply a monochrome image made with CGImageMaskCreate.  I have also seen the same white-out problem if I change my pattern to a rectangle filled with a solid color, and set the alpha inside the pattern to something less than 1.0.  Interestingly, objects drawn with a solid fill when the alpha is set outside the pattern proc print without a problem. I have printed to PDF, and it looks fine onscreen.  When I print the PDF on a PostScript printer, from preview, it prints with the white-out problem.  I also tried printing the PDF from Acrobat, and it came out quite slowly.  The white-out problem was gone, but there was a slight discoloration in the transparent areas of the image. Are image masks, and transparency not legal inside the pattern proc?</body>
  </mail>
  <mail>
    <header>CGPDFPageGetDrawingTransform refuses to upscale</header>
    <body>pageTransform = CGPDFPageGetDrawingTransform (pageRef, kCGPDFArtBox, What I am seeing is that the image scales as long as inRect is smaller than the artBox of the PDF, but as soon as inRect is larger than the artBox of the PDF no scaling occurs. So I can effectively downscale my image to fit a smaller inRect, but not upscale it to fit a larger inRect. This is on 10.3.8. Am I doing something wrong? Should I use another method for drawing? Is there something I need to do to get upscaling or is this a bug? david.</body>
  </mail>
  <mail>
    <header>what does shared window mean in QuartzDebug?</header>
    <body>Hi all: When using QuartzDebug, there is a field "shared". And it is explained in document as "Whether the window is shared or not. Shared windows can be manipulated by multiple applications. Non-shared windows are only modifiable by the application specified in the Application column." Does it mean that windows can be manipulated by other applications? How to do it? I am a confused for windowref is a pointer. Thanks. ‰ΩøÁî®‰∏ñÁïå‰∏äÊúÄÂ§ßÁöÑÁîµÂ≠êÈÇÆ‰ª∂Á≥ªÁªü‚Äï   &lt;a href="http://g.msn.com/8HMBCNCN/2752??PS=47575" target="_top"&gt;MSN Hotmail Get 2 months FREE*.</body>
  </mail>
  <mail>
    <header>Re: PNG images drawing blank</header>
    <body>It's been brought to my attention that one or both of these workarounds may not work. One approach that should work is to create a bitmap context using CGBitmapContextCreate. Then draw the PNG image into this bitmap context. Now one can create a CGImageRef from the bits backing the bitmap context but specify the alpha info parameter to ignore the alpha.</body>
  </mail>
  <mail>
    <header>Re: PNG images drawing blank</header>
    <body>I'm assuming you are using CGImageCreateWithPNGDataProvider. If so, Quartz will respect the alpha data in the image. I haven't tried this, but you could try this workaround : You could get the CGDataProviderRef from the image ref using CGImageGetDataProvider. Then create a new custom data provider that sits on top of the PNG data provider but when asked for data, sets the alpha to 0xff. Or create a new CGImageRef using CGImageCreate but use the PNG data provider from the original PNG CGImageRef, but specify the right alpha info (either kCGImageAlphaNoneSkipLast, or kCGImageAlphaNoneSkipFirst) when creating the new image. On Mar 28, 2005, at 3:34 PM, John Kerr wrote:</body>
  </mail>
  <mail>
    <header>translation of windows fonts</header>
    <body>We're helping port an app from windows to mac, and need to come up with a table of commonly used windows fonts and pair them with their mac equivalents.  Does anyone know if such a thing already exists?</body>
  </mail>
  <mail>
    <header>Create an OpenGL texture from a CIIMage from the GPU directly</header>
    <body>Is it possible to create an openGL suitable for drawing in an opengl context directly from the output of a CIImage (which might be in the GPU). Currently the only solution I have is to draw to a NSBitmapRep, flip the pixels and create/update the texture. Could anything faster be done? ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Re: Efficient drawing OpenGL vs. CoreImage</header>
    <body>Correct me if I am wrong but I am not sure a GeForce 5200 is hardware accelerated at all. From what I have read those a slower than the CPU when it comes to the pixel shader and use of the GPU on this configuration is disabled by default. Or is it some other variant of the 5200? I have faced this dillema when writing my app. One of the filters (zooming, offseting and pixellating) has to be done in real time (on each draw). If run on an accelerated machine (like radeon 9600 or better) then it's very fast since everything happens in the GPU, but if I use my iBook then drawing openGL textures is much much faster (since you only set antialiasing to GL_NEAREST and you have your pixellation). It all depends on the machine on which you're gonna run it in the end. If it's a mac book pro or an imac core duo then Core Image will be very fast. If your mouse is notreactive this is probably all the rendering happens in the CPU. Look at the CPU usage when you run your app. If it all happened in the GPU then your mouse would be fine. In my case I decided to go for both options: users with a non supported GPU will use OpenGL , others will use 100% Core Image. ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Efficient drawing OpenGL vs. CoreImage</header>
    <body>I am trying to draw an animation of two seperate dj turntables that react to mouse events. The drawing should be as performant as possible. I also have to constantly update two textfields and 4 meetering views. I tested two solutions, one with openGL and one with CoreImage. bitmap = [[NSBitmapImageRep alloc]initWithData: [image djImage[i][j][0] = (GLubyte)data[[bitmap bytesPerRow]*i+j*[bitmap djImage[i][j][1] = (GLubyte)data[[bitmap bytesPerRow]*i+j*[bitmap djImage[i][j][2] = (GLubyte)data[[bitmap bytesPerRow]*i+j*[bitmap glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, TEXTURE_SIZE, TEXTURE_SIZE, For the animation task we use a timer with an interval of 0,02 seconds. For drawing we bind each texture and then draw it. //I do the following for each textures ___________________________________________ ___________________________________________ We load the 7 images and then we draw each image. We use here also a timer with the same interval as above. In both solutions the turntable views are juddering. The machine I was testing on is a g4 powerbook with 1.5 GHz and 1.25 GB RAM. I have an GeForce FX Go5200 graphics card with 64 MB VRAM. What is genarally the better solution regarding performance? Would it help to thread the drawing? If threading is reasonable how can I guaranty that the drawing in seperate threads is save? Furthermore I made the observation that OpenGL reacts a lot better to mouse events than CoreImage, is there a logic behind that? Best regards, Federico</body>
  </mail>
  <mail>
    <header>Efficient drawing OpenGL vs. CoreImage</header>
    <body>I am trying to draw an animation of two seperate dj turntables that react to mouse events. The drawing should be as performant as possible. I also have to constantly update two textfields and 4 meetering views. I tested two solutions, one with openGL and one with CoreImage. On initialization time we load 7 textures the following way: djImage[i][j][0] = (GLubyte)data[[bitmap bytesPerRow]*i+j*[bitmap djImage[i][j][1] = (GLubyte)data[[bitmap bytesPerRow]*i+j*[bitmap djImage[i][j][2] = (GLubyte)data[[bitmap bytesPerRow]*i+j*[bitmap glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, TEXTURE_SIZE, TEXTURE_SIZE, For the animation task we use a timer with an interval of 0,02 seconds. For drawing we bind each texture and then draw it. //I do the following for each textures ___________________________________________ ___________________________________________ We load the 7 images and then we draw each image. We use here also a timer with the same interval as above. In both solutions the turntable views are juddering. The machine I was testing on is a g4 powerbook with 1.5 GHz and 1.25 GB RAM. I have an GeForce FX Go5200 graphics card with 64 MB VRAM. What is genarally the better solution regarding performance? Would it help to thread the drawing? If threading is reasonable how can I guaranty that the drawing in seperate threads is save? Furthermore I made the observation that OpenGL reacts a lot better to mouse events than CoreImage, is there a logic behind that? Thanks a lot Federico</body>
  </mail>
  <mail>
    <header>CoreImage Accelerated or not How do I detect it?</header>
    <body>I need to handle things differently in my apps if CoreImage is accelerated by the GPU or not. However I found no simple solution to get whether yes or not it is supported. How does the &amp;quot;System informations&amp;quot; app get it's CoreImage acceleration info? Currently I have setlled with checking if GL_EXT_ARB_fragment_program is available. But that would be true for a FX5200 which is too slow and not used in reality. Of course I still allow the user to chose what method he wants to use for display but I'd like to set a proper default. So so far I just do this: Anything better, safer? ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Re: CIColorMap inputGradientImage?</header>
    <body>I created such an image (below) and it messes up the picture, so I that guess must be wrong. NSBitmapImageRep    *bitmap = [[NSBitmapImageRep allocWithZone: [self zone]] initWithBitmapDataPlanes:NULL pixelsWide:1024 pixelsHigh:1 bitsPerSample:8 samplesPerPixel:4 hasAlpha:YES isPlanar:NO colorSpaceName:NSDeviceRGBColorSpace bytesPerRow:0 for (x = 0; x &amp;lt; 1024; ++x) NSColor *color = [NSColor colorWithDeviceHue:(float)x /</body>
  </mail>
  <mail>
    <header>Re: CIColorMap inputGradientImage?</header>
    <body>What is the &amp;quot;original spectrum&amp;quot;? Is it the hue component from 0 to 1 after converting RGB to HSB?</body>
  </mail>
  <mail>
    <header>Re: problem with coalesced updates and OpenGL</header>
    <body>Eric Schlegel &amp;lt;email@hidden&amp;gt; wrote: Yes, it fixes the problem, though the rest of the interface does more flashing.  Does that suggest some workaround? -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: CIColorMap inputGradientImage?</header>
    <body>Check out the Core Image Fun House sample app. You can give the filter a test run there. The inputGradientImage is a 1 pixel high gradient that maps the colors from the original spectrum to what you specify in that image.</body>
  </mail>
  <mail>
    <header>CIColorMap inputGradientImage?</header>
    <body>The very short documentation about CIColorMap isn't enough to make me understand how to use it. Has anybody any experience with this filter? Martin Wennerberg email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGEventKeyboardSetUnicodeString not working?</header>
    <body>On Apr 16, 2006, James W. Walker wrote: The API works, but the problem is that the Carbon Event Manager does not extract the Unicode data from the CGEvent, and so neither Carbon nor Cocoa apps actually get a chance to receive the Unicode data.</body>
  </mail>
  <mail>
    <header>Re: problem with coalesced updates and OpenGL</header>
    <body>On May 17, 2006, at 8:24 PM, James W. Walker wrote: I don't know offhand what's happening here (we don't have much experience combining OpenGL and composited HIViews in-house; we need to investigate this further and see if we should be adding more explicit support to the HIView Manager for OpenGL), but I'm curious what happens if you use QuartzDebug to turn on &amp;quot;auto-flush drawing&amp;quot;. Does that make a difference?</body>
  </mail>
  <mail>
    <header>problem with coalesced updates and OpenGL</header>
    <body>I'm not sure if this is the right mailing list for this question, but I know I'd get yelled at if I cross-posted... My application often renders incorrectly when coalesced updates are on.  Tech Note 2133 tells some cases in which one coalesced updates might impact performance, but does not explain why you'd see the wrong stuff on the screen. This is a Carbon application with a composited window.  Certain HIViews in the window are set up to draw with OpenGL.  On kEventControlDraw, the view renders, ending with aglSwapBuffers and glFinish.  What sometimes happens (maybe half the time) is that there is a flash and the OpenGL content disappears, revealing whatever was underneath it in the window.  Turning off coalesced updates with Quartz Debug makes this problem go away. Any idea what I could do, other than turning off coalesced updates in my app? -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Intel-only issue with QCRenderer -initWithOpenGLContext:::</header>
    <body>I have an ObjC app that displays live video into an NSOpenGLContext using a simple QC Video Input / Billboard composition. It all works quite well. renderer = [[QCRenderer alloc] initWithOpenGLContext:_glContext pixelFormat:_glPixelFormat But on Intel (with either Intel native code, or Rosetta'd PPC code), I get this: *** Message from &amp;lt;QCVideoInput = 0x13A49FB0 &amp;quot;VideoInput_1&amp;quot;&amp;gt;: Sequence grabber settings restoration failed (error -2004) The code continues, but the resulting video looks &amp;quot;ghostly&amp;quot; and greatly overexposed, almost as if it were inverted, but not quite. (gdb) p renderer $1 = (class QCRenderer *) 0x13a5fd80 (gdb) p _glContext $1 = (class NSOpenGLContext *) 0x13a590e0 (gdb) p _glPixelFormat $2 = (class NSOpenGLPixelFormat *) 0x13a5ddf0 (gdb) p path $3 = (class NSString *) 0x13a4b530 (gdb) po path /Users/Roland/Video/build/Development/Video.app/Contents/Resources/ LiveVideo.qtz Can anyone offer possible causes for this? Again, it works fine on PPC. But even if I run a PPC version on Intel via Rosetta, I still get the same error message and video problem. And, I can run the LiveVideo.qtz composition directly in QC and it works fine. I've tried this with both the internal iSight and an external one, with no difference.</body>
  </mail>
  <mail>
    <header>Re: keynote output to Quartz</header>
    <body>On May 16, 2006, at 7:03 PM, Ron Coleman wrote: It's not entirely clear to me what you are trying to do. Off hand your question sounds like it would be more appropriate on a Keynote applications list than for the developers on this list. If you simply want to get an image of each slide in a Keynote presentation then you can export the presentation as a PDF.  Each page of the PDF would be a separate slide.  You might be able to integrate that into a Quartz Composer composition.</body>
  </mail>
  <mail>
    <header>keynote output to Quartz</header>
    <body>Is there a way to send the output of a Keynote presentation to a Quartz Composition or WackedTV? Other than outputting the KN presentation as a .mov. I would like to run the presentation in realtime and be able to move randomly through the presentation just as if i were sending it to another monitor. Ron</body>
  </mail>
  <mail>
    <header>Re: White and Black Points</header>
    <body>Preview.app has a white point adjust feature where the results are vastly different from Core Image Fun House (and my own code which uses CIWhitePointAdjust). How can I achieve the white point results of Preview.app, but using CoreImage?  If they're not the same thing, why are the named the same?</body>
  </mail>
  <mail>
    <header>Color table of indexed image</header>
    <body>Hi, how do I get the color table out of an indexed CGImageRef ? I read CGColorSpace.h and searched in mostly all quartz headers and documentation, but I haven't found it yet. arne</body>
  </mail>
  <mail>
    <header>Re: Upgrading to Quartz</header>
    <body>Create the array with the pixel image, then create a bitmap context with that image data. Then draw the image into the bitmap context. Bingo, you have your new image, without even having to copy it again =). -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Upgrading to Quartz</header>
    <body>Hi, I have a simple QuickDraw problem I'd like to solve with Quartz, if possible. I have a 64x64 pixel image, currently stored as an RGBA array of data, which I'd like to superimpose a PNG onto. After that, I'd like to copy the new image back into the 64x64 pixel RGBA array. Where should I start? Thanks, -Chilton</body>
  </mail>
  <mail>
    <header>Re: Indexed Bitmaps and CGImageCreateWithMaskingColors</header>
    <body>Yes, thanks. As I need this in a function for a generic UI library, I think it's less work to convert all indexed bitmaps to 8 bits per component on load, so I don't have to deal with color tables. thanks arne</body>
  </mail>
  <mail>
    <header>Re: Indexed Bitmaps and CGImageCreateWithMaskingColors</header>
    <body>Re: On May 15, 2006, at 6:41 AM, Steve Mills wrote: The important thing to remember is that the colors in the color array that you supply to CGImageCreateWithMaskingColors are the undecoded color values in your image. For an image characterized by an index color space, that is a 1 component image so you supply a masking color array of 2 values, the min and max color values you want to be transparent (i.e. mask out). The color value you would use for your array would be the index of the color in your color table that you want to be transparent. You should use the same color for the minimum and maximum value of your masking color. Say the white color that you want to be transparent is entry 5 in your color table (note that indexes start at 0). The code could look like: myColorMaskedImage = CGImageCreateWithMaskingColors (image, Hope this helps, David</body>
  </mail>
  <mail>
    <header>Re: Indexed Bitmaps and CGImageCreateWithMaskingColors</header>
    <body>Did you read this page? _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Indexed Bitmaps and CGImageCreateWithMaskingColors</header>
    <body>Hi, I've an index bitmap and I like to create an image where the white color will be transparent. I'm not sure how I would create the masking color components for the second parameter in CGImageCreateWithMaskingColors. Any Hints ? thanks arne</body>
  </mail>
  <mail>
    <header>Re: Difficulty masking an image with another image</header>
    <body>According to the documentation: &amp;quot;A mask. If the mask is an image, it must be in the DeviceGray color The CGImageCreateWithPNGDataProvider function likely returns an image in a RGB + alpha colorspace. So using CGImageCreateWithCreateMask with that image as a mask image will fail. -Corey -- -Corey O'Connor</body>
  </mail>
  <mail>
    <header>Difficulty masking an image with another image</header>
    <body>I'm just getting my feet wet with Quartz 2D and built a small app to try out some functions.  The following code doesn't behave as I expected.  Specifically the asterisked line below leaves maskedImage as nil.  Either MyImage or aGrayScaleImage will display correctly. Any help appreciated. Thanks, Pete void drawMaskedImage(CGContextRef context, CGRect contextRect) NSString *k9TeethImage = [[NSBundle mainBundle] NSString *maskImage = [[NSBundle mainBundle] imagePath = CFStringCreateWithCString (NULL, imageFileName, imageUrl = CFURLCreateWithFileSystemPath (NULL, imagePath, maskPath = CFStringCreateWithCString (NULL, maskFileName, maskUrl = CFURLCreateWithFileSystemPath (NULL, maskPath, image = CGImageCreateWithPNGDataProvider (imageProvider, NULL, true, mask = CGImageCreateWithPNGDataProvider (maskProvider, NULL, true, CGLayerRef imageLayerRef=CGLayerCreateWithContext (context, theSize, //	Create the masked image maskedImage=CGImageCreateWithMask(image, mask);		// **************************************** //	Draw it to the layer //	Then draw the layer to the window graphics context - (void)drawRect:(NSRect)rect CGContextRef theContext=[[NSGraphicsContext currentContext]</body>
  </mail>
  <mail>
    <header>Re: Speed up drawing using CGContextFlush</header>
    <body>On May 11, 2006, at 3:51 PM, email@hidden wrote: Depends on your definition of an awful lot of work, I suppose. It's about 10-15 lines of code.</body>
  </mail>
  <mail>
    <header>Re: Speed up drawing using CGContextFlush</header>
    <body>That's an awful lot of work, isn't it?  If you can use Cocoa, there's a class called NSAnimation which vastly simplifies all this. Brendan</body>
  </mail>
  <mail>
    <header>Re: Speed up drawing using CGContextFlush</header>
    <body>Or look into using a CVDisplayLink for the rendering. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Speed up drawing using CGContextFlush</header>
    <body>On May 10, 2006, at 9:49 AM, Photog Phred wrote: There's no point in flushing after every line segment, because the display is only going to update at a fixed refresh rate anyways, and you are undoubtedly flushing faster than the refresh rate. Try determining your main display's refresh rate from the kCGDisplayRefreshRate key in the dictionary returned by CGDisplayCurrentMode, and keep track of when you last flushed. Only flush again when the refresh interval has been exceeded.</body>
  </mail>
  <mail>
    <header>Re: Speed up drawing using CGContextFlush</header>
    <body>Did you look at it with Shark? I suspect your slowdown comes from the cost of the flush as they are coalesced on Tiger. Looking at the problem that you are trying to solve, you would have to change your approach by braking the process down into frames being rendered as a given interval (60 fps) not by the path components being generated.</body>
  </mail>
  <mail>
    <header>Speed up drawing using CGContextFlush</header>
    <body>Here's a function that I wrote to demonstrate a problem I'm encountering. I'm NOT actually using this function, but it shows what I'm talking about. It draws very short line segments, alternating in green and blue, in a spiral. Using CGContextFlush in the loop to show each segment as it's being drawn is extremely slow. Putting the CGContextFlush outside of the loop and doing it just once at the end is unbelieveable fast, almost instantaneous. The problem I have is I need to see each segment as it's being drawn to the screen. This will be used in a CAD/CAM product I'm developing and it will display a cutter path as it's being generated.&amp;nbsp;   &lt;FONT class=Apple-style-span face=courier size=2&gt;&lt;SPAN class=Apple-tab-span style="WHITE-SPACE: pre"&gt;&lt;SPAN class=Apple-tab-span style="WHITE-SPACE: pre"&gt;&lt;SPAN class=Apple-tab-span style="WHITE-SPACE: pre"&gt; &lt;SPAN class=Apple-style-span style="FONT-SIZE: 10px"&gt;0.2&lt;SPAN class=Apple-style-span style="FONT-SIZE: 10px"&gt; angle;&lt;SPAN class=Apple-style-span style="FONT-SIZE: 10px"&gt; step =   &lt;DIV style="MIN-HEIGHT: 14px; MARGIN: 0px; FONT: 10px Monaco"&gt;&lt;SPAN class=Apple-tab-span style="WHITE-SPACE: pre"&gt; &lt;FONT class=Apple-style-span size=2&gt;&lt;SPAN class=Apple-style-span style="FONT-SIZE: 10px"&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; x = radius + rect.right / &lt;FONT class=Apple-style-span color=#0000ff size=2&gt;&lt;FONT class=Apple-style-span size=2&gt;&lt;SPAN class=Apple-tab-span style="WHITE-SPACE: pre"&gt; &lt;SPAN class=Apple-tab-span style="WHITE-SPACE: pre"&gt;&lt;SPAN class=Apple-tab-span style="WHITE-SPACE: pre"&gt;&lt;SPAN class=Apple-tab-span style="WHITE-SPACE: pre"&gt;&lt;SPAN class=Apple-style-span style="FONT-SIZE: 10px"&gt;0.0&lt;FONT class=Apple-style-span size=2&gt;&lt;FONT class=Apple-style-span size=2&gt;&lt;FONT face=courier&gt;&lt;SPAN class=Apple-tab-span style="WHITE-SPACE: pre"&gt;&lt;SPAN class=Apple-style-span style="FONT-SIZE: 10px"&gt; ((angle += step) &amp;gt;= &lt;FONT class=Apple-style-span color=#0000ff size=2&gt;  &lt;DIV style="MARGIN: 0px"&gt;Thanks for the help.</body>
  </mail>
  <mail>
    <header>Re: PDF Blending Modes and Custom Algorithms</header>
    <body>On 2006/05/10, at 16:20, Derek Clegg wrote: I see. Thank you. I had read that part, and then later, adobe released a &amp;quot;Blend modes addendum&amp;quot; to clarify certain things about 1.6, one feature was about &amp;quot;customization&amp;quot;  of function of blend modes and their respective blending algorithms, but instead the wording threw me off I guess, especially giving details on the blend function led me to think perhaps they were customizable. In their case custom blending modes was just what ever is not the standard default blend mode. Andre email@hidden</body>
  </mail>
  <mail>
    <header>Re: PDF Blending Modes and Custom Algorithms</header>
    <body>PDF 1.6 doesn't support custom blend modes.  Blend modes are specified by name, and the names must be one of the standard blend modes listed in Tables 7.2 and 7.3 in the PDF 1.6 specification.</body>
  </mail>
  <mail>
    <header>Re: PDF Blending Modes and Custom Algorithms</header>
    <body>On 2006/05/10, at 12:34, Scott Thompson wrote: I see. Is it true though in PDF 1.6? I'm having a hard time clarifying that part... If so, I would like to file an enhancement request for its support in quartz. But first wanted to make sure I was correct about there even being custom blend modes in PDF 1.6 to start with. Andre email@hidden</body>
  </mail>
  <mail>
    <header>Re: PDF Blending Modes and Custom Algorithms</header>
    <body>On May 10, 2006, at 2:24 PM, email@hidden wrote: The blending modes supported by Quartz (in Tiger) are listed in CGContext.h.  I don't see any mention of a custom blend mode.</body>
  </mail>
  <mail>
    <header>PDF Blending Modes and Custom Algorithms</header>
    <body>I've been reading up on the PDF 1.6 descriptions at Adobe.com and came across the mention several times of custom blend modes. Just for clarification, does PDF 1.6 indeed allow for user definable blend modes other than the ones provided, such as color dodge, color burn etc. And if so, does quartz have any functionality in this regard? The ability to define my own transparency blend function would be pretty cool... Andre email@hidden</body>
  </mail>
  <mail>
    <header>Re: Too Lazy Evaluation - Clearing a CIImageAccumulator with	transparency</header>
    <body>On May 6, 2006, at 4:06 PM, Santiago (Jacques) Lema wrote: I've run into something similar; creating a contant-color generator with a CIColor of 0,0,0,0 and then doing a draw of that in a CIContext hooked up to an OpenGL context does nothing.  This is after having configured the GL state to set the blend func to 1/0, of course.</body>
  </mail>
  <mail>
    <header>Too Lazy Evaluation - Clearing a CIImageAccumulator with	transparency</header>
    <body>I have CIConstantColor filter and I want to clear the content (actually some zones only). So I just use: [myAccu setImage: [colorFilter valueForKey:@&amp;quot;outputImage&amp;quot; ] And afterwards I force drawing to the opengl context to make sure it's evaluated. This works perfectly as long as the color in CIConstantColor filter is not 100% transparent (alpha = 0.0).  So I can clear with black blue or whatever, but if I try to clear to a transparent color, it's hopeless. It just won't do it. Is the only way to clear the contents of an accumulator to use the contents of another transparent accumulator ? Sounds a bit weird but I couldn't find a simple solution. that would require having a big additional buffer just for that or doing some loops. Please tell me that, as usual, I am wrong? ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>CGPSConverter and friends</header>
    <body>Dear All, The CGPSConverter API available in 10.3 seems to be somewhat spare in documentation. As I see it, it seems an excellent way of converting PS and EPS into PDF, so here's the questions: 1. Is the same method used by the command-line &amp;quot;pstopdf&amp;quot; and/or the Preview application when it opens PostScript? 2. Does this actually start a new process, thread or task to do its work, use any temp files or is this &amp;quot;in-process&amp;quot;? This question is more out of curiousity and I'm also wondering about performance. 3. Presumably CGPSConverterAbort and CGPSConverterIsConverting would be called from within a converter callback, since the conversion seems to a synchronous call and these functions don't make sense otherwise? 4. Doesn't seem to be anyway of chaining a CGPSConverterConvert to a CGPDFDocumentCreate -- seems rather inefficient in that if I wanted a CGPDFDocument as a result, the system would have to parse twice: once for the PS -&amp;gt; PDF conversion, and once more for the PDF -&amp;gt; Quartz internal structures. The bottom line for my current app (Graphviz) is this: I have an EPS (or PS) and I want to draw it somewhere using Quartz. Am I barking up the right tree, i.e. convert using CGPSConverter to PDF data, then to CGPDFDocumentCreate, and then draw it using CGContextDrawPDFDocument? Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com</body>
  </mail>
  <mail>
    <header>Private APIs</header>
    <body>It is hard to come up with enough adjectives for how unimaginably, hideously ghastly an idea this is.  Making Cocoa classes to help people use undocumented APIs, when the Quartz engineering manager has on *three separate occasions* told you not to use them at all?  It's not just a bad idea, it's damned irresponsible. I've seen first-hand what happens to an operating system when small applications, and then larger applications, start using undocumented APIs and SPIs and variables to hack into the system.  The more people use these routines, the less flexibility Apple has to improve Quartz in ways that might break them, and the more stagnant the OS becomes.  If it's your *goal* to make Mac OS X as unstable as Mac OS 9, or make Mac OS X just as difficult to add new features to as Mac OS 9, this is a great start. Even if this is not your concern - and obviously it's not - I hope that Symantec and Intego find a signature for this code and &amp;quot;disinfect&amp;quot; it from client systems.  I certainly hope Apple isn't wasting a WWDC scholarship on someone who's promoting system instability. I really don't care if you want to learn about Quartz and make academic explorations and turn windows into OpenGL cubes on your own system to demonstrate it at MacHack.  When you start encouraging other people to use it, you've crossed the line, and if you don't step back, people more interested in system stability need to move you back. Keeping Mac OS X functional requires agreement between the OS developers and application developers to follow the rules.  If four shareware programs start using CGSGet/SetWindowTransform for some &amp;quot;feature,&amp;quot; then the programmers at Adobe and Microsoft and Macromedia will start getting requests for it.  Once *they* start using it, Haroon's team suddenly can't change their own internal routines without breaking third party programs. If developers break the rules, it affects future versions of the OS, and that affects *everyone*. Do you really want Apple to spend engineering time on privileges and barriers so that user-level applications _can't_ call CGS functions, just to protect the system?  I'd rather see them impementing new features and fixing bugs than defending the system against programmers who refuse to understand the plainest warnings imaginable. Haroon has asked on several occasions that you file a bug and request access to the Quartz functions you need.  They're _willing_ to listen to you and consider granting access that could be supported and sustainable, but you don't seem to care.  Rest assured that your indifference to the havoc you're attempting to wreak has been noticed. Encouraging people to use these private Quartz functions after all of Haroon's warnings is the single dumbest idea I've heard this year - and I've heard reporters ask Steve Jobs questions about their pet theories on the future of digital music.  Share your academic investigations at MacHack, and stop encouraging people to do what Apple's OS managers have said they should not do -- unless you just hate Mac OS X and Quartz and want them to fail. -- I read this list in digest mode; copy me privately for faster responses</body>
  </mail>
  <mail>
    <header>Re: Capabilities of Quartz: rotation? zooming?</header>
    <body>Wade, I've already asked people not to announce their applications on this mailing list. Please refrain from advertising the availability of new versions of the application on this mailing list. If people are interested in your application, they can visit your website for updates. I am also concerned about the use of private API in this application and I would not like to promote people use those APIs. haroon [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: Capabilities of Quartz: rotation? zooming?</header>
    <body>Again, sorry about the cross-post, but the level of interest seems to be high, so I hope that outweighs the inconvenience to everyone. I've polished off a new version of my little app, and have posted it online.  Since there was *still* some complaints about the file format last time, I've added a few more... ;) (33,086 bytes) (34,505 bytes) (42,372 bytes) (44,444 bytes) (44,533 bytes) (45,400 bytes) (54,241 bytes) (67,503 bytes) (62,582 bytes) Sorry, I'll add rar and a few other esoteric formats soon. ;) Anyway, the new version has had the code cleaned up, with a new NSWindow category to provide the necessary functionality with utmost convenience to anyone wanting to use it.  I've also expanded the demo a little, thanks to Gerriet M. Denkmann, to support rotation/scaling about a variety of points, as well as the provision of a new Reset button for when things go horribly wrong. :) There are still a few minor issues - the window title bar still isn't interacted with properly, and I know that it doesn't always work properly on multiple monitors (I will get around to fixing this next week), etc etc.  I'm currently playing with transforming arbitrary windows of any application, which is all good and dandy, but of course the event transformation doesn't occur for them... I'll probably go the route of an Unsanity APE module, but that still leaves Carbon apps out in the cold. So, I hope people like it, and can make some use of it - a few people have contacted me with questions and details of their own applications of it, which is fantastic.  Just to reiterate, it's all BSD licensed* and I'm of course most happy for people to use it. Wade Tregaskis (aim: wadetregaskis) -- Sed quis custodiet ipsos custodes? * = Except, technically speaking, the code for the reset method, which was copied verbatim from that given to me from Mr. Denkmann.  I haven't confirmed with him his acceptance of any particular licensing scheme.</body>
  </mail>
  <mail>
    <header>re: PICT with alpha from a CGBitmapContext?</header>
    <body>From our (DTS) QT guru: [BEGIN] I would just use the PICT Graphics Exporter but make sure to pick a compression type that can do Millions+ instead of just Millions i.e. ARGB instead of RGB. Looks like if you Save a Compressed Pict using PNG compression you can create a ARGB file. It looks like Phtoshop (CS anyway) when exporting Pict files with alpha uses the NONE compressor and saves it as full 32bit. It doesn't allow you to pick JPEG as the codec. I would have to also assume that they're just using QuickTime underneath so the GraphicsExport method is probably the easiest as long as your GWorld is 32bit with valid alpha. [END] -- Enjoy, George Warner, Schizophrenic Optimization Scientists Apple Developer Technical Support (DTS)</body>
  </mail>
  <mail>
    <header>Another caching question</header>
    <body>Can anyone point me at some documentation about what is really happening (i.e. what is being cached, what are the trade-offs) with NSBezierPath's setCachesBezierPath: Thanks, Bob Clair</body>
  </mail>
  <mail>
    <header>NSImage and caching</header>
    <body>Can someone point me to some more detailed information on what happens when an NSImage is cached and what the trade offs are with not caching ? I notice that when I load an image (TIFF, say) an NSBitmapImageRep is created, but the first time that it is drawn the NSBitmapImageRep is replaced with an NSCachedImageRep. The NSCachedImageRep doesn't seem to change is if the image is drawn rotated and scaled. Why does having the pixels stashed in an off-screen window help, even if they have to be rotated and scaled ? The problem I'm trying to solve is the following: I have relatively small (say less than 300x300) images that get composited many, many times at various scales and rotations. Sometimes they are scaled down so that the number of pixels occupied on the screen are far less than the number in the image. This means Quartz is doing a lot of unnecessary work and things are, naturally, slower than I would like them. My proposed solution is to make a subclass of NSImage that stores multiple representations of the same image at various resolutions (snapped to sizes altivec can use ?), set the cache mode to NSImageCacheNever,  and override - (BOOL)drawRepresentation:(NSImageRep *)imageRep inRect:(NSRect)dstRect, look at the CTM to and calculate what size in pixels is really being asked for, pick the appropriate representation, save the GC, adjust the CTM so we still come out the same size, call super with the new rep and then restore the GC. But I'm not clear about the caching. Should I recache and then if it would be the same rep the next time let it draw from the cache ? Thanks, Bob Clair</body>
  </mail>
  <mail>
    <header>Re: PICT with alpha from a CGBitmapContext?</header>
    <body>Nice! Did you investigate whether the opaque image data needs to be stored premultiplied or not when doing this? Jason</body>
  </mail>
  <mail>
    <header>Re: PICT with alpha from a CGBitmapContext?</header>
    <body>I found the solution after much trial and error. Using the raw data from my CGBitmapContext, I created two PixMapHandles containing the image and mask data. Then I used the QuickTime routine CompressImage to encode the image data using the Animation codec. Then, I called OpenPicture, FDecompressImage and ClosePicture to create a PICT using the mask data as a matte (you'll need to invert the mask to get a correct matte). This creates a PICT with an embedded QuickTime image and alpha channel that Photoshop and other Adobe apps will happily accept. James</body>
  </mail>
  <mail>
    <header>Re: CGDataProviderCreateDirectAccess</header>
    <body>Yes, you can.  For a direct access data provider, you should specify callback and set the other to NULL. I count the two flavors of direct-access separately.  (The two are the &amp;quot;get the byte pointer to all to data&amp;quot; flavor and the &amp;quot;read some bytes at a specified offset&amp;quot; flavor.) Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: Quartz Image drawing Performance problems</header>
    <body>Very interesting. Does it mean that RGB data should be manually converted to the current screen format before creating the CGImage?</body>
  </mail>
  <mail>
    <header>Re: Quartz Image drawing Performance problems</header>
    <body>CGContextClipToRect</body>
  </mail>
  <mail>
    <header>Re: Shadings don't transfer to PDF properly</header>
    <body>On Jul 14, 2005, at 12:26 PM, Derek Clegg wrote: Oh cool.  An aliasing problem. :-) For what I need, your solution should be more than adequate.  Now that I know what's going on, I guess another way to solve the problem would be to  break the shading into multiple sub-shadings.</body>
  </mail>
  <mail>
    <header>Reg : ScaleToFit &amp;amp; CenterImage Option In Print Dialog of Preview	Application</header>
    <body>Hi All, I have seen the new print dialog for Preview Application for jpg , tiff image files.In that there are three options 1) Scale To Fit  2) Center Image 3) Rotate Image. Please send your suggestions on implementing these features In Cocoa and also implementing new print dialog. Thanks in Advance.</body>
  </mail>
  <mail>
    <header>Re: [newbie] drawing a Quartz animated view with transparency</header>
    <body>On Feb 28, 2006, at 3:15 PM, Renaud Boisjoly wrote: One way to do it would be: CGContextRef cgContext = (CGContextRef) [[NSGraphicsContext NSRectFill(portBounds); perhaps?)</body>
  </mail>
  <mail>
    <header>[newbie] drawing a Quartz animated view with transparency</header>
    <body>I hope this isn't too much of a newbie question, please forgive me if it is... I have not yet found this kind of answer in the archives, but I'm still looking, so if you know its there, ignore me, I'll find it. I'm trying to create a transparent window in Cocoa, making it transparent and drawing a Quartz view inside it. The Quartz Composition is an animated image with transparency (basically a PNG file containing transparency info) which moves using Billboard in Quartz Compositor. In compositor, it displays correctly in the Viewer, keeping transparent over the checkered background (not over white). In my Cocoa app, I use a subclass of NSWindow to make it transparent, but when I add my composition to the window, it moves without erasing itself... so it becomes a big mess real quick. I can have it erase to a color as specified in the Quartz view in Interface Builder, or by adding a &amp;quot;clear&amp;quot; patch to clear the background, but it becomes fully white at that point... I'm really new at Quartz and still a novice at Cocoa, but if you suggest areas to search in it would be great! Basically a way to &amp;quot;erase to background&amp;quot; or something... some way not to leave that trace behind somehow... - arby</body>
  </mail>
  <mail>
    <header>Re: colorSpace for bitmapContext in Tiger</header>
    <body>You have to determine what the colorspace is of the originalImage. Even if it is indexed, you need to determine what the base colorspace is. For untagged images, in the past, UserRGB was defined which meant it would pick up whatever was selected in the colorsync preference panel for untagged data, but in Tiger, this is deprecated and replaced by GenericRGB colorspace. Depends on what you had selected in that panel in 10.3.9- if you set that to GenericRGB you would get the same result as in Tiger. Because your source image can be from an arbitrary source, some of these images may have embedded colorspaces (i.e. they are tagged) and some may not (untagged). Given that your purpose is to modify the image pixels, then the colorspace of the bitmap context should also be the same colorspace as the source image. Sometimes this may not be possible as the bitmap context can only be created with a limited set of colorspaces. In general, most applications (e.g. Photoshop) allow one to specify the working colorspace independent of the source image. All source images are matched to this working colorspace. For your purpose you can either choose a working colorspace or derive one from the source image. __ haroon sheikh Ÿáÿßÿ±ŸàŸÜ ÿ¥ŸäÿÆ</body>
  </mail>
  <mail>
    <header>Re: Multiple Image Layers within a View</header>
    <body>I see; and stand corrected. Thanks for the clarification. I have the &amp;quot;Programming With Quartz&amp;quot; book and will toy with it to get a more-solid understanding of this. Ric Neophyte.</body>
  </mail>
  <mail>
    <header>Re: Multiple Image Layers within a View</header>
    <body>On Feb 27, 2006, at 12:55 PM, Frederick C. Lee wrote: The sentence &amp;quot;Which means, that I would set the &amp;quot;doodle layer&amp;quot; CGLayer's context confuses me just a bit.  You're not going to swap out the window's graphics context or anything.  The CGLayer has it's own graphics context and you would draw the doodles on that. Then, whenever your NSView is asked to draw, would draw the background first, and then the CGL on Cocoa's current context.</body>
  </mail>
  <mail>
    <header>QCRenderer to CIImage speed bump</header>
    <body>i'm getting a noticeable performance hit when i'm trying to convert the result of a quartz composer patch to a CIImage.  i've made an instance of an NSOpenGLContext, attached a QCRenderer to the context, and each frame is created by attaching a CVOpenGLBufferRef to the context, and then calling renderAtTime: on QCRenderer (very similar to the 'performer' sample application). this works, but it's slow- converting the CVOpenGLBufferRef to a CIImage using imageWithCVImageBuffer: seems to be where the performance hit occurs.  if i render the context to the buffer but skip the conversion and just render a blank frame in the main output, everything's nice and fast, so it's probably not slower because the system is being overly taxed by an additional context (i've also tried rendering the QCRenderer's context to a view- everything's nice and smooth). is there a more efficient way to convert the output of a quartz composer patch to a CIImage that i'm overlooking? peace : : ray</body>
  </mail>
  <mail>
    <header>Re: Multiple Image Layers within a View</header>
    <body>On Feb 26, 2006, at 7:19 PM, Frederick C. Lee wrote: Generally this is done using a second transparent &amp;quot;overlay&amp;quot; window. I believe this sample code shows how to integrate that with NSView: Nick</body>
  </mail>
  <mail>
    <header>Re: Multiple Image Layers within a View</header>
    <body>On Feb 26, 2006, at 8:19 PM, Frederick C. Lee wrote: How you draw within a view is up to you.  From what you describe, it sounds like CGLayer may be what you need.  CGLayer is, in essence, an offscreen drawing context that is optimized for a given device context.  What you could do is put your &amp;quot;base layer&amp;quot; in an image and then draw your overlays onto CGLayers.  When your NSView gets the drawRect: message, it could then draw the different layers in whatever order you choose. I'd create some kind of object that represents a &amp;quot;layer&amp;quot; in the drawing and a mechanism for keeping track of those layers in drawing order. Adding and removing would be up to you... through whatever mechanism you use to keep track of the layers in a view. As a general rule of thumb, Cocoa does not like it when views overlap.  Some things may work, some things may not.  I'd create my own mini-view system inside of an NSView myself, YMMV.</body>
  </mail>
  <mail>
    <header>Multiple Image Layers within a View</header>
    <body>That is, can each image (layer or context) can be independently added/ removed from the shared NSView? Another way of saying it, is it possible to have multiple Window Contexts for the same view? I want the user to be able to draw upon a layer above a static image being able to erase and re-draw without affecting the base layer. Or, could that only happen with 1 image/view  --&amp;gt; stacking the views as layers? The following are two scenarios (stack of 1 image/view vs multiple images/1 view): #1)  *stack of views* map or photo) #2) *single view* One View with multiple images (transparent drawn upon a static base layer, sharing 1 view) I'm thinking of just stacking customized views. One draw back here, is that I can't create a split view between one (stack of views) and a separate NSTextView. I believe that you loose the INDEPENDENCY/AUTONOMY of an image if the image shares the SAVE view as another image.</body>
  </mail>
  <mail>
    <header>Re: colorSpace for bitmapContext in Tiger</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Drawing a CIImage with transparency in OGL?</header>
    <body>I'm using a CIContext to render an image through CoreImage into an OpenGL context. The problem is it doesn't seem to render the alpha channel (I get the black box around the image instead). _context = [[CIContext contextWithCGLContext: CGLGetCurrentContext() ... (If you'd like to see the code in it's entirety this is based on the code from the FunHouse Demo.) I don't see any options anywhere for working with the transparency of the image when drawing via a CIContext. Does anyone have any pointers on this? --------------------------- Colin Cornaby White Magic Labs -</body>
  </mail>
  <mail>
    <header>Re: [Core Image] Newbie release problem with CIColorControls?</header>
    <body>Hello gentleman, My goodness! Thank you all *so* much. I was so crazy frustrated I was last night, and now look at all the information I have to work with. I can't tell you how grateful I am for all your time and hard work. My apologies to the hatchet job on the code: in my eagerness to isolate the issue and simplify it for the list, it may have actually obscufated it a bit. I will definitely take your suggestions on board Brendan, my Cocoa newness means I might be learning bad habits in the name of getting things to work! I'll look at rebuilding my design, thank you. Guy, you are absolutely correct. I am leaking CIColorControls (the filter) and CFNumbers (from those NSNumbers I guess) according to ObjectAlloc, along with the CIImages. This must have been why those extra releases I had in my earlier code that Frank rightly corrected me on didn't crash me. My program flow is as follows: CameraController is a delegate to the CocoaSequenceGrabber framework, which sends it an NSImage every frame it picks up. CameraController sends out a notification through the defaultCenter that it has a new image. I do it this way rather than call a method directly because multiple classes will want to look at this image down the line. My test class listens for this notification: - (id)init [nc addObserver:self selector:@selector(getCurrentImage:) - (void)getCurrentImage : (NSNotification *)note [thresholdCompositeImageView setImage:[[compositor and it calls the compositor. This happens a *lot* under normal circumstances; around 30 times a second. Both CameraController and TestController are initialized from the NIB file as I was using them to output test images and such. I haven't done any threading at all, I'm still quite new to Cocoa (can you tell? :) )and didn't want to run before I could walk on that front! My book makes only a cursory glance at The One Big Autorelease Pool. Things are recieving  autoreleases, or I would be leaking NSImages generated by the camera framework, but as you say Guy, I definitely am leaking numbers, CIImages and CIFilters. I tried implementing a sub pool per the Apple tutorial () but may have put it in the wrong place (I inited it at the start and released at the end of the  addImageToComposite: function). I haven't been able to find any documentation hinting that Cocoa may make multiple pools, is this happening, and why I would be seeing some objects autoreleased and not others? How do I drain the pool that these CIImages/CIFilters/Numbers are splashing about in? Frank, Guy, Shawn, Brendan, if I could buy you all a beer I most certainly would. It's more than deserved. Your help has made me a much saner man! Chris</body>
  </mail>
  <mail>
    <header>Re: [Core Image] Newbie release problem with CIColorControls?</header>
    <body>On Feb 23, 2006, at 8:41 PM, Guy English wrote: Yup I think he likely is just filling an auto-release pool in some type of loop without draining it. -Shawn</body>
  </mail>
  <mail>
    <header>Re: [Core Image] Newbie release problem with CIColorControls?</header>
    <body>Shawn sed: He's cut some code for the sake of the example that adds the new image on top of the cooling one. That's what I'm thinking too. Somone else sed: The only reason to ever do [[obj retain] autorelease]; is to ensure that the returned object will outlive the object that returned it. In the event that the object that returned the value is released before the end of the runloop is reached *and* the object returned hasn't been retained before hand then you'll end up with a dud pointer. In this case [[returnImage retain] autorelease]; won't help. I also know that because that's what he did the first time and it didn't help then either. :) Take care, Guy</body>
  </mail>
  <mail>
    <header>Re: [Core Image] Newbie release problem with CIColorControls?</header>
    <body>On Feb 23, 2006, at 8:27 PM, email@hidden wrote: ??? What you get from [f valueForKey:@&amp;quot;outputImage&amp;quot;] is already auto- released and in the currently active auto-release pool. Sending it a retain followed by an autorelease doesn't change anything, in only adds overhead. His memory issue lies someplace else unless the following some how causes Core Image to do something wrong... Agreed. -Shawn</body>
  </mail>
  <mail>
    <header>Re: [Core Image] Newbie release problem with CIColorControls?</header>
    <body>Chris, This code looks fine from a quick scan. Are you running this in a while loop or off a timer? Does the autorelease pool get a chance to drain? If you're running in a while loop then you'll end looking like you're leaking memory since the pool will just keep filling up. Commenting our addImageToComposite would look like it's solving the problem because it's only in that call you end up adding anything to the pool. If I'm right you'll be leaking CIFilters and NSNumbers too. Hope that helps, Guy</body>
  </mail>
  <mail>
    <header>Re: [Core Image] Newbie release problem with CIColorControls?</header>
    <body>On Feb 23, 2006, at 6:08 PM, Chris Lewis wrote: I really don't understand how addImageToComposite and addImage interact... If you don't have a composite in addImage: you copy the supplied image and do nothing else. If you have a composite then you do nothing with the supplied image and call addImageToComposite which just operates on the existing image you have. That seem a little weird but it may make sense not knowning what the intent of addImage is and how it inter-plays with addImageToComposite. Anyway how are you calling either addImage? Is it from loop you are running in a thread of your own? etc. -Shawn</body>
  </mail>
  <mail>
    <header>Re: [Core Image] Newbie release problem with CIColorControls?</header>
    <body>It's not hard to solve your memory leak problem, but, to be honest, your code is an awful mess.  I've made comments below. Why is this a global variable?  And why bother making ImageCompositor the sole interface to it?  Most likely, it should be instance variable in your application delegate object or some other global singleton class.  Just to be sure, you aren't accessing this from other threads are you? This class should not exist.  It doesn't do much and it even shares a global variable among its instances. Generally, methods that start with -set&amp;lt;something&amp;gt;: should be passed in exactly what they want to set, not some number of frames that is then used to figure out the cooling factor. Yuck.  Having a method which sometimes sets a new image and sometimes doesn't is not cricket. Try -setImage:nil in the future. To solve your memory problem, you should be returning [[returnImage retain] autorelease];  Also, you probably want to call [f setDefaults] somewhere in there since you aren't guaranteed to get a new filter object each time. Same as above. Note that most of your code is concerned with wrapping logic into an ImageCompositor class which probably shouldn't exist.  The above two methods -overlayImage: and -adjustImageSaturation:etc. should just be called from whatever code actually needs them. Brendan Younger</body>
  </mail>
  <mail>
    <header>Re: [Core Image] Newbie release problem with CIColorControls?</header>
    <body>Hi Frank, anyone else reading, Thank you ever so much for bearing with me, your help is invaluable. ObjectAlloc has been the predictor of my woe, and is how I know it's CIImages that are leaking. Since I am doing this from an iSight in real-time, leaking these causes very fast performance degradation, hence my current upsetness. I hope this isn't too out of order, but I am going to paste in the two files in question. I've removed the image input. addImage: is called externally, and if it doesn't have an image it makes a copy. Otherwise, it hands off to addImageToComposite: which is where my problems lie. I have an overlayImage: function to make the composite, but that has the same problem as the one that is tweaking the levels. Commenting the function call means everything runs fine, so the parameter image to addImage: is being released correctly. It's somewhere in these two files, and it begins when I enter addImageToComposite: ======== // //  ImageCompositor.m //  Comparison Module // //  Created by Chris Lewis on 5/2/06. //  Copyright 2006 Chris Lewis. All rights reserved. // @interface ImageCompositor (PrivateAPI) @end @implementation ImageCompositor - (id)initWithCoolingFrames : (int)numberOfFrames - (void)setBrightnessCoolingFactor : (int)numberOfFrames if (numberOfFrames &amp;gt; 0) else - (id)initWithCoolingTime : (double)seconds return [self initWithCoolingFrames:[TimeMethodLibrary - (id)init - (void)dealloc - (void)addImage : (CIImage *)aImage if (composite == nil) else - (CIImage *)composite - (void)clearComposite - (void)setCoolingFrames : (int)numberOfFrames - (void)setCoolingTime : (double)seconds - (void)addImageToComposite CIImage *coolComposite = [composite adjustImageSaturation:1.0 brightness:brightnessCoolingFactor @end =============== // //  CIImageCategory.m //  Comparison Module // //  Created by Chris Lewis on 7/2/06. //  Copyright 2006 Chris Lewis. All rights reserved. // //  Image conversion code credit: //  convertCIImageToNSImage and convertNSImageToCIImage //  are copyright of Dan Wood, 2005. // @implementation CIImage (CIImageCategory) - (NSImage *)convertToNSImage - (NSSize)nsSizeOfImage - (CIImage *)overlayImage : (CIImage *)overlayImage - (CIImage *)adjustImageSaturation : (double)saturationValue brightness : (double)brightnessValue contrast : (double)contrastValue [f setValue:[NSNumber numberWithDouble:saturationValue] [f setValue:[NSNumber numberWithDouble:brightnessValue] [f setValue:[NSNumber numberWithDouble:contrastValue] @end ======== Any ideas? I'm flat out, it's 2AM in the morning and I've been at it for 8 hours. Your help has been absolutely wonderful, thank you. Chris</body>
  </mail>
  <mail>
    <header>Re: EPS From Quartz?</header>
    <body>On Feb 23, 2006, at 5:09 PM, Nick Nallick wrote: Going through the PostScript path is not likely to help you with EPS, I'm afraid.  Apart from generating the EPS yourself from the same data source that generates your Quartz graphics, the only other solution that comes to mind is to post-process the PDF using something like Ghostscript.</body>
  </mail>
  <mail>
    <header>Re: [Core Image] Newbie release problem with CIColorControls?</header>
    <body>Its a little unclear from just this code snippet what your overall retain/release mechanism is. One thing that does stand out is that the following in your adjustImageSaturation method is redundant You just want to return the outputImage of the filter. The caller of the method should be responsible for retaining it as long as he needs to. One thing you might want to look at for tracking down release/retain problems is ObjectAlloc in /Developer/Applications</body>
  </mail>
  <mail>
    <header>Re: [Core Image] Newbie release problem with CIColorControls?</header>
    <body>Hey, Thanks you your reply Frank, it confirmed what I thought, I am just a little shaky about Cocoa memory management, I seem to get myself into a terrible mess. I've refined the function, and placed it back into my category that I have created on CIImage... - (CIImage *)adjustImageSaturation : (double)saturationValue brightness : (double)brightnessValue contrast : (double)contrastValue [f setValue:[NSNumber numberWithDouble:saturationValue] [f setValue:[NSNumber numberWithDouble:brightnessValue] [f setValue:[NSNumber numberWithDouble:constrastValue] I recieve this in a function here: - (void)addImageToComposite : (CIImage *)aImage if (brightnessCoolingFactor != 1.0) CIImage *coolComposite = [composite adjustImageSaturation:1.0 brightness:brightnessCoolingFactor // Commented out because I have the same leak problem as above composite is an instance variable of a CIImage. The composite isn't set anywhere else, except at initialization. When I comment out the [coolComposite retain]; [composite release]; composite = coolComposite, everything runs smoothly, and my CIImage count remains static. If I do the retain/release cycle, I leak CIImages. I can't for the life of me figure out why. Again, thanks in advance for any help. I understand I am probably being quite newbieish, and it's an easy answer for Cocoa gurus such as yourselves. I've read the retain/release pages of my Cocoa Programming for Mac OS X (Hillegrass) over and over, and tried as my different ways as I can think of, but I leak, and I do not understand why. I want to understand, and get it right, and not just paper over the cracks. Thank you ever so much, Chris Lewis</body>
  </mail>
  <mail>
    <header>EPS From Quartz?</header>
    <body>I'm interested in getting EPS output from a collection of CoreGraphics (not Cocoa) objects.  This seems to be ignored in Quartz in favor of PDF, but I have regular requests for it from people who still use EPS in their workflow (not to mention that InDesign has better support for EPS than PDF).  Can anyone suggest a way to do this without writing it from scratch?  Perhaps through the print system's PostScript mechanism? Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: [Core Image] Newbie release problem with CIColorControls?</header>
    <body>[NSNumber numberWithDouble:1.0] is returning by default an autoreleased object as it is a class function. You add it to the autorelease pool again which will double release it. So instead of [f setValue:[[NSNumber numberWithDouble:1.0] autorelease] you should use [f setValue:[NSNumber numberWithDouble:1.0]</body>
  </mail>
  <mail>
    <header>[Core Image] Newbie release problem with CIColorControls?</header>
    <body>Hello all, I'm a bit of a Cocoa newbie, and I have a serious memory leak in regards to using CIColorControls. I have CIImages absolutely overflowing, but I have no idea why. The behaviour I am seeing is bizarre. Here's the code I reduced it to: - (void)addImageToComposite : (CIImage *)aImage [f setValue:[[NSNumber numberWithDouble:1.0] autorelease] [f setValue:[[NSNumber numberWithDouble:1.0] autorelease] [f setValue:[[NSNumber numberWithDouble:1.0] autorelease] Without fail, this will crash everytime, unless I perform a retain on returnImage. Otherwise, the NSAutoRelease pool will seg fault the program. But here's the deal: If I *do* retain it, then I leak CIImages (at a quite phenomenal rate, because I'm doing this with a real-time iSight feed). As far as I am aware, the outputImage from the filter should be set to autorelease and be managed correctly all on it's own. I shouldn't be adding retains or releases unless I want to. Does anyone know why I am seeing this? I haven't had a problem using CIAffineTransform at all. aImage is handled fine: if I comment out all the code from this function I won't leak. I'm doing the copy at the beginning just to prove that it's unrelated :) Any help would be very much appreciated, this is driving me absolutely mad. Thank you so much guys, Chris Lewis (BTW if I don't autorelease the NSNumbers, they leak as well. But surely they are set to autorelease already? I'm so confused!)</body>
  </mail>
  <mail>
    <header>Re: Grabbing NSAttributeString from PDF</header>
    <body>On Feb 23, 2006, at 12:10 PM, Frederick C. Lee wrote: Exactly.  In terms of Quartz 2D, PDF is a metafile format.  What that means is that Quartz 2D uses PDF as a means to record a series of drawing commands. Having said that, the PDF file format can contain more than just drawing commands.  It is possible for a PDF to store the kinds of information you were curious about. For example, the last time I looked (a couple of years ago) Adobe Illustrator files were &amp;quot;just PDF files&amp;quot; with custom data that allows Illustrator to recover the &amp;quot;editable&amp;quot; document from the PDF. Quartz 2D, however, currently only uses PDF as a metafile format. (my favorite way of saying it is that &amp;quot;All Quartz 2D metafiles are PDF, not all PDFs are Quartz 2D metafiles&amp;quot;). To my knowledge the system doesn't give you any way to inject custom data into a PDF that Quartz 2D is generating.</body>
  </mail>
  <mail>
    <header>RE: colorSpace for bitmapContext in Tiger</header>
    <body>I think it is about two color spaces, one for the source and one for the destination. And when I draw the image on the screen or print it, the color matching mechanism applies the difference between the color spaces of my image and the screen/printer. bitmapContext -&amp;gt; (manipulate pixels) -&amp;gt; newImage. My original image may come from any source - in that special case from a bmp file of indexed color space. I use deviceRGB for the bitmapContext - this way the rgb values in the context will be the same as in originalImage. Then I manipulate the pixels (I set the alpha value to 0 for one specified color) and create a new image from the bitmapContext. The question is: what color space should I use when I create newImage? The original color space can not be used any more since it is an indexed one with no alpha. I would use deviceRGB again. Geza Fabry Graphisoft -----Original Message----- From: Damian Frank [] Sent: 2006. February 23. Thursday 16:48 To: Scott Thompson Cc: Fabry, Geza; Quartz Development Subject: Re: colorSpace for bitmapContext in Tiger A related question, then: how do we keep the color correction from happening twice?  That is, say I create a CGImage from the contents of a bitmap context in order to actually draw it somewhere.  Assume the bitmap context was created with the generic RGB colorspace.  How do I prevent the pixels from being corrected yet again?  Right now, when I create the CGImage, I'm using a system colorspace (obtained from CGColorSpaceCreateWithPlatformColorSpace).  Is that correct? Damian Frank</body>
  </mail>
  <mail>
    <header>Re: Transforming CGImage from one profile to another</header>
    <body>-kevin On 2/22/06 2:52 PM, &amp;quot;Kevin Tieskoetter&amp;quot; &amp;lt;email@hidden&amp;gt; wrote: best way to transform it into another CGImage with another profile called &amp;quot;dstProfile&amp;quot;, using some particular rendering intent? This is the method I'm trying right now: &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;uint32_t bytesPerPixel = ( bitsPerComponent / 8 ) * ( &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;bitmapContext = CGBitmapContextCreate( bitmapData, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;width, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;height, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;bitsPerComponent, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;bytesPerPixel * width, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;dstProfile, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;// Set the rendering intent &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;CGContextSetRenderingIntent( bitmapContext, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;// Copy the original image into this context, causing the color conversion to take place &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;CGContextDrawImage( bitmapContext, CGRectMake( 0, 0, width, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;CGImageRef convertedImage = CGBitmapContextCreateImage( This works, except the image is converted using the Perceptual rendering intent, NOT Relative Colorimetric like I specified using CGContextSetRenderingIntent(). Thanks, -kevin &amp;nbsp;_______________________________________________ Do not post admin requests to the list. They will be ignored. Colorsync-dev mailing list &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;(email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden &amp;nbsp;_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;(email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: colorSpace for bitmapContext in Tiger</header>
    <body>A related question, then: how do we keep the color correction from happening twice?  That is, say I create a CGImage from the contents of a bitmap context in order to actually draw it somewhere.  Assume the bitmap context was created with the generic RGB colorspace.  How do I prevent the pixels from being corrected yet again?  Right now, when I create the CGImage, I'm using a system colorspace (obtained from CGColorSpaceCreateWithPlatformColorSpace).  Is that correct? Damian Frank</body>
  </mail>
  <mail>
    <header>Re: colorSpace for bitmapContext in Tiger</header>
    <body>On Feb 23, 2006, at 7:43 AM, Fabry, Geza wrote: Device RGB means that you want to use the raw color values of the destination device. If you use device RGB and draw to the screen, the computer will simply transfer the color components to the display and let it interpret them. That means that you want to use no color correction whatsoever and, as a result, your colors may look very, very different on different devices. Generic RGB is actually a device independent color space with all that entails.  If you use Generic RGB and draw on the screen the computer will run color matching from Generic RGB space to the Display's color space.  Your drawing will look as close to the same as possible when displayed on different devices.</body>
  </mail>
  <mail>
    <header>RE: colorSpace for bitmapContext in Tiger</header>
    <body>Hi, Thank you for your suggestion: by using deviceRGB color space my colors will not be altered. However there is some confusion at least in my head. orSpace/Reference/reference.html CGColorSpaceCreateDeviceRGB says: &amp;quot;In Mac OS X v10.4 and later, this color space is no longer device-dependent and is replaced by the generic counterpart-kCGColorSpaceGenericRGB-described in &amp;quot;Generic Color Then what is the difference between CGColorSpaceCreateDeviceRGB () and CGColorSpaceCreateWithName (kCGColorSpaceGenericRGB) in 10.4? Geza Fabry Graphisoft -----Original Message----- From: quartz-dev-bounces+gfabry=email@hidden [] On Behalf Of Yoshiaki Katayanagi Sent: 2006. February 23. Thursday 01:48 To: email@hidden Subject: Re: colorSpace for bitmapContext in Tiger Hi, Geza. I don't know how kCGColorSpaceUserRGB affects your progrm, but recommend to use &amp;quot;DeviceRGB&amp;quot;. ... context = CGBitmapContextCreate ( bitmapDataPtr, width, height, 8, bitmapBytesPerRow, colorSpace, Yoshiaki Katayanagi</body>
  </mail>
  <mail>
    <header>Re: colorSpace for bitmapContext in Tiger</header>
    <body>Hi, Geza. I don't know how kCGColorSpaceUserRGB affects your progrm, but recommend to use &amp;quot;DeviceRGB&amp;quot;. ... context = CGBitmapContextCreate ( bitmapDataPtr, width, height, 8, bitmapBytesPerRow, colorSpace, Yoshiaki Katayanagi</body>
  </mail>
  <mail>
    <header>Transforming CGImage from one profile to another</header>
    <body>Given the CGImage &amp;quot;anImage&amp;quot;, tagged with profile &amp;quot;srcProfile&amp;quot;, what is the best way to transform it into another CGImage with another profile called &amp;quot;dstProfile&amp;quot;, using some particular rendering intent? This is the method I'm trying right now: uint32_t bytesPerPixel = ( bitsPerComponent / 8 ) * ( bitmapContext = CGBitmapContextCreate( bitmapData, width, height, bitsPerComponent, bytesPerPixel * width, dstProfile, // Set the rendering intent CGContextSetRenderingIntent( bitmapContext, // Copy the original image into this context, causing the color conversion to take place CGContextDrawImage( bitmapContext, CGRectMake( 0, 0, width, CGImageRef convertedImage = CGBitmapContextCreateImage( This works, except the image is converted using the Perceptual rendering intent, NOT Relative Colorimetric like I specified using CGContextSetRenderingIntent(). Thanks, -kevin</body>
  </mail>
  <mail>
    <header>colorSpace for bitmapContext in Tiger</header>
    <body>Hi, Which color space should I use while creating a bitmap context in Tiger? After creating it I fill a rect with RGB color (0, 128, 128). On Mac OS X 10.3.9 the resulting argb pixels will be 0xff008080 as expected, but on Mac OS X 10.4.5 I got 0xff0f817f. (I used kCGColorSpaceUserRGB on 10.3.9). Setting the renderingIntent to kCGRenderingIntentAbsoluteColorimetric results in pixels 0xff11817f but still not 0xff008080. At least I do not know why the red component is not zero. Please find my sample code below: CGColorSpaceRef cs = CGColorSpaceCreateWithName CGContextRef bmcontext = CGBitmapContextCreate (myBM, width, height, CGContextSetRGBFillColor(bmcontext, 0.0, 0.5, 0.5, 1.0);  // float RGBA // examine memory at myBM here Thanks for any help Geza Fabry Graphisoft</body>
  </mail>
  <mail>
    <header>Re: Different behaviors of CGPDFDocumentCreateWithURL on Mac OS	X	10.2.8 and 10.3</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Different behaviors of CGPDFDocumentCreateWithURL on Mac OS X	10.2.8 and 10.3</header>
    <body>Hi, I have an PDF file. In its Document Properties, it shows that: &amp;quot;PDF Producer: Acrobat Distiller 7.0 (Windows)&amp;quot;, &amp;quot;PDF Version: 1.6 (Acrobat 7.x)&amp;quot;. I have developed an application to open and display PDF files. In Mac OS X 10.2.8, by using CGPDFDocumentCreateWithURL(...), I can open the above file, but when draw the file later by CGContextDrawPDFDocument(...), comes out this error message &amp;quot;Unrecognized filter name `JPXDecode'.&amp;quot; In Mac OS X 10.3, I tried to open it again, still using CGPDFDocumentCreateWithURL(...). This time the CGPDFDocumentRef returned is NULL. Does it mean the function CGPDFDocumentCreateWithURL(...) has been changed from 10.2 to 10.3? Can anyone clarify me on this matter? In the first case (Mac OS X 10.2.8), when drawing the file by CGContextDrawPDFDocument(...), the error message comes out and the application also crashes. &amp;quot;JPXDecode&amp;quot; is a new image filter introduced in PDF Library 6.0 or PDF 1.5. It seems the Quartz API doesnt support this new PDF feature. In this case, is there any way to check the version of the PDF file so that we only call the Quartz API functions to process the file if it is supported by the API, and for PDF file of new version, we can just skip calling the functions, so avoiding the application crash! regards, YU, Liang</body>
  </mail>
  <mail>
    <header>Rotating Monitor =&amp;gt; rotated display</header>
    <body>Can someone help me find documents / procedure for rotating the&amp;nbsp;display by 90 degrees , 180 degrees&amp;nbsp;? I&amp;nbsp;am rotating the monitor and I need the whole screen/display also to be rotated correspondingly( say from landscape to potrait ). Thanks in &lt;SPAN style="FONT-SIZE: 10pt; COLOR: navy; FONT-FAMILY: Arial"&gt;&lt;FONT face=Verdana&gt;Niyaz N</body>
  </mail>
  <mail>
    <header>Re: Pixel Aspect Ratio of Device/Context</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGColorSpaceCreateIndexed and CGBitmapContextCreate</header>
    <body>David, Thanks for your input.  What I have is a series of 20,000 row X column byte arrays containing rgb values.  My data provides a custom colortable that changes throughout the rendering of data.  I have written code that indexes into my color table and fills a pixel matrix; however, this process over the course of 20,000 frames is being very time intensive.  What would be the best way, Quartz, NSImage, etc to index into my colortable and create the frames for display? Thanks, Ben Quartz does not support contexts that represent indexed color. The indexed color space can be used as a source color space for drawing images and, while unusual, you can use it as the current fill and stroke color space. The following document lists the different contexts which Quartz supports and what version of Mac OS X they became available. http://developer.apple.com/qa/qa2001/qa1037.html Hope this helps, David All, I am trying to create a CGBitMapContext with my own index color table (the data provides a very small colortable of its own).¬† However, whenever I call CGBitmapContextCreate after creating my CGColorSpaceCreateIndexed it ruturns nil;¬† Strange thing is, if I pass a as the ColorSpaceRef it successfully returns the context.¬† Any suggestions would be great. Regards, Ben Dunton Below is my code: ¬†_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list¬† ¬† ¬† (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGColorSpaceCreateIndexed and CGBitmapContextCreate</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Photoshop layer styles</header>
    <body>Still, I'd need this for Panther, too. ... Thanks for the tip. Java is not an option. I wasn't aware of ImageMagick and have had a look at it, it doesn't seem to do what I want to though - which is applying layer styles to images I generate programmatically. It's not an essential function at the moment, so I'll postpone implementing it for now... -- Belo Horizonte, Brazil &amp;quot;It's extremely unlucky to be superstitious, for no other reason than it is always unlucky to be colossally stupid.&amp;quot; (Stephen Fry) Weblog:</body>
  </mail>
  <mail>
    <header>CGColorSpaceCreateIndexed and CGBitmapContextCreate</header>
    <body>All, I am trying to create a CGBitMapContext with my own index color table (the data provides a very small colortable of its own).  However, whenever I call CGBitmapContextCreate after creating my CGColorSpaceCreateIndexed it ruturns nil;  Strange thing is, if I pass a as the ColorSpaceRef it successfully returns the context.  Any suggestions would be great. Regards, Ben Dunton Below is my code: colorSpace = CGColorSpaceCreateIndexed(CGColorSpaceCreateDeviceRGB(), , ) context = CGBitmapContextCreate (bitmapData, CDG_FRAMEWIDTH, CDG_FRAMEHEIGHT, bitmapBytesPerRow, colorSpace, )</body>
  </mail>
  <mail>
    <header>Re: Photoshop layer styles</header>
    <body>Yeah, that's a good point - you can use ImageMagick to read/write PSD files.  This is probably how I'll do it rather than purchase the SDK. On Sep 30, 2004, at 6:57 AM, Robin Berjon wrote:</body>
  </mail>
  <mail>
    <header>Re: Photoshop layer styles</header>
    <body>[NB: I don't have access to Tiger, so by discussing it I'm only conjecturing, and not breaching any NDA :)] I believe that the functionality you are looking for is to be made available as part of Core Image, included in Tiger. More detail can be found below: If you cannot wait, similar functionality (though perhaps reduced, and likely not as fast on Apple hardware) is certainly available in other image libs. You might want to look into the likes of Gimp and ImageMagick to see what available there. If Java is an option, Batik supports all the SVG filters, and they can of course be applied to rasters as well. -- Robin Berjon</body>
  </mail>
  <mail>
    <header>Re: Photoshop layer styles</header>
    <body>It's a blind alley, then. I don't plan to spend $200 just to read about a file format, either... thanks to all who replied. -- Belo Horizonte, Brazil &amp;quot;It's extremely unlucky to be superstitious, for no other reason than it is always unlucky to be colossally stupid.&amp;quot; (Stephen Fry) Weblog:</body>
  </mail>
  <mail>
    <header>Re: Photoshop layer styles</header>
    <body>I just spoke with an Adobe's developer rep about this the other day. The psd file format is indeed documented in the Photoshop SDK, which is available only to qualified members of the Adobe developer program (which costs $200 to enter). On Sep 29, 2004, at 6:45 PM, Darrin Cardani wrote:</body>
  </mail>
  <mail>
    <header>Re: Photoshop layer styles</header>
    <body>It used to be documented in the Photoshop SDK. I don't know if it still is, but if you can find a version 4 or 5 SDK, it contains a document describing the format. In fact Jason is correct that Photoshop files do include a preview in their data fork. It's essentially the first layer (if I recall correctly) and is what you would get in Photoshop if did a &amp;quot;Merge Visible Layers&amp;quot; command. Some applications even put a message in it that says &amp;quot;you need version xyz of app abc to properly view this image&amp;quot; in several different languages. In Photoshop 7, you can turn off the previews by opening the prefs and choosing &amp;quot;File Handling.&amp;quot; Uncheck the &amp;quot;Always Maximize Compatibility for Photoshop (PSD) files&amp;quot; check box, and I believe it will no longer put the composite layer in the output. Darrin -- Darrin Cardani - email@hidden President, Buena Software, Inc. Video, Image and Audio Processing Development</body>
  </mail>
  <mail>
    <header>Re: Photoshop layer styles</header>
    <body>Thanks. I've tried to disprove that before posting by removing preview resources and other stuff, but hadn't considered that the data would be duplicated inside the data fork itself. Is the PSD format documented somewhere? -- Belo Horizonte, Brazil &amp;quot;It's extremely unlucky to be superstitious, for no other reason than it is always unlucky to be colossally stupid.&amp;quot; (Stephen Fry) Weblog:</body>
  </mail>
  <mail>
    <header>Photoshop layer styles</header>
    <body>Hi folks, I would like to apply Photoshop layer styles (like bevelling) to images using Quartz. As PS files which include layer styles are displayed correctly by Mac OS X, I presume that an API exists for rendering them, but I've had no luck searching for them in Apple's docs. Any hint? TIA, -- Belo Horizonte, Brazil &amp;quot;It's extremely unlucky to be superstitious, for no other reason than it is always unlucky to be colossally stupid.&amp;quot; (Stephen Fry) Weblog:</body>
  </mail>
  <mail>
    <header>Cocoa, Quartz, Colorspaces and ColorSync</header>
    <body>Apologies in advance for the multiple post, but the subject spans several areas and information is scarce. I'm writing a Cocoa graphic arts app and I'm trying to make it at least minimally color managed. (Or, at least, I'm trying to write it so that adding full color management later doesn't require a complete rewrite.) I've searched the docs, the web and the archives and I haven't found much in the way of information. The docs are outdated - the section of the ColorSync documentation titled &amp;quot;Developing Your ColorSync-Supportive Application&amp;quot; is written in terms of QuickDraw and PICT images. I understand in principle how color management works, it is just the specifics and detailed information on what is really happening that are missing. For the moment I'm not concerned about reading in files or images with embedded profiles, just with things that I'm creating in Cocoa. I'm not sure what the definition of &amp;quot;device indenpendence&amp;quot; is here. Presently when my app saves a file (in its own file format) the colors are saved by archiving the NSColor objects that were created as &amp;quot;device independent&amp;quot; colors. If I then open the file on a system with a different display is the color on the new screen a ColorSync match for the color on the screen of the first system ? Or should I get the components from the NSColor object, store them and use them to initialize a new NSColor object when opening the file? The NSColor class method colorWithCalibratedRed:green:blue:alpha: supposedly returns a calibrated or &amp;quot;device independent&amp;quot; color in the of a Cocoa program using calibrated RGB is NSCalibratedRGBColorSpace. WHAT SPACE IS THIS ???? If it is a real color space, it should have a defined relationship to, say, CIE XYZ. Is it the spaced defined by the profile &amp;quot;Generic RGB Profile&amp;quot; ? (This would be at least consistent, because that's what the Cocoa printing system is putting in the spool pdf.) Is it always the same or does it depend on what is set as the default RGB profile ? By way of contrast, in CoreGraphics you must first set the colorspace and then the color before you draw. The Quartz function to obtain a Calibrated RGB space, CGColorSpaceCreateCalibratedRGB, requires that you specify a white point, a black point, gammas and a matrix - all the info needed to relate the space to CIE 1931 XYZ. Or you can use CGColorSpaceCreateICCBased with a specific profile. NSColor* myCalibratedColor = [NSColor colorWithCalibratedRed: myRed green: myGreen blue: CGColorspaceRef myColorSpaceRef = CGColorSpaceCreateCalibratedRGB( myWhitePoint, myBlackPoint, CGColorspaceRef myColorSpaceRef = CGColorSpaceCreateICCBased ( 3, myRangeArray, myProfileProviderRef, for some default data or profile. Then life would be (relatively) simple if someone could tell me what space the default data corresponds to. Stepping  with the debugger, this seems not to be the case. NSColor is apparently a class cluster and the code creates an instance of a NSCaliabratedRGB color. Since this is internal there is no information available. So, again, the plea: what is happening under the hood here ? Also: what is ColorSync really doing here when things are drawn to the screen ??  I supply an RGB to to Cocoa via [NSColor colorWithCalibratedRed:green:blue:alpha:], set that color, draw, and some (other) RGB is applied by the graphics card to the inputs of the display. Where and when do the various transformations take place ? A related question: What exactly is the Digital Color Meter app measuring for RGB ?? It is clearly getting an RGB somewhere before the display correction is applied - the numbers it reports are the same for a given thing on the screen even when the display profile is changed to wildly different things. Any information, confirmations or pointers to reliable information would be greatly appreciated.</body>
  </mail>
  <mail>
    <header>Re: pattern &amp;amp; CGPatternRef</header>
    <body>Responding to my own post.... I apologize for the mis-information but as was pointed out to me off-list,  patterns do NOT transform with the current CTM.  A pattern has a transform that is associated with it when it is created.  The code I'm working with uses that transform (and repeatedly creates patterns) in order to mangle them intentionally. Without effort on your part, however, the pattern won't follow the CTM. (it was also mentioned that if you have a pattern that is defined in the context of, for example, a PDF document, if you ask the PDF document to draw rotated, or otherwise transformed, then the pattern will transform along with it). -- Macintosh Software Engineering Consulting Services</body>
  </mail>
  <mail>
    <header>Accelerated Video</header>
    <body>&lt;span style='font-size: 10.0pt;font-family:"Trebuchet MS"'&gt;Hi, &lt;span style='font-size: 10.0pt;font-family:"Trebuchet MS"'&gt; &lt;span lang=EN-GB style='font-size:10.0pt;font-family:"Trebuchet MS"'&gt;Has anyone an idea of how to accelerate the playing of vidfeo sequences (may be how to use the board acceleration) on a software using Quartz APIs or, if necessary and if applicable, switching to OpenGL ? &lt;span lang=EN-GB style='font-size:10.0pt;font-family:"Trebuchet MS"'&gt; &lt;span style='font-size: 10.0pt;font-family:"Comic Sans MS"'&gt;Cordialement, Jean-Claude ________________________________________ France Telecom &lt;span style='font-size: 10.0pt;font-family:"Comic Sans MS"'&gt;Division R&amp;amp;D &lt;span lang=IT style='font-size:10.0pt;font-family:"Comic Sans MS"'&gt;MAPS/DMG&amp;nbsp;- Piece 247E Tel: (+33) 1 45 29 58 06&amp;nbsp; Fax: (+33) 1 45 29 52 94 e-mail: email@hidden ________________________________________ &lt;span lang=IT &lt;span lang=IT style='font-size:12.0pt'&gt;</body>
  </mail>
  <mail>
    <header>Re: pattern &amp;amp; CGPatternRef</header>
    <body>On Sep 27, 2004, at 10:08 AM, email@hidden wrote: Not Really.  One way to do it would be to change the pattern to a 1-bit CGImage and then draw the CGImage in your pattern draw callback. You have to be a bit careful, however, because using this technique the pattern will scale and rotate with the current context.  QuickDraw patterns, in contrast, will always draw at the pixel level.  This can lead to some unexpected behavior if when trying to convert QuickDraw drawings to CG. -- Macintosh Software Engineering Consulting Services</body>
  </mail>
  <mail>
    <header>pattern &amp;amp; CGPatternRef</header>
    <body>Hi I would like to draw the Pattern. Is there an easy way to convert Pattern to CGPattern Thanks Vincent</body>
  </mail>
  <mail>
    <header>Re: bug using image masks with dataWithPDFInsideRect</header>
    <body>Hi Haroon - Actually there is a (very ugly) workaround - at least for my app: Set a state variable before calling dataWithPDFInsideRect and clear it afterward. Make the objects that are image masks check it before they render themselves and if they are drawing inside a call to dataWithPDFInsideRect, draw them into a full color image with alpha and then draw that image to the context instead of the image mask. Very un-aesthetic , but I need the functionality. I promise to remove it when the bug gets fixed. :-)</body>
  </mail>
  <mail>
    <header>Re: bug using image masks with dataWithPDFInsideRect</header>
    <body>This is a known bug that we are tracking already.¬† No need to file a bug report. Unfortunately there is no workaround.¬†</body>
  </mail>
  <mail>
    <header>Re: Replacing ScrollRect</header>
    <body>On Sep 21, 2004, at 9:56 AM, R. Scott Thompson wrote: Not all, effective communication is *my* weak point, not yours :) And that's exactly why Apple should implement this! If the optimal method (shifting pixels) isn't appropriate for a given context, then Apple should provide the best possible alternative for any given context (make an image from from the area to be scrolled, erase, draw). That way, the API always does the best possible thing for any given context. Apple is in a MUCH better position than anyone else to do this. I've never agreed with Apple's decision to NOT include more helper-type APIs. (And this is not just Quartz.) I think the idea of a lean and mean API makes sense theoretically, but fails in the real world. Helper-type APIs make it easier for developers to Do The Right thing, allow Apple to optimize as only they can and, in the end, make it easier and faster to write solid code. I'd settle for that :) It's as if the people at Apple developing this API did so in an ivory tower with no real world exposure. That has been very frustrating about Quartz. I'm so fortunate that I can require Panther or later for my app. I wish it could be Tiger or later. The APIs gaps in Quartz have been frustrating. The lack of engaged response from Apple even more so. Yes, Haroon responds telling us to file a bug report (even though it'll likely be ignored ;), but there's little back and forth communication that Apple &amp;quot;gets it&amp;quot; with regards to our API needs. This year though, WWDC was very encouraging in that respect. For the first time since OS X came out many of my API requests seem to be getting addressed. Not bad after all this time. Hopefully it'll be better going forward. Excellent! More supporting data for why Apple should do this and not every developer. Apple can clearly determine these optimal paths for any given context and just Do The Right Thing. Why burden every developer with this? Absolutely, and I have no problem with the conceptual shift and goals of Quartz. My issues are more about how the Quartz API was developed and how it continues to evolve.</body>
  </mail>
  <mail>
    <header>App ideas - visual screen highlights</header>
    <body>(Background: a thread on the student-dev list has been focusing on the creation of utilities to highlight parts of the screen, such as the results of find operations, the cursor, or other things, using technology similar to Expos√©) I like this idea.  And the water-drop one (eye-candy that involves time is always better, and more attention getting).  A slight magnification effect., which would go away after a slight delay when you moved the cursor after a slight delay (if you wanted it to) One (boring) idea would be to draw a highlight box on the screen that would blink a few times.  More interesting ideas are a darkening, like you said, or a &amp;quot;lozenge&amp;quot; magnification effect (text distorted as seen through a half-lozenge piece of glass placed on the screen), or a color &amp;quot;throb effect&amp;quot; - a milder version of the 'blinking box'. I personally think these ideas are great, and really do push the idea of the desktop interface beyond what it is today, into kind of the '2.5D realm' (2D+time).  I'm not sure how many are on Apple's drawing board, and for what timeframe.  They seem to dribble out a few really useful visual enhancements every OS version of so - so that would be what - one to three years?  I don't want to wait that long. As for 'leveraging Expos√© technology, that isn't going to happen until (and unless) Apple releases more APIs for the WindowServer. We've all seen some example of this that basically shows us how Expos√© might work in shrinking and moving windows.   But that's not nearly all of what's possible.  The window darkening effects that Expos√© uses. The genie effect that has been used by the Dock since X *1.0* (and thus the Window Server calls responsible for it haven't changed much at all).  The Dock most likely uses a textual-mapping point transformation of some kind for windows.  But it *has* to be at the 'window view' level (not just a static image), since if you shut down the dock while a window is minimizing - the window will continue to work, just warped in appearance. I think a lot of people would write all *sorts* of things along these ideas, and come up with new ones, if only Apple *would* release the APIs for the Window Server.  In OS9, I remember an extension called 'Gravit√©', which hooked into the Finder's drag routine, and then did a rotation animation on the icon.  It was a harmless little hack, but it could have possibly turned into some little idea for a different kind of spatial-metaphore 2D interface.  I'm pretty sure it used the jGNE mechanism, or some some head or tail patch. In OSX, there APE, but there's no OSS or Apple-sanctioned way to do this.  I don't know exactly why - Apple seems to want to keep a very tight leash on what the OSX interface looks like, and what people can do with it 'under the hood' (witness the former debacle with Theminator..)  Some of this is good - is makes the system for stable, and *possibly* makes tech-support calls easier (I myself can't really believe that). But another thing it does it limits the creativity of the developer community, and (to some degree), the Mac community as a whole.  Another person I talked to said that having something like Terminal Services in Windows would be nice.  Security problems aside, it would be a wonderful diagnostic tool - of course, that would cut into Apple Remote Desktop's territory. There are of course other directions in which these &amp;quot;future interface&amp;quot; ideas could be taken:  One idea is that of moving windows between different users in Fast User Switching, making FUS even more like virtual desktops (which are common in interfaces like KDE).  I suggested the idea of dragging a window into the FUS menu (it would shrink with a genie effect to a small rectangle, or perhaps a 'rotated teardrop' shape, and then down to a particular user name.  Then the window would continue to shrink into the item, with a little pop sound, and the desktop would switch, then the window would expand from the FUS menu in reverse, to the same position on the window as it was before, or perhaps position itself so it overlapped other windows as least as possible (you don't want to change the I don't think).  We can't do this without a documented and 'official' API to the window server, and possibly not even then (transferring windows between users probably means changing who 'owns' them in the Window Server as well, as well as being able to throughly patch into the FUS menu handling routines) When I say 'we', I do mean we.  There are almost certainly academic HCI (human-computer interaction) researchers who would be interested. I am also sure there are ordinary users who would be interested in discussing interesting, and possibly more intuitive (at least once you learn then), graphic interface ideas, who would not dream of joining the student-dev list. I believe there is also a certain 'dilution' effect to having this thread be just 1 thread of about 60 at any one time on student-dev. Not to mention that this discussion is not completely related to 'development'.  And so I propose the creation of a separate mailing, either on Yahoo groups or some other list site or server (perhaps I or someone else could get something set up on a university server somewhere in the world).  I would propose three basic 'branches' for discuss: 1.	Development - reverse-engineering of the ways Expos√©, Dock, etc works.  Creation of hacks and other programs. 2.	Discussion - &amp;quot;code-free&amp;quot; discussion of graphic interface directions and ideas.  This includes 2D interfaces as well as possibilities for 3D interfaces.  My personal favorite idea is an immersive-VR simulation, with wrap-around stereoscopic goggles and motion-control sensors on the user.  See John Cramer's hard-SF book &amp;quot;Einstein's Bridge&amp;quot; for background on the idea. 3.	&amp;quot;Politics and Evangelism&amp;quot; (not the best words) - Discussion of how to persuade Apple to release more information about the Window Server or other private frameworks, and &amp;quot;evangelizing&amp;quot; the list to other people who would be interested.  One idea is approaching academic researchers, who would presumably have some academic clout, and could *possibly* (though this is probably remote) secure grant money for such an exploration (yes, very remote indeed..) Email me off-list (or on-list - might be better in terms of 'exposure, though it would make it more 'public' if you know what I mean..) if you're interested in this idea, or have suggestions as to a place to put the list and/or website, etc. Jim Witte email@hidden Indiana University CS</body>
  </mail>
  <mail>
    <header>Re: Replacing ScrollRect</header>
    <body>On Sep 21, 2004, at 1:32 PM, Haroon Sheikh wrote: LOL! I totally read it differently.... I think that would be great. Please consider looking at some of the other 2D APIs out there: vImage, Java 2D and of course QuickDraw! All of these offer a lot for bitmaps whereas Quartz offers so very little. (It'd be cool to see vImage-powered Quartz APIs for manipulating bitmap contexts.) I know Apple is really big on &amp;quot;API purity&amp;quot; (and not just the Quartz team), but I really think there's been too many cases of immature API sets over the past few years and I believe that the root cause is not time or resources to implement a larger API, but a lack of understanding or experience by Apple's API reviewers in what is involved in writing complex real-world applications. And I'm mainly only talking about Carbon HLTB and Quartz APIs -- I imagine many other areas have similar problems. Of course, I need to find some time to just take a few days and focus totally on Tiger development: I need to try and write some apps that use the new APIs and get bug reports in now for gaps. Too bad there's a not a mailing list to discuss that (and yes, I filed a bug report for that too :)</body>
  </mail>
  <mail>
    <header>Re: Replacing ScrollRect</header>
    <body>I requested a bug report as that is our standard tracking mechanism. Thanks for filing it and I'll try and champion it¬† internally. I wasn't indicating that it will not make it past our API review process - instead I was taking a friendly jab at our API reviewer and not yourself :-) I do think we need to start providing better BitmapContext specific functions that work at the pixel level. ¬† On Sep 21, 2004, at 7:31 AM, Bryan Pietrzak wrote:</body>
  </mail>
  <mail>
    <header>Re: Replacing ScrollRect</header>
    <body>On Sep 21, 2004, at 1:21 AM, Haroon Sheikh wrote: By the way... another reason Apple should do this? You can make the API smarter. In order to draw the image on top of itself, I first have to erase the old content correct? So now I'm doing an erase and a draw. With internal access, you could make this API faster. And let's not even get into hw accel :) Anyway. I've made my point. Many of my arguments apply to other API gaps. I just tend to disagree with Apple's approach to APIs. And given the changes from Jaguar to Tiger, clearly Apple must be changing there mind that the less-is-more approach works. (Hint: it doesn't in all cases!) OK, so I lied. rdar://3809328</body>
  </mail>
  <mail>
    <header>Re: Replacing ScrollRect</header>
    <body>On Sep 21, 2004, at 1:21 AM, Haroon Sheikh wrote: So you're telling me to file another bug that will be ignored? Great, that'll be a lot of fun. What more detail could I possibly provide in an enhancement request that would get by your API reviewer when even you clearly see no need for it. I mean if you don't support it given what I've said on this mailing list, why would I expect anyone else at Apple to champion my API request? Last time I checked no one at Apple was working with my team on our app. This is why Apple is frustrating at times. There is some kind of implementation already available for HIViewScrollRect why should I have to request this? You already have it. Clean it up, if needed, and export it. But no, since Apple and its API reviewers know so much about my application (and given the gaps in the Quartz API prior to Tiger, clearly no one at Apple has ever tried to write advanced 2D drawing apps) they have deemed there is only one true way to do something. God forbid there might be developers that are are doing something a bit different and could use a helping hand from the API. Sorry, but it's getting old. Yes, you guys seem to have addressed many (most?) of my API requests for Tiger (even though many were requested during the Jaguar era -- but no matter, we can wait until Tiger to ship our app -- not!).  This kind of thing gets frustrating time after time. Some APIs are so obvious that I shouldn't have to be asking for them! How hard would it be to have a CGScrollRect that worked fine for bitmap contexts or contexts associated with windows but would return an error for other types of contexts. Oh wait, This is CG and we don't believe in error reporting ;). At the very least, you should consider updating that transitioning from QD to Quartz document to say: if you are using ScrollRect, sorry, we have nothing for you. Rearchitect your code if you can to use HIViewScrollRect or invalidation. If not, there is no CG API equivalent nor any sample code to help you focus on your application and our API reviewers will laugh at you if you request one.</body>
  </mail>
  <mail>
    <header>Re: Replacing ScrollRect</header>
    <body>I don't think I did a good job of making my point. At the level of a generic CGContextRef the API doesn't make a distinction between the two types of contexts you've described.  In the public space there are two different types of contexts at the moment, but who is to say what the future holds and what types of CGContexts we may see.  For that matter, Quartz 2D may have more than just these two context types today. The point is that It seems entirely possible that we could end up with something like a window context that, instead of storing a bitmap, stores an OpenGL display list.  If that ever were to be the case, scrolling would not make much sense in that &amp;quot;context&amp;quot; because you can't just shift the primitive commands that are stored in the display list. In other words, adding a CGScrollRect at the level of a generic CGContextRef does not make sense because not every type of context can be scrolled. It does make sense at the level of CGBitmapContext, though.... maybe a CGBitmapContextScrollRect would be the right way to go. The fact that CGBitmapContexts don't allow more bitmap specific operations is something that has annoyed me in the past as well. Unfortunately there doesn't appear to be a good generic mechanism for downcasting (in the RTTI sense) a CGContextRef from a generic CGContextRef to a CGBitmapContext.  What it looks like you have to do is call CGBitmapContextGetData to get a base pointer to the context's image data.  If that returns successfully, you can use other routines to get the rowBytes and stuff.  Once you have the bitmap's characteristics you should be able to create a CGImage with the part of the image you're interested in.  You could then then draw that image back on top of the original context (with an offset to account for the scrolling). (What's not clear, however, is what happens if the context was created using a JPEG provider or something like that.  Does CGBitmapContextGetData return the pixel data, or the compressed JPEG data?  Probably pixel data but I don't know for sure.) Another thought would be to use the base pointer returned by CGBitmapContextGetData and pass it to routines in vImage (if the image data is in a vImage compatible format).  The advantage there is that vImage is able shuffle the bits around super fast. I think there's going to be a lot of learning to be done on the best way to move things from QuickDraw to Quartz which is part of the reason that the topic fascinates me so much.  QuickDraw is intrinsically pixel based and much of legend and lore of good QuickDraw programming is learning how to efficiently combine and shuffle those pixels.  Quartz 2D on the other hand is more device independent and in some cases a CGContextRef doesn't even have the concept of the &amp;quot;pixel&amp;quot; to work with (well except in so much as the generic context can draw sampled images).  That means that many techniques which are almost second nature to QuickDraw programmers don't make much sense in Quartz. Finding the &amp;quot;right&amp;quot; way to do things under Quartz will be pretty interesting. -- Macintosh Software Engineering Consulting Services Attachment:</body>
  </mail>
  <mail>
    <header>Re: Replacing ScrollRect</header>
    <body>As Scott put it, we strive hard to make sure that a CGContext function works across many context types. This is not always the case some time, but for now, there is no ScrollRect equivalent at the CGContext level. Scrolling is generally a windowing operation and bitmap context operation. If you are working on a window, ideally you would use HIViewScrollRect. If you are working at the bitmap context level, then Scott's suggestion of drawing the same image on top of itself at the appropriate offset will work. That being said, you could put in an Enhancement Request at bugreporter for this. Remember if it gets implemented, it will never work across all contexts. It will have to go through our API reviewer who rarely allows context specific functions in our API :-) Please comment (in some detail if possible) about your case and why you need the function. On Sep 20, 2004, at 2:50 PM, Bryan Pietrzak wrote: Attachment:</body>
  </mail>
  <mail>
    <header>Re: Replacing ScrollRect</header>
    <body>There isn't anything to find. HIViewScrollRect does some internal-only things in its implementation.</body>
  </mail>
  <mail>
    <header>Re: Replacing ScrollRect</header>
    <body>On Sep 20, 2004, at 3:51 PM, R. Scott Thompson wrote: Sure it does! From the link above: &amp;quot;There are two kinds of Quartz contexts: - PDF and printing contexts convert your drawing‚Äîsuch as vector art, text, and images‚Äîinto a structured PDF content stream. - Bitmap and window contexts rasterize your drawing immediately and It's reasonable for me to assume in this context (ha!) that the type of context I'm working with will always be of the rasterized kind. That's a reasonable constraint for the problem I'm trying to solve. So, from that point of view, it makes plenty of sense to ask if there is a method to scroll pixels around. I'll grant you, what I'm doing is not ideal, but I have a large existing body of code (PAIGE from DataPak Systems fwiw) that is VERY abstracted away from the rest of our app and I need to convert it from QuickDraw to Quartz. I'd like to do so with a minimal amount of re-architecting of the library. It may be that in this particular case there simply is not a good one-to-one mapping of the old QD logic to Quartz, and if that's the case, then I'm ok with that, I'll bite the bullet and look at architectural changes as needed. I was just hoping that since HIViewScrollRect existed there may be an analogous CGScrollRect</body>
  </mail>
  <mail>
    <header>Re: Replacing ScrollRect</header>
    <body>What you're asking for doesn't make much sense.  CGContexts can exist for devices that don't have the concept of pixels (and therefore don't have the concept of scrolling pixels).  For those contexts something like CGScrollRect wouldn't make any sense (e. g. what does it mean to scroll a context that is used for drawing a PDF file). If you have a context that you know is going to a bitmap then you can use CGImage to create an image from that bitmap and use CGDrawImage to replicate the image somewhere else (i.e. in a scrolled position). Tiger will have some useful tools that make it easier to create CGImages from parts of other images. Beyond that you're pretty much left with the solution of redrawing the entire canvas. Attachment:</body>
  </mail>
  <mail>
    <header>Re: Replacing ScrollRect</header>
    <body>On Sep 20, 2004, at 3:21 PM, David McLeod wrote: It probably does what I need, but it's at higher level than the code I'm converting. I should have been more clear about this, but at the level of abstraction I'm working with, all I have is a CGContextRef -- and whether it comes from an HIView in a window or an offscreen bitmap context I don't really know. I was hoping that HIViewScrollRect might be built on top of some kind of CGScrollRect, or that CGScrollRect might be available as sample code or something (didn't see anything though in a quick search).</body>
  </mail>
  <mail>
    <header>Re: Replacing ScrollRect</header>
    <body>Does HIViewScrollRect do what you need?</body>
  </mail>
  <mail>
    <header>Replacing ScrollRect</header>
    <body>I'm working to convert an old QuickDraw-based app and while I've seen the Transitioning to Quartz stuff QuickDrawToQuartz2D/index.html&amp;gt; it doesn't mention ScrollRect. Is there any particular &amp;quot;best practice&amp;quot; when it comes to moving away from ScrollRect when moving to Quartz? Considering the compositing behavior of quartz, I almost feel like it'd be best to not even bother, but to just invalidate the area and let everything redraw. But the old QD guy in me cringes at that too :) fwiw, this is in a carbon app that will be using compositing windows along with HIViews.</body>
  </mail>
  <mail>
    <header>RE: flashing overlay window</header>
    <body>Thanks.  We'll have to get some time to look into this. andy -----Original Message----- From: R. Scott Thompson [] Sent: Monday, September 20, 2004 11:38 AM To: Andy Skinner Cc: email@hidden Subject: Re: flashing overlay window Hmmm...  Sounds like you need to do just a bit more Mac-specific work. Basically what it amounts to is that you should only be doing drawing in response to events from the system.  You probably won't have to handle any events other than the ones to draw but if you don't handle those events properly then you're likely to run into trouble. The amount of stuff you have to add is pretty minimal.  You need to create an event handling proc to handle the drawing (or to call your drawing routines) then you have to learn enough to attach that event handler proc to the window.  No need to buy a book, you could probably documentation at Apple's web site: Carbon_Event_Manager/Intro/chapter_16_section_1.html#//apple_ref/doc/ The trickiest part is the fact that you're working with an overlay window.  Overlay windows don't follow the same update model that document windows follow.  What I might recommend is installing a handler for kEventWindowDraw to the root view of the window (gotten using GetRootControl) and then adding kWindowCompositingAttribute to your window attributes. Another way to handle it would be to override the kEventWindowPaint event.  (usually, for document and floating windows you'd override kEventWindowDrawContent) Somewhere in here you need to set the window up to handle events. Move the drawing commands into the event handler.  What will happen is, during ShowWindow, your event handler to draw the contents of the window before it is actually put on the screen.  You will draw the window's back buffer and then the system can blit it forward (in all it's transparent glory) in one operation.  That should avoid the flicker. Scott</body>
  </mail>
  <mail>
    <header>Re: flashing overlay window</header>
    <body>On Sep 20, 2004, at 9:25 AM, Andy Skinner wrote: Hmmm...  Sounds like you need to do just a bit more Mac-specific work. Basically what it amounts to is that you should only be doing drawing in response to events from the system.  You probably won't have to handle any events other than the ones to draw but if you don't handle those events properly then you're likely to run into trouble. The amount of stuff you have to add is pretty minimal.  You need to create an event handling proc to handle the drawing (or to call your drawing routines) then you have to learn enough to attach that event handler proc to the window.  No need to buy a book, you could probably documentation at Apple's web site: Carbon_Event_Manager/Intro/chapter_16_section_1.html#//apple_ref/doc/ The trickiest part is the fact that you're working with an overlay window.  Overlay windows don't follow the same update model that document windows follow.  What I might recommend is installing a handler for kEventWindowDraw to the root view of the window (gotten using GetRootControl) and then adding kWindowCompositingAttribute to your window attributes. Another way to handle it would be to override the kEventWindowPaint event.  (usually, for document and floating windows you'd override kEventWindowDrawContent) Somewhere in here you need to set the window up to handle events. Move the drawing commands into the event handler.  What will happen is, during ShowWindow, your event handler to draw the contents of the window before it is actually put on the screen.  You will draw the window's back buffer and then the system can blit it forward (in all it's transparent glory) in one operation.  That should avoid the flicker. Attachment:</body>
  </mail>
  <mail>
    <header>RE: flashing overlay window</header>
    <body>I'll have to look into these issues.  The program is not a very Mac-oriented one, and uses much from X and/or Java (depending on the situation).  This little bit to do the overlay window is pretty much seperate from everything else.  If we need to handle events as well as just opening the window and drawing, then I'll probably need to get a book and look further.  (Got a book recommendation? :) We create the window with: =================== // Create a window. WindowAttributes overlayAttributes = kWindowNoShadowAttribute | kWindowIgnoreClicksAttribute | OSStatus err = CreateNewWindow( kOverlayWindowClass, =================== We've tried moving the ShowWindow around different places, and it doesn't seem to help. We clear the window with CGContextClearRect(). We draw into the window with  CGContextMoveToPoint() and CGContextAddLineToPoint() and CGContextStrokePath(). thanks, andy -----Original Message----- From: R. Scott Thompson [] Sent: Friday, September 17, 2004 10:20 AM To: Andy Skinner Cc: email@hidden Subject: Re: flashing overlay window How are you drawing in the window?  What events are you catching to do the drawing? You should override kEventWindowPaint and probably kEventWindowUpdate. If you perform your drawing in those routines then the overlay window will draw inside of the call to ShowWindow just before the window is put onto the screen.  That way the clear happens before the window is first displayed. If you don't respond to those events, however, the window may come up opaque and won't actually clear until you do something later to cause it to draw. Scott You need to override the carbon events. Your attributes look fine to me. Scott</body>
  </mail>
  <mail>
    <header>bug using image masks with dataWithPDFInsideRect</header>
    <body>imageMask = CGImageMaskCreate( width, height, 8, 8, to get pdf data for the pasteboard the image masks wind up shown as black rectangles when they are pasted. All of my other objects, Bezier paths and full color images, render properly.</body>
  </mail>
  <mail>
    <header>Re: line thickness</header>
    <body>Yes, but I using integer so the smallest value could not be between 0 and 1. Now It works. Thanks Vincent</body>
  </mail>
  <mail>
    <header>Re: line thickness</header>
    <body>&amp;gt; Hi I don't completely understand your question/problem. Whats wrong with the &amp;quot;void CGContextSetLineWidth(CGContextRef context, float width);&amp;quot; function? If you want to draw the thinnest line possible, then you need to set the line width to 0 (atleast with the NSBezierPath class).</body>
  </mail>
  <mail>
    <header>Re: line thickness</header>
    <body>On Sep 19, 2004, at 2:03 AM, email@hidden wrote: Have you tried CGContextSetLineWidth()?  This takes a floating point value so you can set fractional point thicknesses, just as with PostScript.  The line drawn on a monitor will be anti-ailiased to appear thinner by being lighter, but on a higher resolution printer it will actually be thinner. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>line thickness</header>
    <body>Hi I would like to draw thin line on the printer. The thinest line I can draw are 1/72 inch. There must be a way to draw such line. Is there a way to control the thickness like &amp;quot;setlinewidth&amp;quot; in postscript? Regards Vincent</body>
  </mail>
  <mail>
    <header>Re: flashing overlay window</header>
    <body>How are you drawing in the window?  What events are you catching to do the drawing? You should override kEventWindowPaint and probably kEventWindowUpdate. If you perform your drawing in those routines then the overlay window will draw inside of the call to ShowWindow just before the window is put onto the screen.  That way the clear happens before the window is first displayed. If you don't respond to those events, however, the window may come up opaque and won't actually clear until you do something later to cause it to draw. You need to override the carbon events. Your attributes look fine to me. Attachment:</body>
  </mail>
  <mail>
    <header>profile for NSCalibratedRGBColorSpace?</header>
    <body>I asked this once before, but maybe now that vacation season is over someone is around who knows the answer. If I have an application that draws some things using calibrated RGB colors (obtained from [NSColor colorWithCalibratedRed:green:blue:alpha:]) and I print from it, Quartz will make a pdf as a spool file. The docs say, and an experiment by a friend who has some pre-press software agrees, that Quartz will put a ColorSync profile in that pdf. My question is... what profile ? Which profile should be the source profile here ? I write my own pdf's because I need some features that you don't get with the generic spool pdf's. I know how to write the profile into the pdf. But despite several times through the docs I don't know where to get the profile that corresponds to NSCalibratedRGBColorSpace.</body>
  </mail>
  <mail>
    <header>Re: using core image to apply filters to images</header>
    <body>FWIW, I had filed a DTS incident about a month ago in the hopes that an NDA list could exist to discuss Tiger.  Unfortunately, no luck with that.  Hopefully such a list can be created soon because I too have numerous CoreImage questions.  I'm currently re-writing my application foundation in Cocoa and was hoping to integrate CI before Tiger ships to the public.  Isn't that the ultimate goal (i.e. to have many applications ship alongside the OS update that take advantages of hot new OS features)? I encourage all to send feedback to Apple on this issue.  You wouldn't have to take the extreme approach I did with the DTS incident, but at least an enhancement request in Radar or an e-mail to the appropriate channel.  I think their may be a &amp;quot;developer relations&amp;quot; e-mail address floating out there. Finally, if you find the API confusing or lacking somehow, then also file API enhancement requests.  I've already filed one against CoreImage. -- Rick Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: using core image to apply filters to images</header>
    <body>On Sep 15, 2004, at 10:01 PM, Kevin Barabash wrote: Tiger is under NDA is cannot be discussed publicly beyond what has be announced by Apple themselves (specifics of CoreImage is still NDA).</body>
  </mail>
  <mail>
    <header>using core image to apply filters to images</header>
    <body>I've looked over the scant documentation provided with the developer preview of Tiger and I just don't understand how I'm supposed to apply a filter to an image.  There's two methods: - apply:arguments:options: - apply:k, . . . but both of them require a kernel and I just want to use one of the filters already provided with Tiger like Gaussian Blur.  Should I just pass &amp;quot;nil&amp;quot; as the kernel or what?</body>
  </mail>
  <mail>
    <header>Re: Captur of Mouse event for any window</header>
    <body>Attachment:</body>
  </mail>
  <mail>
    <header>Captur of Mouse event for any window</header>
    <body>Hello All, I have to write an application  wherein I have to capture mouse drag, mouse click , region selection  event on  any of the opened window whether that window is a desktop , or any application's window. So basically I want my application to be running in background that can capture mouse events for any window opened on the screen . I will be thankful if you can let me know the direction where  I should move, any API calls or documentation that can help me develop this application . Thanking You</body>
  </mail>
  <mail>
    <header>Re: QuickDraw Transition vs. OpenGL</header>
    <body>Something we are working on. While QD is deprecated, there are still other parts of Carbon that still rely on QD data structures. Hopefully for Leopard more of this is cleared up so that the QD dependency is minimized or eliminated.</body>
  </mail>
  <mail>
    <header>CoreImage vs. OpenGL GLSL questions</header>
    <body>I am new to development under OSX, and trying to come up to speed, as I will start contributing to an OSX-based project fairly soon. I need to implement some simple image transforms (wavelets) for GPU acceleration, and I am in the process of learning GLSL for this purpose. However, I was informed on the OpenGL list that there is currently no acceleration for GLSL under OSX (disclaimer: this was not a statement from an Apple employee). In looking over developer documentation, I came across and skimmed through the file on CoreImage. In particular, I noticed that you can specify kernels using a subset of GLSL, and that CoreImage currently supports hardware acceleration (with certain conditions). How difficult would it be to implement a wavelet--take the simple Haar for starters--in CoreImage? From what I've read it looks like it can be done, but I don't know for sure and I don't want to waste time running down dead-end streets. Which would be more advantageous for us in the long run? OpenGL/GLSL, or CoreImage? Most importantly: is there an Apple person on this list who can point me to status updates/timetables/roadmaps regarding Apple's level of support for GPU acceleration (or provide information upon request, offline)? This information would be most valuable, as it would allow us to plan some things more concretely. -Josh Senecal</body>
  </mail>
  <mail>
    <header>QuickDraw Transition vs. OpenGL</header>
    <body>I could be wrong, but Carbon applications that use AGL use AGLDrawables that are CGrafPtr. With the Deprecation of QuickDraw, is there going to be an alternate mechanism for connecting OpenGL to a Carbon window?</body>
  </mail>
  <mail>
    <header>How to do PDF encryption/decryption?</header>
    <body>Hi, I want to enable my application to read encrypted PDF files. After I searched through the Quartz API, I found that there are only three functions related to PDF encryption: &amp;quot;CGPDFDocumentIsEncrypted&amp;quot;, &amp;quot;CGPDFDocumentIsUnlocked &amp;quot; and &amp;quot;CGPDFDocumentUnlockWithPassword&amp;quot;. I have tried to using these functions to access an encrypted PDF file. However, When I used &amp;quot;CGPDFDocumentCreateWithURL&amp;quot; to try to get a document reference from the URL of the PDF file, I got the following error: &amp;quot;unsupported security handler level: 4&amp;quot;. It seems the Quartz API doesnt support the latest PDF encryption yet. Is there any other way or other APIs available in Mac OS X 10.2 and 10.3 for decrypting and then reading encrypted PDF files. Can anybody give me some help on how to do it? regardsy Yu</body>
  </mail>
  <mail>
    <header>Re: Reusing CGImageRef</header>
    <body>you may be trying to be too efficient with CG.  i believe the optimizations that can be realized from reusing a CGImageRef result from either writing the image data once into a pdf document (an then referencing it indirectly multiple times), or potentially caching the image data on the gpu as a gl texture.  if you are changing the image data, neither of these optimizations are possible. now, if you are only rendering to screen, you may want to take a look at CGGlContexts, and if that's not enough...open gl; and specifically apple's texture range example:</body>
  </mail>
  <mail>
    <header>Reusing CGImageRef</header>
    <body>In my readings, I understand that for optimal graphics performance, I should reuse my CGImageRefs.  How would I go about reusing a CGImageRef whose data provider is updated with a different objects rgb data each 0.0033 seconds.   I have an array of rgbmatrix that update different parts of a view.  How do I use the power of the DirectAccess provider, create the image using the provider, and not have to release the image when I render the next part of the updated view (i.e. animated view).</body>
  </mail>
  <mail>
    <header>CGContextScaleCTM to scale a tile updated CGImage</header>
    <body>I have a NSView that I am updating small rect of with a CGImage.  My CGImage comes from a RGB matrix.  I want to be able to scale the image which is being updated every 0.0033 seconds.  Scaling would occur when the window is updated.  I figure the approach to take is to use the CGContextScaleCTM method to update the current Transformation matrix of the view.  Anyone have a suggestion on how to use CGContextScaleCTM and also maintain the high update rate I am currently rendering.  In particular how do I scale the overall image when I am specifying the location of the small rects that update the image.</body>
  </mail>
  <mail>
    <header>Re: CGImageImport from Python</header>
    <body>CGImageImport does this by calling QuickTime (GraphicsImportCreateCGImage). So it should be whatever GraphicsImportCreateCGImage supports.</body>
  </mail>
  <mail>
    <header>CGImageImport from Python</header>
    <body>I note that the python sample on Panther uses CGImageImport to read an image out of a file. Is that routine (CGImageImport) an artifact of Python or is it merely a wrapper for the routines like CGImageCreateWithJPEGDataProvider?  What types of files will CGImageImport accept?</body>
  </mail>
  <mail>
    <header>Re: Fastest way to render NSString to screen?</header>
    <body>On Oct 27, 2004, at 11:28 AM, Maksim Rogov wrote: The code sample posted the other day should work... instead of using the context of the offscreen, just use the context returned by [[NSGraphicsContext currentContext] graphicsPort] Using CGContextShowText is not going to work out well for general NSStrings.  So long as your strings are ASCII I bet it will work, but if you ever intend to show non-ascii text then you may be in trouble.</body>
  </mail>
  <mail>
    <header>Re: Fastest way to render NSString to screen?</header>
    <body>Would anyone happen to have an example using the ATSUI to render text onto an NSView or an NSImage or something alike? And another question, using NSLayoutManager would be more efficient than using CGContextShowText? Or the other way around? Right now I am using CGContextShowText, however most of my CPU is now wasted by: Can I avoid having to do this every frame somehow? Thanks! ~ Maksim -- Maksim Rogov Nullriver Software -- Maksim Rogov Nullriver Software</body>
  </mail>
  <mail>
    <header>Re: Fastest way to render NSString to screen?</header>
    <body>Would anyone happen to have an example using the ATSUI to render text onto an NSView or an NSImage or something alike?</body>
  </mail>
  <mail>
    <header>Re: Fastest way to render NSString to screen?</header>
    <body>I got significantly better performance by moving to a CGBitmapContext and using ATSUI to render the attributed text. In my case though, I create OpenGL textures out of the rendered text to composite into a GL scene. So much of the improvement was due to the bitmap being ARGB and statically allocated in AGP-mapped memory, whereas with NSString and NSBitmapImageRep you have a series of runtime allocations (not AGP mappable) and RGBA-&amp;gt;ARGB conversions.</body>
  </mail>
  <mail>
    <header>Re: Fastest way to render NSString to screen?</header>
    <body>One more thing to add to this:  The Worm example in /Developer/Examples/AppKit also demonstrates ways to speed up drawing, including this technique. Ali Begin forwarded message: October 26, 2004 12:29:53 PM PDT Re: Fastest way to render NSString to screen? What I am trying to do is render an NSString onto the screen as optimally as possible. The text is attributed (font, underline, color) but does not wrap, its usually about 2-5 words in length. I need to draw a bunch (~ 10) of them per frame (at ¬†60 frames per second). NSString drawAtPoint:... consumes too much CPU. Could anyone please help me out in any way? Thanks so much for your time!</body>
  </mail>
  <mail>
    <header>Re: Fastest way to render NSString to screen?</header>
    <body>What I am trying to do is render an NSString onto the screen as optimally as possible. The text is attributed (font, underline, color) but does not wrap, its usually about 2-5 words in length. I need to draw a bunch (~ 10) of them per frame (at ¬†60 frames per second). NSString drawAtPoint:... consumes too much CPU. Could anyone please help me out in any way? Thanks so much for your time!</body>
  </mail>
  <mail>
    <header>Fastest way to render NSString to screen?</header>
    <body>What I am trying to do is render an NSString onto the screen as optimally as possible. The text is attributed (font, underline, color) but does not wrap, its usually about 2-5 words in length. I need to draw a bunch (~ 10) of them per frame (at ¬†60 frames per second). NSString drawAtPoint:... consumes too much CPU. Could anyone please help me out in any way? Thanks so much for your time! ~ Maksim&amp;lt;/x-tad-bigger&amp;gt; _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Ovals and Rounded Rects</header>
    <body>On Oct 25, 2004, at 12:56 PM, Bryan Pietrzak wrote: The problem was that I was trying to put a 3 pt rounded rect frame around my HIView. I knew the line was center based and would be clipped, so I did this to account for that: Well... color me stupid. It would help if I actually, really, truly, honestly, inset the bounds. CGRectInset doesn't take a pointer to the bounds to be changed, it returns the new bounds as a result. Doh! Changed to bounds = CGRectInset() and everything works as expected.</body>
  </mail>
  <mail>
    <header>Ovals and Rounded Rects</header>
    <body>Are there any other sources of algorithms for Quartz for drawing circles/ovals and rounded rects? The code in the above example creates some very ugly results when using line widths other than 1.0 (even then it's not very appealing). void UContext::AddOval(CGPoint center, float a, float b) I seem to remember someone else posting a different method of doing ovals and rounded rects, but I couldn't find anything in the archives.</body>
  </mail>
  <mail>
    <header>Re: how to draw art box content</header>
    <body>Unfortunately, CGContextDrawPDFDocument only allows you to specify the destination rectangle (artBox in your code), not the source rectangle. It always uses the media box as the source rectangle. Most likely, there's another (private) function that CGContextDrawPDFDocument is calling that does accept a source rectangle. That's the function I need.</body>
  </mail>
  <mail>
    <header>Re: how to draw art box content</header>
    <body>The following (which would require Panther) should work: CGContextRef		theContext = ... CGPDFDocumentRef	thePDFDocument = ... size_t				thePageNumber = ... CGPDFPageRef		thePageRef = CGPDFDocumentGetPage (thePDFDocument, CGContextDrawPDFDocument (theContext, artBox, thePDFDocument, ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: how to draw art box content</header>
    <body>On Oct 20, 2004, at 3:35 PM, Derek Clegg wrote: According to the PDF 1.4 spec, there are no constraints on the size and location of the art box  relative to the media box (or any other box). Generally speaking, the art box is clipped to the media box, unless it's being placed into another application, in which case &amp;quot;the media box is ignored&amp;quot; and the art box ( or crop box) is used instead. That's the usage I need. (See PDF 1.4 spec, pp. 676, 679.) I know Apple's PDF library can do what I need, because Motion is able to render elements outside the media box (but inside the art box) and it uses that library. I just don't know how they're doing. My fear is that they're using a private API. :-(</body>
  </mail>
  <mail>
    <header>Re: how to draw art box content</header>
    <body>The &amp;quot;content that lies in the media box&amp;quot; is the entire content of the PDF page.  The art box is always (or should always) be entirely contained with in the media box.</body>
  </mail>
  <mail>
    <header>how to draw art box content</header>
    <body>My application needs to be able to draw the entire content of a PDF page (i.e. the content in the art box), not just the content that lies in the media box, into a Quartz context (in my case, a PDF context). Is there any way to do this with the Panther API? A quick Google search and some reasonably complete API digging didn't turn up anything.</body>
  </mail>
  <mail>
    <header>Re: Question on Quartz PDF</header>
    <body>CGPDFDocumentUnlockWithPassword() Manfred</body>
  </mail>
  <mail>
    <header>Question on Quartz PDF</header>
    <body>Hi, I am working on supporting PDF in my application. I noticed that there are some API functions in Quartz for processing PDF files, but it seems quite limited. Can anyone help me on this question: with the APIs available in Mac OS X 10.2 and 10.3, am I  able to support display of encrypted PDF files, or is there any way to decrypt an encrypted PDF file? regards, Yu Liang</body>
  </mail>
  <mail>
    <header>bmp type image resolution change</header>
    <body>I wonder if there is any method to save bmp file into different resolution other than 72 dpi. bmpDate can be created&amp;nbsp;by QuickTime sample code&amp;nbsp;but I can't save other resolution than 72 dpi. If anyone knows how to do it, please let me know. G. Kim</body>
  </mail>
  <mail>
    <header>Re: Using CG private APIs (Was: Anti-Aliasing Quality when	Drawing	PDF)</header>
    <body>you can specify that your dummy prototype is for a weak reference:</body>
  </mail>
  <mail>
    <header>Re: Using CG private APIs (Was: Anti-Aliasing Quality when	Drawing	PDF)</header>
    <body>for plain C functions? What will dyld do if this function is removed or the signature changes in 10.4 (or 10.3.6 for that matter)? Basically, is there any semi-safe way to check that this API exists before using it?</body>
  </mail>
  <mail>
    <header>[solved] pdf page fails to draw in pdf context</header>
    <body>Thanks, that fixed it. I didn't suspect that because most of the file is still written properly when I return zero. Odd, huh? Perhaps the caller only checks the return value if it has more than one chunk of data to send. Anyway, thanks. Regards, Eric Ocean you need to return the number of bytes written from putBytes. size_t putBytes( *buffer, size_t count)</body>
  </mail>
  <mail>
    <header>Re: pdf page fails to draw in pdf context</header>
    <body>deja vu... *buffer, size_t count)</body>
  </mail>
  <mail>
    <header>pdf page fails to draw in pdf context</header>
    <body>...but the same page will draw into a bitmap context. I've included a short program that demonstrates the problem. I'm running Mac OS X 10.3.5 7M34 and Xcode 1.5. I badly need a workaround for this. I've tried Google as well as both Quartz pdf page drawing functions. I also tried to search the quartz-dev archives, but was unable to find a search page for it. I've tried running the app below with different pdf files. I'm out of ideas. Thanks for any help. Regards, Eric Ocean ------- To use: create a new Cocoa app and replace the entire contents of main.m with the text that follows. Modify the two macros (see source). Hit Run and examine the Run Log; the test files should also appear on your desktop. I've chosen funky names so they won't accidently overwrite something important, but you better check anyway... NSString *VALID_PDF_ABSOLUTE_PATH = size_t putBytes( *buffer, size_t count) * argv[]) gfx_context = CGBitmapContextCreate(	bitmapData, pixelsWide, pixelsHigh, , bitmapBytesPerRow, colorSpace, NSBitmapImageRep *imageRep = [[NSBitmapImageRep alloc]	initWithBitmapDataPlanes:&amp;amp;planes pixelsWide:pixelsWide pixelsHigh:pixelsHigh bitsPerSample: samplesPerPixel: hasAlpha: isPlanar: colorSpaceName:NSDeviceRGBColorSpace bytesPerRow:bitmapBytesPerRow [[image TIFFRepresentation]</body>
  </mail>
  <mail>
    <header>Re: CGColorSpaceCreateIndexed and CGBitmapContextCreate</header>
    <body>ben, if you already have the indexed image samples and simply want to render them, you could create a CGImageRef with the indexed colorspace around your pixel data, and then draw that indexed image either into the drawing context: (in cocoa: or into your own custom rgb/cmyk bitmap context or pdf context. regards, nibs David, Thanks for your input.  What I have is a series of 20,000 row X column byte arrays containing rgb values.  My data provides a custom colortable that changes throughout the rendering of data.  I have written code that indexes into my color table and fills a pixel matrix; however, this process over the course of 20,000 frames is being very time intensive.  What would be the best way, Quartz, NSImage, etc to index into my colortable and create the frames for display? Thanks, Ben Quartz does not support contexts that represent indexed color. The indexed color space can be used as a source color space for drawing images and, while unusual, you can use it as the current fill and stroke color space. The following document lists the different contexts which Quartz supports and what version of Mac OS X they became available. http://developer.apple.com/qa/qa2001/qa1037.html Hope this helps, David All, I am trying to create a CGBitMapContext with my own index color table (the data provides a very small colortable of its own).¬† However, whenever I call CGBitmapContextCreate after creating my CGColorSpaceCreateIndexed it ruturns nil;¬† Strange thing is, if I pass a as the ColorSpaceRef it successfully returns the context.¬† Any suggestions would be great. Regards, Ben Dunton Below is my code: ¬†colorSpace = CGColorSpaceCreateIndexed(CGColorSpaceCreateDeviceRGB(), ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† , ¬† ¬†  ) ¬† ¬† ¬† ¬† context = CGBitmapContextCreate (bitmapData, ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† CDG_FRAMEWIDTH, ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† CDG_FRAMEHEIGHT, ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† bitmapBytesPerRow, ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† colorSpace, ¬† ¬† ¬† ¬† ) ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†¬† ¬† ¬† ¬† ¬† ¬† ¬†_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list¬† ¬† ¬† (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Using CG private APIs (Was: Anti-Aliasing Quality when Drawing	PDF)</header>
    <body>for plain C functions? What will dyld do if this function is removed or the signature changes in 10.4 (or 10.3.6 for that matter)? Basically, is there any semi-safe way to check that this API exists before using it?</body>
  </mail>
  <mail>
    <header>Re: CGPostKeyboardEvent() with dead keys;</header>
    <body>No offense taken here, I just found it funny to read this message only minutes after replying to a reply on your question on the Carbon list. Unless of course the CG folks do have something useful to say on this, after all, you are right CGRemote is part of CG. david.</body>
  </mail>
  <mail>
    <header>Re: CGPostKeyboardEvent() with dead keys;</header>
    <body>When I posted to the Carbon one, I got a private reply from an Apple guy suggesting that I also post it in the Quartz forum since the CGRemoteOperation header is actually part of the quartz framework (this is what he said in a nutshell). So basically I repackaged the original email that said that no on replied on the Accessibility list and added the &amp;quot;Carbon&amp;quot; into the same sentence. You are right, and I acted a bit hastily...I didn't mean to offend anyone on the Carbon list. For anyone insterested, there is good discussion going on on this topic in the Carbon-dev mailing list, so I suggest we quash this thread in this quartz-dev list. Sorry for all the confusion everyone... On Oct 14, 2004, at 3:26 PM, David Niemeijer wrote:</body>
  </mail>
  <mail>
    <header>Re: image resolution change smoothly</header>
    <body>By default the resolution of an NSImage is taken into consideration when displaying it in an NSView. So a 720x720 pixel image which is 72dpi will occupy 720x720 pixels on the screen; but if the image were 144dpi, then it would occupy only 360x360 pixels. The image would be displayed by scaling, and you can change the default image interpolation parameters used in this scaling with the NSGraphicsContext API setImageInterpolation:. The size method on NSImage or NSBitmapImageRep return the actual size it will occupy (in points), taking dpi into account. If you want the image to be displayed such that each image bitmap pixel occupies 1 pixel on the screen, you would need to either set the size, or use the appropriate NSImage or NSBitmapImageRep method which lets you specify the destination rectangle size you want the image to be displayed in. Begin forwarded message: Hi, ¬† I'm working on an obj-C cocoa application for editing images. Is there a way¬†to¬†change image resolution(dpi) using interpolation smoothly¬†by Quartz? ¬† And I'm curious why some images other than 72 dpi¬†on NSView¬†look fuzzy. Please let me know how to display the images clearly. ¬† Thanks</body>
  </mail>
  <mail>
    <header>image resolution change smoothly</header>
    <body>I'm working on an obj-C cocoa application for editing images. Is there a way&amp;nbsp;to&amp;nbsp;change image resolution(dpi) using interpolation smoothly&amp;nbsp;by Quartz? And I'm curious why some images other than 72 dpi&amp;nbsp;on NSView&amp;nbsp;look fuzzy. Please let me know how to display the images clearly. &lt;FONT size=2&gt;Thanks</body>
  </mail>
  <mail>
    <header>Re: CGPostKeyboardEvent() with dead keys;</header>
    <body>Matt, there are two same day Carbon list replies there waiting for you. They may not be offering the solution, but to say no one replied is a bit of an overstatement ;-) david.</body>
  </mail>
  <mail>
    <header>CGPostKeyboardEvent() with dead keys;</header>
    <body>I sent this to the Carbon and Accessibility lists, but no one there replied, so I thought I'd quiz the Quartz experts here. I am trying to emulate key strokes with the CGPostKeyboardEvent() API. I am trying to emulate the following key on the US keyboard: [√°] (without braces). This is done by hitting Option+E and then the A key. More precisely, this is done by sending keycodes 58 (for Option) and 14 (for E), and then 0 (for A). I have tried the following code and it does not work: ...all that is typed is the letter [a]. It seems that doing this consumes the dead key that you get if you physically do this on your keyboard. If I try emulating the dead key twice: ...it does work. I've also tried this repeat-dead-key trick on dead keys that don't require modifiers (see the Bulgarian keyboard and try typing [`] (back-tick/grave) + [w]) and that trick doesn't work. An example (using the Bulgarian keyboard): ...doesn't work, ...doesn't work, ...does work Does anyone know what's up?</body>
  </mail>
  <mail>
    <header>Take raw image data (argb) and create an NSImage</header>
    <body>All, I am trying to create a NSImage from RAW argb data.  Below is my code (its experimental).  My view launches; however I don't get the image I am expecting. Thanks, Ben pixelsHigh: hasAlpha: colorSpaceName:NSCalibratedRGBColorSpace</body>
  </mail>
  <mail>
    <header>Using CG private APIs (Was: Anti-Aliasing Quality when Drawing PDF)</header>
    <body>I noticed that Preview draws embedded bitmap images anti-aliased, but when I draw PDFs myself bitmap images are aliased. How do I control the anti-aliasing quality when drawing PDF? CGContextSetInterpolationQuality() on the context has no apparent effect.</body>
  </mail>
  <mail>
    <header>Re: one pixel discrepancy &amp;amp; antialiasin</header>
    <body>Are you drawing your lines on integer coordinates? Like (1,1) and (2,5)? Assuming the transformation matrix is set up to map one point to one pixel then a pixel is represented by a unit square with the edges on integer coordinates. E.G: The rectangle (0,0)-(1,1) is a pixel with it's center at (0.5,0.5). If a one point wide line was drawn from (1,1) to (2,1) then the pixel coverage of the line would cover the rectange (0,0)-(3,2). And the line would likely be drawn lighter than you'd expect since none of the pixels would have 100% coverage by the line. -Corey O'Connor (Not sure if I should write about pixel coverage after a 10 hr day... :-P)</body>
  </mail>
  <mail>
    <header>Re: How to control mouse movement with CGEventSetLocation?</header>
    <body>On May 30, 2007, at 8:09 AM, Bill Cheeseman wrote: No, CGAssociateMouseAndMouseCursorPosition() only works for the foreground app.  To do what you want from a daemon or background process isn't easily possible.</body>
  </mail>
  <mail>
    <header>Re: How to control mouse movement with CGEventSetLocation?</header>
    <body>Thanks, that's very helpful. I had considered using CGAssociateMouseAndMouseCursorPosition(), but the documentation and some older mailing list traffic suggest that it only works in the active application. I'm using event taps in order to get global control of user input, even while my app is in the background. Will this function work for my purposes? (I will of course try it as soon as I get home, but at the moment I can only ask.) -- Bill</body>
  </mail>
  <mail>
    <header>Re: How to control mouse movement with CGEventSetLocation?</header>
    <body>On May 30, 2007, at 3:44 AM, Bill Cheeseman wrote: In this case, there actually are two processes controlling mouse cursor movement, your process and the HID system.  You can ask the HID system to 'let go' of the mouse cursor by calling: CG_EXTERN CGError CGAssociateMouseAndMouseCursorPosition(boolean_t CGAssociateMouseAndMouseCursorPosition(false) tells the HID system to stop updating the mouse cursor position.  Your tap will then see mouse events with kCGMouseEventDeltaX and  kCGMouseEventDeltaY values, but the location returned by CGEventGetLocation() in the incoming events will be unchanged. You can then check the incoming event for modifier key state by examining the flags returned by CGEventGetFlags(), and when you see the desired state, add the desired deltaX and deltaY to the location and set it in the event via CGEventSetLocation().</body>
  </mail>
  <mail>
    <header>How to control mouse movement with CGEventSetLocation?</header>
    <body>I want to force the cursor to remain in one location on the screen unless a modifier key is down. However, Quartz event tap techniques that work to block keystrokes don't seem to work to block mouse movement. Specifically, returning null in my event taps callback function for the mouse moved event doesn't stop the mouse from moving. When I try to work around that issue by using CGEventSetLocation to set the mouse location back to its value at the previous mouse moved event, by subtracting the mouse delta values kCGMouseEventDeltaX and kCGMouseEventDeltaY, I get a cursor that rapidly alternates between two different onscreen locations. The cursor behaves as if there are two separate system processes controlling mouse movement, one that can be controlled with event taps and the other that cannot. Is there anything I can do? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: Speeding drawing of images as backgrounds</header>
    <body>In Carbon you would create a window group, put your main and overlay window into it, then turn on the kWindowGroupAttrMoveTogether attribute so that they stick together. You probably also want kWindowGroupAttrLayerTogether, since the user will see the group as a single window. as you're using Nano you can just call SetContentTransparent, which overrides the kEventWindowGetRegion and kEventControlDraw events to make the window content area transparent. I've used this technique (couple of transparent windows, grouped together, letting the window server handle compositing) to improve performance before, and it can work very well. -dair ___________________________________________________</body>
  </mail>
  <mail>
    <header>Re: Newbie - Transparent Path stroking/filling question [Solved]</header>
    <body>You need to write code that generates a stroked path. It's not trivial, but you can probably find examples on the web. You'll also need to get the actual path points out of the CGPath, which is simple. Use CGPathApply and supply a function that builds the list of points and kinds. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Newbie - Transparent Path stroking/filling question [Solved]</header>
    <body>On 13 Jul 2006, at 15:11, Vinay Prabhu wrote: You'd have to deal with image masks etc. Thankfully I require 10.4 or later so I didn't get around to doing it that way. Matt Gough</body>
  </mail>
  <mail>
    <header>RE: Newbie - Transparent Path stroking/filling question [Solved]</header>
    <body>This code works on Tiger. But on OS Panther, most of the masking API's are not supported. How to create the mask for path on OS Panther? Regards Vinay -----Original Message----- From: quartz-dev-bounces+vinayprabhu=email@hidden [ n Behalf Of Matt Gough Sent: Wednesday, July 12, 2006 8:31 PM To: Nick Nallick Cc: Quartz; Steve Mills Subject: Re: Newbie - Transparent Path stroking/filling question [Solved] Thanks everyone, in the end Nick's solution did exactly what I wanted: In longhand for future reference: CGContextRestoreGState(ctx); // To restore the old clip Obviously this could be optimized using a CGPathRef instead of calling MySetupPath multiple times. Thanks once again Matt Gough</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate</header>
    <body>Yes, it's pretty much the way to go. Load the image , apply the filter or whatever, draw the output CIIMage onto a CIContext created with a CGBitmapContext. At least that's the way I do it. It usually worked when using: Also I have never been able to create an image with bitsPerComponent = 16. It works fine when using 8 or 32 bits. So basically I force 32 bits everytime the image's bits are set to 16.  I believe(?) it 's is also safe to use 4 component (add alpha). I never tried with only three components. Le 12 juil. 06 √† 01:44, Tom Marchand a √©crit : ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Re: CIImage saving/Implementing Undo</header>
    <body>No I am not saying that you should have multiple copies. I am not even sure what you mean :-) There's a rather easy way to &amp;quot;duplicate&amp;quot; an accumulator and it's just like you do it in your code. Just create  a new accumulator and then do a setImage using the source accumulator. I am not very familiar with prepareWithInvocationTarget. I use registerUndoWithTarget which is fine here. [[self undoManager] registerUndoWithTarget: self This should be fine if you replace the complete accumulator. However this can be huge if every time you draw a 32 pixel stroke on a 3000x2000 image you backup the whole image... Of course if you wish to backup an area then you'd have to create a backup accumulator of a smaller size and use a CIAffineTransform filter to move that area into your rectangle. And then your undo method would juste use setImage on the correct area. Now, the picky trick is that using a setImage too fast might replace the image before it was applied. You can only be sure the content has changed after a drawImage. I had this problem when undoing/redoing fast with smaller areas using setImage. I had to create a 'forceRender' method that basically does a drawImage in some dummy CIContext. Le 11 juil. 06 √† 16:36, email@hidden a √©crit : ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Re: CIFilter Exception</header>
    <body>On Jul 12, 2006, at 5:04 PM, Frank Doepke wrote:</body>
  </mail>
  <mail>
    <header>Re: CIFilter Exception</header>
    <body>IRef is a CGImageRef.  Here's the code: int main (int argc, const char * argv[]) if(DRef) if(IRef) if(CGImageDestinationFinalize(DestFile)) -------------- Original message ----------------------</body>
  </mail>
  <mail>
    <header>Re: CIFilter Exception</header>
    <body>First question is, what is IRef as that is not clear from your code - it has to be a CIImage! Second by calling setDefaults you set all parameters to their default values. As CIColorControls has more keys than just the inputImage, you either need to set them or call setDefaults BEFORE setting the inputImage.</body>
  </mail>
  <mail>
    <header>CIFilter Exception</header>
    <body>Hi, I am having trouble applying a simple filter to an image.  Here's the code: This code results in the following exceptions: *** -[NSCFType _feImage]: selector not recognized  [self = 0x50a4b0] *** Uncaught exception: &amp;lt;NSInvalidArgumentException&amp;gt; *** -[NSCFType _feImage]: selector not recognized  [self = 0x50a4b0] I've tried different filters and found that anytime I change an input value from it's default, the exception is generated.  For example: the following code works: While the following code does not work: Thanks..</body>
  </mail>
  <mail>
    <header>Converting to CGPath from in-context paths</header>
    <body>If I want to replace my existing CGContext path creation code with code that creates CGPaths, is it simply a case of doing the following: QuickDrawToQuartz2D/tq_other/chapter_3_section_6.html CGPathRef createOvalPath(CGContextRef context, CGRect r) m = CGAffineTransformTranslate(m,r.origin.x + r.size.width/2, Matt Gough</body>
  </mail>
  <mail>
    <header>Re: Newbie - Transparent Path stroking/filling question [Solved]</header>
    <body>Thanks everyone, in the end Nick's solution did exactly what I wanted: Obviously this could be optimized using a CGPathRef instead of calling MySetupPath multiple times. Matt Gough</body>
  </mail>
  <mail>
    <header>Re: Newbie - Transparent Path stroking/filling question</header>
    <body>On 12 Jul 2006, at 15:37, Steve Mills wrote: I don't know if this works in Quartz, but it doesn't work in PostScript because paths created from strokes have all sorts of internal structure resulting from the line joins and caps.</body>
  </mail>
  <mail>
    <header>Re: Newbie - Transparent Path stroking/filling question</header>
    <body>Subtract the path from a rectangular path whose bounds is the bounds of the path plus stroke slop. Of course, Quartz doesn't supply any such path operations, so you'd have to roll your own. Subtraction might be fairly simple, since the resulting path is really one rectangular contour and a second contour that's the original path. I think you only need to make sure the winding direction of each is the same or different (I can never remember which way does what). Basically: strokePath = the stroked path that Scott mentioned with CGContextReplacePathWithStrokedPath clip is now a path you can use to clip the context and draw the fill. Or I could be 100% wrong. :) _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Newbie - Transparent Path stroking/filling question</header>
    <body>On Jul 12, 2006, at 9:08 AM, Scott Thompson wrote: D'oh.  I was wrong.  Nick was right.  You want to invert the stroke path and clip to that. :-(</body>
  </mail>
  <mail>
    <header>Re: Newbie - Transparent Path stroking/filling question</header>
    <body>On Jul 12, 2006, at 3:40 AM, Matt Gough wrote: Unfortunately there's no easy way to do this with the standard drawing and clipping functions.  On Tiger you can set the current path to the equivalent of the stroke of the current path, so you can clip to the stroke.  However there's no way to perform an inverse clip function.  This would be a reasonable thing to do, but it's not part of PostScript so it's  not part of PDF or Quartz either. You could probably do it with a transparency layer or a CGLayerRef. In this case you'd want to open a layer, draw the fill, clip to the stroke, erase the clipped area (to transparent), draw the stroke, and close the layer. Nick</body>
  </mail>
  <mail>
    <header>Re: Newbie - Transparent Path stroking/filling question</header>
    <body>On Jul 12, 2006, at 4:40 AM, Matt Gough wrote: There is not a better way of doing that, I'm afraid.  The stroke of a path always straddles the path. This routine is a bit hard to explain, but basically it takes the current path and identifies which area of user space would be covered if you were to stroke that path.  It then constructs a new path that covers that exact same area and sets it to be the current path.  The long and short of it is that if you were to fill the context with the second path, you would draw the same area that would have been covered by a stroke on the original path. CGContextReplacePathWithStrokedPath(cgContext) CGContextFillPath(cgContext) Scott</body>
  </mail>
  <mail>
    <header>Newbie - Transparent Path stroking/filling question</header>
    <body>I have an arbitrary path (open or closed) that I want to fill and then stroke with a translucent color. However I don't want the fill to bleed through into the parts where the stroke intersects it. (i.e the fill should not conceptually overlap the stroke even though it does mathematically.) For simple rectangular paths, I can just use a separate path for the fill and stroke, but how can I go about this for an arbitrary path? I am guessing that I will need to create a clipping mask of the stroke's path and clip against its inverse to draw the fill, then draw the stroke with no clip. Unfortunately being a newbie, i know roughly what I need to do, but am not sure of the best way to implement it. Softchaos Limited</body>
  </mail>
  <mail>
    <header>Shader Sample Count To PDF</header>
    <body>I'm drawing a particular shader both to the screen and to a PDF context (e.g., for printing).  The gradient is near white with a smooth bump to gray and back to white in the first 10% or so. When drawing to the screen my shader function is being called 4097 times.  To a casual inspection the input value seems linear with an increment of 0.000244141.  When I draw the same shader, in the same way, to a PDF context my shader function is only called 8 times with an input increment of 0.142857. Because of the shape of this particular gradient the PDF sampling rate is filtering out the entire interesting part.  In other words, at zero the function returns white and at 14% the function returns white but between those values there is a large change that is completely missing in the PDF output.  I'm not aware of any way to influence how the shader picks its function query points.  How do I get this gradient to reproduce correctly to PDF? Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: CIImage saving/Implementing Undo</header>
    <body>Are you saying I should have multiple CIImageAccumulator's, and every time I update one, I apply the same update to all of them? Or is there a way to duplicate/copy an existing CIImageAccumulator? This is what I am trying to do: Right before my drawing command is executed I have this (imageAccumulator is the CIImageAccumulator for the drawing pad): CIImageAccumulator* imageUndo = [[CIImageAccumulator alloc] initWithExtent:[imageAccumulator extent] format:[imageAccumulator [[[[layerDelegate document] undoManager] This has no effect on the image. If I insert: 	NSLog(@&amp;quot;%@&amp;quot;, I can see that the same CIFilterShape is being used, they all seem to have the same content. On Jul 8, 2006, at 3:03 PM, email@hidden wrote:</body>
  </mail>
  <mail>
    <header>Re: Deadlock in CIMicroPaint</header>
    <body>But wasn't this fixed in 10.4.7? It doesn't seem to lock up anymore. At least not immediately like it did in 10.4.6. ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Deadlock in CIMicroPaint</header>
    <body>Resizing the window of the CIMicroPaint window results in what looks like a deadlock deep in quartz (backtrace below). While the lock up obviously shouldn't happen, I'm wondering if this is a bug (API misuse) in the sample or a bug in quartz. I'll submit a bug report in the latter case. This is on a MBP. #0  0x900251a7 in semaphore_wait_signal_trap () #1  0x90028d24 in pthread_cond_wait () #2  0x94001e22 in fe_buffer_begin () #3  0x94075e5d in image_accum_setup () #4  0x940005ca in fe_tree_render_apply () #5  0x93ffeeab in fe_tree_render () #6  0x93ffe157 in fe_tree_render_image () #7  0x93fde3fa in image_accum_render () #8  0x9407a663 in fe_tiled_buffer_foreach () #9  0x93fde53c in image_accum_sync () #10 0x94075e90 in image_accum_provide_buffer () #11 0x94001682 in fe_context_texture_load () #12 0x94001564 in image_buffer_texture_ref () #13 0x940792f0 in texture_retain () #14 0x94001143 in fe_texture_new () #15 0x94000d92 in fe_tree_create_texture () #16 0x940004fc in fe_tree_render_apply () #17 0x93ffeeab in fe_tree_render () #18 0x93ffee1a in fe_tree_render () #19 0x93ffe157 in fe_tree_render_image () #20 0x93fde3fa in image_accum_render () #21 0x9407a663 in fe_tiled_buffer_foreach () #22 0x93fde53c in image_accum_sync () #23 0x94075e90 in image_accum_provide_buffer () #24 0x94001682 in fe_context_texture_load () #25 0x94001564 in image_buffer_texture_ref () #26 0x940792f0 in texture_retain () #27 0x94001143 in fe_texture_new () #28 0x94000d92 in fe_tree_create_texture () #29 0x940004fc in fe_tree_render_apply () #30 0x93ffeeab in fe_tree_render () #31 0x93ffe157 in fe_tree_render_image () #32 0x93ffdd06 in fe_image_render_ () #33 0x93ffdc14 in fe_image_render () #34 0x93ffdb50 in -[CIOpenGLContextImpl renderAccel:matrix:bounds:] () #35 0x93ffc872 in -[CIOpenGLContextImpl render:] () #36 0x94014bf7 in -[CIContext drawImage:inRect:fromRect:] () #37 0x94016dc6 in -[CIContext drawImage:atPoint:fromRect:] () #38 0x000036ff in -[SampleCIView drawRect:] (self=0x34b890, SampleCIView.m:169 #39 0x9342260f in -[NSView _drawRect:clip:] () #40 0x93421669 in -[NSView _recursiveDisplayAllDirtyWithLockFocus:visRect:] () #41 0x934206d1 in -[NSView _recursiveDisplayRectIfNeededIgnoringOpacity:isVisibleRect:rectIsVisible RectForView:topView:] () #42 0x9342129f in -[NSView _recursiveDisplayRectIfNeededIgnoringOpacity:isVisibleRect:rectIsVisible RectForView:topView:] () #43 0x9342129f in -[NSView _recursiveDisplayRectIfNeededIgnoringOpacity:isVisibleRect:rectIsVisible RectForView:topView:] () #44 0x9341fdd8 in -[NSThemeFrame _recursiveDisplayRectIfNeededIgnoringOpacity:isVisibleRect:rectIsVisible RectForView:topView:] () #45 0x9341f5c4 in -[NSView _displayRectIgnoringOpacity:isVisibleRect:rectIsVisibleRectForView:] () #46 0x9341eef4 in -[NSView displayIfNeeded] () #47 0x934198ff in -[NSWindow _setFrameCommon:display:stashSize:] () #48 0x9341e352 in -[NSWindow setFrame:display:] () #49 0x93557783 in -[NSTitledFrame resizeWithEvent:] () #50 0x9355674a in -[NSTitledFrame mouseDown:] () #51 0x935565cf in -[NSThemeFrame mouseDown:] () #52 0x934804b7 in -[NSWindow sendEvent:] () #53 0x93472524 in -[NSApplication sendEvent:] () #54 0x9339d106 in -[NSApplication run] () #55 0x93391037 in NSApplicationMain () #56 0x000023d4 in main (argc=1, argv=0xbffff8bc) at /Developer/ Examples/Quartz/Core Image/CIMicroPaint/main.m:13 I'm</body>
  </mail>
  <mail>
    <header>OCR</header>
    <body>Is anybody aware of any Quartz based OCR libraries?</body>
  </mail>
  <mail>
    <header>Re: Drawing text using quartz</header>
    <body>On Jul 7, 2006, at 10:25 AM, Vinay Prabhu wrote: Yes, it is possible to do these things, but you'll have to learn some fairly complex APIs.  You'll probably want to use ATSUI or Cocoa to render the text.  To fill with a gradient you'll have to set the clip to the character shape and use a CGShader.  Since you're also planning to outline the characters it's probably best to convert the glyphs to paths, then use those paths to define the shader mask and outlines. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Determining if original and filtered image are identical?</header>
    <body>I'm running an image through a series of CIFilters, and want to find out if the filters are configured in such a way that they've changed the image. Aside from doing a pixel by pixel comparison, I can't find a good way. In some cases, defaults for CIFilters do alter the input image, and the output data from the CIFilters will always have formatting differences, making it so a simple image data comparison doesn't work. A pixel by pixel comparison is slow. Is there a better way to figure out if the result of the CIFilters is visually identical to the original image? Thanks, Colin</body>
  </mail>
  <mail>
    <header>Re: CIImage saving/Implementing Undo</header>
    <body>A CIImage is not an image that contains pixels, it is simply the result of a series of instructions to build it, the output of a CIFilter for example. So if you copy the CIImage you just copy those instructions, which when modified would modify the ouput. To keep the content of your image as it is, you would simply use another CIImageAccumulator to store each copy. That's what I do in my app and it works well.  It's not a good idea to use a NSBitmapRep because your accumulator could in the Graphics card memory, and to make an NSBitmapRep you have to copy it back to the main memory. Copying to a new CIImageaccumulator will be done instantly. ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Re: Clearing an accumulator without a second one?</header>
    <body>Well, I'd say the answer is no. Since every filter that outputs transparency (transform, deformation) is not able to overwrite pixels which are not transparent. Le 7 juil. 06 √† 18:36, samiam work a √©crit : ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Tone mapping floating point CIImage</header>
    <body>How do I specify my own tone-mapping (tone compression) function when drawing a CIImage created from a floating point GL texture? Do I need to write a CIFilter? Some details: I created the texture by using setTextureImageToPixelBuffer on a floating point NSOpenGLContext with an attached NSOpenGLPixelBuffer. The pixel format has NSOpenGLPFAPixelBuffer, NSOpenGLPFAColorFloat, and 128 bit color size plus a 24 bit depth buffer. Thx in advance, DW</body>
  </mail>
  <mail>
    <header>Drawing text using quartz</header>
    <body>Hi, I am developing a application, where I need to draw the text in the rect and apply various attributes to it. I could draw the text along with the characters border. Also separate color's for text and border. Is it possible to increase the border thickness for each characters? Also filling the gradient color's for text? Thanks in advance for help... Regards Vinay The information contained in this electronic message and any attachments to this message are intended for the exclusive use of the addressee(s)and may contain confidential or privileged information. If you are not the intended recipient, please notify the sender or email@hidden</body>
  </mail>
  <mail>
    <header>CIImage saving/Implementing Undo</header>
    <body>I have this application that has a drawing pad component.  The user's drawings are stored in a CIImageAccumulator. I am attempting to implement undo support, and it seems to me the best thing to do would be to store a copy of [CIImageAccumulator image] before each drawing command. However I found it impossible to retain a CIImage. Any copy always had the same [CIImage definition] object.  So if the original CIImage was modifed by the user drawing some more, all the CIImage's would be updated as well. I feel like I have a serious mis- understanding of something here.... Right now I am rendering the CIImage into a NSBitmapImageRep and storing that, and then using [CIImage initWithBitmapImageRep] to recreated the CIImage later. This works okay, but I am still wondering why I can't save the CIImage directly. Michael</body>
  </mail>
  <mail>
    <header>CIBlendWithMask only uses green?</header>
    <body>As the documentation says 'The CIBlendWithMask filter uses values from a grayscale mask'. Of course since we don't have any way to get One-Component images in CoreImage we have to use RGB input. I just noticed that if the mask is 100% or 100% red nothing is visible. Which leads me to believe that only the green component is used in this filter. Is this correct? Can we rely on this in future versions? ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Clearing an accumulator without a second one?</header>
    <body>I have been asking this same question for months, and everytime I just had to find a workaround (like getting used to it...) since nobody came up with another answer. Is it really *IMPOSSIBLE* to clear an accumulator without having a second accumulator of the same size? Do we really have to double memory usage just because of this? Isn't there anyway that a setImage would clear the contents of a zone in the accumulator using a CIFilter of some sort? Not only you can't fill with transparency using a CIConstantColor (just has no effect on the image), but you cannot even use an affinetransform to tile a transparent accu on the target accumulator. The goal being to have just one 512x512 image in order to clear other accumulators instead of a big one at least as big as the biggest. In an NSGraphicsContext you can set the composite method to NSCompositeCopy ... isn't there any equivalent that applies to setImage in an accumulator? ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>ROI not tilable</header>
    <body>I saw a couple of posts on the list about people having some issue with a CoreImage message saying: CoreImage: ROI not tilable. DIVX DOD [0,0 1024x778] ROI [0,0 34x27] ARGB_8 APPLY matrix_ DOD [0,0 1024x778] ROI [0,0 18x27] ARGB_8 IMAGE CIImage:0x3ab830 DOD [0,0 1024x778] ARGB_8 APPLY matrix_ DOD [0,0 1024x778] ROI [0,0 34x27] ARGB_8 0xf0484888 Has anyone had any feedback on what the issue is? I also have a bug (#4599144) placed, but so far I haven't seen any responses to my queries. Its kind of frustrating, since this code was working in January, and then I left it alone for a while and have just come back to it, only to find that it no longer works. I am in the process of testing it on various versions of 10.4 and Quicktime to see when the problem appeared. All, I know right now is that 10.4.2 worked.... M.</body>
  </mail>
  <mail>
    <header>RE:[SOLVED] Fill the oval with gradient color's</header>
    <body>Hi, with the few changes the code started working fine, 1) the restore was in wrong place, 2) While calculating start and end point, the rect should be recalculated with CGContextGetPathBoundingBox. CGContextTranslateCTM(currentContext, rect.origin.x + rect.size.width/2, CGRect currentPathRect = CGContextGetPathBoundingBox(currentContext);	//This is important for calculating start and end //point //CGContextClipToRect(currentContext , *(CGRect *)&amp;amp;rect);	//This is where the action happens CGShadingRef    myCGShading = CGShadingCreateAxial(colorspace, startPoint, Regards Vinay -----Original Message----- From: quartz-dev-bounces+vinayprabhu=email@hidden [ n Behalf Of Steve Mills Sent: Tuesday, July 04, 2006 10:35 PM To: email@hidden quartz-dev Subject: Re: Fill the oval with gradient color's Just a thought, but could it be that you have the Restore in the wrong place? If you draw a gradient and only see a solid fill of one of the colors from the gradient, it means the gradient is being drawn somewhere other than where you think it should be, and the rest of the context is filled with the start/end color. I'd try moving the Restore down one line. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>RE: Fill the oval with gradient color's</header>
    <body>Hi, I have a working shader code for filling the rectangle with gradient color's. I am using the same code to fill the oval, but it fails to draw gradient colors, :( Regards Vinay -----Original Message----- From: Nick Nallick [] Sent: Tuesday, July 04, 2006 9:21 PM To: Vinay Prabhu Cc: email@hidden Subject: Re: Fill the oval with gradient color's Your approach seems correct (at least without building it and testing every parameter).  If you're getting an oval with a solid color I'd probably suspect your shader function is doing something wrong.  You could try filling a rectangle instead of an oval to simplify your options in an attempt to rule things out.  You might also try a really simple shader function like one that simply returns the input value for the output in each channel. Regards, Nick Nallick The information contained in this electronic message and any attachments to this message are intended for the exclusive use of the addressee(s)and may contain confidential or privileged information. If you are not the intended recipient, please notify the sender or email@hidden</body>
  </mail>
  <mail>
    <header>Re: Fill the oval with gradient color's</header>
    <body>Just a thought, but could it be that you have the Restore in the wrong place? If you draw a gradient and only see a solid fill of one of the colors from the gradient, it means the gradient is being drawn somewhere other than where you think it should be, and the rest of the context is filled with the start/end color. I'd try moving the Restore down one line. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Fill the oval with gradient color's</header>
    <body>Your approach seems correct (at least without building it and testing every parameter).  If you're getting an oval with a solid color I'd probably suspect your shader function is doing something wrong.  You could try filling a rectangle instead of an oval to simplify your options in an attempt to rule things out.  You might also try a really simple shader function like one that simply returns the input value for the output in each channel. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Re: RE: Fill the oval with gradient color's</header>
    <body>CurveToPoint(ctx, r.x, r.y + r.h - b, r.x + a, r.y + r.h, r.x + r.w * CurveToPoint(ctx, r.x + r.w - a, r.y + r.h, r.x + r.w, r.y + r.h - b,</body>
  </mail>
  <mail>
    <header>RE: Fill the oval with gradient color's</header>
    <body>The API, CGContextAddEllipseInRect is supported for OS 10.4 and above. But my application is supported from OS 10.3. So I can't use above API. -----Original Message----- From: quartz-dev-bounces+vinayprabhu=email@hidden [ n Behalf Of Vinay Prabhu Sent: Tuesday, July 04, 2006 8:46 PM To: email@hidden Subject: Fill the oval with gradient color's Hi, I am trying to create a oval path, and trying to fill the gradient colors. However, could create oval path and fill single color in it. The code is like this, CGContextRef currentContext = (CGContextRef)[[NSGraphicsContext currentContext] CGShadingRef    myCGShading = CGShadingCreateAxial(colorspace, startPoint, endPoint, CGContextTranslateCTM(currentContext, rect.origin.x + rect.size.width/2, CGContextScaleCTM(currentContext, rect.size.width/2, //CGContextClipToRect(currentContext , *(CGRect *)&amp;amp;rect);	// This is where the action happens Please suggest, if there is any other approach. Regards Vinay The information contained in this electronic message and any attachments to this message are intended for the exclusive use of the addressee(s)and may contain confidential or privileged information. If you are not the intended recipient, please notify the sender or email@hidden</body>
  </mail>
  <mail>
    <header>Fill the oval with gradient color's</header>
    <body>Hi, I am trying to create a oval path, and trying to fill the gradient colors. However, could create oval path and fill single color in it. The code is like this, CGContextRef currentContext = (CGContextRef)[[NSGraphicsContext currentContext] CGShadingRef    myCGShading = CGShadingCreateAxial(colorspace, startPoint, endPoint, CGContextTranslateCTM(currentContext, rect.origin.x + rect.size.width/2, CGContextScaleCTM(currentContext, rect.size.width/2, //CGContextClipToRect(currentContext , *(CGRect *)&amp;amp;rect);	// This is where the action happens Please suggest, if there is any other approach. Regards Vinay The information contained in this electronic message and any attachments to this message are intended for the exclusive use of the addressee(s)and may contain confidential or privileged information. If you are not the intended recipient, please notify the sender or email@hidden</body>
  </mail>
  <mail>
    <header>Re: NSBitmapImageRep does not support 32 bits ? (only 1 to 16)</header>
    <body>CGBitmapContextCreate is probably a good option, yes. CGBitmapContext/Reference/reference.html#//apple_ref/c/func/ CGBitmapContextCreate Basically, start with your image width and height.  bitsPerComponent will be 32 for floating point.  bytesPerRow will be width*4*bitsPerComponent/8.  malloc a buffer that is bytesPerRow*height bytes large and use that as 'data'.  Create a colorspace (I've been assuming RGB).  'bitmapInfo' will have the bit for kCGBitmapFloatComponents set. On Jul 3, 2006, at 8:42 AM, Santiago (Jacques) Lema wrote: Attachment:</body>
  </mail>
  <mail>
    <header>Re: Converting arbitrary color spaces to RGB</header>
    <body>Ok. It would actually be sufficient if I could determine whether a CGColorSpaceRef is an RGB space or not. I do not need to identify each single color space. I just need a check for an RGB color space. So I guess I could simply check CGColorSpaceGetNumberOfComponents() against 3 to see if the color space is RGB... Andreas --</body>
  </mail>
  <mail>
    <header>Re: Converting arbitrary color spaces to RGB</header>
    <body>There is no conclusive method. You may be able to use use CGColorSpaceGetNumberOfComponents in its place, but that will confuse your algorithm if you get a color space that is not Gray/RGB/CMYK but has 1/3/4 components. If your input comes from a file, this may not be a concern. -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Converting arbitrary color spaces to RGB</header>
    <body>Thanks alot, that's working like a charm. I've one more question, though. I might be missing the obvious here but could someone tell me how to find out the type of the CGColorSpaceRef I'm getting back from CGImageGetColorSpace()? I just want to find out whether the color space returned by CGImageGetColorSpace() is RGB, CMYK, grayscale, whatever. But I don't seem to find any way to determine this... The only functions that could do this are CGColorSpaceGetModel() and CGColorSpaceCopyName() but both do not work on 10.4 AFAICS. How do I determine the type of a CGColorSpaceRef on 10.4? Tks, Andreas</body>
  </mail>
  <mail>
    <header>Porting a tile-based game to Mac - PNGs to RGBA-8888?</header>
    <body>I'm currently attempting to port Crossfire, a tile-based game that has clients for X-windows and GTK, to Mac. I need help with the graphics - this is my first project with complex graphics. The project is in Cocoa, in that I'll end up drawing to an NSView, but I'm open to any method. At the moment I'm attempting to use CGImages, and it's going a bit pear-shaped. Basically, the cross-platform part of the code calls a couple of functions to tell me what to do to implement the graphics. I'll sum these up in a sentence each: png_to_data() I'm given a pointer to some PNG data, and told its length. I must return bitmap data, in RGBA with one byte for each channel per pixel. I must also return the width and height of the image. create_and_rescale_image_with_data() I'm given the bitmap data returned from png_to_data(), and the width/ height, and I'm told to turn it into a native image format which can be drawn on screen. display_map_doneupdate() Loops through all the tiles on the visible map (a 2 dimensional array). For each one, there's a flag that says if it's been updated and needs redraw. If the flag is set, I'm supposed to draw its images onto the screen. (A tile has 3 layers, so the images get composited on top of each other.) My question is how best to do this. It's annoying that I have the png to data function and the data to native image function completely separate, as that means I can't just use a convenience PNG importer. Currently, I was doing the PNG to data by creating an NSBitmapImageRep using imageRepWithData:, and then asking for bitmapData. This does not work, however, as some of the PNGs are 1- bit or 4-bit, and some do not contain alpha, etc. The cross platform code uses libpng here, and just uses a load of if statements to test for each possible format and converts them appropriately. Also, how is best to do my drawing of tiles? Currently I save the tiles as CGImages, and draw them into a CGLayer. My NSView's drawRect: function simply draws the CGLayer onto the screen. Can anyone advise me on how they would implement something like this? Someone on MacNN told me to look at Quicktime's GraphicsImportCreateCGImage function to do the PNG conversion. I've heard horror stories about the Quicktime API before, though, so I'd like to know if Quartz can help me. My current code, btw, has an unknown bug. Aside from the fact that I'm not converting the PNGs properly, when I draw them into the CGLayer they seem to draw too many times. I.e. I draw one CGImage and it's repeated about 5 more times in random places. Probably a bug in my code, but I'm thinking of starting from scratch and doing it _right_ this time! Amorya</body>
  </mail>
  <mail>
    <header>Re: How to measure the size of a single character?</header>
    <body>On Aug 30, 2005, at 11:40 PM, Jim Wrenholt wrote: OK... to be very, horribly pedantic, you can't measure the width of a character.  You CAN measure the width of a glyph. A character is just an idea like &amp;quot;The letter A&amp;quot;, or &amp;quot;A period&amp;quot;.  A glyph is a picture that represents that idea.  If you draw the &amp;quot;letter A&amp;quot; in 5 different fonts, then you will get five glyphs that all represent the same character.  Ideas don't have widths.  Glyphs do. In order to measure some text, therefore, you are going to have to convert the character(s) into glyphs.  One way to do that for a bunch of ATSUI text is with ATSUGetGlyphInfo.  This routine is reported as &amp;quot;Deprecated&amp;quot;, but that is so you don't use it to create your own layout engine when you can plug into the ATSUI engine with the Direct Access functionality.  At any rate, this routine will return you an array with lots of information about the individual glyphs in a layout.  Among this information you will find the glyph IDs. ATSUGlyphGetScreenMetrics ATSUGlyphGetIdealMetrics The first one will give you the metrics of a glyph as it would be drawn on the screen as part of a drawn string. The second one offers the &amp;quot;ideal&amp;quot; size of the glyph, i.e. a resolution-dependent (presumably 72 dpi?) representation of the glyph.  Since you are drawing with Quartz which is, itself, resolution independent, this routine is probably your best bet.  With either routine, the information returned about a glyph should contain the height and width. If you are doing something scrabble-like, it may be easiest for you to create a long string of every character you might want to draw, create an ATSUI layout, and then use ATSUGetGlyphInfo to obtain glyph IDs corresponding to of the characters in the particular font you are interested in.  With the glyph IDs in hand, you can call ATSUGlyphGetIdealMetrics to retrieve the geometric characteristics of the glyphs.</body>
  </mail>
  <mail>
    <header>Re: How to measure the size of a single character?</header>
    <body>I haven't actually used CGContextShowText before, but if you have an ATSUStyle, you might consider switching to ATSUI to render your text. You should only need to do a few things differently, if you're already generating the style the way you want it to. I believe the steps from there are: ATSUCreateTextLayout[WithPtr]() ATSUSetRunStyle() ATSUDrawText() Once you create a ATSUTextLayout, you can measure the text by calling ATSUGetUnjustifiedBounds. For more information, see the ATSUI Reference (&amp;lt;&amp;gt;). Hope this helps, Jon -- Jonathan Johnson REAL Software, Inc.</body>
  </mail>
  <mail>
    <header>Re: CGEventKeyboardGetUnicodeString only works in U.S.?</header>
    <body>The Quartz event services documentation talks about using the UCKeyTranslate() function in Unicode Utilities, coupled with the ID of the keyboard you would like to emulate, to find out what character will be generated by a particular key or combination of keys on the other keyboard. Search on &amp;quot;UCKeyTranslate&amp;quot; in the Xcode documentation viewer and look at the I think what the documentation is telling us is that the Quartz event taps functions work with the physical keyboard at a low level, and you have to go to Unicode Utilities to see the consequences of using other keyboards. This is done for you if you use higher-level APIs. But I could be wrong. You might try downloading my free PreFab Event Taps Testbench utility, to see whether it sheds any light on this issue. Using the utility, you could install an event tap for keydown and keyup, then monitor the events in the Event Window as you type on the keyboard with different keyboard layouts selected, to see what virtual keycodes and characters are generated. -- Bill Cheeseman - email@hidden Quechee Software, Quechee, Vermont, USA www.quecheesoftware.com PreFab Software - www.prefabsoftware.com</body>
  </mail>
  <mail>
    <header>CGEventKeyboardGetUnicodeString only works in U.S.?</header>
    <body>I had assumed that the unicode characters associated with a keyboard event would depend on the current language. But it seems to be giving me the character that would be produced by the U.S. keyboard layout, even if some other keyboard layout is active.  Is that how it's supposed to work?</body>
  </mail>
  <mail>
    <header>Re: Saving images without color profile</header>
    <body>barring knowledge of how to make the APIs do this, I would probably try rendering the image into another CIImage or NSBitmapImageRep, and then save that out.   A bit ghetto, but probably would work. marc.</body>
  </mail>
  <mail>
    <header>Conceptual Issues using Core Video</header>
    <body>In implementing a generalized Core Image processing interface in my application Isadora, I've come up with a set of conceptual questions regarding the location of data, i.e., is it on the GPU or in CPU RAM, as I would like to understand this fully so that I can optimize performance. Hopefully someone here can give me some insights. The reason I'm concerned about all of this is because Isadora supports video output to multiple monitors. In the past, when all imagery was ARGB bitmaps, you could easily mix and combine these bitmaps because all of the data was in CPU RAM. The last step was to render the data to the screen, which meant taking the CPU RAM data, uploading it into a texture map, and then rendering. But now, it would seem that at least some of the data (specifically QuickTime movies) are already on GPU. That means you combining this data with stuff in CPU RAM, it will have to be read back into CPU RAM, which is certainly going to mean a performance hit. For instance, when I create QuickTime visual context, QTOpenGLTextureContextCreate needs to know the OpenGL context into which it is going to render. This has an implication for me that as this movie is played, the visual data is uploaded directly to the GPU. The further implication is that this movie data is available to this OpenGL context only -- i.e., if you cannot render this data to multiple multiple screens because each will have their own OpenGL context, and you can't ship data from one GPU to another. (Unless you pass it through CPU RAM using glRead, etc.) Now, in the case of still images, I am using CGImageCreate followed by [CIImage imageWithCGImage] to create the CIImage. There doesn't seem to be any need to specify the target OpenGL context -- which then leads me to believe this data is in CPU RAM. So when the moment comes that I'm using a plugin with two image inputs (e.g., CI Addition) to combine the movie and the still image, what exactly is happening? Assuming I'm right in my two points above, the Movie data is already on the GPU. So, it would seem that the still image data needs to be shipped to the GPU (creating a texture map??) so that the two images can be combined. Am I right about this? Movie/QTVisualContext (dest = screen 1) -&amp;gt; Render to Screen 1 \ ---&amp;gt; CI Addition -&amp;gt; Render to Screen 3 ???? / Movie/QTVisualContext (dest = screen 2) -&amp;gt; Render to Screen 2 I've got the two movie contexts set up to render to screen 1 and screen 2, respectively. I them combine these two images using CI Addition, and then render to screen 3. What's happening in this case? Is the GPU data being uploaded to CPU RAM and then out to the 3rd GPU? Sorry if this has been a bit long winded. But for optimizing performance I really need to understand what's going on under the hood. Many thanks in advance for your insights. Best Wishes, Mark -- ======================================================================= Mark Coniglio             |  email@hidden TroikaTronix              | Isadora¬Æ                  | |  (interactive media processing environment) =======================================================================</body>
  </mail>
  <mail>
    <header>Saving images without color profile</header>
    <body>I am using CIImage to generate scaled down JPEG images for use on the web, but the color profile is being embedded in the JPEG which doubles it's size. Does anyone know how I can stop the ICC color profile from being embedded in the JPEG? Rob.</body>
  </mail>
  <mail>
    <header>Re: getting CIImage file data?</header>
    <body>Try CGImageSourceCopyPropertiesAtIndex().  An image source can contain multiple images - think of a TIFF file with multiple resolutions available.  You're after the properties of, likely, the first and only image in your source. -Ken</body>
  </mail>
  <mail>
    <header>Re: getting CIImage file data?</header>
    <body>You get what's available, so if the image didn't have everything defined, then it won't be synthesized. If you look in CGImageProperties.h in ImageIO.framework/Headers then you can see the properties that it can return. For a quick and dirty of course, you can always CFShow() the dictionary you get back :). -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: getting CIImage file data?</header>
    <body>fair enough. i should have said:  the 75000 classes that make up the NSImage system :-) ... okay, i've started playing with this, and it's good, good, good. but the one piece i'm missing then is how to get the Exif (or TIFF dict, etc) from the CGImageSource. if I call CGImageSourceCopyProperties, I get one only one thing in the dictionary, which appears to be &amp;quot;FileSize'. thanks much! marc.</body>
  </mail>
  <mail>
    <header>Re: getting CIImage file data?</header>
    <body>Yah, i've long realised that CIImage is just a recipe class, and it's pretty darn cool all the things you can lump together into a single operation with it. however, i had hoped that, like NSImage and its 75000 sub-classes, somewhere would be hiding the original image data and/or access to some of the other interseting stuff such as EXIF data or TIFF dictionary stuff.  I'm more interested in this extra meta-data than actual image bits. There is the CGImageSource class, which gives me hope, but I am really trying to avoid loading the image data twice ... once for the CIImage class, and once to get its data.  Any suggestions how to do this? thanks, marc.</body>
  </mail>
  <mail>
    <header>Re: drawing just a rect of a CGLayer?</header>
    <body>You can do this by setting a clip region on the context via CGContextClipToRect(). This will limit your drawing to a subsection of the layer and Quartz should do the right thing to limit the amount of drawing necessary (which in this case is fairly easy). The actual storage mechanism behind a CGLayer varies depending on what kind of context it was created from. -- David Duncan Apple DTS Quartz and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGColor vs. NSColor (again, how to serialize a CG something)</header>
    <body>You should note that this only works if the NSColor is in the RGB colorspace already. If this color is coming from a Color Picker for example, it might not be in that color space. -- David Duncan Apple DTS Quartz and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Re: PDF Parsing</header>
    <body>I understand that I have to scan more creating another scanner but I do not know how to create the proper stream to scan... the code I have written apparently does not work. I can not find enough informations on the documentation: do someone knows where to find an example or have a suggestion on how to fix this code ? Many Thanks Marco Pifferi Il giorno 25/giu/07, alle ore 22:09, Derek Clegg ha scritto:</body>
  </mail>
  <mail>
    <header>Re: PDF Parsing</header>
    <body>You have the correct general idea ‚Äî   creating a new scanner is the right thing to do.  You'll need to check which type of XObject you have, though ‚Äî you only need to do more parsing if you have a Group or Form XObject.</body>
  </mail>
  <mail>
    <header>drawing just a rect of a CGLayer?</header>
    <body>So in my new Quartz-based drawing program, I am accumulating &amp;quot;marks&amp;quot; (where a mark is a color, blend mode, and some overlapping set of CGPathRefs of differing widths) on a &amp;quot;sheet&amp;quot;. As each mark is finished, I &amp;quot;add&amp;quot; it to a CGLayerRef by drawing that mark into the context of the layer.  In this way, I end up with a layer that has all the marks that have been drawn, and as I start a new mark (i.e. a mouseDown, and then mouseMoved) at each redraw in the view, if I'm tracking the dirty rect of the new mark, I only need to redraw the dirty rect of the layer and the new piece of the new mark since the last time I drew.  Finally, when I mouseUp, I add this new mark to the layer, as disccussed above. The problem is that I don't see a way to draw a sub-rect of the layer - I can only do the whole thing: Where would I would really like is something like what NSImage has, where I can pick a src rectangle from the image and composite just that, say something like: CGContextDrawLayerAtPointFromRect(context, CGPointZero, layer, Of course, this is all predicated on the fact that there's some sort of speed win here.  My purposely vague understanding of what's hiding behind that CGLayerRef on my multi-core Intel machine with a fast graphics card is that the CGLayerRef is pointing at something akin to either: -  an OpenGL display list -  a texture. In the display list case, it's way more work to think about getting to some sub rectangle, but you should be able to at worst set up a clipping rect and push the data through, and let the silicon decide what gets done, so it doesn't seem like there would be a cost, and there might be some upside in the future. In the texture case, though, there's a clear advantage to pushing less texels around, and you have a higher chance of keeping things peppy the fewer there are, especially in the highly interactive case (i.e. user drawing). So do I have a broken mental model of what a CGLayer is, or is this some API I should be filing an enhancement request for, or? - wave</body>
  </mail>
  <mail>
    <header>Re: getting CIImage file data?</header>
    <body>When you say you want to get at the raw bits, do you want the RGB Pixel values from the graphic of the image, or do you want to get at the data in TIFF/JPEG/PNG format? For the pixels: A CIImage is not really a container for raw bits.  It's more like a series of instructions (an &amp;quot;image program&amp;quot;) to draw an image.  To actually get at the results, you're going to have to draw the CIImage somewere... you have to &amp;quot;run the program&amp;quot;. One way to do that would be to create a CGBitmapContext on some memory that you own.  You could then draw the CIImage into that context and examine the memory that you've set up for the context.  In setting up the bitmap context, you will also have the opportunity to put the pixels in the color space and pixel format of your choice. For the raw TIFF/JPEG/PNG data... just use CFData or NSData  either will get you at the raw bytes of an image file.  If you need image metadata (like the color space, or the name of the camera that took the data or the like) then look at the Image I/O framework.  It allows you to import and export data in a variety of formats as well as examining and setting the metadata in formats that support them.</body>
  </mail>
  <mail>
    <header>Re: CGColor vs. NSColor (again, how to serialize a CG something)</header>
    <body>On Jun 24, 2007, at 10:36 AM, Michael B Johnson wrote: My two cents on this is that Core Graphics is not set up to do the kinds of things you want to do. :-( It's a bit like the difference between an Illustrator file and a PDF. Illustrator needs to be able to edit the curves, but PDF only needs to be able to display them. As a result, Illustrator has to save additional information about the graphics in order to be able to recover the &amp;quot;editability&amp;quot;. (ignore for the moment that they actually store this additional information in a PDF container... the point is that there is extra information there). Quartz 2D is the PDF analog, it's the presentation format,  not the &amp;quot;editable&amp;quot; format. A lot of the information you might like to save has probably been lost (or is owned by other objects) by the time you've created the CGColor. What we do is handle serialization at a higher level than Quartz.  To carry through your particular example, we have a higher-level object that represents an RGBA color. At runtime, we can ask that higher- level object for a CGColorRef representation of itself, but the CGColorRef is just transient representation of our &amp;quot;real&amp;quot; color object.  When serializing, we ask the higher level object to store itself. It does so by putting four floating point numbers (the RGBA components) in an XML framework.  When reading it back in, we assume that the color is in the &amp;quot;Generic RGB&amp;quot; color space. If our application had stronger color management requirements, or wanted to store grayscale colors and CMYK colors and who knows what else, we would have to use a more sophisticated serialization model that noted (for example) the color space and potentially a variable number of components.  Fortunately we don't have to do that. To make a long story short, the way I see it Core Graphics doesn't try to anticipate the needs of the application by presenting a single serialization format because it's a complex problem and whatever it chooses might be overkill for some apps, or too simplistic for others. P.S.  Because we are very cavalier about color spaces, our NSColor to CGColor routine is dirt simple.  It looks something like (typed into mail): - (CGColorRef) createCGColorRef [self getRed: &amp;amp;components[0] green: &amp;amp;components[1] blue: &amp;amp;components[2]</body>
  </mail>
  <mail>
    <header>Re: getting CIImage file data?</header>
    <body>On Jun 25, 2007, at 6:47 AM, Marc Wan wrote: I don't know this for a fact, but I doubt you can get the image data from the CIImage.  Because CoreImage does lazy image processing, CIImage isn't so much a conventional bitmap image container as it is a reference to an image.  I wouldn't be surprised if a CIImage created with a URL simply stores the URL until it needs to process some part of the image. Nick</body>
  </mail>
  <mail>
    <header>Re: CGColor vs. NSColor (again, how to serialize a CG something)</header>
    <body>On Jun 25, 2007, at 10:19 AM, Derek Clegg wrote: This is a Leopard only app. For now, I'm holding onto an NSColor* all the way down, and converting to a CGColor when needed.</body>
  </mail>
  <mail>
    <header>Re: CGColor vs. NSColor (again, how to serialize a CG something)</header>
    <body>On Jun 24, 2007, at 9:36 AM, Michael B Johnson wrote: There's no standard way to encode/decode a CGColorRef object.  You'll need to save enough information about the color space and components to recreate the CGColorRef in your application.  If you're just using solid colors with a specific color space that's pretty easy.  If you're using a variety of color spaces or your CGColorRef is created from a pattern it becomes more complicated. Nick</body>
  </mail>
  <mail>
    <header>Re: CGColor vs. NSColor (again, how to serialize a CG something)</header>
    <body>This is a bit of a pain in Tiger.  You can get the color components and write them out easily, but the color space is problematic.  Are you trying to write out arbitrary colors? If you know that they all come from one (or a few) color spaces, it's a much simpler problem.</body>
  </mail>
  <mail>
    <header>PDF Parsing</header>
    <body>I am trying to parse PDF files using CGPDFScanner and I am having the following problem: I imagine that I should instantiate another scanner to recursively scan the object but I am not sure how to do it: static void operat_Do (CGPDFScannerRef s, void *info) if (!CGPDFScannerPopName(s, &amp;amp;name)) CGPDFObjectRef xobject = CGPDFContentStreamGetResource(cs, CGPDFContentStreamRef localContent = CGPDFScannerRef scanner = CGPDFScannerCreate(localContent, myTable, My question is: How do I instantiate the new CGPDFScanner to read the information in a Do operator ? Is the new scanner the proper way to do it ? Many Thanks Marco Pifferi</body>
  </mail>
  <mail>
    <header>getting CIImage file data?</header>
    <body>I am using [CIImage imageWithContentsOfURL: ....] to load in images from a file, with spectacular success! however, i, at some points in my application, need to get at the underlying file image data, sort of like when you get the raw bits from NSImages and the like, or getting the EXIF data from file types that support this. so .... uh  ... how is this done?  the CIImage class seems to be pretty spartan, and the CIImageProvider seems to be going the wrong way :-P thanks, marc.</body>
  </mail>
  <mail>
    <header>Simple Image to float</header>
    <body>Hello, I&amp;#39;m quite new to Quartz and Cocoa as well, though I have some background in programming. What I need to do is very simple but a bit hard to achieve for a newbie like me. I need to do some very basic motion detection. For this I&amp;#39;m using a (modified version of) macro by Sam Kass which outputs a differential image, in which white pixels draw the variation in the image while black pixels stand where there&amp;#39;s no variation detected between frames at the given sample period. From this image I just need to output a value (could be float or integer, doesn&amp;#39;t matter) which represents the amount of white (and so movement) in the image, which on a live installation will controll the variation and motion of a kaleidoscope made of images and QT clips. A sum of the red, or blue, or green values of all the pixels would be fine. My problem is that I can&amp;#39;t find a suitable patch for this. I understood how to use the Core Image Kernel patch, but Core Image Filters only output images (vec4). I also used a template for XCode to make custom QC patches, but I&amp;#39;m finding it a bit difficult to program, especially treating input images (which is immediate in core image kernel programming) in objective C. The operation I need to do is sooooooo simple, the only problem for me is the setup and syntax of all. May some one give a hint on how to achieve it? Thanks to everyone!! f</body>
  </mail>
  <mail>
    <header>Re: CoreVideo / CIImage</header>
    <body>The answer to #2 SEEMS to be caused by running out of video memory. My program is correctly using openGL textures, but as I have been monitoring it using the OpenGL Driver Monitor, certain combinations of tasks cause the &amp;quot;Texture count&amp;quot; to increment forever (and as related the amount of free video memory to decrease), and I would guess once the VRAM runs out CV automatically switches to using pixel buffers which &amp;quot;causes&amp;quot; my main memory leak. Now for the reason for leaking textures.  I'm not sure! :P  The OpenGL profiler (unless I am missing something) does not show that I am leaking textures in the resources.  For example, OpenGL profiler will show in resources that I am peaking at around 30 textures, yet I can see from watching the openGL monitor that my program has (somehow) allocated a few thousand textures (when I quit, that number drops back to what it was before I launched).  Here is where I seem to be leaking the textures.  If I draw any texture (via CIImage) into a single CIContext more than once, it leaks. Example: My CV callback runs at 60fps (what my refresh rate is set to).  The movie is only 30fps (29.997 DV), as a quick test if I nix the part that checks if a new frame has been made available and just draw whatever CIImage I currently have each time throw, it leaks like a sieve. While I don't have a reason to draw a single image more than once in my preview GL views, I do for the mixed main view.  An example of this is, for example, movie 1 that runs at 30fps and movie 2 that runs at 15fps.  If I mix the two, every other drawing will leak a texture of movie 2 because it has been drawn twice into the same CIContext. I can also get this same behavior with a single movie by simply doubling my &amp;quot;draw preview&amp;quot; function call.  It draws once, all is well. It draws a second time, textures leak. Am I doing something wrong?  I can't imagine that textures are &amp;quot;supposed&amp;quot; to leak just because I draw them twice into the same GL view. In the mean time I have been able to slow the memory leak, but it is still there. :/</body>
  </mail>
  <mail>
    <header>CGColor vs. NSColor (again, how to serialize a CG something)</header>
    <body>So I'm making a lot of progress in rewriting my drawing stuff to use Quartz directly, but I'm struggling with understanding how to go from an NSColor to a CGColor, and vice versa.  That's one question I'd love to know more about it, but the concrete problem I have is writing out a CGColor in some way to a file such that I can reconstruct it later. Again, I know how to write out an NSColor in an -encodeWithCoder:, and how to read one back in later in initWithCoder:, but I don't see how to do that with CGColor. --&amp;gt; Michael B. Johnson, PhD --&amp;gt;  (personal) --&amp;gt;  (alum) --&amp;gt; MPG Lead --&amp;gt; Pixar Animation Studios</body>
  </mail>
  <mail>
    <header>Re: When do I really need to call CGContextBeginPath()?</header>
    <body>On Jun 23, 2007, at 3:29 PM, Scott Thompson wrote: Interesting idea, but the screenshot on this page will show you why subsequent orbits wrap to the opposite edge of the window. -- Rick</body>
  </mail>
  <mail>
    <header>Re: When do I really need to call CGContextBeginPath()?</header>
    <body>To get the effect you want, you would call CGContextMoveTo, CGContextAddLineToPoint, and CGContextStrokePath for each segment.</body>
  </mail>
  <mail>
    <header>Re: Speed And CGGLContext</header>
    <body>In general, no. You are likely going to see more of a performance boost from optimizing your usage of NSBezierPath than you will see from switching to using Quartz directly, as NSBezierPath sits right on top of Quartz. CGGLContexts are not recommended. I'm afraid that I don't know why this is offhand, it's just what is stated in the documentation :). You can't create one from a view directly anyway, they require an OpenGL context (at best you'd have to create them from an NSOpenGLView). If you have specific speed issues, you are better off asking about them specifically than doing a wholesale conversion of code. If you don't know your specific speed issues, then running Shark is your best first step. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Speed And CGGLContext</header>
    <body>At the moment we are using NSBezierPath (cocoa in general) but I wonder if converting everything to CoreGraphics will get a speed bump. In particular I would like to understand if using a CGGLContext (instead of getting the context from the view using : CGContextRef myContext = [[NSGraphicsContext currentContext] graphicsPort]; ) will get an increase in speed (for both Tiger and Leopard). Many Thanks Piff</body>
  </mail>
  <mail>
    <header>Re: serialize CGPathRefs or introspect on a CGPath to get the	points?</header>
    <body>Yes, take a look at CGPathApply().</body>
  </mail>
  <mail>
    <header>serialize CGPathRefs or introspect on a CGPath to get the points?</header>
    <body>Let's say I'm drawing a series of connected lines of different widths but the same color. --&amp;gt; Michael B. Johnson, PhD --&amp;gt;&amp;nbsp;&amp;nbsp;(personal) --&amp;gt;&amp;nbsp;&amp;nbsp;(alum) --&amp;gt; MPG Lead --&amp;gt; Pixar Animation Studios</body>
  </mail>
  <mail>
    <header>Re: When do I really need to call CGContextBeginPath()?</header>
    <body>On Jun 22, 2007, at 3:33 PM, Scott Thompson wrote: Ah! Okay. I thought the current point remained where it wasAs I was waiting, I experimented by calling CGContextMoveTo() right after stroking the line, with the last point used in the path. This fixed things, and now I know why. Yeah, that kind of sucks, but I want the effect of the line fading away. It's a predicted ground track of a satellite from now to a few orbits into the future. These tracks have a tendency to overlap a lot, so I want to vary the alpha component along the path so that it fades away. I meant &amp;quot;end&amp;quot;, not close, which is not quite what you mean by &amp;quot;consume,&amp;quot; but I get it now. Yeah, I misspoke. Thanks! -- Rick</body>
  </mail>
  <mail>
    <header>Re: help me understand blend mode...</header>
    <body>On Jun 21, 2007, at 6:59 PM, Michael B Johnson wrote: This was discussed briefly during the WWDC session on Quartz 2D.  The term &amp;quot;Layer&amp;quot; is pretty overloaded in Quartz/Core Graphics.  There are CGLayers, Transparency Layers, and now with Core Animation there are CALayers which are a different beast altogether. Of course we run into the same kind of problem with NSImage which is very different from CGImage which is not the same as CIImage (none of which should be confused with a vImage_Buffer) I'm glad you worked out your problem though.  I look forward to seeing your work again at next WWDC.</body>
  </mail>
  <mail>
    <header>Re: When do I really need to call CGContextBeginPath()?</header>
    <body>On Jun 22, 2007, at 4:29 PM, Rick Mann wrote: When you call CGContextAddLineToPoint(), it consumes the path in the context and begins a new one.  This includes un-setting the current point. To get the effect you want, you would call CGContextMoveTo, CGContextAddLineToPoint, and CGContextStrokePath for each segment. However, if you do that, it's not going to be one path, it's going to be a series of small paths.  That means that you would not get joins between the segments of the path the way you would if it were drawn as one path. No, stroking a path does not close the path, it consumes the path leaving no path and no current point. You would call CGContextBeginPath() if you wanted to make doubly absolutely sure that there is no current path in a context. CGContextBeginPath also consumes the current path and clears the current point.  It just does so without putting down any &amp;quot;paint&amp;quot;. Closing a path is a different operation.  If the current point, and segment between the current point and the first point of the path, then it &amp;quot;melds&amp;quot; the two endpoints together (so that there is a line join there)</body>
  </mail>
  <mail>
    <header>When do I really need to call CGContextBeginPath()?</header>
    <body>I have some code that's been working fine for a while now. In it, I draw a long path by making a series of calls to CGContextMoveToPoint () and CGContextAddLineToPoint(), and then maybe some calls to set line width &amp;amp; color, and finally CGContextStrokePath(). Then I draw a plus symbol by calling CGContextMoveToPoint() and CGContextAddLineToPoint() a bit more, and call CGContextStrokePath() to stroke that. All this has worked, and I've successfully changed colors and stroke style, and at no time have I called CGContextBeginPath() or CGContextClosePath(). Then I made a change. In one part of my drawing, I change the alpha value of the color and call StrokePath() after almost every CGContextAddLineToPoint() call (I wanted the line to fade away as it was drawn). When I do this, I get only one segment of my long path drawn, and the rest don't show up. It's possible they're invisible, but the alpha values are still very high (0.99), and I can see the calls being made. The plus symbol still shows up. I'm not sure what I've done wrong, but as I was reading through the _Quartz 2D Programming Guide_, I saw that I'm supposed to call CGContextBeginPath() before every new path. So, what am I missing? I'm operating on the belief that stroking a path also closes the path. -- Rick</body>
  </mail>
  <mail>
    <header>CoreVideo / CIImage</header>
    <body>I know it is not best practice to post 2 questions in a single e- mail, but I think they might be related. The application in question that I am writing is a small video mixer application used for presentations we do.  It runs 2 video tracks (from Quicktime movies) that each run via CoreVideo display link (so there are 2 CV threads running concurrently there).  Each thread displays the resulting CIImage in a preview openGL view and also saves it for use by the mixing CV thread.  The mixing CV thread is created via display link as well and simply polls the A/B objects to get their current CIImage and then either displays directly or creates a CIFilter (dissolve) to mix the two during a fade to display in the full-screen openGL view. 1) If I display to the preview window only, there is no memory leak (well, there is one but it is about 12k/minute, I can live with that while I track down the bigger one). If I display to both the preview and the full-screen view (wether mixed or not) I get a massive memory leak, about 800k/minute.  I have tried bypassing the 2 render threads and having the mix thread render both channels and then display everything, but the leak is still there so I do not think it is specifically a thread issue. 2) While trying to debug this memory leak I have noticed something odd.  The memory that is leaking is being allocated in 2 places, first in a DeferredTaskUPP callback and is shown to be CVPixelBuffer textures.  The second place is during drawing and is again, CVPixelBuffer textures.  I specifically setup my CV rendering to use openGL buffers instead of pixel buffers because I wanted the speed and everything I am doing to work with them is via CIFilter's.  I am really curious because I do not seem to be getting nearly the performance that I see in other CV/openGL applications that do similar things to what I am doing.  Again, I ask both questions together because I wonder if I am doing something wrong that could be causing both of these problems? Thanks for your time, I realize I have not given specific code examples and I will if requested try to snip out relevant sections for posting but I am hoping too to at-least get a &amp;quot;yeah I have done something similar so it is certainly possible to do what you want, Daniel Hazelbaker</body>
  </mail>
  <mail>
    <header>Re: help me understand blend mode...</header>
    <body>On Jun 21, 2007, at 4:03 PM, Ken Ferry wrote: To be more specific, and to explain it in a 10.4 context :-), my misunderstanding was due to conflating CGLayers and Transparency Layers. I knew about CGLayers, but wasn't going to deal with them here until I got the unoptimized case working first.  That made me ignore the transparency layers, which are a different beast (I had some vague recollection that they were a particular kind of CGLayer - wrong!). I didn't realize that in order to have blend mode work the way I expected I needed to wrap that drawing with a call to CGContextBegin/ EndTransparencyLayer (). I just went back and reread the section in the Quartz 2D book comparing the two, and it's clear.  Nothing like writing a little code you care about to clear away the cobwebs! --&amp;gt; Michael B. Johnson, PhD --&amp;gt;  (personal) --&amp;gt;  (alum) --&amp;gt; MPG Lead --&amp;gt; Pixar Animation Studios</body>
  </mail>
  <mail>
    <header>Re: help me understand blend mode...</header>
    <body>The blended drawing was working, but it wasn't producing the expected result because of something else - there was a little layering problem in the test app. -Ken Cocoa Frameworks</body>
  </mail>
  <mail>
    <header>Re: help me understand blend mode...</header>
    <body>On Jun 21, 2007, at 3:23 PM, Michael B Johnson wrote: CGContextSetBlendMode is pretty easy to use.  You just set a context's blend mode and draw something.  When you're done you set another blend mode or restore a saved graphics state.  It works very similarly to setting a context's alpha value. The blend modes in CG perform various mathematical functions when you're drawing into a context and conceptually overlap with what you can do in Photoshop between layers.  However this isn't the same as Porter-Duff compositing (over, in, out, etc.).  It sounds like that might be what you're after.  You can accomplish some compositing tasks within CG using alpha, but for the whole Porter-Duff suite you might look into CoreImage. Nick</body>
  </mail>
  <mail>
    <header>Re: help me understand blend mode...</header>
    <body>Can you describe the incorrect results you are getting? My understanding of blend mode (based on the PDF spec) is that you set it as a context-level parameter and any subsequent paint that is laid down in that context will be blended with the existing content using the blend mode specified.  To get the &amp;quot;right effect&amp;quot; however, you may have to draw your content into a transparency layer and then drop that down to the context so that all the paint is &amp;quot;laid down&amp;quot; all at one time. Scott</body>
  </mail>
  <mail>
    <header>help me understand blend mode...</header>
    <body>So I'm taking some code I wrote years ago at the Cocoa level and am trying to drop it down into CG to squeeze maximum performance out. As a beginning step to doing this, I'm trying to reproduce what I do at the AppKit level by drawing an NSBezierPath opaquely into an offscreen NSImage and then fractionally compositing it in to another NSImage where I'm accumulating my drawing. This has the advantage that I can use NSBezierPath path segments that are have coincident end points but are of different widths and still get a smooth transparency when my drawing color happens to be partially transparent (think about the overlapping half circles of the end points otherwise - it's ugly). This had the added benefit that by changing my composite from source over to destination out I could easily achieve an anti-aliased raster erase, which is great. Again, I wrote this years ago, and it works swell.  It's all dirty rects and optimized, but the problem is, it's using NSImages for doing the heavy lifting, and these days, on large displays, that doesn't scale well, so I'm trying to refactor it into CG calls, with the hope of shaving my memory requirements down. So I wrote a simple app at WWDC (which I have since further distilled down) that basically just shows that I don't understand how to call CGContextSetBlendMode(). If anyone can offer any insight, I'm happy to send the source of my test app (although you'll need the WWDC seed) - I know this must be something silly, since the algorithm works fine up one level... --&amp;gt; Michael B. Johnson, PhD --&amp;gt;  (personal) --&amp;gt;  (alum) --&amp;gt; MPG Lead --&amp;gt; Pixar Animation Studios</body>
  </mail>
  <mail>
    <header>Re: Core image treating alpha=0 special?</header>
    <body>For the mailing list record... &amp;nbsp;The solution/workaround is to use a custom implementation of CIColorMatrix that does not premultiply. I have an image that has some rgba color values (1 1 1 0) in it. I apply a CIColorMatrix to invert alpha using the following parameters for the vectors: inputRVector = [1 0 0 0] inputGVector = [0 1 0 0] inputBVector = [0 0 1 0] inputAVector = [0 0 0 -1] inputBiasVector = [0 0 0 1] Notice that this should leave all colors alone and invert the alpha value. However, all rgba values of (1 1 1 0) are converted to (0 0 0 1)! They should be (1 1 1 1). If I change the rgba values (1 1 1 0) to (1 1 1 epsilon) where epsilon is 0.0001, then I get expected behavior. Is this special treatment of alpha=0 intentional within CoreImage or is this a bug?</body>
  </mail>
  <mail>
    <header>Determing if a display is CoreImage compatible</header>
    <body>When I go to System Profiler, I can look in the Graphics/Displays section and it will tell me whether a particular display is CoreImage compatible.How do I do this from Objective-C? I would like to render with CoreImage when possible, but fall back to an OpenGL solution when CoreImage is not available.</body>
  </mail>
  <mail>
    <header>Re: constrain cursor inside a circle</header>
    <body>On Jun 20, 2007, at 4:50 PM, Derek Clegg wrote: And, informationally, if the center is at a point (cx, cy) it's (x-cx) ^2+(y-cy)^2 &amp;lt; r^2 Scott</body>
  </mail>
  <mail>
    <header>Re: Memory leak from PDFPage drawWithBox: ?</header>
    <body>Here's another implementation of your code that uses Core Graphics directly instead of wandering around through NSImage, and writes the result directly to a file (instead of writing it to data in memory first).  In my experiment it runs about 25% faster. YMMV. - (void) transcodePDF_CG NSDictionary *imageProperties = [NSDictionary dictionaryWithObjectsAndKeys: [NSNumber numberWithFloat: dpi], (id) kCGImagePropertyDPIHeight, CGColorSpaceRef rgbColorSpace = CGColorSpaceCreateWithName CGPDFDocumentRef pdfDocument = CGPDFDocumentCreateWithURL((CFURLRef) for(size_t pageIndex = 1; pageIndex &amp;lt;= CGPDFDocumentGetNumberOfPages (pdfDocument); pageIndex++) CGContextRef bitmapContext = CGBitmapContextCreate(nil, imageWidth, imageHeight, kBitsPerComponent, imageWidth * 4, rgbColorSpace, NSString* filename = [NSTemporaryDirectory() stringByAppendingPathComponent: [NSString stringWithFormat:@&amp;quot;file [[NSFileManager defaultManager] removeItemAtPath: filename error: CFURLRef outputFileURL = CFURLCreateWithFileSystemPath(nil, CGImageDestinationRef imageDestination = CGImageDestinationSetProperties(imageDestination, (CFDictionaryRef)</body>
  </mail>
  <mail>
    <header>Re: Memory leak from PDFPage drawWithBox: ?</header>
    <body>He's using PDFKit. It might be worth a bug report overall. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Memory leak from PDFPage drawWithBox: ?</header>
    <body>On Jul 8, 2009, at 10:37 AM, Duncan McGregor wrote: I took your code and reworked it as shown at the end of this message. I ran the code with a 256 page, 8.5x11 inch PDF.  I ran it against the Leaks tool in instruments and it did not show any leaks. You are using classes for which you have not provided source code (PDFDocument for example).  In my transcoding I had to replace your code with calls to CGPDFDocument and such.  Could the problem lie within your PDFPage class? - (void) transcodePDF CGPDFDocumentRef pdfDocument = CGPDFDocumentCreateWithURL((CFURLRef) for(size_t pageIndex = 1; pageIndex &amp;lt;= 50 / *CGPDFDocumentGetNumberOfPages(pdfDocument)*/; pageIndex++) NSBitmapImageRep* bitmapImage = [[NSBitmapImageRep alloc] initWithBitmapDataPlanes:nil pixelsWide: imageWidth pixelsHigh: imageHeight bitsPerSample: 8 samplesPerPixel: 4 hasAlpha: YES isPlanar: NO colorSpaceName: NSCalibratedRGBColorSpace bytesPerRow: 4 * imageWidth [bitmapImage setSize: NSMakeSize(pageBounds.size.width, [NSGraphicsContext setCurrentContext: [NSGraphicsContext CGContextRef imageCGContext = (CGContextRef) [[NSGraphicsContext NSString* filename = [NSTemporaryDirectory() stringByAppendingPathComponent: [NSString stringWithFormat:@&amp;quot;file [[NSFileManager defaultManager] removeItemAtPath: filename error: NSData* pngData = [NSBitmapImageRep representationOfImageRepsInArray: [NSArray arrayWithObject: bitmapImage] usingType: NSPNGFileType</body>
  </mail>
  <mail>
    <header>Re: Memory leak from PDFPage drawWithBox: ?</header>
    <body>As it is, this code writes 256 PNG files, but leaks memory at the rate of about 25Mbytes per second (YLMV)! If I don't call [page drawWithBox:kPDFDisplayBoxMediaBox] it doesn't leak, but then of course it writes blank PNGs. I may be doing something obviously wrong, but I just can't see it. Please help! My users are throwing whole PDF books at my app, and the engines cannae take it.</body>
  </mail>
  <mail>
    <header>Memory leak from PDFPage drawWithBox: ?</header>
    <body>I'm rendering PDF pages to PNG files, and have run into a memory leak problem with multi-page files. I have the following test case, which writes each page in a 256 page PDF as a PNG -(void) testDirect PDFDocument* document = [[PDFDocument alloc] initWithURL:[NSURL NSRect pageBounds_72DPI = [page boundsForBox: NSBitmapImageRep* rep = [[[NSBitmapImageRep alloc] initWithBitmapDataPlanes:nil pixelsWide:width_px pixelsHigh:height_px bitsPerSample:8 samplesPerPixel:4 hasAlpha:YES isPlanar:NO colorSpaceName:NSCalibratedRGBColorSpace bytesPerRow:4*width_px [NSGraphicsContext setCurrentContext: [NSGraphicsContext NSString* filename = [NSTemporaryDirectory() stringByAppendingPathComponent: [NSString stringWithFormat:@&amp;quot;file [[NSFileManager defaultManager] removeItemAtPath: filename STAssertFalse([[NSFileManager defaultManager] NSData* pngData = [NSBitmapImageRep representationOfImageRepsInArray:[NSArray arrayWithObject:rep] STAssertTrue([[NSFileManager defaultManager] As it is, this code writes 256 PNG files, but leaks memory at the rate of about 25Mbytes per second (YLMV)! If I don't call [page drawWithBox:kPDFDisplayBoxMediaBox] it doesn't leak, but then of course it writes blank PNGs. I may be doing something obviously wrong, but I just can't see it. Please help! My users are throwing whole PDF books at my app, and the engines cannae take it. Duncan</body>
  </mail>
  <mail>
    <header>Re: Unpixellated CALayer under scaling</header>
    <body>You could override -setTransform: / -transform to behave something like this: - (void)setTransform:(CATransform3D)transform [super setTransform:CGTransform3DConcat(myAncestorCancelingTransform, - (CATransform3D)transform If you're using NSString UIKit additions to do the drawing, that could be a problem. You could use subviews of the content view instead of sublayers of the content view's layer, I guess. Yeah, CATiledLayer's approach is not great for rendering text. Unfortunately UIScrollView seems to have been designed pretty much solely with CATiledLayer in mind. Hamish</body>
  </mail>
  <mail>
    <header>Re: Unpixellated CALayer under scaling</header>
    <body>Interesting idea... to get an object-oriented modular solution where the CALayer subclass handles its own display, I would have to walk the layer ancestry and concatenate all the transforms to see what the final transform is, then invert that? I suppose the only side effect would be that I can't set the transform of the subclass indepedently any more... and the layer in question would have to be a sublayer instead of the main layer that the UIScrollView was scaling... Yeah but CATiledLayer only has a factor-of-2 bitmaps so that at some zooms it looks a little flaky, and I'm worried about using some of the NSString(UIDrawing) stuff /* noted to be thread unsafe? */ in a UIView backed by a CATiledLayer. --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Re: Unpixellated CALayer under scaling</header>
    <body>Hmm... I throw away the scrolled content view and not the UIScrollView and that works well enough. Still means I have to fill in unscrolled areas though like Safari does, so I'm hoping to avoid some of that with CAShapeLayers and friends. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Re: Unpixellated CALayer under scaling</header>
    <body>UIScrollView seems to store its zoomScale in a private property of the view returned by viewForZoomingInScrollView:. Setting this property affects the transform of the view, but not vice versa, as you've discovered (I too wrestled with this for a long time). When Glen said &amp;quot;... especially if the transform comes from a parent layer...&amp;quot; I assumed he's adding a sublayer to the layer of the zoomed view (I found this approach best, even if using a CATiledLayer, rather than altering the layerClass of the view). I'm not suggesting changing the transform of the zoomed view's layer, but rather, canceling the effect of that transform by setting the sublayer's transform to its inverse (at least, for its scale component). Of course, this may not work either; myself, I ended up using CATiledLayer too. There may not be a &amp;quot;proper solution&amp;quot; because CATiledLayer may have access to that same private property. Hamish</body>
  </mail>
  <mail>
    <header>Re: Unpixellated CALayer under scaling</header>
    <body>On 8 Jul 2009, at 13:37, Hamish Allan wrote: Alas, no. You just get into worse problems due to the UIScrollView keeping an internal idea of what scale it's drawing at and there's no way to reset that. This problem drove me mad. After weeks of struggling with it, I saw the light and my solution was to throw away the UIScrollView and create a new one at the end of every zoom. There must be a proper solution though because CATiledLayer works. In my case, switching to CATiledLayer solved the pixellation problem but at the cost of rendering about a hundred times slower.</body>
  </mail>
  <mail>
    <header>Re: Unpixellated CALayer under scaling</header>
    <body>Hi Glen, I don't know the answer to this question (and I would be interested to find out), but perhaps if you take the scale component of the concatenated ancestors' transforms and set the transform of the layer to its inverse, the new content would be rendered unpixellated? Hamish</body>
  </mail>
  <mail>
    <header>Unpixellated CALayer under scaling</header>
    <body>I'm using CALayers in iPhoneOS and when there is a scaling transform applied to them i.e. when zoomed in a UIScrollView, the contents appear pixellated. It looks as if the CALayer is imaging to contents, and then applying the transform after the imaging. Now CAShapeLayers remain sharp under similar scaling, and CATiledLayers have some sort of magic where the closest resolution bitmap gets used. I wonder if it's possible at all to subclass CALayer to achieve unpixellated contents under scaling? I would suppose overriding - display would do it -- figuring out the transform and applying it to the context, then imaging, then saving it as the content property -- but how do I prevent the machinery from again applying the transform to the imaged content? ... especially if the transform comes from a parent layer... Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Re: Image IO alternative for Mac OS X 10.3</header>
    <body>Hi Glen, thanks! If it&amp;#39;s some other format, there are other libraries around. Hi David, Thanks for the response! &amp;gt;&amp;gt; You will have to bring your¬†own¬†TIFF¬†writing¬†that¬† I am not really sure how I am gonna do this but I am thinking that this is kind of difficult to do but I&amp;#39;ll try. thanks again. hernan</body>
  </mail>
  <mail>
    <header>Re: Image IO alternative for Mac OS X 10.3</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Image IO alternative for Mac OS X 10.3</header>
    <body>Hi Paul, thanks! Hi David, Thanks for the response! I am not really sure how I am gonna do this but I am thinking that this is kind of difficult to do but I&amp;#39;ll try. thanks again. hernan</body>
  </mail>
  <mail>
    <header>Re: What's causing my text to become more bolder?</header>
    <body>Hi, I just wanted to add these links to the thread, because when somebody google it.. he can see this The Sub-pixel antialiasing that effects the text drawing with background is explained here: and cheers, marc</body>
  </mail>
  <mail>
    <header>PMServerLaunchPrinterBrowser()</header>
    <body>After a call to PMServerLaunchPrinterBrowser() is there a way to determine whether a printer was actually added to the system? Or, alternatively a way to determine whether the close box of that dialog was pressed? The dialog is displayed asynchronously so there is not an error I can check after the dialog is dismissed. Aaron Alpher CSE, Inc. 44710 Cape Court Suite 142 Ashburn, VA 20147 USA 703-723-8580 Phone 703-723-8148 Fax email@hidden</body>
  </mail>
  <mail>
    <header>Re: NSView -&amp;gt; NSBitmapImageRep ,	under what circumstances is it 32bpc?</header>
    <body>Forgive the traffic, I neglected to set my background color of my window to [NSColor clearColor], and this seems to do the trick. Apologies. I also meant to say 32bits per pixel, not channel.</body>
  </mail>
  <mail>
    <header>NSView -&amp;gt; NSBitmapImageRep , under what circumstances is it 32bpc?</header>
    <body>qa1325.html, I am wondering what I have to do to get an NSBitmapImageReg to supply me with alpha from the example code in earlier technote? The above code seems to handle that case, but I cant seem to hit what the requirements of the view/window are to make the retuned NSBitmapImageRep have an alpha. I pretty consistently get My view subclass returns isOpaque NO, and my window containing the view is also not opaque, however it is typically not visible.</body>
  </mail>
  <mail>
    <header>Re: What's causing my text to become more bolder?</header>
    <body>&lt;a href="http://www.sky4studios.com/quartz/macbookProWhiteOnBlack.png</body>
  </mail>
  <mail>
    <header>capturing an NSView -&amp;gt; bitmap,	including cocoa Ui control interaction?</header>
    <body>I have been working on a webkit -&amp;gt; openGL plugin for Quartz Composer, which captures an offscreen WebView to a bitmap using [[NSBitmapImageRep alloc] initWithFocusedviewRect:someView] This works fine, however since adding in some faux NSEvents to convince the offscreen webview to receive mouse events, i notice that clicking cocoa NSControls escape from the captured bitmap/context and then mess things up royally after they have been pressed. forgive the length, the 'bug' is at just passed 1 min, where I press an pop up menu, the menu appears on screen and not within any window (its on the left of the screen, which is not what id like, id want it to be in the captured bitmap so its in the texture onscreen). So in short, is there a way to capture a bitmap of the view, including any subviews that have interactive NScontrols? Im fairly certain popup menus are their own windows, so im not sure how this would work. Ive attempted to do some OpenGL trickery using surface textures, but WebKit does not render to GL on its own it seems, and putting a webview into a CAOpenGLLayer seems to not work well, let alone I cant get a texture from a Core Animation layer.. Are there other possible techniques to doing fast offscreen, interactive rendering of a view? I have seen some other webkit based ports/frameworks that appear to do what I want (Awesomium/Clutter-WebKit comes to mind), but unfortunately they do not handle flash content with any grace (at least on the Mac ports it seems).</body>
  </mail>
  <mail>
    <header>Re: Set kCGWindowWorkSpace?</header>
    <body>The multiple displays all share a common coordinate space.  To put a window on a particular display you simply have to ask the display what its frame is in that coordinate space and put the window within that same frame.</body>
  </mail>
  <mail>
    <header>Re: Image IO alternative for Mac OS X 10.3</header>
    <body>Hi David, Thanks for the response! I am not really sure how I am gonna do this but I am thinking that this is kind of difficult to do but I&amp;#39;ll try. thanks again. hernan</body>
  </mail>
  <mail>
    <header>Re: Set kCGWindowWorkSpace?</header>
    <body>Hi David, Thanks for your response. Somewhat related - Is there perhaps an API to put a window on a particular display in case of a multi-display client? Thanks! Ronald</body>
  </mail>
  <mail>
    <header>Re: How to get CIColor from CIImage?</header>
    <body>unsigned char *PixelsFromCIImage(CIImage *inputImage, CGRect extent) [curContext render:inputImage toBitmap:outputPixels rowBytes:width * bytesPerPixel bounds:extent format:kCIFormatARGB8 On 3-Jul-09, at 2:36 AM, email@hidden wrote:</body>
  </mail>
  <mail>
    <header>Image IO alternative for Mac OS X 10.3</header>
    <body>Hi,</body>
  </mail>
  <mail>
    <header>How to get CIColor from CIImage?</header>
    <body>Hi Folks, I'm using CIAreaAverageFilter to get the average colour for a CIImage. Apparently CIAreaAverager returns a single-pixel image that contains the average color for the region of interest. So the question is, if I've got my single-pixel CIImage, how do I get it's CIColor? I thin it could be converted to a bit map, etc. but that seems sort of roundabout. Any advice would be much appreciated! Thanks, Max</body>
  </mail>
  <mail>
    <header>Re: Set kCGWindowWorkSpace?</header>
    <body>No, there is no API to put a window on a particular space. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Set kCGWindowWorkSpace?</header>
    <body>Hi, CGWindowListCreateDescriptionFromArray() returns info about specified (as in the term of Expose &amp;amp; Spaces) in which the window is currently showing. Is there a way - when creating a new window using Cocoa CreateCustomWindow() or whatever other API (perhaps HIView*()) - to specify on what &amp;quot;space&amp;quot; the window should appear? I guess I'm looking for something along the lines of kHIWindowVisibleInAllSpaces, but slightly different (i.e. not &amp;quot;one space&amp;quot; or &amp;quot;all spaces&amp;quot;, but &amp;quot;this space %d&amp;quot;). I.e. is there a way / API to &amp;quot;set&amp;quot; this property (kCGWindowWorkSpace)? Thanks, Ronald</body>
  </mail>
  <mail>
    <header>NSOpenGLView in NSCollectionView</header>
    <body>Hi all, I've been trying to use a custom view with an NSOpenGLView subview as the view prototype for an NSCollectionView but have found that all I see is garbage in the OpenGL views.  My intuition tells me that this is because the &amp;quot;magic&amp;quot; the AppKit uses to copy the view prototype doesn't jibe with OpenGL contexts.  Is there some sort of workaround for this, or is it impossible to use OpenGL? Thanks! David</body>
  </mail>
  <mail>
    <header>core image transition background colour</header>
    <body>Hello! My application transitions between various CIImage objects a lot using various transition CIFilters. the problem is, the &amp;quot;background&amp;quot; of these always seems to be black, and if I fade between two images that are not of the same size using a different coloured blackground, there is a sudden flickering of black as the fading image disappears. Is there any way to set the &amp;quot;base colour&amp;quot; of a transition? Thanks, marc.</body>
  </mail>
  <mail>
    <header>CI usage of IF statements (Was: Trouble writing simple CI Kernel)</header>
    <body>I've now been hit by another problem: before trying to code the sample below using the tertiary operator, I actually planed to use IF statements to differently handle each pixel, depending in it's RAW pattern position. Now Core Image does not support IF statements unless the &amp;quot;... condition can be inferred at the time the code compiles&amp;quot;, as written in the Core Image Reference. That makes it impossible to use IF based on anything related to the pixel position (like the vec2 type, as on the code below). To get around that I though I'd be smart and use nested tertiary operators and function calls within them. Not so smart - I just learned the tertiary operator will evaluate each function, making it unsuitable for my means. Best Mark</body>
  </mail>
  <mail>
    <header>Re: Trouble writing simple CI Kernel</header>
    <body>I believe you found a compiler bug; my guess is that the tertiary operator, when testing for vector for equality, really only tests the last component of the vector. You should report a bug about that. In the meantime, as a workaround, one can simply check the components individually. I'd do something like this: kernel vec4 simpleDebayer(sampler image) return (type.x &amp;gt; 0.5  ? (type.y &amp;gt; 0.5  ?  vec4(1,0,0,1)  :  vec4(0,1,0,1))  : Hope this helps, - Ralph</body>
  </mail>
  <mail>
    <header>Trouble writing simple CI Kernel</header>
    <body>I am having trouble writing a pretty primitive CI Kernel. I am trying to implement the most basic kind of debayering. For this, I need to treat pixels inside the kernel routine differently depending on their position and CFA color. So far I am just trying to draw a grid that shows each pixel colored after it's actual type (R, G or B). So basically I am dividing all pixels in even/odd, both horizontally and vertically. Heres the code. //CFA pixel types - for GBBG CFA ordering kernel vec4 simpleDebayer(sampler image) type = vec2(mod(floor(coord.x), 2.0), mod(floor(coord.y), 2.0)); // get the pixel type return (type == tG1 ? vec4(0,1,0,1) : (type == tR ? vec4(1,0,0,1) : (type == tB ? vec4(0,0,1,1) : vec4(0,0,0,1)))); //does not work??? //return vec4(type.x, type.y, 0.0, 1.0); //shows that type is working: dividing even/odd both vertically and horizontally ------------------------------ The problem seems to be the nested tertiary operators: I am just seeing vertical lines colored green and red, so only the first two expressions are returning TRUE, the bottom ones are always false. Another explanation would be that the vec2 comparison is only taking into account the x coordinate. But why - isn't it supposed to work. Best Mark</body>
  </mail>
  <mail>
    <header>Re: CGImage, NSImage from RAW data results in thumbnail jpeg?</header>
    <body>I always only get back 1 representation, or an image source with one image. - -initWithData returns creates an image with the thumbnail - CGImageSourceCreateWithData without the type hint creates the thumbnail - CGImageSourceCreateWithData with the type hint creates the full image on 10.4.x (YMMV on other OS releases) - -initWithURL or CGCreateImageSource creates the full image I can't use the last one (without an intermediate temp file) because these aren't stored in the file system as separate entities; they are streamed from elsewhere.</body>
  </mail>
  <mail>
    <header>Re: CGImage, NSImage from RAW data results in thumbnail jpeg?</header>
    <body>The RAW files typically have multiple image representations stored within them, and I believe the default representation is often the thumbnail.  Check for other image representations.  I've only used CGImageSource* to do this personally though, so I can't speak for NSImage. On Jul 27, 2007, at 9:47 AM, Jim Correia wrote:</body>
  </mail>
  <mail>
    <header>Re: Is CGEventPost[ToPSN] synchronous?</header>
    <body>I didn't know about CGRegisterScreenRefreshCallback --I'll have to look into it, but because I want to make it work whether there is a UI update or not, it might not work. Unfortunately, I'm trying to implement the interface for a pre- existing modeling framework which emits actions as mouse coordinates and raw key presses, so the Accessibility APIs are out, I believe. I _am_ able to modify the application that is being watched, so I didn't expect needing any sort of inter-application event passing -- merely doing NSApplication -sendEvent:'s should have worked.  The problem was that NSButtons et al. take over the run loop upon mouse- down, so the mouse-up event would not get sent.  As a work around, my code does a -postEvent: of the mouse-up event before doing a - sendEvent: of the mouse-down event.  This works fine for many cases, but not when there is a button that doesn't commandeer the run loop. Actually, now that I think of it, perhaps I can do a postEvent: of the mouse-up event, then _also_ sendEvent it afterwards.  I suppose a spurious mouse-up event does not cause a problem. Anyways, I'm afraid I'm straying off-topic.  Thank you for your help -- it saved me from writing the CGEventPost implementation merely to find that it wouldn't work!</body>
  </mail>
  <mail>
    <header>Re: Is CGEventPost[ToPSN] synchronous?</header>
    <body>On Jul 27, 2007, at 9:10 AM, Daniel Dickison wrote: You have guessed correctly.  The CGEventPostToPSN and related functions add events to a buffer to be delivered to the application when that application next returns to it's runloop to pick up events.   These mechanisms are intended more for the use of novel input devices and mechanisms than for synchronous interaction simulations. With these low level interfaces, some folks have had good luck in posting events, and then watching for UI responses using other APIs such as the low level: /* * Register a callback function to be invoked when an area of the display * is refreshed, or modified.  The function is invoked on the same thread * of execution that is processing events within your application. * userParameter is passed back with each invocation of the callback function. */ CG_EXTERN CGError CGRegisterScreenRefreshCallback( CGScreenRefreshCallback function, This is sometimes used in interactive test harnesses in conjunction with event posting to provide automated testing of applications. At a higher level, the Accessibility APIs may be more appropriate for your application. The Accessibility APIs provide you with a way to inspect and interact with the controls of other applications at a much higher level than the CG primitives.</body>
  </mail>
  <mail>
    <header>CGImage, NSImage from RAW data results in thumbnail jpeg?</header>
    <body>If I create an NSImage view initWithData, or a CGImage via CGImageSourceCreateWithData for a .NEF (generated by a Nikon D80) image, the resulting image is the embedded thumbnail. Is this the expected behavior? (And does it work this way with RAW formats for other cameras/manufacturers?) If I create the images by URL, or in the case of using CGImageSourceCreateWithData with a type identifier hint, I get the large RAW image as expected. I was(am) using the data initializer because the data isn't stored as a discrete filesystem entity, and I'd like the result to be an NSImage if possible. Any practical advice? (Besides using CGImageSourceCreateWithData with the type identifier hint and ended up with a CGImage?) Thanks, Jim</body>
  </mail>
  <mail>
    <header>Is CGEventPost[ToPSN] synchronous?</header>
    <body>I have a quick question regarding CGEventPost, which I haven't been able to find the answer to online.  I would very much appreciate if someone can clear this up for me. Does the function CGEventPostToPSN block until the event has been processed by the process? The process in question is a Cocoa app, so by &amp;quot;processed&amp;quot; I specifically mean that the derived NSEvent has been -sendEvent:'ed by the NSApplication object, or that it has been dequeued via - nextEventMatchingMask:untilDate:inMode:dequeue:. I'm guessing the answer is &amp;quot;no&amp;quot;, that the event gets added to a queue but CGEventPostToPSN returns before the event is dispatched.  But, if I'm wrong, that would be great news for me. The problem I'm trying to solve is implementing a way for models that simulate humans to interface with Cocoa applications.  I need the model to perform clicks and key presses that get sent to the application, but the model must wait until the event has caused any GUI changes before continuing.  If there is a better way to do this than via the Quartz Events API, I would appreciate input on that as well.</body>
  </mail>
  <mail>
    <header>Soft Proof</header>
    <body>I would like to allow the user to see the softproof version of the graphics drawn by my application. The application draws the following entities: 1. CGMutablePath were the colors are using calibrated RGB color space. 2. Images that are using various colorspaces. 3. PDF Files again with different colorspaces. The idea is that the user select the proper output device (maybe a CMYK printer) and the output is displayed correctly as the Soft Proof option does in the Print Preview application. The ImageApp does something like that but only for images: I wonder if it is possible to do something that applies for both CGMutablePaths and Images. Many Thanks Marco</body>
  </mail>
  <mail>
    <header>Re: Snow Leopard draws different colors from Leopard and before</header>
    <body>To be clear, the default gamma was changed to 2.2 in SnowLeopard from 1.8 previously. See &amp;quot;Gamma 2.2&amp;quot; in</body>
  </mail>
  <mail>
    <header>Quartz developer wanted</header>
    <body>Cardinal Peak, LLC, specializes in embedded product development for clients in video, audio and signal processing markets.  We hire only top-notch engineers with deep expertise and great attitudes.  We are based in Lafayette, Colorado. Cardinal Peak is seeking a senior engineer meeting the following requirements: - Bachelor‚Äôs degree in computer science - 10+ years professional development experience - Proficiency in C and Objective-C - Experience with Linux - Experience with browser plugins - Experience with Quartz - Experience with Core Audio - Experience with Quicktime Kit This position will be filled on a contract basis.  Individuals must (a) currently live in the Denver/Boulder area, and (b) already have authorization to work in the United States. Interested individuals should submit a resume to email@hidden.  Principals only, we are not considering candidates submitted by recruiting firms.</body>
  </mail>
  <mail>
    <header>Re: Snow Leopard draws different colors from Leopard and before</header>
    <body>&amp;gt; I create CGBitmapContext and draw on it by setting pixel data directly. Oh, it's not the Snow Leopard issue, but depends on Gumma. It is the same in Leopard, if display's gumma is set other than 1.8. I should consider this, any hint ? Yoshiaki Katayanagi</body>
  </mail>
  <mail>
    <header>Snow Leopard draws different colors from Leopard and before</header>
    <body>Hi, I create CGBitmapContext and draw on it by setting pixel data directly. Create a CGImageRef from it and draw it onto HIView context or PDF context. Result color on the screen is differ from leopard. 1. Create BItmapContext CGColorSpaceRef colorSpace = CGColorSpaceCreateWithName( context = CGBitmapContextCreate( buffer, width, height, 8, bytePerRow, 2. Set a color data to pixels UInt32 colorData = ((UInt32)(rgb.red &amp;amp; 0xFF00 ) &amp;lt;&amp;lt; 8) + ((UInt32)(rgb. 3. Create CGImageRef CGImageRef image = CGImageCreate( width, height, 8, 32, rowBytes, colorSpace, kCGImageAlphaNoneSkipFirst | kCGBitmapByteOrder32Host, provider, 4. Draw to HIView context If draw pixels using CGContextFillRect() at the process 2, showed color on the screen is correct. I examine the functions result on the buffer, and found that it convert thr color data ( maybe to adapt to Gamma 2.2 ). What can I do in this situation? I can't use CGContextFillRect() because of performance, the function itself is slow, and it is not thread safe for one CGBitmapContext. If I can convert color data as CGContextFillRect(), it maybe the easiest way. Regards. Yoshiaki Katayanagi</body>
  </mail>
  <mail>
    <header>CGContextDrawImage, alpha premultiplication, and dark halos.</header>
    <body>When using CGContextDrawImage, does it unpremultiply the pixels it is putting down on a CGBitmapContext?  I'm thinking it isn't, as I'm getting some interesting halos when I use CGContextDrawImage to repeatedly draw an image over (in this case, as a brush): If you're having a hard time seeing the halos, here's an image with the contrast turned up: The original image is created using CGContextFillEllipseInRect, which if I use instead of CGContextDrawImage, works great.  However, I'd rather use images since sometimes the brushes are different shapes. Do I have any good options to keep the halos from happening, or should I look into using some other framework? August 'Gus' Mueller Flying Meat Inc.</body>
  </mail>
  <mail>
    <header>CoreImage and NSImage on 10.6</header>
    <body>I have a very annoying crash/hang on snow leopard 10A432 when converting CIImage to NSImage. Here is the stack : #0	0x7fff88588994 in fe_tree_node_simplify #1	0x7fff885879f4 in fe_tree_refactor_transforms_ #2	0x7fff8858857e in fe_tree_refactor_transforms_ #3	0x7fff885885a3 in fe_tree_refactor_transforms_ #4	0x7fff8858857e in fe_tree_refactor_transforms_ #5	0x7fff885885a3 in fe_tree_refactor_transforms_ #6	0x7fff8858857e in fe_tree_refactor_transforms_ #7	0x7fff88587986 in fe_tree_refactor_transforms #8	0x7fff885870d9 in fe_tree_prepare_tree_ #9	0x7fff885852ca in fe_tree_render_image #10	0x7fff88584cab in fe_image_render_ #11	0x7fff885e270b in fe_image_render #12	0x7fff885e122e in -[CIOpenGLContextImpl renderWithBounds:matrix:function:info:] #13	0x7fff885e1118 in -[CIContextImpl render:] #14	0x7fff883d56b2 in cgxcoreimage_instance_render #15	0x7fff86f49395 in CGXCoreImageInstanceRender #16	0x7fff835ef27a in ripc_AcquireCoreImage #17	0x7fff835e4490 in ripc_DrawShading #18	0x7fff86e61439 in CGContextDrawShading #19	0x7fff885de687 in -[CICGContextImpl render:] #20	0x7fff885de25f in -[CIContext drawImage:inRect:fromRect:] #21	0x7fff88622013 in -[CIContext drawImage:atPoint:fromRect:] #22	0x100035cce in -[CIImage(IMImagesUtilities) nsImageWithSize:] at IMImagesUtilities.m:643 Is anyone familiar with this issue? if (([self extent].size.width == 0) || ([self extent].size.height == 0)) if (CGRectIsInfinite([self extent])) imageToAdd = [IMImagesUtilities cropImage:self else // Create a new NSBitmapImageRep. NSBitmapImageRep * theBitMapToBeSaved = [[NSBitmapImageRep alloc] initWithBitmapDataPlanes:NULL pixelsWide:newRect.size.width pixelsHigh:newRect.size.height bitsPerSample:8 samplesPerPixel:4 hasAlpha:YES isPlanar:NO colorSpaceName:NSDeviceRGBColorSpace bytesPerRow:0 // Create an NSGraphicsContext that draws into the NSBitmapImageRep. (This capability is new in Tiger.) NSGraphicsContext *nsContext = [NSGraphicsContext // Save the previous graphics context and state, and make our bitmap context current. // Get a CIContext from the NSGraphicsContext, and use it to draw the  CIImage into the NSBitmapImageRep. [[nsContext CIContext] drawImage:imageToAdd atPoint:CGPointZero // Restore the previous graphics context and state. @end Romain Piveteau Author of LiveQuartz, &amp;quot;Free photo reTOUCHing for Leopard&amp;quot;. Paris, France email@hidden</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics and Python</header>
    <body>It's really hard to find documentation on all of this things ! In the old way this function gets a context from a file with that : file = CGDataProviderCreateWithFileName(theFileName) originalImage = CGImageImport(file) But now I don't really see how to handle that with CGImageSourceCreateImageAtIndex.. I try to use it with CGImageSourceCreateWithURL, but I didn't find how to create a URL from a path no the disk. If somebody knows how it work.. Is there still some apple documentation on line or in the ADC site, I didn't find anything !! This script was very useful for me to call in command line from a web site to get the power of CoreGraphics on the server. Thanks Cedric</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics and Python</header>
    <body>I had been bitten by this some time ago when trying to call setLineDash(). ¬†The solution turned out to be as simple as: ¬† dashes = CGFloatArray( 2 ) ¬† dashes[ 0 ] = 2.0 ¬† dashes[ 1 ] = 2.0</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics and Python</header>
    <body>I'm not familiar with CGImageImport (I'm not entirely familiar with the CGBindings either, but enough people ask me about them!), but I would imagine it would be covered by ImageIO. CGImageSourceCreate followed by CGImageSourceCreateImageAtIndex sounds like it would do what you want. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics and Python</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics and Python</header>
    <body>best, Hi all, I&amp;#39;m using since years a short python script to resize my images and others very useful functions which works great !! Now i try to use them in a Snow Leopard Server, but I have a big problem : my &amp;quot;import CoreGraphics&amp;quot; doesn&amp;#39;t work anymore.. Do you know how to use this CoreGraphics library under python 2.5 or newest ? Thanks for your help !</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics and Python</header>
    <body>I'm using since years a short python script to resize my images and others very useful functions which works great !! Now i try to use them in a Snow Leopard Server, but I have a big problem : my "import CoreGraphics" doesn't work anymore.. Do you know how to use this CoreGraphics library under python 2.5 or newest ?</body>
  </mail>
  <mail>
    <header>CoreGraphics and Python</header>
    <body>I'm using since years a short python script to resize my images and others very useful functions which works great !! Now i try to use them in a Snow Leopard Server, but I have a big problem : my &amp;quot;import CoreGraphics&amp;quot; doesn't work anymore.. Do you know how to use this CoreGraphics library under python 2.5 or newest ? Thanks for your help !</body>
  </mail>
  <mail>
    <header>Re: Getting data out of an image.</header>
    <body>Is it a loss of precision or bad values? Raphael Hi Raphael, that&amp;#39;s great thanks. I tried option one but the numbers were slightly wrong. There was some loss of precision in the floating point numbers. I&amp;#39;ll give option 2 a go and see how that goes. Many thanks, Max 2009/8/28 Raphael Sebbe &amp;lt;&amp;gt;: &amp;gt; Hi,</body>
  </mail>
  <mail>
    <header>Re: Getting data out of an image.</header>
    <body>Hi Raphael, that's great thanks. I tried option one but the numbers were slightly wrong. There was some loss of precision in the floating point numbers. I'll give option 2 a go and see how that goes. Many thanks, Max 2009/8/28 Raphael Sebbe &amp;lt;email@hidden&amp;gt;:</body>
  </mail>
  <mail>
    <header>Re: Getting data out of an image.</header>
    <body>Hi, Hi All, I&amp;#39;ve seen a post about how to do this dated back to 2006... ¬† ¬†&amp;quot;For float pixel processing you need to read back the pixels from the GPU using glReadPixels or texture readback. ¬† ¬† Frank I&amp;#39;m trying to do the same thing. I have my OpenGL context created. Then I render my CIImage to that context using the method This is where I get confused. To access my pixel data, would I now access the bitmap created by method &amp;quot;render:toBitmap:rowBytes:bounds:format:colorSpace:&amp;quot;? Or, would I use glReadPixels? Does the render draw the image both to the bitmap memory location and into the GL context? In which case I assume I&amp;#39;m best to access it from the GL context to get the floating point precision? Sorry folks. This probably looks like a baby trying to ride a motorbike, let alone take a few first steps! Any advice much appreciated. Cheers, Max ¬†_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list ¬† ¬† ¬†() Help/Unsubscribe/Update your Subscription: This email sent to</body>
  </mail>
  <mail>
    <header>Re: Getting data out of an image.</header>
    <body>Hi All, I've seen a post about how to do this dated back to 2006... &amp;quot;For float pixel processing you need to read back the pixels from the GPU using glReadPixels or texture readback. Frank I'm trying to do the same thing. I have my OpenGL context created. Then I render my CIImage to that context using the method This is where I get confused. To access my pixel data, would I now access the bitmap created by method &amp;quot;render:toBitmap:rowBytes:bounds:format:colorSpace:&amp;quot;? Or, would I use glReadPixels? Does the render draw the image both to the bitmap memory location and into the GL context? In which case I assume I'm best to access it from the GL context to get the floating point precision? Sorry folks. This probably looks like a baby trying to ride a motorbike, let alone take a few first steps! Any advice much appreciated. Cheers, Max</body>
  </mail>
  <mail>
    <header>_subtreeDescription: What do the ampersands mean?</header>
    <body>In the line of GDB output below, what do the ampersands following the h= and v= mean? (gdb) po [self _subtreeDescription] [ D A     W#] h=--&amp;amp; v=&amp;amp;-- NDView 0x10012fca0 f=(0,113,480,473) b=(-) =&amp;gt; &amp;lt;CALayer: 0x1001304d0&amp;gt; TIME drawRect: min/mean/max 0.00/0.00/0.00 ms A=autoresizesSubviews, C=canDrawConcurrently, D=needsDisplay, F=flipped, G=gstate, H=hidden (h=by ancestor), O=opaque, P=preservesContentDuringLiveResize, S=scaled/rotated, W=wantsLayer (w=ancestor wantsLayer), #=has surface</body>
  </mail>
  <mail>
    <header>Re: Converting from float [0-1] to int [0-255] range</header>
    <body>I suggest error diffusion. This will distribute quantization error throughout your image and keep the total energy of the quantized image closest to the source image. The simplest way, when scanning horizontally : * set error to zero Then, for each pixel : * add error to source value, quantize pixel * compute new error : quantized value - source value * carry the error to the next pixel With this, the quantization algorithm won't matter much. You can use quantized = round(source). If you convert a subtle gradient, you'll see the difference right away. Raw pixel by pixel quantization will give you banding, error diffusion will give you something smooth. - if you were to compute the total quantization error for each image, you'd get a big number for the left one and something close to zero for the right one.)</body>
  </mail>
  <mail>
    <header>Re: Converting from float [0-1] to int [0-255] range</header>
    <body>Hi all, First of all, thanks for all the replies. What I am after is the correct way to convert RGB triplets from floating-point to integer representation. In Quartz RGB(A) values are usually expressed with floating-point values in the 0-1 range. Usually a Red, Green, Blue and sometimes an Alpha channel. No mistery so far... In my code it is more convenient to have them expressed as raw unsigned bytes, which is what they ultimately are. I am looking for the correct way to implement this conversion. In my previous email I assumed that the correct way to perform this conversion is quantization with equal distribution of all the values. i.e. like Graeme suggested. However, judging by the general tone of the replies maybe my assumption is incorrect. Maybe rounding is the correct way to implement this conversion, even if the first and last value only cover half the range, like the following: In short, what is the right way that ought to be used to convert from floating-point to integer representation when dealing with RGB values? Thanks in advance. - Luigi</body>
  </mail>
  <mail>
    <header>Re: Converting from float [0-1] to int [0-255] range</header>
    <body>Actually it's symmetrical, the result 255 also only covers half the range. This is the correctly rounded result. You seem to be after something else though. Something that should do what you want within the tolerance of floating point precision is: if (i == 256) Note that for an even distribution of floats, the chances of landing on 1.0 is 1/ the precision of the significand (mantissa), so 255 will get an infinitesimal number of extra hits. (You've got to put that extra possible value into one of the slots though, no matter how you do it!). The extra value could be moved to the middle by doing something like Graeme Gill.</body>
  </mail>
  <mail>
    <header>Re: Converting from float [0-1] to int [0-255] range</header>
    <body>I use (float)f / 255.0f to convert to float and fmaxf(0.0f, fminf(f * 256.0f, 255.0f)) to convert from float, which fulfills my primary requirement: round-trip compatibility, i.e. every byte value converted to float and back again produces the original value. Looking at it, a more theoretically pure version of the same approach would be fmaxf(0.0f, fminf(1.0f, f * nextafterf(256.0f, -1.0f))), which maps [0, 1] to [0, 256) rather than [0, 256]. -- Jens Ayton</body>
  </mail>
  <mail>
    <header>Converting from float [0-1] to int [0-255] range</header>
    <body>Hi, I have an RGB triplet of floating-point values in range [0, 1] which I need to convert to a RGB triplet of unsigned bytes in range [0, 255]. ...A seemingly simple problem, but in reality quite complicated: The obvious solution looks like this: unsigned char QuantizeFloat(float f) This works in so far that I get all numbers from 0 to 255, but the distribution of the integers is not even. The function only returns 255 if a is exactly 1.0. Not a good solution. If I do proper rounding I just shift the problem: unsigned char QuantizeFloat(float f) Here the the result 0 only covers half of the float-range than any other number. How do I do a quantization with equal distribution of the floating point range? I would like to get an equal distribution of integers if I quantize equal distributed floats. Thanks for any suggestion. - Luigi</body>
  </mail>
  <mail>
    <header>Re: Kernel to return vec4 coordinate values from destCoord()? 	Possible?</header>
    <body>Hi Steve, looks like I have some sensible looking values to play with. Not sure if they're correct but at least they look like proper numbers! I had to cast the unsigned char to a float pointer. The trick was to render to CIContext using kCIFormatRGBAf. This article was helpful too... I'm starting to realise there's a whole lot more to this than I initially thought. I hope my posts aren't raising too many groans out there in list land... Take care, Max 2009/8/26  &amp;lt;email@hidden&amp;gt;:</body>
  </mail>
  <mail>
    <header>Re: Kernel to return vec4 coordinate values from destCoord()? 	Possible?</header>
    <body>thanks Steve. I think I'm getting back 16 bit float values from the kernel. So I need to convert pairs of unsigned char's into floats. I'm going to try that and see how the values look. Cheers, Max 2009/8/25 Steve Israelson &amp;lt;email@hidden&amp;gt;:</body>
  </mail>
  <mail>
    <header>ChromaKey CIFilter?</header>
    <body>I asked this in QT-API, but didn't get a response.  It seems like this is more the place for CIFilter discussions.  I'm really surprised that I can't find much of anything through Google, even on algorithms. Several apps, e.g. iChat, use either chromakey or background removal.  I haven't found anything on how to do this, other that at the QuickTime level. I would like to do this by using a CIFilter on a QTCaptureLayer or QTMovieLayer.  Am I going to have to reinvent the wheel here by writing my own CIFilter?  Any help would be appreciated.</body>
  </mail>
  <mail>
    <header>Re: Kernel to return vec4 coordinate values from destCoord()?	Possible?</header>
    <body>The image data can only be values between -1.0 and +3.0. If your coords are outside that, then they will not be correct. You can scale them to be by dividing by the width and height so they are a number from 0-1.0</body>
  </mail>
  <mail>
    <header>Kernel to return vec4 coordinate values from destCoord()? Possible?</header>
    <body>Hi Folks, question for you guys regarding Core Image kernel filters. I'm trying to write something that should return some coordinate values. I know this is non-standard &amp;amp; no doubt I'm making a mistakes on a few different fronts. My question is, if I return a vec4 that has destCoord() values for x and y, should I expect to see some reasonable values for my 640 x 480 video? And if so, is there anyway to get at them without CIContext rendering the bitmap? I know the rending is pre-multiplying and converting to int's. I'd guess it's doing some other funky stuff as well. Any tips as always much appreciated. Many thanks, Max</body>
  </mail>
  <mail>
    <header>Re: How to get a bitmap image from a CIImage?</header>
    <body>A CIImage doesn't actually have data - its just a &amp;quot;recipe&amp;quot; of how to process whatever source data you provide to it, which is applied on demand. So you have to render a CIImage (or part of it) to get data, e.g., to a CGImage. You can then get data from that with the usual CGImageGetDataProvider(), etc calls.</body>
  </mail>
  <mail>
    <header>How to get a bitmap image from a CIImage?</header>
    <body>Hi. I'd like to process a movie frame by frame, and I get each frame through this method: - (CIImage *)view:(QTMovieView *)view willDisplayImage:(CIImage *)image which geves me a CIImage. But to process a CIImage I need to build a CIFilter (because the built in filters supplied by Apple are not the ones I want). But to perform that I need to know about OpenGL shading language and design the filter to operate in a pixel by pixel fashion, something that is not easy sometimes. So I was just wondering if I could get the CIImage bytes, operate on it and then build another CIImage with the result. Performance has to be considered. Thank you for any suggestion. Marcelo Cicconet. -- www.marcelocnt.com www.impa.br/~cicconet</body>
  </mail>
  <mail>
    <header>Re: Initialising a NSBitmapImageRep using toBitmap data from 	CIContext?</header>
    <body>Hi Patrick, David, brilliant, I got this going. Worked first time with the initWithBitmapDataPlanes method. Thanks both of you for your advice on this. Here's the method I used... Mostly cut and pasted from one of the Apple examples. &amp;quot;Cocoa Drawing Guide&amp;quot; had some good info on this as well. Cheers, Max //-------------------------------------------------------------------------------------------------- - (NSBitmapImageRep*) _bitmapImageForRawBitmapBuffer:(unsigned char *)buffer withWidth:(int)width andHeight:(int)height //User NSBitmapImageRep to allocate a memory buffer of ARGB 32 bits pixels - We use the &amp;quot;NSCalibratedRGBColorSpace&amp;quot; so that no color profile is embedded in the bitmap bitmap = [[NSBitmapImageRep alloc] initWithBitmapDataPlanes:&amp;amp;buffer pixelsWide:width pixelsHigh:height bitsPerSample:8 samplesPerPixel:4 hasAlpha:YES isPlanar:NO colorSpaceName:NSCalibratedRGBColorSpace bitmapFormat:NSAlphaFirstBitmapFormat bytesPerRow:bitmapRowBytes 2009/8/25  &amp;lt;email@hidden&amp;gt;:</body>
  </mail>
  <mail>
    <header>Re: Initialising a NSBitmapImageRep using toBitmap data from 	CIContext?</header>
    <body>Hi Patrick, if I create a NSCIImageRep though, I still have some work to do to get that into a bitmap. I guess somewhere I'd still need to go with one of the init with planes methods David's mentioned. Do you know if there's any shortcuts? These look pretty hefty. Some of the parameters I know but others are going to take some digging. Onwards and upwards on the learning curve.... initWithBitmapDataPlanes:pixelsWide:pixelsHigh:bitsPerSample:samplesPerPixel:hasAlpha:isPlanar:colorSpaceName:bitmapFormat:bytesPerRow:bitsPerPixel: initWithBitmapDataPlanes:pixelsWide:pixelsHigh:bitsPerSample:samplesPerPixel:hasAlpha:isPlanar:colorSpaceName:bytesPerRow:bitsPerPixel: Cheers, Max 2009/8/24 Patrick Geiller &amp;lt;email@hidden&amp;gt;:</body>
  </mail>
  <mail>
    <header>Re: Initialising a NSBitmapImageRep using toBitmap data from	CIContext?</header>
    <body>initWithData: expects a compressed image, along the lines of what ImageIO is capable of reading. You need to use one of the initWithBitmapDataPlanes: methods which allow you to describe how your data is organized. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: CIFilter/CIKernel RGB Values</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Same image but different looks</header>
    <body>Why not assign the sRGB colour space for &amp;quot;plain&amp;quot; BMPs?  The *entire point* of sRGB was that it matched a &amp;quot;typical&amp;quot; PC monitor, so it's pretty clear that if something doesn't provide a profile, assuming sRGB is at least a good default. The only time where it wouldn't be is if you knew that the file format in question was a Mac format (e.g. PICT), in which case you need to use something with an effective gamma of 1.8 rather than 2.2. --</body>
  </mail>
  <mail>
    <header>Initialising a NSBitmapImageRep using toBitmap data from CIContext?</header>
    <body>Dear All, wondering if anyone has any experience with this? I've been struggling with this most of the afternoon. I know this must be a pretty straightforward thing to do, but unfortunately as a newbie I'm not having much luck. Basically, I have a bitmap of¬†outputPixels created successfully as a result of rendering a CIContext. unsigned char *outputPixels = (unsigned char *) calloc(width * [curContext render:image toBitmap:outputPixels rowBytes:width * bytesPerPixel bounds:extent format:kCIFormatARGB8 colorSpace:CGColorSpaceCreateWithName(CFSTR(&amp;quot;kCGColorSpaceGenericRGBLinear&amp;quot;)) I want to then take &amp;quot;outputPixels&amp;quot; and create a¬†NSBitmapImageRep object. First I need to create the NSData... NSData *imageData = [[NSData alloc] initWithBytes: then I try to create the¬†NSBitmapImageRep... NSBitmapImageRep *nsImage = [[NSBitmapImageRep I know this isn't working as &amp;quot;nsImage&amp;quot; is nil. According to the documentation this would happen if &amp;quot;Unable to interpret the contents of bitmapData.&amp;quot; I assume my sizes are OK and that they would be the width * the height * the number of bytes per pixel (which for RGBA would be 4). Any ideas? As always any pointers would be much appreciated. Thanks folks, Max</body>
  </mail>
  <mail>
    <header>Re: Same image but different looks</header>
    <body>On Mon, Aug 24, 2009 at 4:29 AM, Andreas And what about other color profiles?  What if someone saves an image on a Mac and embeds their device profile, or uses the Adobe RGB profile, or‚Ä¶ Perhaps you should be designing for the platform with color management and telling those who don't have color management that it's their problem. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: CIFilter/CIKernel RGB Values</header>
    <body>a) This would probably be a good idea.  In my test cases for now though I'm only using TIFF images with an alpha channel of 0xFF, so I don't think this is a problem currently. b) I'm suspecting this might be a part of the problem.  The code I have creates a CIImage from a NSBitmapImageRep (initWithBitmapImageRep).  If necessary I could use imageWithBitmapData and pass a nil colorspace, but I don't want to have to format the image data myself if it's not necessary. c) The code I submitted has a clamp value between 0 &amp;amp; 1 which is the min and max of the rgb values I believe.</body>
  </mail>
  <mail>
    <header>Re: CIFilter/CIKernel RGB Values</header>
    <body>a) In my experience, you have to do unpremultiply/premultiply: ..................... b) You could also be running into color profile issues - are you sure the color profile in the CIFilter and the profile in the NSBitmapImageRep are the same? If they aren't you could be seeing the effect of translation between color profiles I'm also a bit confused by your clamp - if you want to clamp to 200 on 0-255 scale, shouldn't you be clamping to 0.78 in the CI Filter?</body>
  </mail>
  <mail>
    <header>Re: Same image but different looks</header>
    <body>Well, AFAICS there's not really an easy way around this problem because the users can always mix plain BMPs with sRGB PNGs using my application and then they'll get different results and there's not much I could do against this. So if someone complains to me I'll tell him to upgrade to SL :) Greets, Andreas --</body>
  </mail>
  <mail>
    <header>Re: How to set the video input programmatically?</header>
    <body>Hi vade, Ive tried to put all this together with my jumbled non programming orientated mind. Im really hoping you (or someone here) can take some time and  have a look over it to see what is glaringly wrong? Ive been googling this kinda thing for weeks ( and buying programming books) and you are the only person it seems in the world who can offer any advice on this. Where did Pierre go? Attachment: Attachment: Essentially the code should take the video output form a default attached video camera / iSight and pass its output to the QC published image input &amp;quot;InputImage&amp;quot; for the Comp &amp;quot;InputComp&amp;quot;.  The code does complie, with a few warnings regarding local declarations hiding instance variables. Of NOTE it also comes up with a warning saying QTCaptureVideoPreviewOutput may not respond to _setCaptureSession on line 68. Of course its not working (this is way over my head) ...... but I seriously cant proceed any further without some technical help. Anyone? I'm desperate!</body>
  </mail>
  <mail>
    <header>Re: Same image but different looks</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Same image but different looks</header>
    <body>Thanks for pointing this out! Then I'll simply do nothing and wait for Snow Leopard :) Andreas --</body>
  </mail>
  <mail>
    <header>Re: Same image but different looks</header>
    <body>On Aug 23, 2009, at 9:01 AM, Jacob Gorban wrote:</body>
  </mail>
  <mail>
    <header>Re: Creating a CGImageRef from a .BMP file?</header>
    <body>The Image IO framework is new as of 10.4. They are considerably more modern than the Quicktime importers and are the preferred implementation going forward. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Creating a CGImageRef from a .BMP file?</header>
    <body>I didn't even know that it was possible with CG API. One solution: BMP (or PNG, GIF, JPG, TIF, PSD etc.) data to Handle to QT GraphicsImportComponent (HandleDataHandlerSubType) to QD GWorld (ARGB32) to CGImage. The greatest challenge was for me creating the Handle object for the graphics importer object (eg. setting up a dummy file name (&amp;quot;dummy.bmp&amp;quot;), mime type and finally the actual data). Otherwise it didn't work.</body>
  </mail>
  <mail>
    <header>Re: Creating a CGImageRef  from a .BMP file?</header>
    <body>If you can give Image IO a url to the bitmap instead of loading it yourself, it might get more information that it can use to deduce the file... You should probably use the default CF callbacks instead of NULL here, although I don't think it should cause you problems (unless the directory it's returning is NULL) Image IO uses 1-based indexing. Your 0s should be 1s. Of course this means nothing as long as cnt comes back as 0 :). -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Deprecated APIs</header>
    <body>//No more QT Calls Sandeep This is not enough of a code sample to go on.&amp;nbsp;&amp;nbsp;What is done with the pixels once you&amp;#39;ve gotten ahold of the base address? You left off the parameters to QTNewGWorld so it&amp;#39;s really hard to tell what pixel format the pixels in the PixMap are using. From what you&amp;#39;ve posted the best we might be able to come up with is: pixels = malloc(/*some number of pixels */) Scott</body>
  </mail>
  <mail>
    <header>Creating a CGImageRef  from a .BMP file?</header>
    <body>I'm trying to create a CGImageRef from a .BMP that I have in memory.  The entire file has been read into a block of memory that hndBitMap is a handle to.  I'm trying to use CGImageSourceCreateWithData, passing in a dictionary containing kCGImageSourceTypeIdentifierHint set to kUTTypeBMP.  Everything seems to work until I get to the call CGImageSourceCreateImageAtIndex, which returns NULL, in the two diagnostic calls I added afterward, cnt comes back 0, and stat comes back as kCGImageStatusUnknownType. Am I going about this the completely wrong way? Is there a better way to do this (ultimately I need to recompress the image in memory as a PNG)? Thanks in advance for any help anyone can provide, Stephen Handle      hndBitMap = NULL;            // handle to actual BMP disk data read into memory if (hr == S_OK) // Add the uniform type identifier of the image to our CGImageSource options.</body>
  </mail>
  <mail>
    <header>Re: animation performance</header>
    <body>I don't know if it would be faster to load the mosaic image (the one with all the sprite frames) into one layer; or to create individual layers for each sprite where each layer contains that portion of the mosaic for that sprite; or if it would be faster to create individual layers, one for each frame of each sprite. However a layer is going to give you an advantage because if you create your layers, and supply a screen context as the destination that you pass to CGLayerCreate, the system is going to ensure that whatever you draw into the layer is color matched and set up in memory in an optimal fashion for display on the screen. Secondly, the operating system can move a CGLayer's backing store to VRAM so that each time you draw that layer it is completely composited on the video card and the CPU is not involved (ok... probably minimally involved).  So repeatedly drawing a layer to the screen (or whatever destination it was created with) is supposed to be FAST. If I had to pick a strategy based on what I've done in the past, I would try creating a separate layer for each sprite.  I would have that layer contains the mosaic image for the sprite (perhaps by using CGImageDraw and the larger mosaic image to establish the content of the Layer). I would then arrange your code so that it uses clipping and an offset to draw the layer in such a way that the individual frames of the sprite get drawn. I'd expect that to perform very, very well. But I have to qualify this with saying that I haven't written any sprite animation code for Quartz 2D (and it's been a long time since I wrote any for QuickDraw). They are going to be much more effective about helping you eek the very best performance out you're code.  They look intimidating at first, and they are very full-featured, but I've had pretty good luck just using them in the most basic of ways. Scott</body>
  </mail>
  <mail>
    <header>Re: Wrong image colors?</header>
    <body>I'm not sure that it is.  Core Graphics does not generally deal with Indexed Color models in the traditional sense of color lookup tables.  Some consider it a limitation, but honestly it doesn't come up that often. If I were a betting man, I would say that your best chance at getting at the raw index values in a GIF file, on a modern Mac OS X system, would probably lie through QuickTime. I can't say for sure that such a path exists, and I'd have to do some investigation to find it but if it's anywhere, QuickTime handles enough legacy formats that that would be a good place to start.</body>
  </mail>
  <mail>
    <header>Fwd: Wrong image colors? (now CoreFoundation)</header>
    <body>I responded to Rick off-list, but just in case it&amp;#39;s useful for other people.... Of course, others may not agree with my simplified description ---------- Forwarded message ---------- Date: Aug 22, 2007 7:11 PM Subject: Re: Wrong image colors? (now CoreFoundation) email@hidden Are there any good books on CoreFoundation?&amp;nbsp;&amp;nbsp;It seems to not be covered in</body>
  </mail>
  <mail>
    <header>Re: CoreFoundation (was Re: Wrong image colors?)</header>
    <body>There is documentation on CoreFoundation and Foundation memory management on developer.apple.com. They cover everything you need to know at a high level. If there are exceptions, then they will be covered explicitly on API that violates them, but this is very very rare. Read the docs on Toll-Free bridging (which covers CoreFoundation vs Foundation, and a few other places). Additionally read the documentation on the API your working with, they often speak of what you need to do to convert types, or have explicit functions/methods to do that work for you. -- David Duncan Apple DTS Quartz and Printing email@hidden</body>
  </mail>
  <mail>
    <header>CoreFoundation (was Re: Wrong image colors?)</header>
    <body>Are there any good books on CoreFoundation?  This material seems to be missing from Hillegass. The multiplicity of libraries confuses me.  You can make a Dictionary in Objective C, CoreFoundation, and I think a few other places, and I'm not sure when I would choose one over the other or convert from one to another.  CF and CG have a lot of redundant types, and CG and CI overlap. Rick</body>
  </mail>
  <mail>
    <header>Re: Wrong image colors?</header>
    <body>On 8/22/07, Ok, I give in, you&amp;#39;re right :) But it&amp;#39;s really confusing! Here&amp;#39;s betting that 7 out of 10 developers not familiar with all these CG background (and honestly, who has the time to read up &amp;amp; keep in mind all these detailed docs) infer from the official documentation to &amp;quot;release the damn color space when you&amp;#39;re done with it.&amp;quot; Ok, fine, so I won&amp;#39;t release it.</body>
  </mail>
  <mail>
    <header>Re: Wrong image colors?</header>
    <body>Ok, I give in, you're right :) But it's really confusing! Here's betting that 7 out of 10 developers not familiar with all these CG background (and honestly, who has the time to read up &amp;amp; keep in mind all these detailed docs) infer from the official documentation to &amp;quot;release the damn color space when you're done with it.&amp;quot; Ok, fine, so I won't release it. Problem solved. But back to the issue that's much more important to me: How do I get the original colors of a CLUT based image? I don't want all this clandestine color space conversion. I just want the plain pixels of a CLUT image without writing my own GIF loader. It must be possible somehow through the API?! Andreas</body>
  </mail>
  <mail>
    <header>Re: Wrong image colors?</header>
    <body>Darn it.  This is the wrong link.  It should be: /apple_ref/doc/uid/TP30001066-CH202-CJBIBHHB</body>
  </mail>
  <mail>
    <header>Re: Wrong image colors?</header>
    <body>On Aug 22, 2007, at 10:21 AM, Andreas Falkenhahn wrote: Ahem: &amp;quot;You are responsible for retaining and releasing the color space **as In this case releasing the color space is not necessary because you have not first retained it. That does not change the fact that you are receiving an object from a &amp;quot;Get&amp;quot; call and as the naming convention rules state: source: /apple_ref/doc/uid/20001148-DontLinkElementID_4 Augmented, of course, by the Core Graphics object ownership documentation at: which paraphrases to the idea that Core Graphics follows the Core Foundation memory management naming conventions. This is wholly irrelevant.  Chances are that in drawing the image someone else retains it.  In that case, your superfluous release is undoing that retain.</body>
  </mail>
  <mail>
    <header>Re: Wrong image colors?</header>
    <body>Hmm, I'm a bit clueless concering the terminology here. What exactly does it mean to &amp;quot;retain&amp;quot; the color space? If I pass the color space to CGBitmapContextCreate() do I &amp;quot;retain&amp;quot; it then? Or in other words: Under what circumstances must I release the color space and under what circumstances may I not release it? Andreas --</body>
  </mail>
  <mail>
    <header>Re: Wrong image colors?</header>
    <body>As Scott said, the &amp;quot;get&amp;quot; design pattern applies here.  The documentation could be clearer but I'm sure it's trying to say that if you retain it you have to release it.  Otherwise don't release it.</body>
  </mail>
  <mail>
    <header>Re: Wrong image colors?</header>
    <body>Ahem: CGImageGetColorSpace Return the color space for a bitmap image. &amp;quot;The source color space for the specified bitmap image, or NULL if the image is an image mask. Source: And as I said, after creating a bitmap context and drawing the image to it, the color space retrieved from CGIGCS() actually can be released and all images can also be freed without any crash. Andreas</body>
  </mail>
  <mail>
    <header>Re: Wrong image colors?</header>
    <body>When you are calling a routine with &amp;quot;Get&amp;quot; in the name, you do not release it.  You only release items that you get from routines that contain &amp;quot;Create&amp;quot; or &amp;quot;Copy&amp;quot;.</body>
  </mail>
  <mail>
    <header>Re: animation performance</header>
    <body>On Aug 22, 2007, at 9:27 AM, Brother Josef wrote: I expect part of the problem is that you are trying to drive a loop instead of just letting the system run it's normal event loop. You probably don't want to drive your own loop for the animation, instead, let the operating system handle the looping in the normal course of event processing. You should handle invalidating parts of the display that need redrawing, and handling the drawing when you are asked to do so. I probably don't have enough information to really help you at this point. I can say that if you draw the same CGImage repeatedly, then the system has the opportunity to use cached information about the image to optimize it's drawing.  It sounds like when you change the sprite image, you are causing the system to have to recalculate... something.  In this case Shark may be your friend as it could tell you what new code is being run when you change images. How are the CGImages that you use for the frames of your sprites created? (You're not making changes to the CGImages themselves are you?) One potential slow-down is color matching, are you using the system display color space for your CGImages? Have you considered using CGLayers instead of CGImages? If I were creating a sprite engine,  I might consider creating separate layers for each frame of the sprite and then drawing the appropriate layer when the sprite needs to drawn.</body>
  </mail>
  <mail>
    <header>Re: animation performance</header>
    <body>On Wed, August 22, 2007 10:27 am, Brother Josef said: I'll let others talk about what the 'best' way to do this is, but I did just want to mention that Cocoa is documented as caching images whenever it can.  In-video-card if possible, even. This makes re-drawing the same image _much_ faster than drawing new images, as it doesn't have to load and render the image every time you use it. Daniel T. Staal --------------------------------------------------------------- This email copyright the author.  Unless otherwise noted, you are expressly allowed to retransmit, quote, or otherwise use the contents for non-commercial purposes.  This copyright will expire 5 years after the author's death, or in 30 years, whichever is longer, unless such a period is in excess of local copyright law. ---------------------------------------------------------------</body>
  </mail>
  <mail>
    <header>Re: Wrong image colors?</header>
    <body>Ok, so I found another problems with wrong colors. What can I do in order to receive the correct colors from CLUT based images like GIF? I obviously cannot use the image's color space (as returned by CGImageGetColorSpace()) for my bitmap context, because I want CGContextDrawImage() to draw RGB pixels. So if I create a kCGColorSpaceGenericRGB color space instead, and then use CGContextDrawImage() to draw the image into a bitmap context, I get lots of annoying color drifts. E.g. pixel colors of 0x278283 where the palette color entry is 0x008282 instead. How can I get the original colors of CLUT based images? Is there a way to access the color lookup table of the image? If I had access to that, I could probably first draw the color indices to a memory array, and then map the indices to an RGB array using the color lookup table. Furthermore, I've probably found a bug during my experiments. If you load an image (doesn't have to be CLUT one) using CGImageSourceCreateWithURL(), then create a CGImage of it using CGImageSourceCreateImageAtIndex(), then get its color space using CGImageGetColorSpace() and then free the whole bunch using 1) CGColorSpaceRelease(colorspace) 2) CGImageRelease(image) 3) CFRelease(imagesource) Under these conditions CFRelease() crashes with a &amp;quot;segmentation fault.&amp;quot; If I do not immediately free all the objects after CGImageGetColorSpace(), but draw the image to a bitmap context first, then nothing crashes and everything is freed correctly. Freeing everything without actually using the colorspace &amp;amp; CGImage, however, causes a segmentation fault on CFRelease(). I've attached a brief demonstration source to this mail. If you think that this is a bug, let me know and I will file a bug report. Andreas Attachment:</body>
  </mail>
  <mail>
    <header>Re: animation performance</header>
    <body>On Aug 22, 2007, at 7:33 AM, Brother Josef wrote: Drawing in a loop is probably the wrong idea.  You should probably drive the animation with a timer that calls HIViewSetNeedsDisplay when you want the images to change.  Then in your drawing code you can select the next image and draw it. How fast are you trying to change the images (how many frames per second are you trying to get?) Screen updates on Mac OS X (Tiger) generally occur at the monitors refresh rate.  If you try to drive an animation at a faster rate you can actually end up in a situation where changing the graphics can work against the screen update rate and can actually slow things down.</body>
  </mail>
  <mail>
    <header>DPI setting for CGImageDestination saving to JPEG file.</header>
    <body>I am using the following code to set the dpi saving a CGImage using CGImageDestination [imageOptions setObject:[NSNumber numberWithInt:dpi] forKey:(NSString [imageOptions setObject:[NSNumber numberWithInt:dpi] forKey:(NSString CGImageDestinationAddImage(dest, cgImage, (CFDictionaryRef) 2. The DPI in other image format are displayed in Preview not as the exact integer value but with some extra decimal digits eg: Any suggestion ? Thanks Marco Pifferi</body>
  </mail>
  <mail>
    <header>Re: Create CGImage without Colorspace</header>
    <body>Mark On 21.08.2007, at 23:10, David Duncan wrote:</body>
  </mail>
  <mail>
    <header>Re: Create CGImage without Colorspace</header>
    <body>You can generally use a device profile for generating a file on disk that has no color profile attached (there are some caveats and exceptions to this). A device profile carries only information on the number of colorants and their type without information on how to interpret those colorants. -- David Duncan Apple DTS Quartz and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Re: Loading alpha channel</header>
    <body>Thanks. Works perfect right out of the box. Andreas</body>
  </mail>
  <mail>
    <header>Re: Loading alpha channel</header>
    <body>Thanks, that works like a charm. But I would have never figured that out on my own, because the docs do not even talk about the possibility of passing a NULL colorspace.</body>
  </mail>
  <mail>
    <header>Re: Loading alpha channel</header>
    <body>myDest = CGImageDestinationCreateWithURL(someURL, CFSTR(&amp;quot;public.png&amp;quot;, Should get you there</body>
  </mail>
  <mail>
    <header>Re: Loading alpha channel</header>
    <body>Thanks, that works like a charm. But I would have never figured that out on my own, because the docs do not even talk about the possibility of passing a NULL colorspace. Ok, problem solved, thanks again, I'm very happy :-) Now all I need to figure out is how to use CGImageDestination to write out PNGs with alpha channel. But that should be much easier I hope. Haven't looked at any documentation yet, though. Andreas</body>
  </mail>
  <mail>
    <header>Re: Create CGImage without Colorspace</header>
    <body>I want create a ICC profile for a camera (with lprof), thus my interest in generating an image without a colorspace (that shall be the source image for the profiling). Now, assigning a colorspace to the image coming from the camera could interfere with the profiling - or? Thanks! Mark</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate CMYK with alpha channel.</header>
    <body>Il giorno 21/ago/07, alle ore 17:58, Scott Thompson ha scritto: CGImageCreate works properly with CMYKA format but unfortunately  the ImageIO revert the image to CMYK with Alpha Mask. I have to move the saving code to NSBitmapImageRep .... even if that make the code not so neat and clean... Many Thanks Marco Pifferi</body>
  </mail>
  <mail>
    <header>Re: Loading alpha channel</header>
    <body>Good idea, but fails the practical test. It draws something, but it seems to be pre-multiplied alpha information, too. Here's what I did: "theImage" is the image loaded using CGImageSourceCreateWithURL(). This code draws alpha information into my "dstbuffer," but it's pre-multiplied. It's not the original data. I still have to try your other suggestions (vImage, Core Image), but I'm not</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate CMYK with alpha channel.</header>
    <body>On Aug 21, 2007, at 10:45 AM, Marco Pifferi wrote: CGImage can support pixel formats that CGBitmapContext cannot handle. For example, it's possible to create a CGImage that is 24 bpp RGB, but you cannot create a CGBItmapContext without alpha channel. I don't know for certain that CGImage will support CMYKA, but you could try it with a small sample and see if it works.  Even if it doesn't, there's nothing wrong with using NSBitmapImageRep.</body>
  </mail>
  <mail>
    <header>Re: Loading alpha channel</header>
    <body>Good idea, but fails the practical test. It draws something, but it seems to be pre-multiplied alpha information, too. Here's what I did: &amp;quot;theImage&amp;quot; is the image loaded using CGImageSourceCreateWithURL(). This code draws alpha information into my &amp;quot;dstbuffer,&amp;quot; but it's pre-multiplied. It's not the original data. I still have to try your other suggestions (vImage, Core Image), but I'm not really positive that any of these can do what I want. But I'm hoping it ;-) Andreas</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate CMYK with alpha channel.</header>
    <body>I done a quick test and apparently the NSBitmapImageRep with CMYKA works and save correctly to a TIFF CMYKA file: do someone know if that is supposed to work also with ImageIO ? Using CGImageCreate ? Does CGImageCreate support CMYKA format ? It seems to work only with 32 bit 4 components images. Many thanks Marco Pifferi</body>
  </mail>
  <mail>
    <header>Trouble with GenericRGB -&amp;gt; sRGB conversion on OSX 10.7.5</header>
    <body>Hello I am attempting to write a command line application that will convert images to the sRGB colorspace and then back to generic RGB values. The reason I want to convert back is because I want to be able to feed the original RGB values back into Quicktime, but that is a side issue. The issue I am having seems to be with CoreGraphics. Converting from &amp;quot;Generic RGB Profile&amp;quot; to &amp;quot;sRGB IEC61966-2.1&amp;quot; seems to be working just fine: I allocate the  &amp;quot;Generic RGB Profile&amp;quot; like so: And the sRGB colorspace is allocated like so: My test buffer is a 4x1 array with the following RGB values: My debug code checks the RGB values as integers and they look like this as (R, G, B, A): pixels[0] = (255, 0, 0, 255) pixels[1] = (0, 255, 0, 255) pixels[2] = (0, 0, 255, 255) pixels[3] = (127, 127, 127, 255) I then wrap this buffer as a CGImageRef and use the image to render into another buffer of the same size and BPP (both are 24BPP), but using the sRGB colorspace. After the RGB values are converted to the sRGB colorspace the values are now: pixels[0] = (255, 38, 0, 255) pixels[1] = (0, 249, 0, 255) pixels[2] = (4, 51, 255, 255) pixels[3] = (145, 145, 145, 255) These sRGB values seem to be just fine. If I write this buffer to a file with the sRGB profile attached then the image looks fine in Preview. Now for the problem. When I attempt to render this buffer of sRGB values back into a buffer with the Generic RGB colorspace, the pixel values are not mapped back to the original values: pixels[0] = (251, 4, 7, 255) pixels[1] = (33, 255, 7, 255) pixels[2] = (0, 4, 255, 255) pixels[3] = (126, 126, 126, 255) I double checked my logic thinking that perhaps the colorspace ref got lost during one of the conversions, but it looks like the correct values make it into the CGImageRef before I render into the Generic RGB buffer: (lldb) po (CGColorSpaceRef)CGImageGetColorSpace(imageRef) (CGColorSpaceRef) $5 = 0x00122f00 &amp;lt;CGColorSpace 0x122f00&amp;gt; (kCGColorSpaceICCBased; kCGColorSpaceModelRGB; sRGB IEC61966-2.1) (lldb) po [cgBuffer colorspace] (CGColorSpaceRef) $6 = 0x00124370 &amp;lt;CGColorSpace 0x124370&amp;gt; (kCGColorSpaceICCBased; kCGColorSpaceModelRGB; Generic RGB Profile) So, I am stumped. What don't the values map back to the original values when converting from sRGB to Generic RGB? These output RGB values are close, but they look &amp;quot;washed out&amp;quot; when viewed in Quicktime 7. Is the an issue with the colorspace conversion or does it have something to do with a 1.8 vs 2.2 Gamma mismatch? thanks Mo DeJong</body>
  </mail>
  <mail>
    <header>Re: [CATransaction commit] to force layoutSublayersOfLayer / layoutSublayers</header>
    <body>Probably I did not explain well. We are not looking for chaining Animations but control the layouting of different Layout Managers in one Transaction. We want to layout a layer subtree in one Transaction. Completion Blocks are no help in our case. regards, mahal Am 26.11.2010 um 14:55 schrieb Jim Hillhouse:</body>
  </mail>
  <mail>
    <header>Re: [CATransaction commit] to force layoutSublayersOfLayer / layoutSublayers</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>[CATransaction commit] to force layoutSublayersOfLayer / layoutSublayers</header>
    <body>Dear List Situation: * deep hierarchy of CALayers in a hosting NSView * we want to layout a subtree sometimes with and sometimes without animation * therefore, we don't want several CATransactions inside layoutSublayers but control the animation for the whole tree with one Transaction Pseudo-Code: // do some things to some layers // some layers may get a new position Issue: * in the commit, sometimes our layoutSublayers is called * but sometimes the layouting occurs in a implicit transaction after the commit (instead of in the explicit commit) Question: * how to control layouting of several layers in a hierarchy? Best regards mahal</body>
  </mail>
  <mail>
    <header>Re: Selection Rectangle</header>
    <body>CGContextFillRect. Or use a CALayer. Or a transparent window. It all depends on what you're doing. If I understand what you're talking about (the halftone-shaded thing that was originally used for window resize until Windows 98), there's no direct visual equivalent in Quartz. Depending on what you're doing, you'll probably either want a Finder-esque blue box or a Preview-esque marching ants. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Outline (flatten) fonts in PDF with Quartz</header>
    <body>Hello, I would like to take a PDF file as input, and output the same PDF where all glyphs have been replaced by drawing commands. It seems to be referenced as &amp;quot;outline fonts&amp;quot; or &amp;quot;flatten pdf&amp;quot;. I can do that using Ghostscript. Basically chaining pdf2ps and ps2pdf. However, I would like to do that with Quartz only. I have tried several ways to do that. They failed : they produce the same PDF data, the fonts being kept instead of drawn. For information, here is what I tried : -put data into a CGPDFDocument and get the dataRepresentation of each page -draw the PDF in a NSView and use dataWithPDFInsideRect -using PDFOperationWithView:insideRect:to in NSPrintOperation The last resort is to use the CGPDFScanner and handle callbacks on fonts, but it is far too complex to support  all pdf text operators. Is there someother  idea I could try ? Regards, Pierre Chatelier</body>
  </mail>
  <mail>
    <header>Re: quartz &amp;quot;clipping&amp;quot; geometry when used with glViewport?</header>
    <body>I&amp;#39;m sure others will have more constructive and more robust workaround, but I *think* that if you implement a GL Quad (basically, don&amp;#39;t use Apple&amp;#39;s Sprite), you should have no problems. For example, I believe that the GL Quad implemented by Kineme, in the GL Tools.plugin will not display this kind of clipping. This is some kind of &amp;quot;optimization&amp;quot; that happened between 10.5 and 10.6, with the Sprite, and probably other renderers, like Mesh to my recollection. I know that Mesh and Sprites wouldn&amp;#39;t work when changing the matrix in initial 10.6 releases (models would get sheared at weird places, sprites wouldn&amp;#39;t render, as you describe), and they definitely didn&amp;#39;t work with the shadow effect in Lighting. George Toledo Hello, ¬†I am trying to render a quartz composition in an OpenGL context using a viewport. ¬†I&amp;#39;m seeing geometry in the quartz composition being left out when the viewport origin is not (0,0). This happens particularly often to geometry away from the origin in the +x or +y directions. ¬† For example, a sprite patch with x position +1, so that the image is halfway on screen, is usually omitted. Exactly when the sprite is missing seems to depend on the glViewport, but when it&amp;#39;s gone it&amp;#39;s consistantly not there. An trace in OpenGL Profiler has revealed that the geometry is not being sent. ¬†In other words, quartz is &amp;quot;clipping&amp;quot; in software. ¬†Though it&amp;#39;s not clipping using a plane, since the object is either rendered completely (and in the correct position) or completely omitted. The OpenGL context is a full-screen context (child class of NSOpenGLView). ¬†In this case, it&amp;#39;s 1280 by 1024. ¬†This is the basic usage that is not working: ¬† ¬† ¬† ¬†[renderer renderAtTime:time.getValue() arguments:nil] However, if instead I send: Then this works as expected. Is this a known issue or is there a workaround? ¬†Is there some way to force quartz to send all the geometry to the OpenGL View, so that I can do clipping myself there? Thanks Elizabeth ¬†_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list ¬† ¬† ¬†() Help/Unsubscribe/Update your Subscription: This email sent to</body>
  </mail>
  <mail>
    <header>quartz &amp;quot;clipping&amp;quot; geometry when used with glViewport?</header>
    <body>Hello,  I am trying to render a quartz composition in an OpenGL context using a viewport.  I'm seeing geometry in the quartz composition being left out when the viewport origin is not (0,0). This happens particularly often to geometry away from the origin in the +x or +y directions.   For example, a sprite patch with x position +1, so that the image is halfway on screen, is usually omitted. Exactly when the sprite is missing seems to depend on the glViewport, but when it's gone it's consistantly not there. An trace in OpenGL Profiler has revealed that the geometry is not being sent.  In other words, quartz is &amp;quot;clipping&amp;quot; in software.  Though it's not clipping using a plane, since the object is either rendered completely (and in the correct position) or completely omitted. The OpenGL context is a full-screen context (child class of NSOpenGLView).  In this case, it's 1280 by 1024.  This is the basic usage that is not working: [renderer renderAtTime:time.getValue() arguments:nil] However, if instead I send: Then this works as expected. Is this a known issue or is there a workaround?  Is there some way to force quartz to send all the geometry to the OpenGL View, so that I can do clipping myself there? Thanks Elizabeth</body>
  </mail>
  <mail>
    <header>Re: Text drawing questions</header>
    <body>You can do something like // draw here using -[NSString drawWithRect:options:attributes:] or NSLayoutManager (typed in Mail). &amp;nbsp;You may have to adjust the flipped flag on your graphics context, depending on the API you use. &amp;nbsp;See  for NSStringDrawing API.</body>
  </mail>
  <mail>
    <header>Re: Text drawing questions</header>
    <body>You can do something like // draw here using -[NSString drawWithRect:options:attributes:] or NSLayoutManager (typed in Mail).  You may have to adjust the flipped flag on your graphics context, depending on the API you use.  See  for NSStringDrawing API. Core Text can do this as well: Attachment:</body>
  </mail>
  <mail>
    <header>Re: Text drawing questions</header>
    <body>1) Is there anyway to draw an "outside" stroke like in Photoshop using</body>
  </mail>
  <mail>
    <header>Re: CGContextFillRect taking up all the time in my app</header>
    <body>Sure.  A couple of particularly useful hints: you'll want to do something like this: // Update the viewport // Update the projection to set the OpenGL co-ordinate space up to match the Quartz co-ordinate space. You probably want to be using one of the higher performance methods for passing the rectangles to OpenGL; we use glDrawArrays() with separate arrays for vertices and colours set-up as follows: // Set-up the vertex and color arrays // Enable them You don't have to put all of your rects in the array at once; you can re-fill your arrays over and over if you like... in our implementation we have fixed-size vertex and colour buffers and we keep adding data to them until they're full, then call glDrawArrays(), reset our counter and repeat until we're done. It's actually pretty straightforward; the hard part in iDefrag really has to do with the data structure for storing the fragment information rather than the actual rendering. --</body>
  </mail>
  <mail>
    <header>Re: CGContextFillRect taking up all the time in my app</header>
    <body>Your demo is nice and fast, and looks like pretty much what I'm looking for.  I'm not familiar with using OpenGL, so I guess I'll have to hit the examples now.  :-&amp;gt;  If you have a snippet of code from your little-square-drawing code that you wouldn't mind posting, that would Ben Haller Stick Software</body>
  </mail>
  <mail>
    <header>Re: CGContextFillRect taking up all the time in my app</header>
    <body>Speaking from experience (iDefrag renders *huge* numbers of tiny rectangles, from 1x1 upwards), you'll probably get the best performance for this task from OpenGL.  Grab a copy of the iDefrag demo if you're interested in seeing a GL-based implementation in action; you can even adjust the block size from the Preferences window. e.g. it's dead easy to get a colour fade across the rectangle, and the GPU doesn't even break a sweat rendering such a thing. --</body>
  </mail>
  <mail>
    <header>Re: CGContextFillRect taking up all the time in my app</header>
    <body>Perhaps; I don't know.  I've never worked at that level.  Don't know what a core filter is.  :-&amp;gt;  Is there an example project I ought to take a look at? Ben Haller Stick Software On 16-Oct-09, at 11:22 AM, Vansickle, Greg wrote:</body>
  </mail>
  <mail>
    <header>Re: CGContextFillRect taking up all the time in my app</header>
    <body>Just a thought ... Would you be better off having your application simply write RGB values to a single pixel and use a core filter to render the rectangles? That intermediate buffer never needs to be rendered.</body>
  </mail>
  <mail>
    <header>Re: CGContextFillRect taking up all the time in my app</header>
    <body>Yep, you're right.  This is new to me, thanks for the tip. QuartzDebug shows that disabling beam synch makes my app's frame rate zoom upward, whereas it is normally throttled down to 60 fps.  The key point in the technote you referenced, from my point of view: &amp;quot;Decouple your visualization engine from your data engine so that neither engine will impede the other.&amp;quot;  I had been planning to do this anyway; now I've got another reason to do so! Thanks! Ben Haller Stick Software On 16-Oct-09, at 1:10 AM, Scott Thompson wrote:</body>
  </mail>
  <mail>
    <header>Text drawing questions</header>
    <body>Josef</body>
  </mail>
  <mail>
    <header>Re: CGContextFillRect taking up all the time in my app</header>
    <body>&amp;nbsp;Hi all. &amp;nbsp;I've got an app I'm working on that draws a great many little (4x4) rectangles in a wide variety of colors, at as high a frame rate as possible. &amp;nbsp;(Sounds like a great app, huh? &amp;nbsp;I have my reasons. &amp;nbsp;:-&amp;gt;). &amp;nbsp;There is also quite a bit of calculation involved, as these little rects do represent something interesting; but the calculation is taking only 2.5% of the time, according to Instruments. &amp;nbsp;86.9% of my app's time is spent in CGContextFillRect(). &amp;nbsp;So far so good; that's not a problem in itself, given the app's design as described above. &amp;nbsp;But 72.5% of the total time -- 83.4% of all the time spent inside CGContextFillRect() -- is spent in _CGSSynchronizeWindowBackingStore, all 72.5% of it in mach_msg_trap. &amp;nbsp;Just a couple percent is spent in the actual blitter routines. &amp;nbsp;Ouch. &amp;nbsp;I assume there is some good reason for this; but it still makes me wonder if there is a way that my app could be designed to avoid some of this overhead. &amp;nbsp;Filling multiple rects at once with CGContextFillRects() seemed like a possibility, but they would all have to be the same color, and that's not what I'm doing. &amp;nbsp;I can free up the calculation to go faster by putting it in its own thread and decoupling that logic from the drawing, and I intend to do so. &amp;nbsp;But the fact remains that the frame rate the drawing code itself gets is not good. &amp;nbsp;If I fill up my screen with little rectangles, it gets pretty slow, in fact. &amp;nbsp;If I could make it two or three times faster, I would be very pleased. &amp;nbsp;Advice?</body>
  </mail>
  <mail>
    <header>Re: CGContextFillRect taking up all the time in my app</header>
    <body>Hi Ben, ¬†Hi all. ¬†I&amp;#39;ve got an app I&amp;#39;m working on that draws a great many little (4x4) rectangles in a wide variety of colors, at as high a frame rate as possible. ¬†(Sounds like a great app, huh? ¬†I have my reasons. ¬†:-&amp;gt;). ¬†There is also quite a bit of calculation involved, as these little rects do represent something interesting; but the calculation is taking only 2.5% of the time, according to Instruments. ¬†86.9% of my app&amp;#39;s time is spent in CGContextFillRect(). ¬†So far so good; that&amp;#39;s not a problem in itself, given the app&amp;#39;s design as described above. ¬†But 72.5% of the total time -- 83.4% of all the time spent inside CGContextFillRect() -- is spent in _CGSSynchronizeWindowBackingStore, all 72.5% of it in mach_msg_trap. ¬†Just a couple percent is spent in the actual blitter routines. ¬†Ouch. ¬†I assume there is some good reason for this; but it still makes me wonder if there is a way that my app could be designed to avoid some of this overhead. ¬†Filling multiple rects at once with CGContextFillRects() seemed like a possibility, but they would all have to be the same color, and that&amp;#39;s not what I&amp;#39;m doing. ¬†I can free up the calculation to go faster by putting it in its own thread and decoupling that logic from the drawing, and I intend to do so. ¬†But the fact remains that the frame rate the drawing code itself gets is not good. ¬†If I fill up my screen with little rectangles, it gets pretty slow, in fact. ¬†If I could make it two or three times faster, I would be very pleased. ¬†Advice? Ben Haller Stick Software</body>
  </mail>
  <mail>
    <header>CGContextFillRect taking up all the time in my app</header>
    <body>Hi all.  I've got an app I'm working on that draws a great many little (4x4) rectangles in a wide variety of colors, at as high a frame rate as possible.  (Sounds like a great app, huh?  I have my reasons.  :-&amp;gt;).  There is also quite a bit of calculation involved, as these little rects do represent something interesting; but the calculation is taking only 2.5% of the time, according to Instruments.  86.9% of my app's time is spent in CGContextFillRect(). So far so good; that's not a problem in itself, given the app's design as described above. But 72.5% of the total time -- 83.4% of all the time spent inside CGContextFillRect() -- is spent in _CGSSynchronizeWindowBackingStore, all 72.5% of it in mach_msg_trap.  Just a couple percent is spent in the actual blitter routines.  Ouch.  I assume there is some good reason for this; but it still makes me wonder if there is a way that my app could be designed to avoid some of this overhead.  Filling multiple rects at once with CGContextFillRects() seemed like a possibility, but they would all have to be the same color, and that's not what I'm doing. I can free up the calculation to go faster by putting it in its own thread and decoupling that logic from the drawing, and I intend to do so.  But the fact remains that the frame rate the drawing code itself gets is not good.  If I fill up my screen with little rectangles, it gets pretty slow, in fact.  If I could make it two or three times faster, I would be very pleased. Ben Haller Stick Software</body>
  </mail>
  <mail>
    <header>Re: Big problems posting keyboard events with CGEventPost in Snow Leopard</header>
    <body>Yes, my event taps do handle kCGEventTapDisabledByTimeout, however this problem doesn't directly involve event taps.  I've just been using them to monitor events, and even without one I can see the effects of some of the keyboard events not being handled. wrote:</body>
  </mail>
  <mail>
    <header>Re: Big problems posting keyboard events with CGEventPost in Snow Leopard</header>
    <body>I've just been using NULL this whole time, and that worked without problems in Leopard.  I've never quite understood how to properly use event sources, or how to create one.  Will that make a difference in this case?  And if so, how do I use event sources correctly? wrote:</body>
  </mail>
  <mail>
    <header>Re: Big problems posting keyboard events with CGEventPost in Snow	Leopard</header>
    <body>Does anyone know how to fix this? &amp;nbsp;I desperately need to release a new version of my application that fixes this problem, but I have no idea</body>
  </mail>
  <mail>
    <header>Re: Big problems posting keyboard events with CGEventPost in Snow 	Leopard</header>
    <body>An application I&amp;#39;ve written uses CGEventPost to create keyboard events at the kCGSessionEventTap level, and in Mac OS X 10.4 and 10.5 it worked fine. ¬†Now that I&amp;#39;ve been debugging my program in Snow Leopard,</body>
  </mail>
  <mail>
    <header>Big problems posting keyboard events with CGEventPost in Snow	Leopard</header>
    <body>An application I've written uses CGEventPost to create keyboard events at the kCGSessionEventTap level, and in Mac OS X 10.4 and 10.5 it worked fine.  Now that I've been debugging my program in Snow Leopard, I see that the events I post aren't working consistently. Essentially, when creating a keystroke, I would create events and post them in a specific order so as to mimic what would actually happen when the user made the keystroke.  For example, to create the keystroke for Command-Option-A when no other keys are actually pressed on the keyboard, I would create the following events in this order: kCGEventFlagsChanged, specifying that the command key is down kCGEventFlagsChanged, specifying that both the command and option keys are down kCGEventKeyDown, with the keycode for 'A' and both command and option down kCGEventKeyUp, with the keycode for 'A' and both command and option down kCGEventFlagsChanged, specifying that the command key is down kCGEventFlagsChanged, specifying that no modifiers are down. What happens in 10.6 is sometimes, but not every time, these events will occur out of order, or random events will be dropped.  In fact, the success rate of all of the events actually being posted is about 40% - pretty miserable.  And a lot of the time it'll work out that a key will get &amp;quot;stuck&amp;quot;, since the kCGEventKeyUp won't get posted. This can have *disastrous* results, especially because if the user then uses a keyboard shortcut to switch spaces, the Spaces on screen display won't go away, and every key on the keyboard will be unresponsive until the stuck key is somehow released!  Users have complained to me that they lose all ability to use their keyboard because of this bug, and that is simply unacceptable!! I've observed this happening using the application &amp;quot;Event Taps Testbench&amp;quot; as well as my own event taps to monitor what events are actually getting through the event subsystem to the kCGAnnotatedSessionEventTap or application level.  The only other information I have is that this bug doesn't happen if I post events to the kCGAnnotatedSessionEventTap level.  Unfortunately, that's not suitable for my purposes, as my application needs to trigger system-wide hot keys, and those need to be placed at the kCGSessionEventTap level or higher.  I've also tried sending the events to kCGHIDEventTap, but this has the same issue as kCGSessionEventTap. Does anyone know how to fix this?  I desperately need to release a new version of my application that fixes this problem, but I have no idea what to do! - Brian email@hidden</body>
  </mail>
  <mail>
    <header>Re: Observing animated property changes in a CALayer</header>
    <body>Overriding -display seems to work. In fact so long as you set +needsDisplayForKey, the frame can be zero and -display will act as a timer or metronome during every tick of the animation. Sort of similar to CADisplayLink, presumably. In the end, the environment is so dynamic and fairly tied up to the model layer properties, so I ended up just using a NSTimer/ CADisplayLink and doing the animation manually. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Re: Cutting a radial gradient off to rectangle</header>
    <body>Maybe I missed it, but did clipping not allow you to do this? (a gradient naturally fills the entire clipping path at the time that it is drawn, so if you don't want it to draw somewhere, just set the clipping path such that the area you don't want affected won't be drawn to). -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Cutting a radial gradient off to rectangle</header>
    <body>I have a quick question about drawing radial gradients. Is there any way I can use a fairly large radius but clip the actual drawn gradient to a rectangle? I'm using gradients to darken different rectangular regions in my layer from the center of the region. The problem, however, is that to get a sufficient darkening of the whole rectangle, I have to use a large radius that overlaps with other regions. Is there any way to do this? Thanks for any replies!</body>
  </mail>
  <mail>
    <header>CGPDFPage excessively caching content stream</header>
    <body>Hi, When a CGPDFPageRef is obtained via CGPDFDocumentGetPage(), its memory usage is minimal. However, once that page has been rendered using CGContextDrawPDFPage(), its content stream appears to be cached somewhere. For pages with lots of images this is a problem, because the CGPDFDocument appears to retain the page until it is itself released, even during low memory warnings on the iPhone, and the CGPDFPage does not appear to relinquish its cache until it is deallocated. Has anyone had any success with getting this cache emptied? Thanks, Hamish</body>
  </mail>
  <mail>
    <header>Re: Observing animated property changes in a CALayer</header>
    <body>I'm not certain how these properties are implemented, but perhaps if John has a moment he will chime in on why this may or may not work. If your layer's don't actually have contents, you could potentially work around this by implementing -display or -displayLayer: (in your delegate) to set the layer's contents to nil, which will prevent most of the work that may otherwise go into a display call. Because you don't have any timing guarantees. Your delegate could be called 0s after the animation ends or 10s after the animation ends. While this isn't necessarily the problem, also keep in mind that removeOnCompleted=NO means that you are responsible for eventually removing the animation. Presumably your current auto-scroll is along the lines of &amp;quot;scroll 20 pixels&amp;quot;. What I'm proposing is more along the lines of &amp;quot;in 5 pixels control X will appear, and in 15 pixels control Y will disappear&amp;quot;, so apply 3 animations, to autoscroll 5 pixels, 10 pixels, and 5 more pixels, and enable &amp;amp; disable controls as necessary. So there is no predicting the future, but rather just taking your current intention into account. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Observing animated property changes in a CALayer</header>
    <body>Currently I compute the edge and autoscroll to that, hit detection is going to be complicated otherwise. I was hoping at least to get a custom property on a CALayer to animate, since this is useful in other scenarios. What am I missing here?: @interface CustomLayer: CALayer @property float zoop @end @implementation CustomLayer - (void)setValue:(id)value forKey:(NSString*)key @end When I make a CABasicAnimation to modify zoop and add it to the layer, the setValue:forKey: (and the NSLog therein) doesn't appear to be called at all. It only works if I also declare +needsDisplayForKey and set layer.frame to something, but I don't necessarily need the display contents to be invalidated. Why would I see stutter if I add animations on the fly? If the fillMode is kCAFillModeForwards and removeOnCompleted = NO for the old animation, presumably the presentation of the original animation and the new animation should be identical at the point of replacement? Is adding an animation through addAnimation:forKey: atomic in that it will atomically replace whatever is already there in that animation slot? Since I can't predict whether the user will try to autoscroll in another direction, I can't set up all my animations in advance. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Re: Observing animated property changes in a CALayer</header>
    <body>The simplest method to do this is to just run a timer to poll the presentationLayer. A more complicated method is to use CAAnimations that (auto-)scroll as far as it takes to reveal or hide a particular control and use the delegate to be notified. You'd also have to have a number of animations setup already, otherwise you may see stutter if you try to add new animations during the callback. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Observing animated property changes in a CALayer</header>
    <body>It's an autoscrolling scenario. When the user touches the edge of the screen, I animate the bounds.origin of the scrollview.layer until he either lifts the finger or moves out of the zone. To stop the autoscroll, I remove the animation if present. Whenever the animation stops, I apply the presentationLayer.bounds.origin to the original bounds.origin to represent the actual scrolling. Now as the user autoscrolls, I want to update the user interface e.g. by redrawing items that are selected and deselected. The redrawing is not drawContext: based, rather it simply updates some sublayers. Thus I need to be notified by the animation as the scrollview.layer.bounds.origin is changing, in order to redraw the UI items. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Re: Observing animated property changes in a CALayer</header>
    <body>What are you trying to do? Best approach would likely depend greatly on that. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Observing animated property changes in a CALayer</header>
    <body>I have a CABasicAnimation that animating a property of a CALayer e.g. bounds.origin. I want to be able to observe the property changing over time, but haven't really found a method that works 100%. 1.	I tried using KVO (key-value observation) on the presentationLayer's bounds.origin keyPath. The system complains that the object is freed before its observers are freed, leading me to think that the presentation layer is only temporary. Observing presentationLayer.bounds.origin as a keypath doesn't work. 2.	I tried creating a property on another layer and animating that e.g. by declaring the @property and making it @dynamic on that layer. However this new property only gets changed when the presentation layer is accessed (e.g. at the end of the animation), it doesn't seem to update while the animation is running. 3.	I used +needsDisplayForKey on the property in #2, which does trigger updates during the animation, but for these issues: a.	it only works if the CALayer has non-zero frame. Since this layer might be a CAShapeLayer or subclass, it may have a zero frame. b.	it looks like it triggers -setNeedsDisplay for that layer, but since I'm not actually drawing that layer only monitoring the property change, I don't want to cause it to redraw. 4.	I tried scheduling an NSTimer, and within the timer callback sample the presentationLayer. This also works but for these issues: a.	The timer would probably be slightly out of sync with the animation update. b.	Since occasionally the original animation gets pre-empted by another animation, it's difficult to actually get the timer to fire when the animation is running and only when the animation is running. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Re: feeble anti-aliasing?</header>
    <body>The bug was in the path creation bit. So, problem solved :-)</body>
  </mail>
  <mail>
    <header>Re: feeble anti-aliasing?</header>
    <body>for (element in app.elements) The bug was in the path creation bit. So, problem solved :-) Of course, it's still not &amp;quot;perfect&amp;quot; as you can see from the png which contains clockwise from top-left no anti-aliasing --colin</body>
  </mail>
  <mail>
    <header>Re: feeble anti-aliasing?</header>
    <body>The path creation code isn't going to cause overdrawing.  The first code you should examine is anything drawing-related (that is, pushing pixels to the backing store; things like -drawRect:). --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: feeble anti-aliasing?</header>
    <body>The -drawRect: on its own wouldn't help you much - it's the path creation routines that need looking at - and they're, umm, polymorphic? It'll be quicker for me to look at my code carefully :-) I got sent an example of overdrawing, and yes - it looks very suspiciously similar. It's conceivable that that's what's happening although It Shouldn't Be Doing That! (TM) if (element.back) bit of code. If element.back isn't nil, like I think it is, then the back of one element will get drawn under the front of the next element. Thanks for the (persistent :-) hint. --colin</body>
  </mail>
  <mail>
    <header>Re: feeble anti-aliasing?</header>
    <body>Not that we don't believe you, but you need to post your -drawRect: method.  Describing it is insufficient. What you've posted looks suspiciously like overdrawing. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: feeble anti-aliasing?</header>
    <body>OK, basically I think there's still some aliasing in the anti-aliased image. If that makes sense. The top-front part of the first lens element (on the left) looks better than the identical (reflected) Bezier curve of the back-top part of the first lens element. I applied a 0.25 - 0.33 pixel Gaussian blur in Photoshop, and the result looks better (obviously). If you look at the more or less vertical lines here - you'll see that in the horizontal direction there are only ever 2 pixels shaded. The RGB values add up to 253. In the very-mildly blurred version I have, there are now 4 pixels shaded in the corresponding parts. I wonder if there's a setting where one can, in effect, change the anti-aliasing filter... --colin</body>
  </mail>
  <mail>
    <header>Re: feeble anti-aliasing?</header>
    <body>Nope. At the moment I'm filling the entire view with white and recalculating and redrawing all the lines (+ several that don't even get shown) on each -drawRect: call. Anyway, I have alpha = 1.0. --colin</body>
  </mail>
  <mail>
    <header>Re: feeble anti-aliasing?</header>
    <body>Marc. I posted this to the cocoa-dev list (hoping I was making a Cocoa blunder) and someone suggested I try posting here ¬†instead:</body>
  </mail>
  <mail>
    <header>Re: feeble anti-aliasing?</header>
    <body>Interesting, but shouldn't ColorSync deal with all that? --colin</body>
  </mail>
  <mail>
    <header>Re: feeble anti-aliasing?</header>
    <body>Colorspace makes a difference. For anti-aliasing it should ideally be a linear light colorspace. Most device colorspaces are not linear light, since it has poor visual coding efficiency. (Sorry, I can't help you with how this observation applies specifically to Quartz). Graeme Gill.</body>
  </mail>
  <mail>
    <header>Re: feeble anti-aliasing?</header>
    <body>As was also suggested on cocoa-dev, please post a screenshot so we have some idea of what you're talking about. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>feeble anti-aliasing?</header>
    <body>I posted this to the cocoa-dev list (hoping I was making a Cocoa blunder) and someone suggested I try posting here  instead: ######################################################################### I'm doing some simple drawing (black Bezier curves and lines on a white background) to a custom view which inherits directly from NSView. [self translateOriginToPoint:NSMakePoint( 20.0, [self scaleUnitSquareToSize:NSMakeSize( (frameRect.size.width / 30.0), (frameRect.size.width / 30.0) ) ];     // scale factor is around x25 NSPath *path = //path creation stuff This all works fine. However, while Cocoa (or Quartz) clearly IS anti- aliasing my curves and lines, it isn't, ummm, doing it very well. I still see jaggies (on my MacBook Pro, at slightly greater than normal viewing distance... say 50 cm (20 &amp;quot;). Now I've written anti-aliased Bezier curve drawing routines in assembler, so I thought I knew what to expect. Are there any other settings I can adjust with regard to the anti- aliasing? (Not talking about flatness of Bezier curves here). Is this perhaps due to the floating point user space to device space conversion? and/or non pixel-perfect rendering? It's just odd. The first time I saw the output, I thought, &amp;quot;oh, I need to turn anti-aliasing on&amp;quot;. Then I looked carefully, used the screen magnifier - and was a bit surprised that it WAS anti-aliased. It can't make a difference that Cocoa thinks I have a screen resolution of 72 whereas actually my MacBook Pro has around 110 ppi ?</body>
  </mail>
  <mail>
    <header>Creating texture from raw pixel data</header>
    <body>Hello everyone, I've tried several different ways to create a texture from raw bytes and none of them seem to work. I get either a black screen or trash. I'm wondering the best way to create a texture from data. Is there a specific format the data has to be in (to correspond with the pixel format) and will some just not work? Currently my data is 24 bit RGB 300 x 216 stored in a buffer pixelData[194400] I've tried CVPixelBufferCreateWithBytes(), and then CVOpenGLTextureCacheCreate(), and the CVOpenGLCTextureCacheCreateTextureFromImage(), and several variations. I'm sure I'm probably getting the parameters wrong. I've tried CVPixelBufferCreate(), and then drawing the image into the base address and creating a texture from that. Still no luck. Something tells me I'm creating the right buffer for my type of data. Any help will be appreciated. If I need to change the format of my raw data, that's no problem. Thanks -B</body>
  </mail>
  <mail>
    <header>Re: Identify screen containing application window</header>
    <body>It's possible for a window to be on more than one screen. (For example, you can drag a window so it's half on one screen and half on another). Also, some displays may be &amp;quot;mirroring&amp;quot; other displays (for example, if you have a laptop that's also connected to a projector), in which case some applications may want to treat both displays as being the same logical screen, but some applications may want to treat them as separate displays. These are because the function may need to return multiple displays (see above). You need to allocate a buffer containing space for some number of CGDirectDisplayIDs. You tell the function how much space is in the buffer (maxDisplays) and give it a pointer to the buffer (displays). It hands back the number of displays it wanted to return in *matchingDisplayCount --- if this is larger than maxDisplays, it means that the function ran out of space in the buffer you provided, and so it only returned the first few displays. You can call the function again with a larger buffer if you want. (Or, you can call the function once with a NULL buffer to discover how large the buffer needs to be, and again with a buffer that's the right size.)</body>
  </mail>
  <mail>
    <header>Re: Dumb question about best practices for pointers, objects,	structs, and CG types, e.g. CGRect</header>
    <body>In general CFArrays can't store &amp;quot;plain ol' structs&amp;quot;.  They hold pointer-sized values, which rules out most useful structures in practice.  Of course, you can store pointers in them (and the pointers can point to anything), but that doesn't really help in this case. --</body>
  </mail>
  <mail>
    <header>Re: Dumb question about best practices for pointers, objects,	structs, and CG types, e.g. CGRect</header>
    <body>I believe CFArrays can store plain ol' structs, which would be the way to go if you don't want to mess with the overhead of a wrapper object but still want a usable array. On Oct 5, 2009, at 9:03 AM, Alastair Houghton wrote:</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>Raphael I take it that you&amp;#39;re not running inside an NPAPI plugin i.e. this is a stand-alone Cocoa application calling CA directly and there&amp;#39;s no Webkit involved? If yes, and I&amp;#39;m assuming this to be the case, then that&amp;#39;ll be very interesting. It could well re-enforce a general CAOpenGLLayer issue theory. Interesting, I&amp;#39;m not seeing any VRAM issues - that&amp;#39;s presuming I&amp;#39;m looking in the right place of course! I&amp;#39;ve monitored Current Free Video Memory (vramFreeBytes), Current DMA Memory (gartSizeBytes), Current Largest Free Video Block (vramLargestFreeBytes) and GPU Memory Utilisation and see nothing abnormal; there appears to be plenty of memory available. I do not know. I&amp;#39;m going to modify the CALayer Essentials demonstration app from Apple and see if I can reproduce the conditions. If I can create something reproducible from there then I shall have enough to file a bug report. More ideas and theories are very welcome. Kind regards,</body>
  </mail>
  <mail>
    <header>Getting the size of a CGImage</header>
    <body>Josef</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>I take it that you're not running inside an NPAPI plugin i.e. this is a stand-alone Cocoa application calling CA directly and there's no Webkit involved? If yes, and I'm assuming this to be the case, then that'll be very interesting. It could well re-enforce a general CAOpenGLLayer issue theory. Interesting, I'm not seeing any VRAM issues - that's presuming I'm looking in the right place of course! I've monitored Current Free Video Memory (vramFreeBytes), Current DMA Memory (gartSizeBytes), Current Largest Free Video Block (vramLargestFreeBytes) and GPU Memory Utilisation and see nothing abnormal; there appears to be plenty of memory available. I do not know. I'm going to modify the CALayer Essentials demonstration app from Apple and see if I can reproduce the conditions. If I can create something reproducible from there then I shall have enough to file a bug report. More ideas and theories are very welcome. Kind regards, Christopher _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>Hi, To me, it seems to be linked to the way CAOpenGLLayer manages VRAM. When repeatedly invoking setNeedsDisplay (from mouse events, not automatically with asynchronous), I can see the available VRAM (driver monitor) going from 512MB to 40MB in just a couple seconds. My scene has no textures, but about 2MB of vertex data (VBO). Are your FBOs rendering to textures? My suspicion still centres around the use of textures. The more textures used, the more things slow down (that&amp;#39;s just a theory of course). _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list ¬† ¬† ¬†() Help/Unsubscribe/Update your Subscription: This email sent to</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>Are your FBOs rendering to textures? My suspicion still centres around the use of textures. The more textures used, the more things slow down (that's just a theory of course). _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>I have finally been able to get the OpenGL profiler to work with my plugin - normally, Safari would just freeze when i tried to attach the the WebKitPluginHost, but now I tried running Safari in 32-bit mode, which makes the plugin run in the same process, and there it worked. What I'm seeing, when running content which suffers from the problem is this: CGLFlushDrawable, Avg Time 104294.94¬µs (104 ms!), GL Time: 99.44% (!!!) We never call CGLFlushDrawable from our own code, it's only being called here, from a seperate thread: #0	0x96985c9d in CGLFlushDrawable #1	0x915c1e79 in view_draw #2	0x915c0fd7 in view_display_link #3	0x915c0f2b in link_callback #4	0x9027f440 in CVDisplayLink::performIO #5	0x9027e0a8 in CVDisplayLink::runIOThread #6	0x9027dcd0 in startIOThread #7	0x94df1fbd in _pthread_start #8	0x94df1e42 in thread_start I also noticed that this problem seems to be related to our usage of real-time shadows (for which we use FBOs to render the shadow maps). If I disable those, the problem goes away (at least for the content I tested - there may be other ways to trigger it). (Sorry to be spamming quartz-dev with my daily findings like this, but this is a serious regression from the old plugin model, and we are really stepping in the dark on how to solve it). jonas _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>Hi Jonas, I've noticed that the more textures I have the more things appear to slow down. Perhaps this is your case. I do not see delays there for me. Not exactly sure, but are you doing things in immediate mode instead of using display lists etc.? Anyhow, I do not think that these are anything to do with the core problem. Buffer swaps and CPU Wait for OpenGL Swap to Complete are the big issue from my observations. The question remains, why? Perhaps. Kind regards, Christopher _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>Another interesting observation: For content which suffers from these issues, the OpenGL Driver Monitor will show at lot (&amp;gt;100ms) of CPU Wait for Free OpenGL Command Buffer and  CPU Wait for Free 2D Command Buffer. What do those mean? If I understand correctly, the driver has a buffer where OpenGL commands are queued, and then asychronously executed by the GPU. Is my content queuing too many OpenGL calls, so the buffers overrun, and the system stalls? jonas _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>No, I also don't think it's the only reason, as the problem still shows. But not trying to push for &amp;gt;100 fps makes the lag noticably better, though far from gone. One interesting thing is that it seems to depend on the content we are running. Some content runs just fine in the Core Animation plugin (no difference to direct OpenGL) whereas some other content runs really slow. We are currently collecting source files for slow content, so we can analyze what is slowing it down. One more noteworthy thing is that the whole system becomes very unresponsive when playing back slow content. Just opening an chat window and trying to type while running the web plugin becomes close to impossible, as every keypress takes a second or so to register. The same content runs without any issues when running in direct OpenGL, or when switching to the 9600GT graphics card in my MacBook. jonas _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CAMediaTimingFunction functionWithControlPoints</header>
    <body>On Nov 23, 2009, at 10:43 AM, John Harper wrote: Thanks!</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>I was thinking along the same lines, and I'm actually quite sure now that my issue is some sort of pipelining stall between the plugin and the browser. I have actually now solved the issue for us to some degree. What i was doing was to manually call setNeedsDisplay on my OpenGL layer, instead of using the asynchronous property. I did this, so i could get higher frame rates then the 60 fps asynchronous seems to give me. In reality, 60 fps is ok for most cases, but our plugin has a function for the content to set the desired maximum frame rate itself, and I wanted to implement that function. Also having frame rate displays quoting hundreds of fps is always good for selling technology :) It turned out that this seems to be the cause of my problems, though. When i was pushing frames at very high frame rates, it seemed to stall, resulting in hickups, which made the overall impression worse. So now, I disabled this feature, and use CAOpenGLLayer.asynchronous to let Core Animation decide itself when it wants to update, and the result looks much smoother. I still don't get as consistent frame times as I would get running outside of Core Animation - in fact, every now and then, I see frames taking exactly twice as long as the average, which may still be the skipping frames issue Christopher is talking about. But I see no such extreme hickups as I did before. jonas _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>I was the one who reported having the same issue moving an NPAPI plugin from AGL to Core Animation. For me, it also seems to be hardware related. On my MacPro, GF8800GT, it runs smooth. On my MacBook Pro, GF9400 it runs sluggish (but smooth when running the same content ouside of CoreAnimation). Same machine, but switching to the GF9600 GT, no issues. Maybe it's driver related? VRAM does not seem to be a problem here, OpenGL Driver Monitor reports 170MB free video memory. It does seem (and that's just a subjective impression), that content using more FBOs is more affected then content which doesn't - maybe those FBO switches are causing the driver to hang waiting for something? jonas</body>
  </mail>
  <mail>
    <header>Re: CAOpenGLLayer display link timer on main thread</header>
    <body>Thanks John, calling [layer setNeedsDisplay] then [CATransaction flush] with a [NSRecursiveLock tryLock] around the renderer callback works well. I've posted a request at rdar://problem/7420277. I haven't tested with any heavy duty concurrent layer manipulation, but drawing an OSD and clock over the video continues to work well. Regards, Ryan</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>Hi everyone, I've now performed some initial tests observing the following: Buffer Swaps: rests at around six, increases to 123 (max) when I perform some animation over 3 seconds. CPU Wait for GPU: Spikes at around 30ms (344,592,825 ns) during my animation, otherwise there is no waiting at rest. CPU Wait for OpenGL Swap to Complete: Correlates roughly with CPU Wait for GPU. This must be where we are stalling then (the other CPU Wait Observations look insignificant). Current Free Video Memory: around 250MB on a 256MB video card. Textures: max at 1,407. Any clues? It appears to be performing Buffer Swaps. Kind regards, Christopher</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>Considering the note that the NPApi plugins are using a IOSurface backed FBO for the CALayer, could there be 'synchronization' issues if the background NPApi process takes longer to draw to the FBO, so that the front end Safari portion draws the 'older' (ie last frames) version of the Layer contents, thus looking like a stall? I have myself wondered how exactly the equivalent of &amp;quot;vsync&amp;quot; with IOSurface would work. In my own QC plugin that uses IOSurface, Ive noticed occasionally that updates can get out of 'phase' between the background and the foreground applications. For example: (background update indicates the background app has written to the IOSurface backed FBO, and foreground draw indicates the surface in its current state is drawn to the GL world from the parent/foreground app) background : update: foreground: draw: background : update: foreground: draw: background : update: foreground: draw: foreground: draw:   &amp;lt;-- 'stall' background : update: background : update:  &amp;lt;-- stall foreground: draw: background : update: foreground: draw: background : update: foreground: draw: background : update: foreground: draw: background : update: Something like that may explain the hiccups as a result of 'phasing' between draw and update calls between the foreground and background apps, especially if no display link is being used to synchronize the drawing as best as possible?</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>If you are not using any FBOs that you or a library you are using created , then strictly speaking you should not have to handle fiddling with those glGets and state restoration stuff at all. In that case, I am not too sure and my experience with CA is limited. Good luck!</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>Hi John, Thanks for your helpful and informative reply: Yes indeed I do - several. I shall look and report back. Wilco. Kind regards, Christopher _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>Hi Vade, Thanks for your wonderful guidance. I shall follow through and check back with the forum and yourself. FYI I'm not using FBOs within my own code. Kind regards, Christopher</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>yes. CAOpenGLLayer creates an IOSurface (a buffer that will live in VRAM) for your rendered content, attaches the GL context to the buffer using an FBO, then asks your code to render into it. There's a pool of buffers, so you should see a few different FBOs get created. there should only be a few buffers created. It's possible that if you're right on the edge of using all the available VRAM, some of your textures are having to be reuploaded. You can see if that's happening by using the OpenGL Driver Monitor application, it can report the amount of buffer paging happening every second. If you have a test case showing the problem, please file a radar so we can look into it. John</body>
  </mail>
  <mail>
    <header>Re: CAOpenGLLayer display link timer on main thread</header>
    <body>There's currently no built-in way to have the layer be called on a background thread. Please file a radar requesting that if you think it would be useful. It should work to trigger the update on the CVDisplayLink thread, but I'm not sure if it's ever been tried. You would probably need to do &amp;quot;[glLayer setNeedsDisplay]; [CATransaction flush];&amp;quot; to ensure it gets called. The only potential problem I can think of is that you may also flush changes to the screen that other threads made to the layer tree (threads aren't completely isolated from each other, they're all changing the same shared objective C data structure) John</body>
  </mail>
  <mail>
    <header>Re: CAMediaTimingFunction functionWithControlPoints</header>
    <body>The values for the ease-in-ease-out curve are &amp;quot;.42,0 .58,1&amp;quot;. The separate ease-in and ease-out curves keep one point the same as ease-in-ease-out and set the other to 0,0 or 1,1 John</body>
  </mail>
  <mail>
    <header>Re: how to force execute:atTime:withArguments to run in	kQCPlugInTimeModeNone</header>
    <body>sorry wrong list! was meant for Quartz Composer dev list 29 Ironworks. Dace Rd. London E3 2NX, UK mob : +44 (0) 7958 783 832 tel : +44 (0) 20 8123 9986 fax : +44 (0) 20 8986 5496 play:</body>
  </mail>
  <mail>
    <header>how to force execute:atTime:withArguments to run in	kQCPlugInTimeModeNone</header>
    <body>29 Ironworks. Dace Rd. London E3 2NX, UK mob : +44 (0) 7958 783 832 tel : +44 (0) 20 8123 9986 fax : +44 (0) 20 8986 5496 play:</body>
  </mail>
  <mail>
    <header>CAOpenGLLayer display link timer on main thread</header>
    <body>Hi, I'm using a CAOpenGLLayer to render YUV422 video frames (from an open-source decoder) as part of a media center app. This works well, however I've noticed that the display link callbacks (canDrawInCGLContext:pixelFormat:forLayerTime:displayTime: and drawInCGLContext:pixelFormat:forLayerTime:displayTime:) are called on the main thread, where previously using a NSOpenGLView and a manually-created display link, these were called on a separate (presumably higher-priority thread. This isn't a huge deal, as I've used GCD etc to ensure as little code as possible is running on the main thread. However, actions like displaying an open dialog etc cause stuttering of the video (presumably because they take longer than 1/60th of a second to execute). Is there any way round this? CAOpenGLLayers  don't seem to be compatible with multithreaded OpenGL. Would setting the asynchronous property to false and using a manually created displaylink (hopefully thereby running the timer on a background thread) to trigger the layer update be feasible? Regards, Ryan _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Maximum size of CIImage for offscreen drawing</header>
    <body>Don't think of a CIImage as a picture.  A CIImage is a set of instructions for drawing a picture. Because of this, CIImages can be infinite in extent. The results of some of the CIFilters, for example the Checkerboard filter, are CIImages that are infinite in size.  It is happy to draw checkerboards anywhere on the drawing plane... you usually have to combine the results of such a filter with some kind of crop to make use of them. The surface that you draw the CIImage on to may have size limits, but a CIImage itself can be infinitely large. Scott Thanks,</body>
  </mail>
  <mail>
    <header>CAMediaTimingFunction functionWithControlPoints</header>
    <body>Does anyone know what paramaters to CAMediaTimingFunction functionWithControlPoints would produce the kCAMediaTimingFunctionEaseInEaseOut curve?  (That's not an Apple Secret (TM) is it?) I'd like a curve like kCAMediaTimingFunctionEaseInEaseOut but with an even steeper slope in the middle.  (How steep?  I don't know.  I'm trying to avoid the math and just eyeball it.) Thanks, David</body>
  </mail>
  <mail>
    <header>Re: Fuzzy CGPDF Display</header>
    <body>I had to read this about 5 times, but yes.&amp;nbsp; That worked.&amp;nbsp; Thank you so much.&amp;nbsp; Both of you were a huge help. -alexander Hamish Allan &amp;lt;email@hidden&amp;gt;; email@hidden Fri, November 20, 2009 8:10:10 PM &lt;span style="font-weight: bold;"&gt;Subject: Re: Fuzzy CGPDF Display</body>
  </mail>
  <mail>
    <header>Re: Fuzzy CGPDF Display</header>
    <body>That is how I understand it.&amp;nbsp; I'm basically letting the user drop UIKit views on top of the PDF.&amp;nbsp; For example, UIImageView, UILabel etc... I then convert that to a PDF by rendering each page view as in my code example. -alexander email@hidden Fri, November 20, 2009 4:38:43 PM Re: Fuzzy CGPDF Display It sounds as though you're generating a PDF containing bitmaps of pages from the previous PDF with the user's changes rendered onto them. What kind of changes are you allowing the user to make? Hamish</body>
  </mail>
  <mail>
    <header>Re: Fuzzy CGPDF Display</header>
    <body>It sounds as though you're generating a PDF containing bitmaps of pages from the previous PDF with the user's changes rendered onto them. What kind of changes are you allowing the user to make? Hamish</body>
  </mail>
  <mail>
    <header>Re: Fuzzy CGPDF Display</header>
    <body>Yes, I display a PDF to the user in a UIView. &amp;nbsp;(I can see the quality issue just with that.) The user can then use my software to change the pdf.&amp;nbsp;&amp;nbsp; Next, I generate a final PDF with the user's changes. -alexander email@hidden Fri, November 20, 2009 4:12:03 PM Re: Fuzzy CGPDF Display Hi Alex, Now I'm really confused. You're trying to use this PDF to generate another PDF? In which case, why does UIView come into it? H // for each page, render it the layer 3:50:37 PM</body>
  </mail>
  <mail>
    <header>Re: Fuzzy CGPDF Display</header>
    <body>&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; // render the layer</body>
  </mail>
  <mail>
    <header>Re: Fuzzy CGPDF Display</header>
    <body>Hi Alex, Now I'm really confused. You're trying to use this PDF to generate another PDF? In which case, why does UIView come into it? H</body>
  </mail>
  <mail>
    <header>Re: Fuzzy CGPDF Display</header>
    <body>&amp;nbsp;Hi Hamish, I knew I was leaving some code out :)&amp;nbsp; I The previous e-mail contains the drawing code for the EditorPageView.&amp;nbsp; Here is the code that generates the PDF. Let me know if you need any other contextual info. -alexander ------ rendering code for EditorPageView&amp;nbsp;&amp;nbsp; ------- &amp;nbsp;&amp;nbsp;&amp;nbsp; // Now that the page is rotated and scaled correctly, all that remains is to draw it. --------------- code to generate final pdf -------- &amp;nbsp;&amp;nbsp;&amp;nbsp; // for each page, render it &amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; // get the size of the page &amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; // begin a new pdf page &amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; // flip the page &amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; // render the layer &amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; [page.layer &amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; // end the pdf page &amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp; // increment our page email@hidden Fri, November 20, 2009 3:50:37 PM Re: Fuzzy CGPDF Display Where is the page rotated and scaled? H</body>
  </mail>
  <mail>
    <header>Re: Fuzzy CGPDF Display</header>
    <body>Where is the page rotated and scaled? H</body>
  </mail>
  <mail>
    <header>Fuzzy CGPDF Display</header>
    <body>Hello, The code is fairly simple.&amp;nbsp; It simply creates&amp;nbsp; UIView for each PDF page, and concatenates those on another UIView.&amp;nbsp; I included the Page drawing code&amp;nbsp; below. &amp;nbsp; I also noticed that the Quartz example in the iphone SDK also shows the same problem I see. I really need to solve this problem, and I am stuck.&amp;nbsp; Any help or suggestions would be appreciated. -alexander &amp;nbsp;&amp;nbsp;&amp;nbsp; // Now that the page is rotated and scaled correctly, all that remains is to draw it. that may or may not be related to my problem. -alexander</body>
  </mail>
  <mail>
    <header>Re: reestablishing a CGContextRef as the current context</header>
    <body>It is only good within the focus lock or -drawRect:. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Maximum size of CIImage for offscreen drawing</header>
    <body>Don't think of a CIImage as a picture.  A CIImage is a set of instructions for drawing a picture. Because of this, CIImages can be infinite in extent. The results of some of the CIFilters, for example the Checkerboard filter, are CIImages that are infinite in size.  It is happy to draw checkerboards anywhere on the drawing plane... you usually have to combine the results of such a filter with some kind of crop to make use of them. The surface that you draw the CIImage on to may have size limits, but a CIImage itself can be infinitely large. Scott Thanks,</body>
  </mail>
  <mail>
    <header>reestablishing a CGContextRef as the current context</header>
    <body>Please consider me a newbe to graphics. My question may reflect this.¬† Is the CGContextRef only good while the focus is locked or inside the scope of drawRect? Or is there a way for me to hold onto it and set it to be the current graphicsPort and draw into it at a later time.¬†</body>
  </mail>
  <mail>
    <header>Draw at cursor speed</header>
    <body>Josef</body>
  </mail>
  <mail>
    <header>Re: Implicit animation on iPhone layers</header>
    <body>On Nov 18, 2009, at 7:17 PM, David Duncan wrote: Thanks.  That is what I needed to know.  I suspected something like that, but I couldn't find anything in the docs that even hinted at that.</body>
  </mail>
  <mail>
    <header>Re: Implicit animation on iPhone layers</header>
    <body>UIKit suppresses animations on layers it creates by default. In order to get implicit animations you need to be running between +[UIView beginAnimations:context:] and +[UIView commitAnimations]. Typically you would UIView's methods instead of Core Animation's methods when a UIView method exists. In UIKit to do the same thing here you would do: -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Implicit animation on iPhone layers</header>
    <body>&amp;quot;Animation and Timing Classes Many of the visual properties of a layer are implicitly animatable. By simply changing the value of an animatable property the layer will automatically animate from the current value to the new value. For example, setting a layer's hidden property to YES triggers an animation that causes the layer to gradually fade away. Most animatable properties have an associated default animation which you can easily customize and replace. A complete list of the animatable properties and their default animations are listed in ‚ÄúAnimatable But this does not seem to be the case on the iPhone.  For example, if I have just a simple UILabel, the following code will not trigger an animation: At least, I'm not seeing an animation.  So I tried slowing the animation down, but I still don't see anything. [CATransaction setValue: [NSNumber numberWithFloat: 2.5f] forKey: Thanks, David</body>
  </mail>
  <mail>
    <header>quartz pdf voyeur sample code</header>
    <body>Does anyone know where i can find the quartz pdf voyeur sample code. The docs say &amp;quot;/Developer/Examples/Quartz/PDF/Voyeur&amp;quot; but thats not there and i cant find it online either. thx AC _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>Now filed as a radar:&amp;nbsp;7460093 I have observed that my CAOpenGLLayer based program drops more frames under a similar amount of load as its NSOpenGLView counterpart. This problem manifested itself having needed to transition my NPAPI plugin code from using an NSOpenGLView to a CAOpenGLLayer. My application performs a number of texture operations with textures in the magnitude of 1920 x 1080 pixels each. This did not surface as an issue when using NSOpenGLView given that animations appeared smooth. However when forced to using a CAOpenGLLayer approach (a requirement for NPAPI plugins for Safari 4) my animations became quite jittery. I have attempted to compare the performance of CAOpenGLLayer vs NSOpenGLView with two projects (attached). Each project sets up a 1920x1080 texture and blits it several times per frame. The resultant programs write a message to their respective consoles if more than 18ms has past since the last time a frame was rendered. 18ms is approximately how long it takes to render one frame at a rate of 60 frames per second. My observations have shown that the CAOpenGLLayer outputs more messages than the NSOpenGLView. This indicates that CAOpenGLLayer performance is less than NSOpenGLView i.e. more frames are being dropped.</body>
  </mail>
  <mail>
    <header>measuring plain text line length</header>
    <body>Hi, I have a function which measures the length of a short plain text (single line, single font and size). This function is called many times so it has to be fast. I create an attributed string using Cocoa then I call CTLineCreateWithAttributedString and CTLineGetTypographicBounds(NULL, NULL, NULL). It seems to be slow. In my test it takes 165 miliseconds from which CTLineCreateWithAttributedString takes 107 milliseconds. The previous Carbon implementation which I have to replace uses ATSUGetUnjustifiedBounds and takes only 61 milliseconds. CoreText is said to be about twice as fast as QuickDraw so I must do something wrong. Do you know how to do the CoreText version faster? Thanks, Geza Here is the simplified code: Int32 CoreEngine::MeasurePlainText (const GS::UniString &amp;amp;uString, const IFont &amp;amp;iFont, double *width) [attrString release]</body>
  </mail>
  <mail>
    <header>Re: Affine transforms on iPod Touch = iTouch</header>
    <body>There should be no differences here in terms of iPhone OS, however if you are transforming a view, then you are rotating that view in its parent's coordinate system, not in its own. If you want to rotate an image while clipping it to a view's rect, then the simplest method to do so is to implement -drawRect: and do the appropriate transforms there (which will clip to the view's rect). -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Affine transforms on iPod Touch = iTouch</header>
    <body>I am new to iTouch development but have experience developing for the Mac. I am trying to use Quartz to display a rotated image using affine transforms. The techniques used to do this on the Mac do not transpose to the iTouch and I have tried the methods described in the iTouch documentation but can't get them to do what I need them to. I can successfully rotate the UIView but can't confine the rotation to just the rect that holds my image. I had hoped to find some sample code on Apple's site but no luck. Can anyone help me? Thanks, Charlie Dickman email@hidden</body>
  </mail>
  <mail>
    <header>Re: incremental image saving</header>
    <body>hoi list/lurkers, apologies for posting -just- too soon... right after hitting send, i came across the post below, that contains a link to samplecode that exactly answers my questions. thanks, arri</body>
  </mail>
  <mail>
    <header>incremental image saving</header>
    <body>hi, after extensively searching for a solution, i start to believe i'm on the wrong track, i hope someone gere can say something wise, and point me in the right direction: i want to save the contents of a graphics ocntext to file, preferably png. right now i'm using to get the pixel-data into a buffer, and [[imageRep representationUsingType:NSPNGFileType properties:nil] to write that to a png image file. this works fine, but this needs at least as much RAM as there are bytes the buffer. so if the view is 30000x5000 pixels in size, the above will require close to 600MB (32bit). i've looked into CGDataConsumer and CGImageDestination thinking they would provide the solution to incrementally write-out parts of the data, without having to provide the entire buffer at once, but i'm a bit lost there. the documentation mentions the incremental loading of images () but what would be the scenario of doing the reverse? thanks, arri</body>
  </mail>
  <mail>
    <header>Re: CGPDF retain/release</header>
    <body>I am not really in this part but if I understand the people who deal with money in my company, this is about a few times 10000 dollars. The drawing ones. Basically I would have wanted to deal with DRMed part in PDF , be able to open them and then simply pass it to the usual Apple drawing callbacks. Today it is more like : subclass and do everything yourself or use the black box Best laurent</body>
  </mail>
  <mail>
    <header>Re: CGDisplaySwitchToMode replacement</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>CGDisplaySwitchToMode replacement</header>
    <body>Hello --</body>
  </mail>
  <mail>
    <header>Standard method to convert ASCII character to CGKeyCode?</header>
    <body>Given a character, I need a function that returns the CGKeyCode associated with the position of that character on the current keyboard layout. E.g., given 'b', it would return kVK_ANSI_B if using U.S. QWERTY, or 45 if using Dvorak. I am using this to pass to CGEventCreateKeyboardEvent(), as CGEventKeyboardSetUnicodeString() apparently does not work with modifier flags. I have searched extensively for this but cannot find a decent answer. Currently I am using the following code (found online [1]), which works, but is not exactly elegant and I would prefer not to use it in production code. Are there any standard functions I am overlooking? [1]: CGKeyCode keyCodeForCharWithLayout(const char c, CGKeyCode keyCodeForChar(const char c) currentLayoutData = TISGetInputSourceProperty(currentKeyboard, return keyCodeForCharWithLayout(c, /* Beware! Messy, incomprehensible code ahead! * TODO: XXX: FIXME! Please! */ CGKeyCode keyCodeForCharWithLayout(const char c, const UCKeyboardLayout *uchrHeader) /* Loop through the keyboard type list. */ /* Get a pointer to the keyToCharTable structure. */ UCKeyToCharTableIndex *uchrKeyIX = (UCKeyToCharTableIndex *) /* Not sure what this is for but it appears to be a safeguard... */ stateRecordsIndex = (UCKeyStateRecordsIndex *) if ((stateRecordsIndex-&amp;gt;keyStateRecordsIndexFormat) != /* Make sure structure is a table that can be searched. */ /* Check the table of each keyboard for character */ UCKeyOutput *keyToCharData = /* Check THIS table of the keyboard for the character. */ /* Here's the strange safeguard again... */ if ((keyToCharData[k] &amp;amp; kUCKeyOutputTestForIndexMask) == UCKeyStateRecord *stateRecord = (UCKeyStateRecord *) (uchrData + Thanks for the help.</body>
  </mail>
  <mail>
    <header>Re: CGPDF retain/release</header>
    <body>That should be okay if your document's structure is simple and most of the memory usage comes from images in the content stream. If you have an idea of the price of that, I'd be interested to hear it. I've never considered this. What nature of default callbacks would you consider to be useful? Yes. I fear there is no &amp;quot;cache&amp;quot; per se -- this stuff was written for the desktop in which it would just be swapped out to disk. Still, the more people file a bug, the quicker it'll be fixed. H</body>
  </mail>
  <mail>
    <header>Re: CGPDF retain/release</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGPDF retain/release</header>
    <body>Hi Laurent, It's not good news, I'm afraid. I can't get into devforums.apple.com at the moment, but if you can, there's more here: I think the original bug referenced from that thread is: rdar://problem/6367689 I opened a DTS request on this but there is no known workaround other than releasing the whole document each time you render a page (which is about as much use as no workaround at all as far as I'm concerned). I actually forgot to file a bug until just now: Please, please, file a bug yourself and vote to get this fixed! Best wishes, Hamish</body>
  </mail>
  <mail>
    <header>CGPDF retain/release</header>
    <body>Hi I have a question related to the retain/release mechanism of CGPDF. I am trying to display a multi page document on the iPhone, some with heavy color pages. So I call CGContextDrawPDFPage on object I get through  CGPDFDocumentGetPage. The problem is that I never see memory being released as the pages are turned. Should I call CGPDFPageRelease after having displayed a page (I would think no as this is not Copy but Get). Basically I want to make sure that I have only one page at a time in the memory of the iPhone. Otherwise it can crashes very fast. Thanks for helping laurent</body>
  </mail>
  <mail>
    <header>Re: CIImage to 8 bit in one easy step?</header>
    <body>I think core image filters can definitely do this. The problem is as far as I can see to render the CIImage the format has to be one of these:</body>
  </mail>
  <mail>
    <header>Re: CIImage to 8 bit in one easy step?</header>
    <body>Yes, but don't forget you can interpret that how you please.  It doesn't have to be RGBA data that comes back out... it could be that each channel had data for a different pixel, for instance---e.g. you could take an input image encoded like this (each cell in the table is a single pixel): 0   1   2   3 0  RG  RG  RG  RG BA  BA  BA  BA 1  RG  RG  RG  RG BA  BA  BA  BA 2  RG  RG  RG  RG BA  BA  BA  BA 3  RG  RG  RG  RG BA  BA  BA  BA and output an image encoded like this: 0   1   2   3 0  R   G   B   A 1  R   G   B   A 2  R   G   B   A 3  R   G   B   A (Effectively you're using one &amp;quot;pixel&amp;quot; in the output image to hold the values for four pixels in your 8bpp result.) Kind regards, Alastair. --</body>
  </mail>
  <mail>
    <header>Re: CIImage to 8 bit in one easy step?</header>
    <body>I'm assuming you're talking about converting 8 bit per channel image data to an 8 bit per pixel image (e.g. a .gif)? This isn't the kind of thing that can &amp;quot;just&amp;quot; be done for you because in general to get a good result you'll (a) need to decide how to pick appropriate colours for your image and (b) need to dither the colours in the 8bpp image. For instance, sometimes you might choose to use an octree-based colour selection algorithm and then do an ordered dither; or a diffusion dither; or for a larger image you might even choose to use a halftone screen (particularly for display on a high-resolution device).  For some types of output, you might want a different colour quantisation algorithm, or you might modify your selection to take account of your dither method.  It's even possible that your selection of colours might want to take account of particular visual elements in your image. I don't know whether there are any built-in conversions of this type on Mac OS X, but nothing springs immediately to mind.  There is certainly software out there that can do these kinds of things (ImageMagick, for instance) and that you might be able to use in your code, but at the end of the day this kind of conversion does require that *someone* loops over the pixels somehow. If I were looking to do this on Mac OS X and were starting from scratch, I might be inclined to try to do the work in a Core Image filter, since it seems the kind of thing that the GPU would be good at. Kind regards, Alastair. --</body>
  </mail>
  <mail>
    <header>Re: CIImage to 8 bit in one easy step?  (ben syverson)</header>
    <body>Not really as I will have to loop through every pixel in the whole image to convert the RGBA components to a scalar value. On 2 Dec 2009, at 20:08, email@hidden wrote:</body>
  </mail>
  <mail>
    <header>Re: CIImage to 8 bit in one easy step?</header>
    <body>On Dec 2, 2009, at 12:24 PM, Max wrote:</body>
  </mail>
  <mail>
    <header>Re: PDF bloat in Leopard</header>
    <body>I appreciate the ideas.  I will see if I can get anywhere with them and post the results back. Regards, Sandor Szatmari On Dec 1, 2009, at 6:41 PM, Ken Ferry wrote:</body>
  </mail>
  <mail>
    <header>Re: Determining RGB Brightness</header>
    <body>&amp;gt;&amp;gt; If you want to compare a color against another color Well this is quite what I proposed to you then :) Basically, you can formulate your problem this way : from a set of 2 colors (black, white) choose the color that offers the maximum perceptual contrast on another color. (this is it, right ?) Then depending on the precision you need, you can resolve your problem. Since you end up in only taking a decision (black or white), maybe the evaluation of contrast maybe quite coarse. So for example, even this kind of weighted norm 1 : contrast = 0.2126 * abs (R - Rc) + 0.7152 * abs (G - Gc) + 0.0722  * where RGBc is the background color might have enough precision to take your decision. Now this might be completely OT, but did you consider to get your text have a self bundled contrast ? This is the technique used in film paper ads for example, or in the text in your dock on mac, etc. Basically the idea is to have the text both have white &amp;amp; black colors so that you're sure it will go anywhere. Typical implementations are for example white text with black stroke, or white text with black shadow, or the inverse, etc. Hope this will help, Raphael</body>
  </mail>
  <mail>
    <header>Re: Determining RGB Brightness</header>
    <body>Hmm. The CIECAM models are about 10 times as complicated as the L*a*b*. (quick comparison, 20 lines of code vs. 80 in my implementations), and that's not counting the RGB-&amp;gt;XYZ conversion too (maybe another 10 lines). Graeme Gill.</body>
  </mail>
  <mail>
    <header>Re: PDF bloat in Leopard</header>
    <body>There is no method that I can think of. This seems like the kind of behavior that most people would want to opt out of :). If there was such an API or constant, I would imagine it would be either in the documentation or in the CGPDFContext.h header. There are a very few things added in 10.5, and none of them seem relevant. NSImage on 10.5 may rasterize PDF, which is obviously not what you want. I can&amp;#39;t recall what the incantation is to avoid this, but perhaps Ken Ferry will pop in and mention it :).</body>
  </mail>
  <mail>
    <header>Re: PDF bloat in Leopard</header>
    <body>There is no method that I can think of. This seems like the kind of behavior that most people would want to opt out of :). If there was such an API or constant, I would imagine it would be either in the documentation or in the CGPDFContext.h header. There are a very few things added in 10.5, and none of them seem relevant. NSImage on 10.5 may rasterize PDF, which is obviously not what you want. I can't recall what the incantation is to avoid this, but perhaps Ken Ferry will pop in and mention it :). -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: PDF bloat in Leopard</header>
    <body>Thanks for your response.  Just to clarify things, although it would be nice if there were a way to make this example code work, any solution that produced the desired results would be acceptable. It appears that filing a bug report will be necessary, I just wanted to make sure I wasn't missing something.  I thought, perhaps, there may be some mode, or hint, I might need to enable that was introduced as part of an update to the API. I also created the analog to this code using PDF Kit while exploring this issue.  I found that when using an NSImage the image is indeed included in the PDF document as an XObject and referenced by the PDF drawing command /Fm1.  However, the PDF image rep is not used, a Bitmap image rep is used. While this indeed created small files the quality is inferior and unacceptable.  When I forced the use of the PDF image rep the behavior changed to repeatedly including the drawing commands creating bloated files. Regards, Sandor</body>
  </mail>
  <mail>
    <header>Re: PDF bloat in Leopard</header>
    <body>Your best bet in the case of changed behavior that you feel is incorrect is to file a bug report. Given the nature of your code, it is unlikely that there is anything anyone on this list can really do to help. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>PDF bloat in Leopard</header>
    <body>Has anyone noticed that when using the Core Graphics Framework to compose a PDF there seems to be a fundamental difference between Tiger and Leopard? In Tiger when I repeatedly draw the same object to a PDF page the drawing commands are stored once in an XObject and referenced repeatedly by the /Fm1 command. In Leopard the same object is repeatedly drawn to a page the drawing commands are inserted into the page over and over again causing the document to bloat severely when the image needs to appear many many times. The following code will illustrate what I have described.  It produces drastically different PDF drawing commands when run on Leopard as opposed to Tiger. For this example, when I execute the following code on Tiger the PDF file size is 188K while on Leopard the file size is 228K.  This change is small compared to what I have experienced in a production work flow.  PDFs going from 20M to 190M. Am I missing some API change or does the internals of the Leopard Framework just not behave the same way as Tiger's? CFURLRef pageURL = CFURLCreateWithFileSystemPath(kCFAllocatorDefault, CGContextRef context = CGPDFContextCreateWithURL(pageURL, &amp;amp;pageRect, CFURLRef templateURL = CFURLCreateWithFileSystemPath(kCFAllocatorDefault, CGRect mediaBox = CGPDFPageGetBoxRect( page, kCGPDFMediaBox ); // Calcuate a rect for the diagram for (i=0; i &amp;lt; 100; i++) // Draw the diagram 100 times CGContextDrawPDFDocument(context, CGRectMake(0., (0+i*0.5), This is an image that can be used to test this. Thanks in advance, Sandor Szatmari</body>
  </mail>
  <mail>
    <header>Re: Determining RGB Brightness</header>
    <body>Am 30.11.2009 um 16:14 Uhr schrieb Dinge Raphael: But that's not what I wanted to do. I was just looking for a formula to decide if a text on top of a colored background should be white or black. And the formula I posted did work better than the one given before for Y (luma). I did not try the one you gave for the Lab model. But since it needs quite a bit more work to calculate, I'm not sure it's worth it anyway. Still, I appreciate the background information and I might look into the CIECAM models. Thank you. :) Andreas</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>I did test the program on older G5 dual 1.8 w/ ATI 9700/128MB on 10.5.8. It is running much faster (5x fps wise) with no significant GPU memory drop on the older system than on the newer macbookpro nVidia 9400M/ I also believe that there are too many buffers being allocated. The fact that the GPU memory fills up fast in my sample project is because of FSAA (look at the pixel format). When I disable FSAA, mem requirements drop too, but there remains performance problems. This is all linked, but symptoms are somehow different. Raphael (build and run) Could some of you confirm you can observe this as well on you setup? I will then file a bug. (OpenGL Driver Monitor, Current Free Video Memory) Mine is MacBookPro, 9400M, 10.6.2</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>Raphael (build and run) Could some of you confirm you can observe this as well on you setup? I will then file a bug. (OpenGL Driver Monitor, Current Free Video Memory) Mine is MacBookPro, 9400M, 10.6.2</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>Christopher, Raphael (build and run) Could some of you confirm you can observe this as well on you setup? I will then file a bug. (OpenGL Driver Monitor, Current Free Video Memory) Mine is MacBookPro, 9400M, 10.6.2</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>I can do some more testing, but at the end I would like to know if it is a bug or if this is just a normal situation when working with CAOpenGLLayer, in which case I would need to move to the good old overlay window.</body>
  </mail>
  <mail>
    <header>Re: Display Reconfiguration Callback firing too late</header>
    <body>And it does!  (20KB??) Ok, you can get my test code here: Thanks for your help, -n8 --</body>
  </mail>
  <mail>
    <header>Re: Display Reconfiguration Callback firing too late</header>
    <body>As a test, I just threw together a little app that does nothing but print the display configuration each time it changes, during the pre and post config notifications.  I see the same behavior as in my app. Here's a disconnect and a reconnect of the external monitor: 17:03:26.320 fmn-mini Awake! 17:03:26.323 Activating 17:03:36.120 ----------------------------------------------------- 17:03:36.121 Got pre-display change notification on 0x4271a00, is main 17:03:36.122    Display 0x4271a00: 1440x900 at (1600, 300) 17:03:36.122 ----------------------------------------------------- 17:03:36.122 ----------------------------------------------------- 17:03:36.123 Got pre-display change notification on 0xb401671, is not main 17:03:36.123    Display 0x4271a00: 1440x900 at (1600, 300) 17:03:36.123 ----------------------------------------------------- 17:03:41.752 ----------------------------------------------------- 17:03:41.869 Got post-display change notification on 0xb401671, is not main 17:03:41.871    Display 0x4271a00: 1440x900 at (0, 0) 17:03:41.871 ----------------------------------------------------- 17:03:41.871 ----------------------------------------------------- 17:03:41.872 Got post-display change notification on 0x4271a00, is main 17:03:41.881    Display 0x4271a00: 1440x900 at (0, 0) 17:03:41.881 ----------------------------------------------------- 17:04:14.473 ----------------------------------------------------- 17:04:14.493 Got pre-display change notification on 0x4271a00, is not main 17:04:14.499    Display 0xb401671: 1600x1200 at (0, 0) 17:04:14.504    Display 0x4271a00: 1440x900 at (1600, 0) 17:04:14.511 ----------------------------------------------------- 17:04:14.516 ----------------------------------------------------- 17:04:14.519 Got pre-display change notification on 0xb401671, is main 17:04:14.521    Display 0xb401671: 1600x1200 at (0, 0) 17:04:14.523    Display 0x4271a00: 1440x900 at (1600, 0) 17:04:14.525 ----------------------------------------------------- 17:04:20.510 ----------------------------------------------------- 17:04:20.599 Got post-display change notification on 0xb401671, is main 17:04:20.599    Display 0xb401671: 1600x1200 at (0, 0) 17:04:20.600    Display 0x4271a00: 1440x900 at (1600, 300) 17:04:20.600 ----------------------------------------------------- 17:04:20.600 ----------------------------------------------------- 17:04:20.601 Got post-display change notification on 0x4271a00, is not main 17:04:20.601    Display 0xb401671: 1600x1200 at (0, 0) 17:04:20.601    Display 0x4271a00: 1440x900 at (1600, 300) 17:04:20.602 ----------------------------------------------------- I'll attach the project in another reply (just in case the list has a size limit). Thanks, -n8 --</body>
  </mail>
  <mail>
    <header>Re: Display Reconfiguration Callback firing too late</header>
    <body>That would be astonishing, since the only thing my app does on its event loop is wait for display reconfiguration events.  :-)  Could the problem be that my app is too idle? Actually, now that I think about it, I'm also registered as a DO server to get configuration options from my prefpane.  Hmm... Disabling that doesn't help though. That is interesting.  There is a noticeable delay (~3 sec) between the menu bar time when I physically connect/disconnect the monitor and the timestamp when I get the notification.  But like I said, all the app does is wait for these notifications, so I don't know what I could do to get them faster.  Any thoughts or suggestions on how to debug this? Thanks, -n8 --</body>
  </mail>
  <mail>
    <header>Re: Display Reconfiguration Callback firing too late</header>
    <body>Display reconfiguration callbacks fire as a side effect of your application's processing of new events, and are driven from the same code that picks up new events for your application. You might want to check if your application is being kept busy and not picking up events often enough.     In general, the Begin Configuration notifications are 'synchronous', that is, the configuration process will not proceed until all applications have either processed their notifications, or have not processed the notification within 5 seconds. It looks like your application may not be picking up the notifications until after the timeout period has passed and the configuration change is already underway.</body>
  </mail>
  <mail>
    <header>Display Reconfiguration Callback firing too late</header>
    <body>Hi folks, I've got an app that critically depends on the display reconfiguration callback mechanism [1].  The way we do our thing is: When our callback fires with kCGDisplayBeginConfigurationFlag set: location of menu bar) -  We use the accessibility API to enumerate all windows and store their positions &amp;amp; sizes -  We associate the window states with the display configuration When it's called with kCGDisplayBeginConfigurationFlag not set: -  We enumerate the CGDisplays again -  We try to find a stored configuration matching the current configuration -  If one is found, we restore the states of all the windows that we can (BTW, Apple, *please* steal this idea!!) In Tiger this worked marvelously and all was love and light.  In Leopard, however, we've got a big problem.  It appears that the &amp;quot;begin configuration&amp;quot; notification arrives too late.  For example, when disconnecting a MacBook Pro from a 1600x1200 monitor I will see this log output from my app: ] Got pre-display change notification on 0x4271a00, is main ] ******** Screen configuration about to be changed! ******** ] Current configuration: CGDisplayConfiguration with 1 displays, displays = ( CGDisplay 0x4271a00: 1440.000000x900.000000 at (1600.000000, 300.000000) ), hash = 328 ... ] Got post-display change notification on 0x4271a00, is main ] ======== Screen configuration changed! ======== ] New configuration: CGDisplayConfiguration with 1 displays, displays = ( CGDisplay 0x4271a00: 1440.000000x900.000000 at (0.000000, 0.000000) ), hash = 1572 So the state returned by CGGetActiveDisplayList during the pre-change notification is (almost) the state that it should be during the post-change notification.  The corresponding problem also appears when I reconnect the monitor -- I get two displays in the pre-change configuration. This is fatal to our attempt to enumerate the displays, but that's not hard to work around.  What's more problematic is that the window positions &amp;amp; sizes we get from the accessibility API at this stage are often just nuts.  I will file a bug on this, but I wanted to post here to see if anybody knew if there's an explanation or can suggest a workaround. Thanks, -n8 [1] --</body>
  </mail>
  <mail>
    <header>CATransform3D perspective question</header>
    <body>I'm having some trouble with using Core Animation's 3D transforms to get a perspective effect.  Part of the problem is that the exact nature of the transformations is not documented as far as I can tell, and it seems not to behave the way that I've seen these transforms used before. The CATransform3D type is a 4 by 4 matrix.  Typically these matrices are multiplied by the vector (x,y,z,1) to return a vector (x', y', z', s) and the display co-ordinates are (x'/s, y'/s), with z'/s being used for z-buffer values if drawing is done with z-buffering (which I don't think CA supports). My initial tests supported my view that this was the way CA was going to use the matrix.  Rotations about the z axis put sin(t) and cos(t) values into the m11, m12, m21 and m22 slots; translations effect the example I could find for applying a perspective transformation was three lines of code in the CA &amp;quot;Layer Geometry and Transforms&amp;quot; docs, listing 2, which uses the standard technique of putting -(1/distance) into the m34 slot (this exact same code also appears in the CoverFlow example).  Adjusting the zPosition value for a layer zooms the layer in and out.  So far so good. The problem is simply this; applying a 90 degree rotation about the y axis does NOT turn the image edge-on.  I've hooked up a rotary NSSlider to an action that makes a transform thus: I'm printing this transform and then setting the transform on a layer containing an image.  Rotating the slider rotates the image and if the slider is set to 90 degrees then the transform is the expected: 1.0 in m13, m22, m31 and m44, with zeros everywhere else.  This _should_ produce a transform where the output 'x' co-ordinates are invariant of the 'x' input (and in fact should be solely dependent on the 'z' position).  Unfortunately, when I do this I get an image which looks like it's turned by about 75 degrees, definitely not edge-on, and very much with the output x co-ordinates dependent on the input x value.  I have to turn the slider to about 115 degrees to get the image edge-on. So, my question is what exactly is the process by which the layer co- ordinates are converted to the display co-ordinates?  The documentation on this seems to be missing and while it looks like it should be fairly standard it does not function as expected.  Any help would be much appreciated.</body>
  </mail>
  <mail>
    <header>CGContextSetRenderingIntent() not working</header>
    <body>I am trying to use the call to CGContextSetRenderingIntent() to set the rendering intent (RI) within a particular graphics context. No matter which RI is chosen the results are the same - as if the RI is ignored. I store the graphics context then call CGContextSetRenderingIntent () then restore the graphics context. Am I not using this call properly? Is there another way to have an image drawn with a chosen (not image-specified) RI? On the Quartz mailing list someone from Adobe was having the same problem. He never got an answer to his question but posted this response to his own question: Is he correct? Does this mean there is NO way to specify an image conversion with its RI? If he is correct, is there a call to change the specified RI of an image? Aaron Alpher</body>
  </mail>
  <mail>
    <header>Re: How to make CIContext when not drawing on screen?</header>
    <body>-- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: How to make CIContext when not drawing on screen?</header>
    <body>Thanks, but I'm having a problem with this.  [CIContext createCGImage:fromRect:format:colorSpace] requires 10.5, and I'm using the 10.4 SDK.  I tried [CIContext createCGImage:fromRect:] but it returned NULL.  The CIImage* I'm feeding it is not NULL, and the rect looks OK, so I don't know what else to check. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: QCRenderer speed</header>
    <body>What were you using before that you are comparing the QCRenderer to? How much difference are you noticing? It would be good to take a Shark profile (or Instruments on Leopard) to know where the bottleneck is. I could suggest that you only change the values as necessary to avoid hammering the KVC system but without knowing where the real processing is happening that might not be a good use of your time. I can't see anything in the steps you are using that would indicate a major slowdown. But if you are comparing to a vanilla CI filter pipeline then the convenience you get out of using qtz files may translate to a little bit slower performance. Please file bugs if you believe the Quartz Composer system can or should do better. Also, there is a quartzcomposer-dev list you can use for questions.</body>
  </mail>
  <mail>
    <header>Re: ATSUI text color space</header>
    <body>On Nov 29, 2007, at 12:51 PM, John Kerr wrote: For most fonts, I believe ATSUI uses the CG context's fill color.  If you want to use a CGColorRef you could call CGContextSetFillColorWithColor before drawing.  This assumes your text is all one color.  If you start setting RGB colors with style tags you'll get something else. Nick _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: How to make CIContext when not drawing on screen?</header>
    <body>hmm, I didn't know you could instantiate a CIContext like that.  I've done this in the past by using an NSImage and using lockFocus/ unlockFocus. something like this: CIContext *context = [CIContext contextWithCGContext: [[NSGraphicsContext currentContext] graphicsPort] //do drawing in here... On Nov 29, 2007, at 2:58 PM, Ralph Brunner wrote:</body>
  </mail>
  <mail>
    <header>Re: Effectively &amp;quot;joining&amp;quot; many small CIImages</header>
    <body>I'd guess you need to make the decision based on destCoord(), then output a pixel based on samplerCoord's specific to each sampler. Actually, I decided not to do something off the top of my head because I realized it's probably a little more complicated, involving a samplerTransform or two involving the destCoord().  That's when I fumble-fingered and hit send by mistake.  Sorry.</body>
  </mail>
  <mail>
    <header>Re: Printing pre-managed pixels</header>
    <body>I too would be very interested in doing this using some Apple's API. Until now, a simple solution (for my particular usage) has been to print using a patched version of the GPL printing package &amp;quot;gutenprint&amp;quot;.</body>
  </mail>
  <mail>
    <header>Re: Effectively &amp;quot;joining&amp;quot; many small CIImages</header>
    <body>Just off the top of my head, something like: kernel vec4 sideBySide( sampler image1, sampler image2 )</body>
  </mail>
  <mail>
    <header>Printing pre-managed pixels</header>
    <body>Within our Cocoa application we have pre-color managed pixels (i.e. the pixels are already in the CMYK space of the printer) within a CGImageRef. We don't want to let the print driver color manage the data since it does not have support for third-party papers and inks. However, when printing we don't want the pixel values transformed further. Is there a way to do this? When printing the CGImageRef will be drawn into the CGContextRef of the printer and therefore will be double color managed. Is there a way around this? Is there a better way to do this?</body>
  </mail>
  <mail>
    <header>Re: Effectively &amp;quot;joining&amp;quot; many small CIImages</header>
    <body>On 10 Dec 2007, at 15:59, Aidas Dailide wrote: It would be pretty trivial to write an custom kernel which placed two or more images side by side. You could then stack a few of these together to produce any sort of mosaic you like. kernel vec4 sideBySide( sampler image1, sampler image2 ) return xy.x &amp;lt; size.x ? sample(image1, xy) : sample(image2, xy-</body>
  </mail>
  <mail>
    <header>Question about Display Profile and drawing bits</header>
    <body>I hope this is the correct list, I debated between this and colorsync- dev, but ultimately went here. If it is incorrect, please let me know. I'm testing our painting code to see how well it plays with different color sync profiles. Along the way I came across this GBR profile which changes basically shifts all the channels as far as I can tell. Most applications seem to deal with this fairly well. The problem I have is that I have some bits in memory created in cross-platform code as ARGB (for simplicity we fill it with 0xffff0000 or red) and I want to create a CGImageRef out of them and draw them to the screen. If I create a CGImageRef with the kCGColorSpaceGenericRGB colorspace and then draw it to the screen with CGContextDrawImage(), the color is visually wrong, (it appears blue on the screen). If I get the profile for the main display and draw it, the image appears red on the screen. If I create another chunk of memory, and create a CGBitmapContext on it with the kCGColorSpaceGenericRGB, fill with opaque red, then convert that to a CGImageRef and draw it to the screen, it is correct, the image on the screen is red. I've peeked at the memory for this second chunk of memory and indeed, the pixels aren't 0xffff0000, but something like 0xff23ff08. So, it seems to me that I am doing something wrong with my bits in the first example. From my limited knowledge of display profiles, I would think that the generic color space would do the right thing for me, like it does for the bitmap context, but there seems to be a missing piece somewhere and I'm not sure where to look. -- Trenton</body>
  </mail>
  <mail>
    <header>Effectively &amp;quot;joining&amp;quot; many small CIImages</header>
    <body>I'm having some trouble figuring out what would be the best way to join many small (lets say 128x128 or even smaller) CIImages into one big CIImage. I've tried to use CIImageAccumulator but rendering gets painfully slow. I've also tried to use Source Over filter to join them together but again I don't feel it's very effective. I haven't figured out how to write effective custom filter to do the same job (I don't need the compositing, just to place the images side-by-side). Now the reason I'm doing this is that I want to do my own tiling and I use imageWithTexture:size:flipped:colorSpace: which is hardware- limited but is very effective. I've tried to use CIImageProvider but since it copies bitmap data every time I re-create the CIImage I decided to drop it. Thank you, Aidas</body>
  </mail>
  <mail>
    <header>CGSWindow errors</header>
    <body>We're seeing occasional exception reports in a Cocoa app via our homegrown exception reporting mechanism, which also uses asl to get information from syslog.  The exception is &amp;quot;Error (1000) creating CGSWindow&amp;quot;, and I've no idea where it originates (the stack trace just shows the event loop).  I'm posting here because it's CGS related. I can't reproduce this on any of the systems I use the app on.  The user who reported this most recently has the following CGSResolveShmemReference errors reported; he says other apps also report them, starting with Mail.app.  Is there anything we can do to debug this? Dec  8 14:51:24 BibDesk[6299]: CGSResolveShmemReference : reference offset (34688) exceeds bounds (32768) on shmem obj 0x8992 Dec  8 14:51:24 BibDesk[6299]: CGSResolveShmemReference : reference offset (34576) exceeds bounds (32768) on shmem obj 0x8992 Dec  8 14:51:27 BibDesk[6299]: Error (1000) creating CGSWindow</body>
  </mail>
  <mail>
    <header>Automatically execute some code right before Front Row starts</header>
    <body>I would need to automatically execute some code right before Front Row starts and right after it ends. FR is invoked by remote control. Is it possible to do and how? A related question: Is it possible to start another app or script than FR by using the remote?</body>
  </mail>
  <mail>
    <header>Re: External display reported online and active when the display	isoff</header>
    <body>I investigated this topic in some more detail. DVI/HDMI devices such as monitors, TVs, AV Receivers etc. do respond with a valid EDID block even when powered off completely. Digging into the specifications and implementation guides I discovered that this is actually a requirement. The standards say that a source device (computer in this case) shall provide 5 V on the specified lead so that a sink device (monitor, TV, etc.) can reply to EDID request even when powered off. The rationale behind this, according to the specs, is that in large installations with many monitors etc. you may have a system management application running at nights, when the monitors are off, that needs this information.  A large installation consisting of large number of large full HD flat panel TV devices (which are typical consumer devices) running system management application at nights when TVs are off, requiring up-to-date knowledge about TV resolutions, sizes, and refresh rates? Isn't the need for a source device to know whether the TV is on or off or in standby, and whether the source is connected to currently selected input, i.e. whether the rendering done by the source will actually be visible on screen, much more obvious and immediate? Dear TV and monitor vendors, in the future please include a user accessible setting that will make the TV/monitor to disconnect all the leads from all not currently selected inputs, if not physically (by using relays) then at least electronically (by using high impedance semiconductors), thank you. In the meantime, I solved this problem by connecting a remote controlled HDMI switch between iMac and TV. Then by using a programmable universal remote, both TV and HDMI switch are operated in sync so that HDMI switch disconnects TV from iMac whenever TV is switched off or another input than iMac line is selected on TV. Not all HDMI switches do this right. A large number of switches let EDID pass through even when the actual line or the whole switch is off - they operate only on video signal lines and let control lines be connected all the time. As this information cannot be found in switch specifications, one has to verify the correct function by experiments until the right switch is found. ----- Original Message ----- Sent: Monday, December 03, 2007 5:42 PM Subject: Re: External display reported online and active when the display isoff</body>
  </mail>
  <mail>
    <header>DisplayLink: which field is being drawn?</header>
    <body>Hi Anyone know how to determine which field is being drawn when the CVDisplayLink callback is called for an update on an interlaced display? I am rendering to a PAL monitor (50Hz interlaced). The CVDisplayLink callback is called for every field (i.e. 50 times per second) so I know when a field is drawn, but I don't know whether it is odd or even - i.e. I cannot determine the start of a complete frame. The callback has a CVTimeStamp passed to it, but kCVTimeStampTopField and kCVTimeStampBottomField are always FLASE. I need to know this so I can render interlaced video out correctly - I'm using Cocoa on Leopard. Any ideas would be greatly appreciated. Cheers</body>
  </mail>
  <mail>
    <header>PDF operator TJ</header>
    <body>Hi I'm having a problem with the PDF operator TJ: I cannot figure out how to get glyph's horizontal and vertical displacements while scanning a PDF document for text. These displacements are referred to as w0 and w1 in page 410 of PDF Specs, version 1.7. Are w0 and w1 set by some operator that I cannot see? Or do I need to use the font dictionary? Thank you for any help. JJ ___________________________________________________________ Support the World Aids Awareness campaign this month with Yahoo! For Good</body>
  </mail>
  <mail>
    <header>Re: high cpu use when embedding an NSView into an NSBox</header>
    <body>&amp;gt; The amazing thing is that when I decide to embed my CustomNSView into If your view is opaque (which it sounds like it is), override, -isOpaque to return YES.  That way, invalidating your view will not cause the box and the window background surface to redraw. Sounds like what you built is a test app?  Then you could file a bug complaining about box drawing performance and include a zip of the whole project. -Ken</body>
  </mail>
  <mail>
    <header>CoreText in flipped NSView</header>
    <body>Hi, I'm using CoreText to draw into a flipped NSView.  I've applied the identity transform and set a -1 Y scaling in the context, so the text appears the right way up. However, the text is still drawn from bottom to top, so the lines appear in reverse order and paragraphs are broken onto the 'previous' line instead of the 'next' line.  An example of the resulting 'bottom- to-top' output might be: that everyone self-evident, truth to be We hold this Is there some extra setting required to draw in normal line order (top- to-bottom) using CoreText in a flipped view?</body>
  </mail>
  <mail>
    <header>Re: Display Reconfiguration Callback firing too late</header>
    <body>Ok, I finally found time to file this bug.  It's radar 5631661.  I've attached my test code that demonstrates the bug to the report. Thanks, -n8 --</body>
  </mail>
  <mail>
    <header>[CA] Create a &amp;quot;A la FrontRow&amp;quot; Button</header>
    <body>first of all, I'm sorry for my double mailing: I did not know which list was more appropriate for this question, because it's about CoreAnimation, so it's Cocoa related and also Quartz(Core) related. By the way, here is what I want to do: create a &amp;quot;√† la FrontRow&amp;quot; button (the highlighting rectangle below the current selection). In order to do so I thought I'd create a CALayer subclass which would contain two gradients -- one for the button top, the other for the bottom -- to create a gloss effect and a outline light. I thought I could use CIFilters to create these &amp;quot;effects&amp;quot; (CILinearGradient and CIBloom), but it isn't that easy (well, I think I just don't use them correctly): the two CILinearGradient I create are applied on the layer as &amp;quot;backgroundFilters&amp;quot; (if they are applied as -- foreground -- &amp;quot;filters&amp;quot; they don't appear) and the CIBloom, as a -- foreground -- &amp;quot;filters&amp;quot; (and doesn't appear). I think that if my filters don't appear when they're applied as foreground filters it's because my layer is empty -- which is logical :'). So do I have to provide a &amp;quot;fake&amp;quot; contents to my layer (as a transparent 1 square pixel picture streched to my layer bounds)? Or is there a smarter way to create my &amp;quot;√† la FrontRow&amp;quot; button layer? Paul _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Flipping/mirroring contents of a display</header>
    <body>On Dec 5, 2007, at 01:21, David Duncan wrote: Yes, I'm aware of the hardware option, but unfortunately the (old) projector I'm using doesn't have it. I was already thinking about adding another mirror to bring the image into the correct orientation, but then again, I also thought that implementing the screen flip in software would be more trivial. I might opt for another mirror now. However, if end up having to implement a software solution, I'll share my source code with the list! Many thanks for all the great help, Georg</body>
  </mail>
  <mail>
    <header>Re: Flipping/mirroring contents of a display</header>
    <body>The basic idea is to read the screen image into a texture, then render that texture onto an OpenGL context on the other screen. To get this to work most efficiently you would have 2 Open GL contexts that share resources (hence why they must be on the same video card). There are a number of technologies that you can use to do this, with the latest and greatest being Pixel Buffer Objects (PBOs). I think the OpenGLCaptureToMovie example uses a slightly older tech however. Most of the color change options work by modifying the display gamma table. You can see this by playing with the various options and taking screenshots and you will see that none of them are reflected in the output screenshot. All that said, a colleague just mentioned that typically this is a hardware function of the projector itself, and failing that that there are various aftermarket hardware solutions (google &amp;quot;rear projection mirror&amp;quot;) that may do this as well. Maybe that's a simpler way to go? -- David Duncan Apple DTS Animation and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Re: Flipping/mirroring contents of a display</header>
    <body>Primarily yes. I think that, OpenGLScreenshot and SonOfGrab are our current screen capture samples. At the time that I wrote the readme I was doing my timing application side and hadn't inspected what the window server was doing. I hadn't tried to push the framerate at that time either :). I'll get the ReadMe updated for the next push of the sample. -- David Duncan Apple DTS Animation and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Re: Flipping/mirroring contents of a display</header>
    <body>example, i.e. capturing the main screen and drawing the obtained CGImageRef into a full screen view on the second monitor (i.e. the projector) with the vertical flip applied in the CTM, but it doesn't perform too well. But yes, both displays are connected to the same graphics card, so the idea of duplicating the framebuffer sounds interesting. I've looked at the OpenGLCaptureToMovie and glGrab examples and got a bit of an understanding on how to obtain the necessary OpenGL contexts to do this, but how can the contents of the first framebuffer get flipped vertically when duplicating them in the second framebuffer? Obviously, it would be great to do this right on the graphics card, but I have to admit that I've got absolutely no clue on how to do this. Could you maybe elaborate a bit to give me the right pointers? Another idea I had: How do the &amp;quot;Display&amp;quot; options under &amp;quot;Universal Access&amp;quot; in &amp;quot;System Preferences&amp;quot; work, such as turning on the &amp;quot;Black on White&amp;quot; display mode? It looks as if these settings where messing with the framebuffer directly (in the case of &amp;quot;Black on White&amp;quot;, inverting it), so wouldn't it be possible to do the screen flipping in a similar manner? Thanks again, Georg</body>
  </mail>
  <mail>
    <header>Re: Flipping/mirroring contents of a display</header>
    <body>I don't doubt that at all, but what samples are you referring to? OpenGLCaptureToMovie, maybe? Yes, the Window Server works very hard to fulfill the requests (and I've never tried to push the frame rate). You might want to update the Readme.rtf in Son of Grab. It seems to say that the new API performs as well as any other method as long as you factor out window list creation appropriately.</body>
  </mail>
  <mail>
    <header>Re: Flipping/mirroring contents of a display</header>
    <body>CGWindow certainly does make it easy to capture the contents of the screen, however for the kind of work that Georg wants to do, he might be better served by the OpenGL samples that we've posted previously, especially if both displays are on the same video card, since it would be the equivalent of duplicating the frame buffer from one screen to another. If they are on different cards, the two methods would probably be a toss up as far as speed goes. Also be aware that a lot of the work that goes on in the CGWindowAPI occurs inside the window server. I have some code here that gets a partial screen composite at 30 fps, but it also causes the Window Server to consume around 20% of the CPU (on a 2.3Ghz MacBook Pro). This is about 19% above normal :). This is just looking at Activity Monitor, and the API also only updates the window list it uses periodically. YMMV, as always with performance issues be sure to test often and early to see if the assumptions of today still make sense tomorrow... -- David Duncan Apple DTS Animation and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Newbie Quartz Question on drawing from QTCaptureSession</header>
    <body>drawImage CGPointZero CGRect I&amp;#39;ve noticed that the above code is quite a performance hit whether I&amp;#39;ve initialized the CIContext with a window context or a bitmap context. &amp;nbsp;Basically I&amp;#39;ve modified the new QTKit Still Capture example app to draw the video frame from my builtin iSight immediately into a window using the said Quartz methods. &amp;nbsp;Is there a faster way to do this? Or is going directly from the CVImageBufferRef to an OpenGL Texture the only really quick method? I thought perhaps creating a CGImageRef directly from the CVImageBufferRef might be quicker, but I haven&amp;#39;t got this working. Thanks, David</body>
  </mail>
  <mail>
    <header>Re: Flipping/mirroring contents of a display</header>
    <body>Leopard has a cool new API for getting the contents of the display. CGContextDrawImage in a context with the appropriate coordinate space transform.  Combine the Son of Grab example with some CGDirectDisplay (CGDisplayCapture, CGDisplayGetDrawingContext) to capture and draw on the second display.</body>
  </mail>
  <mail>
    <header>Re: Flipping/mirroring contents of a display</header>
    <body>On Dec 4, 2007, at 11:30 AM, Georg Kaindl wrote: In that case you probably want to search archives for the OpenGL based screen capture methods that have been discussed on this list in the past.  I believe that method uses the lower left corner of the screen as the origin so you might not need to do any flipping.  If you do you'll have to create a CGImageRef for your captured image and draw it into a second buffer configured as a bitmap context with a flipped CTM.  You might also consider looking at vImage in the Accelerate framework.  There are some highly optimized functions there for this kind of thing. Nick</body>
  </mail>
  <mail>
    <header>Re: Flipping/mirroring contents of a display</header>
    <body>On Dec 4, 2007, at 19:08, Nick Nallick wrote: Thanks, Nick. However, I'm not writing a display driver (at least not for the moment), I just want to hook up the projector to the Mac and have it back-project onto a screen. However, the CG context approach sounds like exactly what I'm looking for, but how do I get the appropriate CG context to transform? I thought the Quartz framework would not allow to get a CG context that pertains to the screen context as that would be skipping some levels of abstraction? I actually want to flip whatever currently is on the screen, not only what my own application might draw. Thanks again, Georg</body>
  </mail>
  <mail>
    <header>Re: Flipping/mirroring contents of a display</header>
    <body>On Dec 4, 2007, at 10:22 AM, Georg Kaindl wrote: The standard way to flip vertically is to translate the CG context's current transformation matrix (CTM) by the height, then scale it by -1.0 vertically (or vice versa).  However you may find that it's better to incorporate this into your display driver work flow.  For example, if your driver can simply start at the other end of the frame buffer and increment scan lines in the opposite direction you've probably just flipped the display without any extra computing overhead. Nick</body>
  </mail>
  <mail>
    <header>Flipping/mirroring contents of a display</header>
    <body>I'm looking for a performant way to mirror the contents of a screen, i.e. draw them flipped around the y-axis. I need to do this in order to drive a prototype back-projection display, made out of an off-the- shelf projector. Ideally, I'd want to be able to flip the contents of any screen. Is there, for example, a way to hook the necessary transformation into the Quartz pipeline? Since this is just a prototype project, I'm not afraid of using private APIs for this. However, since it's not necessary for the mirrored display to respond to mouse events, it would also be fine to just draw the flipped contents of the main screen on a secondary display. What would be the fastest (in terms of execution speed) way to implement this? I was thinking it might be possible to capture the main screen, flip it with a core image filter (scaling by -1.0 around y axis should do the trick) and drawing it in fullscreen onto the secondary screen. Is it possible to do this, and if yes, what would be the necessary APIs to use in order to get the best performance? This only needs to work on Leopard, by the way, no backwards compatibility with Tiger necessary. Many thanks in advance, Georg</body>
  </mail>
  <mail>
    <header>Re: CGImageDestination</header>
    <body>I don't know why your metadata isn't being written back to the file, but you have several memory management bugs. You autorelease the instace that you got back from +[NSData dataWithContentsOfFile:], but you are not responsible for releasing it. You *are* responsible for releasing instances returned from the CG &amp;quot;create&amp;quot; and &amp;quot;copy&amp;quot; APIs, but you fail to do so. (I suppose it is possible that you are building a GC application, in which case the missing releases aren't a problem, but the extra - autorelease would then be both misleading and unnecessary.) Jim</body>
  </mail>
  <mail>
    <header>Re: CGImageDestination</header>
    <body>On 04.12.2007, at 05:00, Parrish Myers wrote: The supplied frameworks (QuickTime, ImageIO) are chronically omitting some metadata at export - so chances are you're doing every thing right. File bugreports...</body>
  </mail>
  <mail>
    <header>CGImageDestination</header>
    <body>I have been fighting with CoreGraphics for a bit. &amp;nbsp;I was hoping some one would help... I am attempting to write an application that resizes images from my digital camera (to aid in my picture gallery web page). &amp;nbsp;I typically used the Apple Script Image Events to do the job. &amp;nbsp;I wanted to rewrite the application using objective-c. &amp;nbsp;From what I have read, I need to use the ImageIO framework to preserve the image property data stored by my camera. &amp;nbsp;So far what I have been able to do use read the image using CGImageSource and read all of its properties using CGImageSourceCopyPropertiesAtIndex(...). &amp;nbsp;That all seems to work properly. &amp;nbsp;I even get the kCGImagePropertyMakerCanonDictionary stuff. &amp;nbsp;The problem comes when I try to write the file back to disk (I haven&amp;#39;t gotten to the resize step yet). &amp;nbsp;I tried to use CGImageDestination to write the file. &amp;nbsp;It worked, but not all of the properties got copied back into the image... most notably the kCGImagePropertyMakerCanonDictionary didn&amp;#39;t get copied into the new image. &amp;nbsp;What am I doing wrong? &amp;nbsp;Here is the code I used: - ( CGImageSourceCreateWithData inImageRef = NULL 0 CGImageDestinationCreateWithURL ,</body>
  </mail>
  <mail>
    <header>Re: CGContextSetRenderingIntent() not working #2</header>
    <body>As far as I am aware, yes you are using it correctly, and no there is no way to have an image drawn with a chosen RI (other than the one it is specified with). Nothing that I am aware of. You can always create another CGImageRef using the same parameters as the original but with a different RI. This should be a light-weight operation. -- David Duncan Apple DTS Animation and Printing email@hidden</body>
  </mail>
  <mail>
    <header>CGContextSetRenderingIntent() not working #2</header>
    <body>I am trying to use the call to CGContextSetRenderingIntent() to set the rendering intent (RI) within a particular graphics context. No matter which RI is chosen the results are the same - as if the RI is ignored. I store the graphics context then call CGContextSetRenderingIntent () then restore the graphics context. Am I not using this call properly? Is there another way to have an image drawn with a chosen (not image-specified) RI? On the Quartz mailing list someone from Adobe was having the same problem. He never got an answer to his question but posted this response to his own question: Is he correct? Does this mean there is NO way to specify an image conversion with its RI? If he is correct, is there a call to change the specified RI of an image?</body>
  </mail>
  <mail>
    <header>[CIImage emptyImage]: why?</header>
    <body>This is probably a really silly question, but I read about a new CIImge method &amp;quot;emptyImage&amp;quot; in the class description that was added in 10.5. There doesn't seem to be anything you can do with an emptyImage (like add something to it!) - so, what purpose does it serve? David</body>
  </mail>
  <mail>
    <header>CIImage: how to retain memory control</header>
    <body>My app needs to keep a number of very large bit mapped images open at the same time - not only for display purposes but for real time analysis. I use NSBitmapImages now, but want to transition to CIImages (so I can use filters). In my case, I read in an image off disk in a structured format, as tiles, since various images may be created from this same data set with the tiles rearranged  (and also excluded or included based on various criteria). Thus, I'd like to do maintain memory ownership, and supply image data on an as needed basis without the system re-caching the huge images. As Frank so graciously pointed out, CIImage will cache tiled images (and thus in my case there would be two image footprints in memory). I cannot use CIImage's imageWithBitmapData as the memory is not laid out row by row. I also tried to first create a CGImageRef using my own CGDataProvider, but CIImage just reads in the whole image (and apparently caches it) [I used the MassiveImage example code, which would be great if I was just using CGImages...]. My next thought is to create a custom filter that instead of an inputImage takes an array of tiles, and then outputs the pixel values using some internal conversion strategy. At first blush, this should accomplish what I want to do. [The only lingering questions is, will the system cache the outputImage?] Does this seem like a reasonable strategy to pursue? If so, I would probably first do the filter as an executable filter to just get it working, then later convert it to a non-executable filter. PS: after watching the WWDC grpahics team put a team member in front of a video camera, had him move his hand around with a ball in his palm, and have filters replace the ball with an image of a duck (!!!), I'm pretty well convinced you can do anything with Core Image!</body>
  </mail>
  <mail>
    <header>Re: Duplicated CGEvents from event tap</header>
    <body>Why don't you try working with the dock using my Event Taps Testbench application, and see if you get the same behavior? Testbench contains an option to block all events on an application, which you can invoke by checking the appropriate checkboxes to set it up. It also has options to receive reports of what's going on. (Beware that Testbench appears to have some issues with Leopard. I expect to clear them up this weekend.) -- Bill Cheeseman - email@hidden Quechee Software, Quechee, Vermont, USA www.quecheesoftware.com PreFab Software - www.prefabsoftware.com</body>
  </mail>
  <mail>
    <header>Duplicated CGEvents from event tap</header>
    <body>Hi folks, So I've been playing around with event taps, trying to intercept events for the Dock. I call CGEventTapCreateForPSN passing (for exploration) kCGEventMaskForAllEvents. In my event tap callback I just list all details for that event and return NULL, so from what I understand this the Dock should now ignore any mouse input. Several puzzling things happen: 1) All mouse button ups and downs (both left and right) arrive in duplicate at my callback: the events are identical, but I get two different values for the CGEventTapProxy that's passed in. 2) On some runs, I get &amp;quot;mouse moved&amp;quot; events for the entire screen the Dock is on; after I kill/restart the Dock, I get them just for the actual Dock area. Those events aren't duplicated. 3) Despite my returning NULL from the callback, the Dock still does all its normal stuff when clicking/holding/right-clicking on a Dock icon; I suppose that means the Dock doesn't use events at all, but polls the mouse buttons? Ideally I'd like to suppress the Dock noticing clicks on certain icons completely. 4) When the Dock's contextual menu pops up, I get a type == 21 event; a type which isn't in the headers. Any suggestions welcome! Thanks, -- Belo Horizonte, Brazil &amp;quot;In the affairs of others even fools are wise Weblog:</body>
  </mail>
  <mail>
    <header>Leopard and CIImage/CIFilter</header>
    <body>I recently created a test project to play with Core Image. I created a new Objective C project in Xcode 3, put a scrollview with a custom view inside the default window, and added a custom NSView class to the project. This view creates and displays a CIImage using a CIImage and a CIFilter. I added the QuartzCore framework (and deleted the CoreData framework):  so in addition to QuartzCore the project includes Cocoa, AppKit, and Foundation. When I went to build the project (all defaults), it fails with this linker message: ld: file not found: /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/AE.framework/Versions/A/AE So, I switched the build setting so my project was 10.4 based. It compiles just fine. Switch it back to 10.5, and it fails. I tried adding the ApplicationService framework but then I get another missing file message, this on WebServices (something like that). David</body>
  </mail>
  <mail>
    <header>Re: External display reported online and active when the display	isoff</header>
    <body>I have to correct myself. If I disconnect DVI/HDMI cable at either end (iMac or Sony) then OS X correctly reports that the display is not there. Also, Sony KDL2000W46 LCD communicates correct EDID even when its power line is disconnected! I don't know what the standards say about this, but obviously another method of assessing whether external display is on or off would be required. In many environments the external display is used only from time to time, therefore you need a way to tell whether it is on or off without requiring the user to constantly connect and disconnect the cables - it's inconvenient, not user friendly, and the connectors aren't designed for that. ----- Original Message ----- Sent: Monday, December 03, 2007 4:39 AM Subject: Re: External display reported online and active when the display isoff</body>
  </mail>
  <mail>
    <header>Re: External display reported online and active when the display is	off</header>
    <body>Many thanks for your reply. The external monitor is off because its power line cable isn't there when I test. It is reported as online and active even after reboot. Quartz reports also a number of valid modes for it. All this without power line connected to it. It even doesn't help when I disconnect at the DVI/HDMI junction - leaving only the small mini-DVI/DVI adapter (by Apple). It disappears when I physically remove the adapter jack from connector in the iMac. Also, when I turn the power off in the external display, whether by removing power line, switching it off or to standby, or even when selecting another input than the one where iMac is, OS X clears the internal screen and redisplays the desktop but it thinks the external is still there.</body>
  </mail>
  <mail>
    <header>Re: External display reported online and active when the display is	off</header>
    <body>The cables and hardware are probably completing the signal path that indicates a display is present even without the monitor being turned on. If the reported vendor ID and model are correct, then the monitor isn't really off, and is responding to EDID data queries. Sent from my iPhone</body>
  </mail>
  <mail>
    <header>CIImageProvider and tiled image questions</header>
    <body>Is there any example code using the kCIImageProviderTileSize feature of CIImage / CIImageProvider anywhere? 1) In particular, does the provider get asked for a tile at a time? 2) Are tiles cached by the CIImage?? I'd like to use the tiles to provide chunks of my &amp;quot;massive&amp;quot; image - on demand - and keep the data in a memory mapped file. David</body>
  </mail>
  <mail>
    <header>Re: Display Reconfiguration Callback firing too late</header>
    <body>On Nov 30, 2007, at 5:22 PM, Nathaniel Gray wrote: Please file a bug about this, if you have't already; it doesn't sound like the right behavior to me. Mike is the expert here, and would know for sure, but he probably won't have time to look at your problem unless there's a Radar number assigned for it.</body>
  </mail>
  <mail>
    <header>External display reported online and active when the display is off</header>
    <body>Quartz reports (CGDisplayIsActive, CGDisplayIsOnline) an external display as online and active even if the display is off but physically connected. This is on iMac (20'' alu), Leopard 10.5.1 using Sony KDL2000W46 LCD connected via mini-DVI/DVI/HDMI cable. Is this by design or is it a bug?</body>
  </mail>
  <mail>
    <header>Re: draw multicolor line with cgpath</header>
    <body>A path has no color information of any kind embedded in it.  It does not behave like OpenGL where each vertex also has an associated color.  A path in Quartz 2D is strictly a geometric construct.  If you want to draw different lines in different colors, you will have to use a separate path for each color. Scott</body>
  </mail>
  <mail>
    <header>draw multicolor line with cgpath</header>
    <body>hi list, below you see a code that draws a multicolor line each time drawMulticolorLine is called. void drawMulticolorLine for( int i = 1; i &amp;lt; lineWidth; ++ ) //a function that sets a different color for each i void SetStrokeColor(int i) CGContextSetRGBStrokeColor(secondaryContext,‚Äö√Ñ¬∂ the code above works but if i use it in real live, it is a perfomance killer. so i try to improve the drawing performance with the code below. void drawMulticolorLine for( int i = 1; i &amp;lt; lineWidth; ++ ) //a function that sets a different color for each i void SetStrokeColor(int i) CGContextSetRGBStrokeColor(secondaryContext,‚Äö√Ñ¬∂ this code works with a good performance if the line color is always the same. my issue now is that once CGContextDrawPath is called the whole line is drawn in the color of the last CGContextSetRGBStrokeColor. is there a way that the path keeps the colors of every line segment that was added ? any help is greatly appreciated. cheers, bastian</body>
  </mail>
  <mail>
    <header>Backgrounds filters having effect only on immediate parent?</header>
    <body>In my understanding, if I apply, say, a blur filter to layer's background (CALayer-&amp;gt;backgroundFilters), all layers that are behind that layer - that is, immediate parent, grand parent, etc. and all children and siblings of those parent and grandparents that are lower in the tree of layers - should appear blurred. An experiment however shows that it's not the case. The background blur filter only blurs the direct parent of the target layer, but not its grandparent and other layers. The same applies for compositingFilter. Am I doing something wrong? If this is the way it should work as documented, how do achieve the effect I want? Thanks.</body>
  </mail>
  <mail>
    <header>Re: KVO-related EXC_BAD_ACCESS exception on position property of	CALayer</header>
    <body>Thanks for the tip. Yes, the Allocations instrument shows what is expected as far as the number of live content layers. What's interesting is what it doesn't show. There are no deallocations for the content layer, even though -dealloc for that content layer is getting called (printed to the console). It looks like the program is therefore crashing before or shortly after the content layer is finished being deallocated, and before it's being registered with the Allocation instrument. [...] Discovery! Setting a breakpoint in the -dealloc method and stepping through, I see that #3	0x7fff810bf352 in CA::Transaction::commit is getting messaged just before the crash. This makes sense, since I'm disabling and re-enabling CA animation in the scroller update code using the pattern: // Disable animation temporarily. // Re-enable animation. If I remove this CATransaction animation disabling logic, everything seems to work. It would appear that explicit transactions (such as the wrapper above) are not supported within the context of implicit CALayer transactions tied to KVO, even through the docs say that nested transactions are supported. Perhaps the added element of KVO here is what causes the inability to nest an explicit transaction in an implicit one? From the CATransaction documentation: &amp;quot;CATransaction is the Core Animation mechanism for batching multiple layer-tree operations into atomic updates to the render tree. Every modification to a layer tree must be part of a transaction. Nested transactions are supported. The content layer is still being deallocated whenever the KVO snap-back occurs, but it's not causing any problems. In fact the Allocation instrument does show that many deallocations are occurring, corresponding to the number of snap-back animations of the content layer. It looks like CA is making a local copy of the layer, working with it, and then deallocating. That would seem to make more sense. Thanks to both Hamish and Kyle for the clarifying questions, tips, and suggestions. Best, Dalmazio</body>
  </mail>
  <mail>
    <header>Re: KVO-related EXC_BAD_ACCESS exception on position property of	CALayer</header>
    <body>Run with the Allocations instrument and make sure you really are properly retaining your layer. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Debugging layer trouble</header>
    <body>Hello, I have a custom NSView which is layer hosting and contains a multi-level layer hierarchy. My problem is this: Seemingly randomly and not very often, when a layer in my hierarchy gets updated (painting a selection on top of another layer) as a result of a mouse interaction (a selection that gets dragged), part of my view goes fully black for a extremely short period of time. It's a flicker-like effect, but it's definitely confined to my view or a sub-layer in the hierarchy (depending on the current layout). All layers have their content animation turned off (by using the delegate method -actionForLayer:forKey: returning [NSNull null] when the event is @&amp;quot;contents&amp;quot;). Not sure that is relevant. I've seen this a number of times now so I'm pretty sure it is me doing something wrong, but since I don't know how recreate (and it sometimes doesn't happen for days), it I don't know how to debug it. Also, the layer hierarchy is not simple, I have layers with sublayers with sublayers with lots of layers z-ordered in a certain way (which change - frame, z-ordering - during user-interaction), which makes isolating a simplified test-case in a separate project extremely difficult. Even more so as I have no idea what to look for. Is there something that can help with this kind of stuff? I thought that something like Quartz debug that lets me look into the compositing process by &amp;quot;playing&amp;quot; it in slow motion might help. I'm using Xcode 3.2.5 on an Spring'10 iMac with 8GB of ram. The code is 32 bit right now. I realize this is all very vague, I'd be more than happy to provide more information, I just don't know what is relevant and what isn't. Does Anyone have a clue on what the cause of this phantom flicker might be? Thanks a lot! Regards Markus -- __________________________________________ Markus Spoettl</body>
  </mail>
  <mail>
    <header>Re: KVO-related EXC_BAD_ACCESS exception on position property of	CALayer</header>
    <body>In this case no crash occurs. The code that updates the scrollers does access the content layer position property to determine how the scrollers should be re-positioned, but it makes no changes to this or any other property. No, we're not using GC at present. But turning GC on has the same effect. You're right though. I've added an NSLog in the -dealloc method of the content layer class whose position property is being observed and animated, and it is getting deallocated during animation. Do you know if this is normal and/or expected CA behaviour? Since I maintain this content layer as an instance variable of the scroll layer, when I hit the KVO handling code for the position property, it would seem that I am now referring to the original content layer which has been deallocated by Core Animation for the position property animation, which appears to be causing the crash. But I retained this content layer (scroll layer maintains ownership) so it shouldn't be getting deallocated should it? Is there a recommended way to get around this? Interestingly, if I add an NSTimer to update the scrollers in the KVO handling code for the position property, and schedule it to run ASAP (within 100ms), I get the scroller updates and there is no crash. But I'm concerned that since the content layer has been deallocated in the animation this is not a clean solution, and may cause problems later. This behaviour of deallocating a CALayer during animation for which there is an external (retained) reference is a bit odd though. Am I missing some basic understanding of how Core Animation works in its capacity of animating properties for a layer? It almost appears as if Core Animation is deallocating the layer (ignoring retain counts) and then reallocating to the same state as previously (?) with the exact same memory location so the original reference is no longer stale after all animations are complete. If this is true, this implies that the layer that is being animated should not be accessed during animation, in a KVO handler or otherwise. Is this even remotely possible? This might also explain why the NSTimer approach seems to work (animation is finished by the time it runs, and the temporarily stale content layer reference is no longer stale). Best, Dalmazio</body>
  </mail>
  <mail>
    <header>Re: How best to force PNG UIImages to uncompress</header>
    <body>How large are your images? The simplest way to ensure that you are working with a decompressed image (on iOS 4.0+) is to do the following (type in Mail): The 'decompressed' UIImage will have a fully decompressed version of 'image'. Create image in a way that is appropriate to your application. You should be able to scan back and forth through the timeline for VM Tracker as well. It may help to turn on automatic snapshots for this. Out of curiosity is the image here coming from a file? If it is it would make more sense to use -imageWithContentsOfFile: rather than creating an NSData to wrap the file's contents. -- David Duncan</body>
  </mail>
  <mail>
    <header>How best to force PNG UIImages to uncompress</header>
    <body>I'm a CoreGraphics newbie.  I notice that decompressing large PNG files on the main thread in iOS can make animations jerky, so I've moved that operation onto a background thread. 1.  What's the best way to force a UIImage to use a decompressed backing store with the minimum amount of transient memory allocation? (Right now I am briefly allocating 2 times the required backing store.) 2. Is there a way of monitoring CG allocations dynamically using Instruments or other means?  Instruments doesn't show the CG allocations in the Allocations track -- just the VM Tracker track, which is limited to a single snapshot. Re: 1, I think I'm using more memory than I need to.  Here's what I'm currently doing: CGContextRef newContext = [ImageEditor contextForImage:originalImageContainer backing: &amp;amp;newContextBuffer];   // ALLOCATION 1 CGContextDrawImage(newContext, CGRectMake(0, 0, CGImageGetWidth(originalImageContainer), CGImageGetHeight(originalImageContainer)), originalImageContainer); // ALLOCATION 2 I hope that it shouldn't be necessary to allocate the same large amount of memory for both operations marked ALLOCATION above.  I realize that these allocations are freed very shortly, but I'd rather make just one of them. Thanks, Kevin</body>
  </mail>
  <mail>
    <header>Re: KVO-related EXC_BAD_ACCESS exception on position property of	CALayer</header>
    <body>And what happens if you (having reinstated the animation) remove the code that updates the positions of the scrollers? In my experience, if something works without animation but crashes with, it's usually because one of the objects involved is reallocated during the animation. Are you using GC? H</body>
  </mail>
  <mail>
    <header>Re: KVO-related EXC_BAD_ACCESS exception on position property of	CALayer</header>
    <body>That's mostly correct. Removing all observers from the layer's position property causes the same code path to be followed to the end of the scrollDidEnd, which is the method that handles rubber-banding (mouse up). The only difference without observers registered is that whenever the position property of the content layer is set, no KVO notification occurs, so the positions of the associated scrollers are not updated and there is no crash. (Note, this special scroll layer is scrolled by the user dragging directly on the layer, and the scrollers are informational only -- very similar to iOS scroll views). Actually, I'm using an implicit CALayer animation by simply setting the position property of the layer. Core Animation then animates the layer from it's current position to the new position to create the &amp;quot;rubber-band&amp;quot; effect automatically. But your suggestion raises an interesting point. Having both 1) the position property observer registered for the content layer, and 2) implicit animations turned off by wrapping the original line of code that sets the position property in the scrollDidEnd method with the CATransaction constructs below, results in no crash AND the scroller positions updating correctly -- only there is no longer any animation of the rubber-band effect -- it just instantaneously repositions the content layer. // Disable animation temporarily. _contentLayer.position = CGPointMake(_contentLayer.position.x + snapBackOffset.x, // Re-enable animation. So it appears the implicit CA animation of the layer position property is interfering with KVO on the same position property. Is this type of operation intentionally not supported, or is this perhaps a bug? Best, Dalmazio</body>
  </mail>
  <mail>
    <header>Re: KVO-related EXC_BAD_ACCESS exception on position property of	CALayer</header>
    <body>To clarify: if you remove all observers from the layer's position property, there is no crash? (I understand that you require the observer to implement the &amp;quot;resistance&amp;quot; for the rubber-banding, but you'll still get the scrollDidEnd, right?) Also, assuming you're using an explicit animation, what happens if you remove that animation? Best wishes, H</body>
  </mail>
  <mail>
    <header>Re: KVO-related EXC_BAD_ACCESS exception on position property of	CALayer</header>
    <body>Here is the relevant code: // Scroll layer KVO observation context. [...] // Scroll layer KVO key paths. [...] // Add content layer position property observer (for scroller position updates). [self addObserver:self forKeyPath:kContentLayerPositionKeyPath options:NSKeyValueObservingOptionNew [...] - (void) observeValueForKeyPath:(NSString *)keyPath ofObject:(id)object change:(NSDictionary *)change context:(void *)context if (context == kScrollLayerObservationContext) if ([keyPath isEqualTo:kContentLayerContentBoundsKeyPath]) else if ([keyPath isEqualTo:kContentLayerPositionKeyPath]) else // Be sure to call super. Best, Dalmazio</body>
  </mail>
  <mail>
    <header>Re: KVO-related EXC_BAD_ACCESS exception on position property of	CALayer</header>
    <body>Possible red herring: what are you using as the &amp;quot;context&amp;quot; argument when you register your observers? --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: KVO-related EXC_BAD_ACCESS exception on position property of	CALayer</header>
    <body>Yes, sorry, should be NSZombieEnabled -- which is what I used in Xcode. Best, Dalmazio</body>
  </mail>
  <mail>
    <header>Re: KVO-related EXC_BAD_ACCESS exception on position property of	CALayer</header>
    <body>Hi Hamish, Yes, zombie detection is active as it prints out the expected over-release details (never mind the fact that I mistyped NSEnableZombies instead of NSZombieEnabled above). Well, the endChange and then propertyDidChange methods shown in the stack trace are called after the &amp;quot;position&amp;quot; property is set for my CALayer (which results in the KVO notification -- I've registered observers previously which I haven't shown). I can confirm this, as the observeValueForKeyPath:ofObject:change:context method is called and the required code executes. Only when program flow unwinds out of the observeValueForKeyPath:... method into the methods/functions shown in the stack trace does the crash occur. Here's the stack trace after setting a breakpoint where I actually handle the KVO notification instead of letting it run to completion and unwind the stack: #0	0x1000488dc in -[MRScrollLayer handleContentLayerPositionChange:ofObject:change:] at MRScrollLayer.m:453 #1	0x100048a60 in -[MRScrollLayer observeValueForKeyPath:ofObject:change:context:] at MRScrollLayer.m:486 #2	0x7fff83f2990c in -[NSKeyValueObservance observeValueForKeyPath:ofObject:change:context:] #3	0x7fff83f296ba in NSKeyValueNotifyObserver #4	0x7fff83f291ae in NSKeyValueDidChange #5	0x7fff83f0c945 in -[NSObject(NSKeyValueObserverNotification) didChangeValueForKey:] #6	0x7fff810bcbee in endChange #7	0x7fff810bc9f3 in -[CALayer setPosition:] #8	0x100038678 in -[MRContextualScrollLayer scrollDidEnd] at MRContextualScrollLayer.m:280 Letting the program unwind the stack replaces frames #0-#5 with the topmost frames in my original post where the EXC_BAD_ACCESS occurs. Yes, on the main thread. Best, Dalmazio</body>
  </mail>
  <mail>
    <header>Re: KVO-related EXC_BAD_ACCESS exception on position property of	CALayer</header>
    <body>Is this a typo? Don't you mean NSZombieEnabled? ‚Äî F</body>
  </mail>
  <mail>
    <header>Re: KVO-related EXC_BAD_ACCESS exception on position property of	CALayer</header>
    <body>Hi Dalamzio, Have you tried deliberately over-releasing something in your code, to ensure the zombie detection is indeed active? I'm not clear on where KVO comes into this -- I don't see any KVO-specific code in the stack trace. Is all this happening on the main thread? H</body>
  </mail>
  <mail>
    <header>KVO-related EXC_BAD_ACCESS exception on position property of CALayer</header>
    <body>I'm having some problems debugging what appears to be a KVO-related EXC_BAD_ACCESS exception on 10.6.6. I've set up an observer on the position property of a CALayer subclass. This particular layer is the content layer for a custom CALayer scroll layer. Basically, when the user drags the content layer around, I would like some KVO notifications to occur on the position changes for the layer so that I can update some custom CALayer scrollers. The above all works fine. Now, I've added a rubber-band effect √† la iOS, so that when the user scrolls passed the limits of the content area, some resistance is encountered, and when the user lets go of the mouse, the content layer snaps back to the limit position in both x and y dimensions. Vertical snap-back works like a charm. However, for some bizarre reason, any amount of horizontal snap-back cause the above exception to occur. What I don't understand is why precisely the same code path (vertical snap-back vs. horizontal-snap back) would cause these two situations to behave differently? Here is the stack trace: #0	0x7fff810bbc2b in CALayerTransactionFlagsLocation_ #1	0x7fff810bbe00 in CALayerMark #2	0x7fff810bd394 in propertyDidChange #3	0x7fff810bcbff in endChange #4	0x7fff810bc9f3 in -[CALayer setPosition:] #5	0x100038578 in -[MRContextualScrollLayer scrollDidEnd] at MRContextualScrollLayer.m:280 ... scrollDidEnd is the method that is called when the user let's go of the mouse, and a snap-back calculation is performed to determine how to reposition the content layer for the snap-back effect. I've tried turning on NSEnableZombies by setting the environment variable in the arguments of the executable in Xcode, but it has no effect, i.e., no additional information is logged. This leads me to believe that the problem is not a memory over-release issue, where an object is being accessed that shouldn't be. The final line in the scrollDidEnd method that results in the KVO notification getting posted (which then results in the crash) is simply: _contentLayer.position = CGPointMake(_contentLayer.position.x + snapBackOffset.x, Does anyone have any ideas or helpful hints? Thanks, Dalmazio P.S. I wasn't sure whether to post this to the general cocoa-dev group (KVO issue) or the quartz-dev group (CALayer + KVO issue). I've posted only to the quartz-dev group for now, and if others feel this question deserves wider readership, I can repost to the general group later. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>RE: CGImage supported formats</header>
    <body>&amp;gt; Apparently you have image data that you want to draw. For the in- Yes, I have in-memory image data I would like to draw, and am currently using CGImageCreate with CGDataProviderDirectAccessCallbacks (I'm using GetBytesAtOffset and ReleaseInfo). Thank you. This was not at all clear (to me) from the CGImage docs on the developer website -- the NSBitmapImageRep::initWithBitmapDataPlanes documentation seems clearer, but only lists 1,2,4,8,12 and 16 as supported bit depths, not mentioning 32 bit floating point data. Got it. Thanks. It may be that my google-fu is failing this week, but I was unable to find this information on the developer website. In particular, the CGImageCreate documentation at does not give a range of valid values for any of the size_t parameters. It just fails and returns nil if you provide numbers it doesn't like, or which don't match one of the other parameters. Thanks again for the clarification, -DaveP</body>
  </mail>
  <mail>
    <header>Re: CGImage supported formats</header>
    <body>Re: On Jan 30, 2008, at 10:15 AM, Dave Polaschek wrote: What you are looking at in Table 12.1 is about the formats Quartz and includes creating a bitmap context and drawing to that context to create a bitmap representation of your drawing.. The Quartz book that Bunny and I wrote was written just as 10.4 was complete and of course does not contain any Leopard information. One thing that is new in Leopard is the ability to create a bitmap context that is 16 bits per component. However I don't think this is relevant to your question. Apparently you have image data that you want to draw. For the in- memory case for images with color you probably want to use CGImageCreate and in the book the discussion on that begins on page 211 in the chapter &amp;quot;Creating CGImage Objects&amp;quot;. When you create a CGImage object using CGImageCreate you can specify bitsPerComponent of 1-32 so of course 16 bits per component is supported. Note that in 10.1 and earlier the available formats supported are more restricted but I suspect that isn't a major concern ;-) Re: By &amp;quot;regular image use&amp;quot; I assume you mean images with color, not image masks. As I wrote above, 16 bits per component is supported in 10.2 and later, Regarding: Masks are discussed in Chapter 10 &amp;quot;Image Masking&amp;quot; and the function you probably want to use is CGImageMaskCreate. In the Quartz book, that is discussed starting on p264 in the section &amp;quot;Creating an Image Mask&amp;quot;. If you want to draw with a mask you must use CGImageMaskCreate to create a CGImage object that represents a mask. On p266 there's a section &amp;quot;Drawing and Inverting a 1-Bit Image Mask&amp;quot; with sample code, etc. CGImageMaskCreate (as of 10.4) only supports 1, 2, 4, and 8 bit masks. I don't know if that has been expanded in Leopard. For using CGContextClipToMask, see the section on p282 &amp;quot;Clipping to a Mask (Tiger)&amp;quot;. That function is only available in 10.4 and later. As discussed there, the mask parameter passed to CGContextClipToMask can be either an Image Mask created with CGImageMaskCreate or a 1- component image with no alpha created with the DeviceGray color space. Of course the sense of masking between the two is inverted and that is discussed in various places in the Chapter 10. However you can use a decode array with either to invert the sense of the mask so that's no big deal, use which is easiest. I'm sure that the docs on the developer website contain most if not all of this information, but I don't have pointers to it so perhaps others will chime in about it. However since you said you have the book, I thought providing you with pointers to the relevant text would be potentially useful.</body>
  </mail>
  <mail>
    <header>Re: CGImage supported formats</header>
    <body>On Jan 30, 2008, at 12:15 PM, Dave Polaschek wrote: I don't remember ever seeing documentation of that kind :-(  There's a technote about the support set of bitmap contexts, but I've not seen one about CGImages.</body>
  </mail>
  <mail>
    <header>RE: CGImage supported formats</header>
    <body>&amp;gt; &amp;gt; Is there a list of all the formats, bit-depths, number of components, Umm, not exactly. I've got pixels already in memory, not a doc on a HD. I've already written a data provider (which seems to be both deprecated and the only way to do the job I need to do), and am just trying to puzzle out which (in-memory) pixel formats I can expect CGImageCreate to support without resorting to trial and error. -DaveP Dave Polaschek email@hidden Sr. Computer Scientist 651-766-4705 Adobe Systems Incorporated 3900 Northwoods Dr. Suite 300, Arden Hills, MN 55112</body>
  </mail>
  <mail>
    <header>Re: CGImage supported formats</header>
    <body>On 31/01/2008, at 7:15 AM, Dave Polaschek wrote: Check out the ImageIO docs.</body>
  </mail>
  <mail>
    <header>CGImage supported formats</header>
    <body>Is there a list of all the formats, bit-depths, number of components, what OS began support for the format, etc., supported by CGImageCreate? I find Table 12.1 from the Quartz book (Gelphman &amp;amp; Laden) useful, but I have a suspicion that it does not reflect the current reality (for example, 16 bits per component seems to be supported, but is not listed in the table), and I can't find a more current list of supported formats in the CGImage documentation online. More specifically, I'm looking for info on both 16-bits per component for regular image use, and 1 bit per component for creating CGImages that I'd like to use with CGContextClipToMask. It seems a waste to use an 8-bit grayscale mask when I've already got a bitmapped mask in hand. Thanks, -DaveP Dave Polaschek email@hidden Sr. Computer Scientist 651-766-4705 Adobe Systems Incorporated 3900 Northwoods Dr. Suite 300, Arden Hills, MN 55112</body>
  </mail>
  <mail>
    <header>Re: Distinguish from CATransition and CoreImage transition?</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: What function does Quartz use to replace Quickdraw	GetMainDevice</header>
    <body>CGMainDisplayID() in Quartz Display Services Reference at</body>
  </mail>
  <mail>
    <header>Re: What function does Quartz use to replace Quickdraw GetMainDevice</header>
    <body>On Jan 29, 2008, at 9:25 PM, Fosse wrote: GetThemeWindowRegion  -&amp;gt; HIThemeGetWindowShape.</body>
  </mail>
  <mail>
    <header>Re: What function does Quartz use to replace Quickdraw GetMainDevice</header>
    <body>The general answer would be to check out CGDirectDisplay.h. The specific answer would depend on &amp;quot;what functionality are you trying to -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>What function does Quartz use to replace Quickdraw GetMainDevice</header>
    <body>From the reference doc , function GetMainDevice is deprecated and we should use Quartz function. But I don't find doc describing the new function. The similar thing happens on some Appearance Manager APIs too, for example GetThemeWindowRegion. Reference doc says we need to use HIThemexxx() but I can't find any documents about HITheme functions. What I can do is find the head files in /System/Library/Framework/ ...</body>
  </mail>
  <mail>
    <header>Re: NSBitmapImageRep inconsistency</header>
    <body>So i'll just file a bug, then.</body>
  </mail>
  <mail>
    <header>Re: NSBitmapImageRep inconsistency</header>
    <body>I don't know why you are seeing the initial bitmap rep telling you its RGB, but your getting an inconsistency because your asking for a new bitmap that is RGB (and thus at least 3 component) but asking for only 1 component (which at best would be for a paletted image, which isn't supported in this context). Thats why I said to set the values that don't matter directly and only copy over those values that you need to reflect from the original. -- David Duncan Apple DTS Animation and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Re: NSBitmapImageRep inconsistency</header>
    <body>I was hoping I can just completely avoid having to detect the format entirely, as my stuff works regardless of the image's actual format. Yes, I thought that's what I was trying. I was just trying to create an image with the same parameters as the source image, except for the changed bitmap data. I still don't understand why an RGB color format is returned for a grayscale image by NSBitmapImageRep, or, if that is valid, why I can't create a NSBitmapImageRep with that parameter set. Or am I wrong in seeing this as a bug? Cheers :)</body>
  </mail>
  <mail>
    <header>Re: Current mouse position</header>
    <body>On Jan 29, 2008, at 11:39 AM, David Catmull wrote: On 10.5, there's HIGetMousePosition in CarbonEventsCore.h, which returns an HIPoint that is congruent with a CGPoint.</body>
  </mail>
  <mail>
    <header>Current mouse position</header>
    <body>What I'm trying to do is find the display where the mouse cursor is, so I'm looking for the easiest way to get a CGPoint to feed to CGGetDisplaysWithPoint.</body>
  </mail>
  <mail>
    <header>Re: NSBitmapImageRep inconsistency</header>
    <body>color space at all? You could just always specify RGB/8bps/3spp and never see the inconsistency issue. Again, it comes down to my original question of what attributes of the original bitmap do you actually care about replicating :). Of course, your alternative if your not using Quartz at all to manipulate the image is to simply get the image data and only create an object to draw it when you need to redraw. For work like this where your displayed image is a result of some algorithm this is typically the method used. -- David Duncan Apple DTS Animation and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Re: NSBitmapImageRep inconsistency</header>
    <body>Indeed, I am aware of the non-pre-multiplied alpha issue, and this is something entirely different. As mentioned, its for image analysis / image recognition, so I definitely can't get around manipulating the actual bitmap data. What is so frustrating is that there's no reliable and consistent way to get at it. I fail to find anything sinister, of course except if things fail due to the color space not being NS*WhiteColorSpace, which is a pretty damn stupid reason, and an even more stupid error message for not just saying WHAT is acually inconsistent: 2008-01-29 19:33:00.190 HaarLab[38324:10b] width:7000 height:2748 bitsPerSample:8 samplePerPixel:1 hasAlpha:0 isPlanar:0 colorSpaceName:NSCalibratedRGBColorSpace bitmapFormat:0 bytesPerRow:7000 bitsPerPixel:8 2008-01-29 19:33:00.191 HaarLab[38324:10b] Inconsistent set of values to create NSBitmapImageRep See, see? I am not crazy ;) Why NSCalibratedRGBColorSpace is returned by the given image rep in the first place for a grayscale image is anyone's guess. That would be the obvious step, and of course the first thing I tried. Too bad it keeps the CGImageRef's bitmap data from which the first instance was created, leaving me with the immutability problem. No -mutableCopy, eh :/</body>
  </mail>
  <mail>
    <header>Re: NSBitmapImageRep inconsistency</header>
    <body>Not really, the issue at hand is one of pre-multiplied vs non-pre- multiplied alpha (and I know where Alex was coming from here, as this has been a question raised on the OpenGL list often). Since Quartz doesn't let you draw to any context that isn't set for pre-multiplied alpha, if your image has alpha, then drawing it will pre-multiply that alpha into the RGB channels. In your original message, you mentioned you were working with JPEG. If this is an exclusive format, then you can ignore the alpha issue entirely. Likely it is not, in which case you will have to do the leg work to get the image data into a format that does not use pre- multiplied alpha. But this is really only the case if you must draw or manipulate the changed pixels yourself. If you just want to doodle over a picture for example, then you don't really need to care about the actual image format, as Quartz (and the libraries it sits on top of) provide the tools for drawing. If you really do need to do pixel twiddling, then yes Quartz isn't the best at it - its not a pixel oriented library. However, people have found various tricks over the years that let them do the old pixel-oriented techniques that they used to do. Well, thats why I recommended you print out what you were seeing from the source bitmap. I suspect that you weren't getting what you thought you were getting from the format, and that you might see the inconsistency yourself. I still think its a worthy thing to check out, if only so you can check it against the documentation. Although looking at the docs, I see now that NSBitmapImageRep conforms to NSCopying, so you can probably do what you want with a simple [bitmap copy] :) -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>probleme printing coreimage &amp;quot;result&amp;quot;</header>
    <body>Hi, I am trying to print a NSView page where there is some coreimage object ... If I use traditional imageio all works. If I replace the code with coreimage code i got a SEGV like this : #0      0x90a594c0 in objc_msgSend #1      0x0e06e090 in ?? #2      0x934e54ac in -[NSConcretePrintOperation _continueModalOperationPastPrintPanel] #3      0x934e4f4a in -[NSPrintPanel _dismissSheet:] #4      0x0de1e811 in StopQueueEventHandlerProc #5      0x0de1c037 in PrintingDialogEventHandler #6      0x0de1bb04 in PrintingDialogEventHandler #7      0x92def4d7 in DispatchEventToHandlers #8      0x92deeb7c in SendEventToEventTargetInternal #9      0x92df5f7c in SendEventToEventTarget #10     0x0dfb6a64 in dyld_stub_PMPluginCreateInstance #11     0x93383dbc in -[NSApplication sendAction:to:from:] #12     0x93431d0f in -[NSMenu performActionForItemAtIndex:] #13     0x93431a51 in -[NSCarbonMenuImpl performActionWithHighlightingForItemAtIndex:] #14     0x9343a081 in _NSPopUpCarbonMenu2 #15     0x9343922a in _NSPopUpCarbonMenu1 #16     0x93438e48 in -[NSPopUpButtonCell trackMouse:inRect:ofView:untilMouseUp:] #17     0x933b5b39 in -[NSControl mouseDown:] #18     0x933733e3 in -[NSWindow sendEvent:] #19     0x934e4d20 in -[NSCarbonWindow handleMouseDownEvent:at:inPart:withMods:] #20     0x934e41dc in NSCarbonWindowHandleEvent #21     0x92def4d7 in DispatchEventToHandlers #22     0x92deeb7c in SendEventToEventTargetInternal #23     0x92df5f7c in SendEventToEventTarget #24     0x92df640f in ToolboxEventDispatcherHandler #25     0x92def88e in DispatchEventToHandlers #26     0x92deeb7c in SendEventToEventTargetInternal #27     0x92df5f7c in SendEventToEventTarget #28     0x934e4776 in -[NSCarbonWindow sendEvent:] #29     0x93365384 in -[NSApplication sendEvent:] #30     0x9328fe1e in -[NSApplication run] #31     0x93283d4f in NSApplicationMain #32     0x000020ee in _start #33     0x00002015 in start Any ideas/help would be appreciate... thanks Ce message et les pi?ces jointes sont confidentiels et r?serv?s ? l'usage exclusif de ses destinataires. Il peut ?galement ?tre prot?g? par le secret professionnel. Si vous recevez ce message par erreur, merci d'en avertir imm?diatement l'exp?diteur et de le d?truire. L'int?grit? du message ne pouvant ?tre assur?e sur Internet, la responsabilit? du groupe Atos Origin ne pourra ?tre recherch?e quant au contenu de ce message. Bien que les meilleurs efforts soient faits pour maintenir cette transmission exempte de tout virus, l'exp?diteur ne donne aucune garantie ? cet ?gard et sa responsabilit? ne saurait ?tre recherch?e pour tout dommage r?sultant d'un virus transmis. This e-mail and the documents attached are confidential and intended solely for the addressee; it may also be privileged. If you receive this e-mail in error, please notify the sender immediately and destroy it. As its integrity cannot be secured on the Internet, the Atos Origin group liability cannot be triggered for the message content. Although the sender endeavours to maintain a computer virus-free network, the sender does not warrant that this transmission is virus-free and will not be liable for any damages resulting from any virus transmitted.</body>
  </mail>
  <mail>
    <header>Re: NSBitmapImageRep inconsistency</header>
    <body>Thanks for all the replies. So basically it comes down to having to throw away most of Quartz's bitmap handling for this purpose, analyze the source image format myself, swizzle it manually into a format that I can beat Quartz into drawing on the screen with an additional voodoo chant, even though it gave me the source format in the first place, which it seems to draw quite well, except taking it from me. Sorry for the bitterness, I just set out to write a little toy GUI for some image analysis algorithms, but I am just running around reinventing the wheel. I guess I can't get around writing some (probably redundant) framework that gives me ANY source image in a bitmap format _I_ specify, instead of some obscure variation Quartz itself doesnt really understand when I feed it back. Cheers, - D.G</body>
  </mail>
  <mail>
    <header>Re: NSBitmapImageRep inconsistency</header>
    <body>An alternative on 10.5 and if you have no problem writing code to deal with any layout (or have control over the layout of your source images) is to use CGDataProviderCopyData() to get the source image pixels and then take that data and use it to create a new CGImageRef to draw. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: NSBitmapImageRep inconsistency</header>
    <body>Unfortunately this approach has the tiny side effect of destroying your bitmap data, if it has an alpha channel.</body>
  </mail>
  <mail>
    <header>Re: Access to Published Outputs in Cocoa</header>
    <body>It appears that you have given no Options to the addObserver message so the KVO system doesn't know what you are interested in. In this case you will want to pass NSKeyValueObservingOptionNew. Hope that helps, Troy PS  Sorry for the slow reply. You will find faster answers for QuartzComposer questions on the email@hidden list, whereas this one is more geared toward the 2D drawing system, Quartz.</body>
  </mail>
  <mail>
    <header>Re: NSBitmapImageRep inconsistency</header>
    <body>I'm not certain, I would try logging all the parameters of your source bitmap and checking them against the documentation. Since you mention this works with color JPEGs but not B/W JPEGs, I might consider that some of the attributes of the source bitmap are reporting values consistent with a grayscale image and others with a color images (just a shot in the dark). To be honest, I wouldn't try to clone the bitmap like this, but instead specify all the parameters that matter directly and taking those that don't from the original bitmap. What matters and what doesn't is up to you, but I imagine you don't need to clone the &amp;quot;isPlanar&amp;quot; attribute for example :). Generally if you want to do this, then you would draw the image to a bitmap, manipulate the bitmap, then take a snapshot that bitmap to draw. In Quartz this would be the equivalent of calling CGBitmapContextCreate() to create a bitmap, drawing to it, then calling CGBitmapContextCreateImage() when you want to display it and throwing the image away when you want to update the display. CGImageRefs are considered immutable, so this is the correct behavior. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>NSBitmapImageRep inconsistency</header>
    <body>I am seeing an annoying thing with NSBitmapImageRep. I am trying to basically clone an existing NSBitmapImageRep (which was created from a CGImageRef) to do some bit twiddling with the bitmap data. However, the below code just says [[NSBitmapImageRep alloc] initWithBitmapDataPlanes: NULL pixelsWide:[bitmap pixelsWide] pixelsHigh: [bitmap pixelsHigh] bitsPerSample: [bitmap bitsPerSample] samplesPerPixel:[bitmap samplesPerPixel] hasAlpha: [bitmap hasAlpha] isPlanar: [bitmap isPlanar] colorSpaceName:[bitmap colorSpaceName] bitmapFormat: [bitmap bitmapFormat] bytesPerRow: [bitmap bytesPerRow] bitsPerPixel: [bitmap bitsPerPixel]] Why does this happen even though 'bitmap' can obviously exist with the given parameters? I observed this with a B/W JPEG, by the way, things work fine for color JPEGs. Are there more reliable ways of getting at the bitmap data and then displaying the changed bitmap? This is starting to turn into a rather frustrating endeavour. On a side note, apparently changing the bitmapData returned by an NSBitmapImageRep that was inited with a CGImageRef has no effect on the displayed image. Cheers, - D.G</body>
  </mail>
  <mail>
    <header>Re: Quartz event taps and tablet events [SOLVED]</header>
    <body>I spoke too quickly. My &amp;quot;solution&amp;quot; was only due to a coincidence. I'm still up in the air about how to do this. -- Bill Cheeseman - email@hidden Quechee Software, Quechee, Vermont, USA www.quecheesoftware.com PreFab Software - www.prefabsoftware.com</body>
  </mail>
  <mail>
    <header>Re: Quartz event taps and tablet events [SOLVED]</header>
    <body>Answering my own question: I simply forgot to include the common case where tablet events are implemented as mouse events with the kCGEventMouseSubtypeTabletPoint subtype. -- Bill Cheeseman - email@hidden Quechee Software, Quechee, Vermont, USA www.quecheesoftware.com PreFab Software - www.prefabsoftware.com</body>
  </mail>
  <mail>
    <header>Quartz event taps and tablet events</header>
    <body>The next version of my PreFab Event Taps Testbench utility includes several examples that are supposed to intercept and modify tablet pointer events. However, they don't work. The tablet events that reach the target applications are not modified, even though the events that Testbench is intercepting and issuing in modified form are modified. I'm using the same coding pattern for tablet events that works perfectly well for keyboard, mouse and scroll wheel events. Only tablet events fail. So I'm wondering if there's something special about tablet events that requires different handling. Or, is the Wacom 6.0.5 tablet driver doing something that circumvents Quartz event taps, like reading the hardware directly? Or is there something about Corel Painter Effects 3 or Adobe Photoshop Elements 4.0.1 that circumvents Quartz event taps? For example, I modify all tablet pointer events so that they have pressure set to 1.0 (maximum). Yet Painter Effects and Photoshop Elements continue to respond to pressure differences as I draw on the tablet. -- Bill Cheeseman - email@hidden Quechee Software, Quechee, Vermont, USA www.quecheesoftware.com PreFab Software - www.prefabsoftware.com</body>
  </mail>
  <mail>
    <header>Re: quartz question</header>
    <body>Regarding: On Jan 25, 2008, at 10:38 AM, Steve Mills wrote: I'd really recommend the functions CGContextFillRect and CGContextStrokeRectWithWidth to fill and stroke a rectangle. They can be more efficient than CGContextAddRect followed by stroking and filling operations. Regarding: Sorry for the shameless plug but this is described in quite a bit of detail on p.20-22 of the book &amp;quot;Programming With Quartz&amp;quot; that I wrote with Bunny Laden. There's also a graphic on p21 that shows the difference that the order of painting operations makes. Typically you want to fill then stroke which is the reason why: Hope this helps, David</body>
  </mail>
  <mail>
    <header>Re: Is anyone using IKImageView?</header>
    <body>Additional information: I'm actually using a subclass of IKImageView with isFlipped return YES.  This problem does NOT occur if the imageview is not flipped. Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Is anyone using IKImageView?</header>
    <body>I'm trying to write a couple of applications with an IKImageView with scrollbars. The docs say that it should have scrollbars, but I haven't been able to get them to work. I'm currently putting  the IKImageView inside a subclass of NSScrollView with the following change: if (docSize.height &amp;gt; scrollViewSize.height || docSize.width &amp;gt; scrollViewSize.width) else This works for displaying/hiding the scrollbars and scrolling correctly. However: 1) if it shows either scrollbar, it takes up the screen real estate for both, even if it  doesn't actually show the second scrollbar. (because the work-around either &amp;quot;auto-hides&amp;quot; or it doesn't) 2) and MUCH worse, the scrollview seems to confuse the IKImageView when it comes to the select tool. If the image isn't EXACTLY the height of the scrollView,  and you try to select a region with the select tool, the select box is not in the correct location. The x coordinate is fine, but the height is off. I think that this has something to do with isFlipped, (because it's only in the y dimension) but I haven't figured out what it is doing yet. Has anyone experienced this? and if so, how do I get around it? thanks. Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: Drawing using Quartz on the iPhone</header>
    <body>Your event handling code (touchesMoved, etc) should update a model which maintains the state of the application - for example, what ellipses have been drawn at what coordinates and sizes, in what order (if that is meaningful in your app) etc. Your drawRect method should then redraw all of the shapes in the model on each invocation. Quartz is different from Core Animation (or say, Flash Player programming) in this way - graphics are not persistent. There may be ways to work around this but you'll probably be better served in the long run by having your drawRect method derive from your model. e</body>
  </mail>
  <mail>
    <header>Re: Drawing using Quartz on the iPhone</header>
    <body>Manu</body>
  </mail>
  <mail>
    <header>Drawing using Quartz on the iPhone</header>
    <body>Hi there.  Any help would be greatly appreciated, since I've been going crazy with this issue.  I'm writing an iPhone app with functionality that allows a user to paint to the screen, with different shapes and colors.  I've been able to draw out dynamic shapes using the touchesBegan, touchesMoved and touchesEnded methods.  Basically what I do is: 1. Track current starting point 2. On touchesMoved, send current point to the drawing view 3. Call drawRect on the drawing view, which draws out the ellipse to the view's context This works great, I can draw out ellipses or rectangles or whatever.  Here's the big problem that I can't seem to solve:  when I go to write another shape, the current shape gets cleared.  Because every call to drawRect clears that view's graphics context.  How can I continue writing new shapes to the view's context, without clearing the previous content?  I've tried writing to a layer, then writing the layer to the context.  I've also tried putting code in touchesEnded that takes the layer of the draw view and adds it to the sublayer of the underlying view of draw view.  I may sound like a total newb here, which I am.  But this is driving me crazy.  How do basic sketching apps get created in this way?  How do you keep writing to the view w/o clearing it each time?  Thanks so much ahead of time for your help. --Eric _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>PDFView printWithInfo:autoRotate: fails?</header>
    <body>I saw a 2.5 year old thread on this list about what looks like the same problem, but didn't see a resolution. I was hoping that someone might remember what the solution was: I'm trying to print a PDFDocument that I am constructing from a series of images. In case it matters, I'm doing all of this from within a Mozilla plugin. I create the PDFDocument, and put it into a PDFView, then I call The print dialog comes up (as a separate window, instead of panel. I assume that that comes from being inside a mozilla window, so I wasn't too worried about it.)The dialog shows my document, and I can page through it correctly, and everything looks good. However, when I hit &amp;quot;Print&amp;quot; the dropdown with &amp;quot;Layout&amp;quot; etc becomes empty, and the view under that becomes empty. The window doesn't disappear, and the document doesn't print. Hitting Cancel does exactly the same thing. The only thing I can do then is force-quit Mozillla. I based the program off of PDFKitLinker2 from the apple dev site, and that program works. But I can't see any significant differences between it and my version.  Note that Mozilla is NOT a fully cocoa program, so I guess it's possible that that's at least part of the problem... Any suggestions on where to look? thanks. Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: Image wrapping</header>
    <body>Thanks for all the replies. Yes I'm only sliding it along the x-axis.  Something I did try was doubling the content size of the root and sublayers and doubling the frame width and moving the root position around.  The only issues is it seem it might be a bit heavy with the large content layers they have to work with.  And when I warp the root to a side the position warp gets animated but I just need to play with the animations for that. That would be perfect if I didn't need to support 10.5. I'm guessing my best option is to have double sized content instead of duplicate trees since a couple layers have filters. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: HIThemes documentation?</header>
    <body>Unfortunately, there is no documentation other than what's in the header file. If you have questions about the API, please post them to the carbon-dev mailing list. So far, I've heard of no plans to deprecate the HITheme API. -eric</body>
  </mail>
  <mail>
    <header>Re: Image wrapping</header>
    <body>This is true, however the content of sublayers are not included in a layer's contents, so in order to mirror a layer tree elsewhere, you need to either replicate the entire layer tree or use a CAReplicatorLayer. Delegate drawing is actually not very different at all, it is only a structured method to get Quartz drawing into a layer's contents property. If you have a layer that uses delegate drawing you can assign the contents from those layers to other layers just like you can with any other layer. The only thing you need to take care with is that a layer with a delegate will clobber its current contents if you call -display or -setNeedsDisplay on it. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Image wrapping</header>
    <body>I was under the impression that if you assigned the same content to two different CALayers that CA was smart about reusing the texture. If you're using delegate drawing, of course, that's a completely different story. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>HIThemes documentation?</header>
    <body>So, Apple claims that HIThemes aren't deprecated (yet). They appear to just about be the only part of HIToolkit that isn't. This is somewhat good for me because I need to get UI elements (a scrolled image in particular) into a CGContext that is given to me for a Mozilla Plugin. I'm pretty sure that HIThemes are the only way to do this, other than implementing my own widgets out of rectangles and paths. However, I haven't seen any documentation for HIThemes except for the sparse comments in the .h file. There's absolutely no help on the Apple Dev site... Does anyone know of any better documentation or even just sample code where I can see HIThemes being used? Also, does anyone know if/when Apple plans to deprecate HIThemes? Hints? Rumors?  It definitely looks as though Apple is moving towards a Cocoa/Objective-C or nothing approach... Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: CALayer, CAAnimation performance optimization</header>
    <body>The Core Animation instrument is likely the most useful here. Turn on the options to color blended and misaligned images and minimize your usage of both. The FPS counter is likely your best objective measure of your success. You may also be able to get some measure from the OpenGL ES instrument. If you can, then Renderer and Tiler utilization are most important, followed by Resource Bytes. Although you may only get Renderer &amp;amp; Tiler information in very specific cases, so don't be surprised if they are usually 0%. I would not expect most of alternatives you present to make a significant difference. The only one that may is #3, but unless your paths are particularly complex you may not be able to measure the difference. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Image wrapping</header>
    <body>If you can require Mac OS X10.6/iPhone OS 3.0, then the CAReplicatorLayer can do what you want. We have a desktop sample demonstrating its use, but the basic jist of it would be that you would simply create replicators to duplicate the drawing of your layer tree to implement your tiling. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Rubberband</header>
    <body>I definitely recommend this option. It's much cleaner than attempting to draw your own bezier path rubber band inside your view, and you let Quartz take care of all the dirty work for you. As long as your custom window doesn't swallow mouse events, you shouldn't have to do much more work than drawing inside the view. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: Rubberband</header>
    <body>What have you tried?</body>
  </mail>
  <mail>
    <header>Re: Rubberband</header>
    <body>We do all such accessory drawing in a transparent overlay window. That way we only need to redraw rubber bands, handles, etc each time an action like this happens. The overlay is grouped with the document window, and moves and resizes along with it. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Rubberband</header>
    <body>What is the best way to draw a 'rubberband'  showing a srag selection area? -db</body>
  </mail>
  <mail>
    <header>Re: Image wrapping</header>
    <body>Question:  Are you only moving your image in one-dimension? If so, I might use the layer's delegate to provide the content of my image... perhaps drawing using CGLayers and cropping out the appropriate CGImage.</body>
  </mail>
  <mail>
    <header>Re: Image wrapping</header>
    <body>That sounds good, I'll give it a try.  Is there a way to tell a CALayer to mirror another's content (including sublayers)?</body>
  </mail>
  <mail>
    <header>Re: Image wrapping</header>
    <body>Create a second layer and give it the same contents, and keep them positioned correctly? --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Image wrapping</header>
    <body>I'm trying to find a good way to wrap an image in a CALayer.  If the image is slid to one side it should wrap around to the other side, to simulate a continuous image.  I tried doing this with the CIAffineTile filter and it does what I want but the filter seems to be fairly expensive for this task.  Are there more efficient ways to do this that can be done with live moving? _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>CALayer, CAAnimation performance optimization</header>
    <body>I'm at the stage with Instaviz that I'd like to explore ways of making my CALayer hierarchy faster, but I'm not sure how to approach this. On the iPhone, scrolling through a large CALayer hierarchy is sometimes slightly laggy. How do I objectively measure how fast or slow scrolling/display etc. are? Do I have to use Instruments and check the CPU/GPU (?) (and bearing in mind it looks like Springboard is hosting all the animation stuff) while scrolling? Or can I write my own tests to scroll and check CPU/GPU usage? In particular here are some either/or design decisions I need to make. How much will they affect performance: 1.	A layer has many sublayers and I want to hide it. a.	Hide individual sublayers. b.	Hide the layer itself. c.	Remove the layer from the display hierarchy. i.e. is hiding much more expensive than removing the layer from the hierarchy? 2.	A CAShapeLayer with bounds around the path vs. an unbounded CAShapeLayer (the CAShapeLayer itself has no content only a path). 3.	A CAShapeLayer with a path made up of n subpaths vs. n CAShapeLayers with single paths. 4.	Animating a CAAnimationGroup made up of n subanimations vs. animating all n animations separately. 5.	Assuming actions are disabled, setting the sublayers property of a layer vs. using addSublayer, removeFromSuperlayer etc. to compose the hierarchy. 6.	Applying an animation to a layer that has sublayers vs. apply animations to each sublayer directly e.g. a fade animation (opacity from 1 to 0) on the layer, or on the individual sublayers. 7.	Applying an animation when the duration is zero vs. not applying the animation at all. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>RE: IKFilterUIView - Problem with text fields</header>
    <body>From: email@hidden To: email@hidden Date: Thu, 11 Mar 2010 16:40:36 +0100 Subject: IKFilterUIView - Problem with text fields</body>
  </mail>
  <mail>
    <header>Auto crop?</header>
    <body>Is there anything in CoreImage or vImage that can find the rect bounds of an arbitrary image?  For instance if there's a large clear filled image with something smaller and opaque drawn somewhere, I'd like to get a rect of what part of the image actually has opaque data. I can probably roll my own filter kernel or just scan as a bitmap, but don't want to reinvent the wheel if it's there. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Is using Garbage Collection with Core Image good?</header>
    <body>Thanks, Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Is using Garbage Collection with Core Image good?</header>
    <body>Thanks,</body>
  </mail>
  <mail>
    <header>Combining CIFilter with cpu image filter or how to get a buffer	pixel from a CIImage efficiently</header>
    <body>Hello, I am trying to combine some non local image filters , i.d. a flood fill variant, image segmentation and image reconstruction, with some CIFilters. My filters operate on pixel buffers, while CIFilters produce a CIImage. I have no problem getting a CIImage representation of my buffer data, but I am wondering if there is an efficient way of getting buffer data out of a CIImage. The examples I found sofar write the whole CIImage in a context to get the buffer data. Is there another way? Maybe one where CIImage provides single pixel data without rendering the whole image? Small examples would be greatly appreciated. Thank you in advance. JUMAttachment:</body>
  </mail>
  <mail>
    <header>Re: CAShapeLayer lineDashPattern crash</header>
    <body>This is an error in the documentation (as John Harper has pointed out). It‚Äôs been corrected and will be published soon.</body>
  </mail>
  <mail>
    <header>Re: Trouble switching to CGEvents</header>
    <body>In short, no. The Cocoa API is available to 64-bit apps. As others have pointed out, only some parts of the Carbon API are restricted to 32-bit apps; most are actually available in both 32-bit and 64-bit. GetEventMonitorTarget is available in 64-bit. So if that's your only reason, there's really no need to switch away from GetEventMonitorTarget. -eric</body>
  </mail>
  <mail>
    <header>Re: Trouble switching to CGEvents</header>
    <body>If you look in CarbonEvents.h, you'll see that some of the Get...Target declarations are enclosed in &amp;quot;#if !__LP64&amp;quot; and some are not. GetApplicationEventTarget, GetEventDispatcherTarget, and GetEventMonitorTarget are apparently available in 64-bit code. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: Trouble switching to CGEvents</header>
    <body>This doesn't answer your direct question, but if you look in the headers, you will see that GetEventMonitorTarget is available in 64 bit.</body>
  </mail>
  <mail>
    <header>Re: Trouble switching to CGEvents</header>
    <body>Eric, Since GetEventMonitorTarget is defined in Carbon (and specifically HIToolbox), does this have implications for whether I can use addGlobalMonitorForEventsMatchingMask:handler: in a 64-bit app?  My goal in all of this was to get to something 64-bit clean, but this suggests an unexpected dependency on 32-bit Carbon and I'm not sure how to interpret that. -- Tom Harrington email@hidden AIM: atomicbird1</body>
  </mail>
  <mail>
    <header>Re: Capturing cursor pixmap using carbon / C++</header>
    <body>&lt;blockquote cite="" type="cite"&gt;I'm confused why you can't just use NSCursor. Is there some reason you're avoiding Cocoa?</body>
  </mail>
  <mail>
    <header>Re: Color values from a CGImage</header>
    <body>On Feb 29, 2008, at 4:30 PM, Marcel Borsten wrote: In general, no, because a CGImage doesn't contain pixels. :-) To do what you are trying to do, however, you need to create a CGBItmapContext in the color space of your choice and draw the image into that context.  You can then read the value of the bitmap context's pixels from the memory that you supply when you create it.</body>
  </mail>
  <mail>
    <header>Color values from a CGImage</header>
    <body>Is there an easy way to get the color values (RGB or RGBA) out of a pixel from a CGImage? Attachment:</body>
  </mail>
  <mail>
    <header>Re: Leopard Core Video problem with CIFlashTransition on CbYCrY	movies</header>
    <body>Filed now as bug id 5773293. I attached a sample project that reproduces the problem. The software rendered does not have this problem. I'm using a RadeonHD2600 (aluminium iMac) Cheers, Martin 28 feb 2008 kl. 18.20 skrev Frank Doepke: Attachment:</body>
  </mail>
  <mail>
    <header>Re: CGRegisterScreenRefreshCallback not notifying me of all updates</header>
    <body>I finally got to the bottom of this - it ws due to my blocking the thread somehow - once that was sorted out, I did end up receiving all the updates that I should have done, as far as I can tell. Once I sorted out all my problems I did end up with a strange result. My original code was quite closely based on the OpenGLScreenCapture example, which spawns a seperate thread on each screen refresh callback to grab the dirty rect (in my code, the original grabs the whole screen) and uses a FrameReader object to call glCopyTexSubImage2D/glGetTexImage to grab the frame. Using the original example code, I found that it was either blocking when trying to obtain a lock on [FrameReader class], or during the write (I'm writing the data out rather than passing it to a compression session). Whatever I tried, I couldn't get the OpenGLScreenCapture example to work without serious problems with mechanism for screengrabbing, so this was kind of annoying. OTOH, using glGrabPixels during the screen refresh callback, with certain optimisations (e.g. compressing the data with something RLE on the fly, so that writes end up being much smaller) worked fine. That is unexpected, as everything on the web says that glReadPixels is really slow, and should be avoided at all costs. Maybe I was just doing something wrong, but I would definitely take the OpenGLScreenCapture glCopyTexSubImage2D/glGetTexImage is the most efficient mechanism claims with a pinch of salt. If you think that's definitely wrong, and that OpenGLScreenCapture really is the way to go, I'd love to hear from you. On Fri, Feb 15, 2008 at 4:50 AM, Martin Redington --</body>
  </mail>
  <mail>
    <header>Re: Leopard Core Video problem with CIFlashTransition on CbYCrY	movies</header>
    <body>Can you file a bug on this? It would be helpful to know on which graphic cards you see the problem. Frank Doepke Senior Engineer Core Image Apple Computer, Inc. M/S 302-3PG 2 Infinite Loop Cupertino, CA 95014 phone: 408-974-1266 email: email@hidden</body>
  </mail>
  <mail>
    <header>Leopard Core Video problem with CIFlashTransition on CbYCrY movies</header>
    <body>I use a Core Image transition such as CIFlashTransition to transition between two CIImages of different sizes. In the transition parts of the output image will correspond to no part of the source images and I would expect those parts to be rendered as transparent. This works as expected on Tiger, but on Leopard I get black instead of transparent if the images comes from certain movies. I inspected the images I get out of Core Video, and concluded that the problem is when they are in CbYCrY_8 format. ARGB_8  works fine though. Most other transitions (eg CIModTransition) does not have this problem. So, is this expected or should I file a bug report? Is there any way I can fix it? I've tried specifying Core Video options such as working color space and pixel format type and I also specify the color space for imageWithCVImageBuffer in the hope that the frames would be converted to ARGB_8 and thus get alpha. Is that possible? I still get only CbYCrY_8 and only black instead of transparent. - (CGColorSpaceRef) workingColorSpace if (!myWorkingColorSpace) // // An RGB color space with linear gamma curves will make a good working space NSDictionary* workingProfileSpec = [NSDictionary dictionaryWithObjectsAndKeys: @&amp;quot;displayRGB&amp;quot;,                 @&amp;quot;profileType&amp;quot;, one,                           @&amp;quot;gammaR&amp;quot;, one,                           @&amp;quot;gammaG&amp;quot;, one,                           @&amp;quot;gammaB&amp;quot;, @&amp;quot;HDTV&amp;quot;,                       @&amp;quot;phosphorSet&amp;quot;, [NSNumber numberWithInt:6500], @&amp;quot;whiteTemp&amp;quot;, CMMakeProfile(workingProfile, myWorkingColorSpace = /* Create QT Visual context */ NSMutableDictionary	*attributes = [NSMutableDictionary NSValue	*sizeValue = [qtMovie NSMutableDictionary *contextAttributes = [NSMutableDictionary if (sizeValue) NSDictionary    *targetDimensions = [NSDictionary dictionaryWithObjectsAndKeys:[NSNumber numberWithFloat:size.width], kQTVisualContextTargetDimensions_WidthKey, [NSNumber numberWithFloat:size.height], [contextAttributes setObject:targetDimensions forKey:(NSString NSNumber        *pixelBufferFormatType  = [NSNumber NSDictionary    *pixelBufferAttributes  = [NSDictionary dictionaryWithObjectsAndKeys:pixelBufferFormatType, (NSString *)kCVPixelBufferPixelFormatTypeKey, [NSNumber [contextAttributes setObject:pixelBufferAttributes forKey: theError = QTOpenGLTextureContextCreate(kCFAllocatorDefault, [currentOpenGLContext CGLContextObj], pixelFormat, (CFDictionaryRef)attributes, if(myVisualContext == NULL) else QTVisualContextSetAttribute(myVisualContext, kQTVisualContextWorkingColorSpaceKey, [[[self scene] screen] theError = SetMovieVisualContext([qtMovie if (theError != noErr) CIImage *image = [CIImage imageWithCVImageBuffer:currentMovieFrame options: [NSDictionary dictionaryWithObjectsAndKeys:(id) [self // Create a new CIContext using the new output color space NSDictionary* options = [NSDictionary dictionaryWithObjectsAndKeys: (id)[self workingColorSpace], kCIContextWorkingColorSpace, (id)displayColorSpace, kCIContextOutputColorSpace, [NSNumber numberWithBool:NO], kCIContextUseSoftwareRenderer, CIContext *ciContext = [CIContext contextWithCGLContext: [openGLContext CGLContextObj] pixelFormat:[pf CGLPixelFormatObj] Thanks, Martin Attachment:</body>
  </mail>
  <mail>
    <header>Re: Is there an efficient way to draw the Caret in Quartz?</header>
    <body>As long as you're drawing with Quartz, I guess it will interesect the primitives with the current clipping rect to know if it is going to actually render it or not. Here, we have GUIs with hundreds of pictures and drawing a tiny surface is not a problem at all. However, if you're using a portable buffer rendering approach (rendering yourself), the problem can be more tricky. Then you should probably review with your architect on the possible approaches.</body>
  </mail>
  <mail>
    <header>Re: Is there an efficient way to draw the Caret in Quartz?</header>
    <body>On Feb 28, 2008, at 12:57 AM, Eric Schlegel wrote: Unfortunately the drawing of its background is not cheap. In fact, it is a big custom HIView. Its drawing is a bit complex. I notice that Apple Mail mail body view draws the caret fast. How does it achieve that?</body>
  </mail>
  <mail>
    <header>Re: Bypassing ColorSync while printing</header>
    <body>On Feb 27, 2008, at 2:55 AM, Dinge Raphael wrote: I had considered tagging the image with the destination profile as a possible solution, but I wasn't sure if the act of copying was guaranteed not to pipe the pixels through a rendering intent at some point. The fact that the function demands the same number of color components would suggest a laissez-faire attitude toward color, but you never know.</body>
  </mail>
  <mail>
    <header>Is there a CI ripple distortion?</header>
    <body>I didn't see it in the CIFilter reference, but I might be missing something.  I know there's a ripple transition filter, but I was wondering if there was also just a regular ripple image distortion.</body>
  </mail>
  <mail>
    <header>Re: Is there an efficient way to draw the Caret in Quartz?</header>
    <body>On Feb 27, 2008, at 2:15 AM, bady.yu_gmail.com wrote: Is the background of your text-editing view opaque? If so, you can set the Opaque view feature flag for your view, and when you invalidate the view in preparation for hiding the caret, only your view will be called to draw its background (which I assume is cheap). No other view behind your view will be called to draw because the view system knows that your view is opaque and will overwrite anything drawn behind it anyways. By the way, please don't cross-post to multiple lists. Stick to one list.</body>
  </mail>
  <mail>
    <header>Re: Bypassing ColorSync while printing</header>
    <body>You can do this by assigning the TIFF a new profile (the destination profile) CGImageCreateCopyWithColorSpace is meant for that.</body>
  </mail>
  <mail>
    <header>Re: Is there an efficient way to draw the Caret in Quartz?</header>
    <body>You might want to draw the background to a CGLayer, then draw the CGLayer in your view redraw function.  When you need to refresh the background or portions using the CGLayer will be pretty fast. On Feb 27, 2008, at 3:15 AM, bady.yu_gmail.com wrote:</body>
  </mail>
  <mail>
    <header>Image Editing</header>
    <body>Hi All,</body>
  </mail>
  <mail>
    <header>Re: change the CGColorSpace when using a CIFilter to use	16bits/channel images</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGPDFPageRef memory leak (Carlo Eugster)</header>
    <body>I think it's similar to a problem mentioned in Quartz-dev Digest, Vol 7, Issue 9. Apparently when loading a PDF page from a PDFDocument using CGPDFDocumentGetPage that memory is not released until the CGPDFDocument is released. David Duncan suggested the following (Jan 13th, 2010) Maybe he can tell us if that issue has been resolved since then (if it's actually a bug in the first place). Also I think the UIGraphicsEndPDFContext call is out of place since you're creating a context with UIGraphicsBeginImageContext.</body>
  </mail>
  <mail>
    <header>Re: CGPDFPageRef memory leak</header>
    <body>Thanks for the responses! And I assume there is no way to force it to clear the cache, correct? Constantly loading and unloading the document doesn&amp;#39;t seem very efficient... Even if it only happens every 10 or so pages as suggested.¬† That&amp;#39;s my understanding of it too. But the problem is that the cache is not released when a low memory warning is issued on the iPhone OS. That&amp;#39;s the bug. Depending on the size / complexity of the document, reloading it all the time is really not an option. For instance, the Adobe PDF Guide PDF takes about 10-15 seconds to CGPDFDocumentCreate... Best wishes, Hamish</body>
  </mail>
  <mail>
    <header>Re: CGPDFPageRef memory leak</header>
    <body>That's my understanding of it too. But the problem is that the cache is not released when a low memory warning is issued on the iPhone OS. That's the bug. Depending on the size / complexity of the document, reloading it all the time is really not an option. For instance, the Adobe PDF Guide PDF takes about 10-15 seconds to CGPDFDocumentCreate... Best wishes, Hamish</body>
  </mail>
  <mail>
    <header>Re: CGPDFPageRef memory leak</header>
    <body>What we did in our iPad app is to draw a certain number of pages to not have to reopen the doc every time and close it and reopen it like every 10 pages On Apr 7, 2010, at 9:02 PM, Hamish Allan &amp;lt;email@hidden&amp;gt; wrote:</body>
  </mail>
  <mail>
    <header>Re: CGPDFPageRef memory leak</header>
    <body>If this is what I think it is, it isn't a leak but rather a cache. Instead of retaining the PDF document between calls, try creating the document, getting and drawing the page, then releasing the document when your done. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: CGPDFPageRef memory leak</header>
    <body>File a bug (with a minimal test case), mark it as a dupe of rdar://problem/6367689, and join the club :( H</body>
  </mail>
  <mail>
    <header>CGPDFPageRef memory leak</header>
    <body>Hi, Does anyone have an idea why this code would leak memory? The method is used to flip through the pages of the PDF document, but something along the line does not seem to be released properly. I narrowed the problem down to this chunk of code since the footprint of my application shoots up drastically whenever I pull a page that has graphics on it. It&amp;#39;s been driving me crazy for the past several days, so any help would be greatly appreciated!</body>
  </mail>
  <mail>
    <header>change the CGColorSpace when using a CIFilter to use 16bits/channel images</header>
    <body>Hi,</body>
  </mail>
  <mail>
    <header>Re: CGDisplayBaseAddress deprecated</header>
    <body>thanks a lot David for u r kindly help :). i just checked and found the OpenGL material on the developer website. i think it will help my cause. no. i have not tried it yet. so is there no way to do it using Quartz anymore?</body>
  </mail>
  <mail>
    <header>Re: CGDisplayBaseAddress deprecated</header>
    <body>no. i have not tried it yet. so is there no way to do it using Quartz anymore?</body>
  </mail>
  <mail>
    <header>Re: CGDisplayBaseAddress deprecated</header>
    <body>You may make an overlay window that would cover the entire screen(s), then you can draw with quartz with it. If you have a tile rendering engine, you might make a bitmap context with quartz once, and every time you want to draw to screen, draw to your tiles, extract an image from the bitmap context and display this image in the window.</body>
  </mail>
  <mail>
    <header>Re: CGDisplayBaseAddress deprecated</header>
    <body>no. i have not tried it yet. so is there no way to do it using Quartz anymore? madhu Have you tried OpenGL? --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: CGDisplayBaseAddress deprecated</header>
    <body>Have you tried OpenGL? --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: CGDisplayBaseAddress deprecated</header>
    <body>since CGDisplayBaseAddress is deprecated in Mac OS X 10.6 (snow leopard). what is the alternative to obtain the base address in framebuffer memory of an online display? pls help.</body>
  </mail>
  <mail>
    <header>Re: getting rid of QuickDraw</header>
    <body>hi all, thanks so much for the detailed feedback. i am making progress. best, bill &amp;gt; Thanks for the feedback. I have some constraints because I am working on a large common code base that is compiled under Windows or Mac. So there are portability routines that do one thing or another on either platform. So i have been upgrading individual functions, but a holistic rewrite is difficult with so much shared code. In some cases I need to mimic what the Mac used to do or what Windows currently does. If you don&amp;#39;t mind some unsolicited advice... The trick for cross platform code of this type is to take advantage of the Model-View-Controller paradigm. ¬†The trick is to have your model be cross platform code, your view be platform-specific. The Controllers would then be split into two parts. On one side of the controller you should concern yourself with what kinds of &amp;quot;commands&amp;quot; you can perform on your data... that is what manipulations can you do. On the other side of the controller you concern yourself with how things like mouse movements and such translate into those commands. The commands portion of your controller can then be cross platform. ¬†The part that is interpreting events and control messages can be platform specific. I gave a presentation on this at WWDC in 2006, if you&amp;#39;re interested, and if I can find it, I will send it along. A strategy you might follow for your application would be to take your application on a window-by-window basis factor to a MVC triad for that window. ¬†Then you could replace the Carbon implementation of the view side of the window with a Cocoa implementation. See... it was really easy for me to type that, not knowing any of the details :-P All drawing SHOULD be done in response to an event. ¬†If you want to draw something in response to a mouse event, you would invalidate some portion of your HIView and then do the drawing in response to the related carbon event. For the longest time there wasn&amp;#39;t any such thing as IB for Carbon applications. ¬†HIViews were created programmatically. :-) Not exactly. ¬†A WindowRef in Carbon is most closely related to an NSWindow in Cocoa. ¬†HIViews are similar to NSView. The title is a feature of an NSWindow.</body>
  </mail>
  <mail>
    <header>CGDisplayBaseAddress deprecated</header>
    <body>hi all, since CGDisplayBaseAddress is deprecated in Mac OS X 10.6 (snow leopard). what is the alternative to obtain the base address in framebuffer memory of an online display? pls help. rgds, madhu</body>
  </mail>
  <mail>
    <header>Re: getting rid of QuickDraw</header>
    <body>If you don't mind some unsolicited advice... The trick for cross platform code of this type is to take advantage of the Model-View-Controller paradigm.  The trick is to have your model be cross platform code, your view be platform-specific. The Controllers would then be split into two parts. On one side of the controller you should concern yourself with what kinds of &amp;quot;commands&amp;quot; you can perform on your data... that is what manipulations can you do. On the other side of the controller you concern yourself with how things like mouse movements and such translate into those commands. The commands portion of your controller can then be cross platform.  The part that is interpreting events and control messages can be platform specific. I gave a presentation on this at WWDC in 2006, if you're interested, and if I can find it, I will send it along. A strategy you might follow for your application would be to take your application on a window-by-window basis factor to a MVC triad for that window.  Then you could replace the Carbon implementation of the view side of the window with a Cocoa implementation. See... it was really easy for me to type that, not knowing any of the details :-P All drawing SHOULD be done in response to an event.  If you want to draw something in response to a mouse event, you would invalidate some portion of your HIView and then do the drawing in response to the related carbon event. For the longest time there wasn't any such thing as IB for Carbon applications.  HIViews were created programmatically. :-) Not exactly.  A WindowRef in Carbon is most closely related to an NSWindow in Cocoa.  HIViews are similar to NSView. The title is a feature of an NSWindow.</body>
  </mail>
  <mail>
    <header>Apply bevel effect to UIImage?</header>
    <body>Can anyone give me any pointers on how I would add a bevel effect to a UIImage ? I have a picture, and want to make it look like a framed picture with a bevel effect, like you could do in photoshop. Eg: I&amp;#39;m not sure how to approach this in Quartz 2D for iPhone. Perhaps using masks? Thanks, Paul</body>
  </mail>
  <mail>
    <header>Re: Drawing via Quartz</header>
    <body>Ahoj Lukas, Nice to see someone from Czech republic :) For 2D game animation on the iPhone / iPad I have used a great game kit called Cocos2D. It has many freatures which you can use to build games. See the videos and tutorials here: Paul</body>
  </mail>
  <mail>
    <header>Re: getting rid of QuickDraw</header>
    <body>When Eric says that you should start planning a rewrite in Cocoa, he is definitely right. However the term &amp;quot;rewrite&amp;quot; is probably a lot scary to you, but in practice it is just like using another API when having the following in mind : - you can first try to build your C or C++ files using Objective-C or Objective-C++. If it goes without complaining, then you're lucky and your port will be rather easy. - if it complains (like with keyword 'id' and such), then you should adopt pimpl approach where you tie cocoa calls in separate files, so that you can compile your whole project in C/C++ and have a few files compiling in Objective C/C++ If you are going to use HIView then remember : - it's going to be deprecated sooner or later - it is actually very tied to the NSView model, so why not directly go to an API that is (at least for now...) future proof ? It is better to draw in response of a draw event. However if your invalidate/draw cycle is handled in your shared code base, then this is going to be simpler to handle than when working with MacOS8/9 GrafPorts and the likes. If your shared code base, for graphics implements a compositing engine, then you'll be ok. If not, you'll have to remember that the windows model at the moment you write for it was not a compositing engine. It can be created programatically. Basically this is really like any API except it is OO.</body>
  </mail>
  <mail>
    <header>Re: getting rid of QuickDraw</header>
    <body>Hi All, Thanks for the feedback. I have some constraints because I am working on a large common code base that is compiled under Windows or Mac. So there are portability routines that do one thing or another on either platform. So i have been upgrading individual functions, but a holistic rewrite is difficult with so much shared code. In some cases I need to mimic what the Mac used to do or what Windows currently does. With the usage of HIView for example, must all drawing be in response to an event? Or for example could I get a mouse event and then just start drawing stuff in a window? Can HiViews be programatically created, or only with Interface Builder? I haven&amp;#39;t been able to figure that one out from the docs. With Cocoa i would replace WindowRefs with NSViews. So i could pass a NSView into a C function and do stuff like set the window title, etc. Right? In other words I need to use a NSView like an old fashioned HWND or WindowRef. Happy Easter, Bill In the long term, you should start planning a rewrite in Cocoa. The user interface portions of Carbon are no longer actively being enhanced, and while we&amp;#39;re not going to remove the API from the system anytime soon, it will become increasingly hard to maintain or develop Carbon apps overtime. You&amp;#39;re definitely smart to start by moving from QuickDraw to CoreGraphics, since that portion of your drawing code can probably be reused when you rewrite for Cocoa. The best way to stop using QDBeginCGContext is to convert your windows to use the compositing mode of drawing supported by HIToolbox in 10.2 and later. When you do that, you rewrite your drawing code so that it is placed inside a kEventControlDraw handler for an HIView in your window. You could, for example, just create one HIView that is the size of the entire window, and its kEventControlDraw handler contains all of your drawing code for your entire window (although you will often want to separate out portions of your window into separate views). A kEventControlDraw handler for a view in a compositing window is passed a CGContextRef as an event parameter, so to get the CGContext, you just call GetEventParameter with the appropriate constants. -eric</body>
  </mail>
  <mail>
    <header>Re: getting rid of QuickDraw</header>
    <body>In the long term, you should start planning a rewrite in Cocoa. The user interface portions of Carbon are no longer actively being enhanced, and while we're not going to remove the API from the system anytime soon, it will become increasingly hard to maintain or develop Carbon apps overtime. You're definitely smart to start by moving from QuickDraw to CoreGraphics, since that portion of your drawing code can probably be reused when you rewrite for Cocoa. The best way to stop using QDBeginCGContext is to convert your windows to use the compositing mode of drawing supported by HIToolbox in 10.2 and later. When you do that, you rewrite your drawing code so that it is placed inside a kEventControlDraw handler for an HIView in your window. You could, for example, just create one HIView that is the size of the entire window, and its kEventControlDraw handler contains all of your drawing code for your entire window (although you will often want to separate out portions of your window into separate views). A kEventControlDraw handler for a view in a compositing window is passed a CGContextRef as an event parameter, so to get the CGContext, you just call GetEventParameter with the appropriate constants. -eric</body>
  </mail>
  <mail>
    <header>getting rid of QuickDraw</header>
    <body>Hi all, i&amp;#39;m getting rid of QuickDraw and replacing that with core graphics right now i am getting my CGContextRef from a WindowRef with QDBeginCGContext but QDBeginCGContext is part of QuickDraw... so are WindowRefs also going away? Is all of Carbon going away? can anyone give me some advice about future proofing what we are doing (other than rewrite everything in cocoa) thanks Bill Appleton</body>
  </mail>
  <mail>
    <header>Re: CIFilter and grayscale image</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>RE: CIFilter and grayscale image</header>
    <body>Hi Alex, Paul Subject: Re: CIFilter and grayscale image From: email@hidden Date: Fri, 2 Apr 2010 15:43:55 -0700 CC: email@hidden To: email@hidden</body>
  </mail>
  <mail>
    <header>Re: Combining CIFilter with cpu image filter or how to get a	buffer	pixel from a CIImage efficiently</header>
    <body>Hi Jens, As you know, Core Image only actually does any work when you ask it to render to a destination (up until then all you've done is provide Core Image with a recipe to produce a final image.) That being said, when you perform the actual rendering, many steps need to occur and that includes doing a whole-bunch of tree optimizations in order to ensure that only the necessary bits of the textures that are used to build up your destination image are being read in, tiling appropriately, and so on (it's a long list.) Said otherwise, if you can at all avoid rendering a single pixel at a time please do! Now if you're ok with rendering the entire image and then working on it once its been put into a buffer then I suggest create a CG context via CGBitmapContextCreate() and an associated CIContext using the newly created CG context. Hope this helps, Alex.</body>
  </mail>
  <mail>
    <header>CIFilter - output image format</header>
    <body>Hi,</body>
  </mail>
  <mail>
    <header>FW: Problem to save a CIImage using a CGImageRef</header>
    <body>Hi ,</body>
  </mail>
  <mail>
    <header>CI and blurring - porting from GLSL questions.</header>
    <body>Hello I am in the process of porting a GLSL based gaussian separable blur to Core Image Kernel Language in order to make an Image Unit. In porting the main kernel code (which was very straight forward), I noticed that while the main algorithm is exactly the same my results are vastly different for blur widths greater than 2: Here is an example image where I specify a kernel width of 5 (for a 5 pixel blur - but over 5 passes in each implementation) The top is my Core Image unit, the bottom is my GLSL implementation as a Custom QC Plugin. Notice the obvious sampling artifacts. I attempted to isolate the issue by offsetting my sampler coordinates by 0.5 in CI, which did very little smoothing out of the artifacts. I also noticed that switching to nearest filtering vs linear filtering in my sampler options in the unit resulted in no visual change. This tells me I am missing something obvious, but I am unsure as to what it might be. The core logic of both the CIKL code and the GLSL code is exactly the same, simply replacing texture2DRect with sample, the texture coordinates from the vertex shader with samplerCoord, etc, and the image looks identical for kernel widths below 2.0 To debug and make sure that I have not fouled up in my Image Unit (near as I can tell), I also tried using Quartz Composers Core Image Custom Filter patch and entered in my code there. The same artifacts / sampling issues occur there. Are there any special considerations one has to have in mind when porting code from GLSL to Core Image besides the lack of Matrix transforms and other syntax based differences? Any subtle sampling differences that could account for this issue I might not be aware of? Is there a maximum sampling distance from the current &amp;quot;fragment&amp;quot; (in GL speak) where you no longer get linear filtering? Thanks, very curious about this. Note, I also have tested on a variety of GPUs both NV and ATI, and all have this issue. This tells me its something I am doing or misunderstanding. At least its consistent! Thanks again for any pointers. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CALayer: frame automatically animated?</header>
    <body>after I set the frame but this seems quite odd to me so I am wondering if I do something wrong. Do I?</body>
  </mail>
  <mail>
    <header>Re: CALayer: frame automatically animated?</header>
    <body>CALayers animate all animatable properties by default (and as you see, the frame isn't directly animatable, but rather spawns animations for the aforementioned properties). If you don't want animations, you can use CATransaction to disable them temporarily by disabling actions (there is a convenience method available on 10.6+ and a key that takes boolean for pre-10.6 systems) -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>CALayer: frame automatically animated?</header>
    <body>Hello, I position and resize a CALayer with myLayer.frame = CGRectMake(...) I want the layer to jump to the new position instantaneously but it slides there smoothly in about half a second. I do not add any animations manually but when I log the [myLayer animationKeys] it prints [myLayer animationKeys] = ( position, transition, bounds ) I seem to be able to get rid of the animation by adding after I set the frame but this seems quite odd to me so I am wondering if I do something wrong. Do I? Thanks and regards, Sebastian Mecklenburg</body>
  </mail>
  <mail>
    <header>Re: CGShading problem in 10.6.7</header>
    <body>The domain of your function is incorrect. For a shading, the function is 1:N but you have a 4:N function here. I can only assume that the silent failure is due to CG being unable to correctly fill out the extra parameters that your function is asking for. I would file a bug asking for this failure to be made less silent. We should probably at least log a console message in this case, and probably should return a NULL shading (although that may be more problematic). -- David Duncan</body>
  </mail>
  <mail>
    <header>Tiling and sampler coordinates</header>
    <body>And while I'm at it, I've got another one. Core Image renders image as tiles when needed, depending on configuration, GPU/CPU, etc. Could someone please comment on how these kernel functions interact with tiling? - destCoord - samplerCoord - sample - samplerExtent - samplerTransform A particular example I have is that counting on samplerExtent() to return the full size of the associated image doesn't work, which I suspect is due to tiling. I would like to understand this better. Thank you very much, Raphael Raphael Sebbe Creaceed ‚Äî Creative iPhone &amp;amp; Mac apps ‚Ä¢‚Ä¢‚Ä¢ Twitter: ‚Ä¢‚Ä¢‚Ä¢ Web:</body>
  </mail>
  <mail>
    <header>Difference between kCIApplyOptionDefinition and kCIApplyOptionExtent</header>
    <body>Hi, I've been using Core Image intensely in past couple years, but there remains some things I still don't understand. Could someone please shed some light on this? What's the difference between kCIApplyOptionDefinition and kCIApplyOptionExtent? Should they both be set when defining a new filter, what is there exact purpose? Thank you! Raphael Raphael Sebbe Creaceed ‚Äî Creative iPhone &amp;amp; Mac apps ‚Ä¢‚Ä¢‚Ä¢ Twitter: ‚Ä¢‚Ä¢‚Ä¢ Web:</body>
  </mail>
  <mail>
    <header>Re: CGShading problem in 10.6.7</header>
    <body>Thanks for the reply. There are no build warnings or errors and when the program runs there are no errors being logged to the console. Here is the relevant code: I would check the console and your intermediate steps to see if there are any errors. Beyond that, it may help to post the relevant code from your reduced case. -- David Duncan</body>
  </mail>
  <mail>
    <header>VM problem in CGContextDrawPath ?!?</header>
    <body>Hello, This is weird. Path drawing in the simulator gets really slow and the profiler shows me strange stuff, functions like NSFileProtectionReadWrite or vm_fault_copy_cleanup being called from CGSFillDRAM8by1which is in turn called from CGContectDrawPath. Does anyone know how to get rid of this? It does not happen on the device where the app runs fast and smooth. I use the iOS SDK 4.2. Thanks and regards, Sebastian Mecklenburg _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: gradient layer always on top?</header>
    <body>Whoops, sure thing. Absent minded I am... Ahh, that was the vital information. I put it into a ViewController and works fine now. Thanks and regards, Sebastian Mecklenburg _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Quartz iOS Performance</header>
    <body>OK, CGContextDrawImage seems to be reduced to a simple memcpy. That sped up things a bit. You are right, that seems logical... All right, it seems that when I keep the graphics really simple I can get some fluid animations now. Nice. Thanks and regards, Sebastian Mecklenburg</body>
  </mail>
  <mail>
    <header>Re: CGShading problem in 10.6.7</header>
    <body>I would check the console and your intermediate steps to see if there are any errors. Beyond that, it may help to post the relevant code from your reduced case. -- David Duncan</body>
  </mail>
  <mail>
    <header>CGShading problem in 10.6.7</header>
    <body>Hello All,</body>
  </mail>
  <mail>
    <header>Re: Quartz iOS Performance</header>
    <body>Try using LittleEndian32 | AlphaPremultipliedFirst instead. What I'm saying is that the CTM doesn't matter, but the pixel size does, which is the same thing your saying. The difference is that I'm saying you don't need a &amp;quot;SetCTM&amp;quot; function to do this - just create a 640x960 image when the view's scale is 2.0 and draw it into the same 320x480 bounds with the default CTM and you'll get exactly what you want. Even more so, I'm saying if you want drawing performance, you want to be inside of -drawRect: as little as possible. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: Quartz iOS Performance</header>
    <body>Hello Sander, I feel with you... Thanks for the CALayer tip, I'll check it out. Thanks and regards, Sebastian Mecklenburg _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>source-over compositing (CIFilter/CALayer)</header>
    <body>I'm trying to &amp;quot;render&amp;quot; a Core Animation layer tree into a CIImage. This works really well, and writing the filters to reproduce the shadow and opacity properties was quite easy, but I just can't figure out how to do the default source over compositing (compositingFilter=nil). Using the CISourceOverCompositing filter produces a quite noticable different result. It just would be nice if this whole thing could be done by just chaining the filters together, because I have a fixed output size for the final image, and this way I wouldn't have to worry about infinite extents in the middle of the process and which parts of an image are needed for a filter (is this even possible?). So, my question is, am I just missing something obvious (newbie here...), or is this just not easy to do? I also tried to write a filter for the compositing, but the best i could come up with looks exactly like the CISourceOverCompositing filter... and trying to get there by tweaking some parameters until it might look right just doesn't feel right ;-) I also tried the CARenderer, but this seems to be quite memory intensive and I couldn't get it to work when there are filters applied to layers. Any help would be appreciated. Thanks. -silvio</body>
  </mail>
  <mail>
    <header>Re: Edge repeating with CI Gaussian Blur</header>
    <body>Indeed, thanks Ryan. That one has been a thorn in my side for ages. I was looking for the solution in completely the wrong place.</body>
  </mail>
  <mail>
    <header>Re: Edge repeating with CI Gaussian Blur</header>
    <body>Ryan Britton wrote .. Blammo -- that's exactly what I need. Thanks Ryan! - ben</body>
  </mail>
  <mail>
    <header>Re: Edge repeating with CI Gaussian Blur</header>
    <body>Chain a image crop patch after the blur, and pass in the image size from the image dimensions patch?</body>
  </mail>
  <mail>
    <header>Re: Edge repeating with CI Gaussian Blur</header>
    <body>Paul Sargent wrote .. Really? I see the same problem in QC... This is one of those things that makes me feel like Core Image is about 90% done. That unfinished 10% means I spend a lot of time creating hacky workarounds. Is there some avenue for feature requests for CI? - ben</body>
  </mail>
  <mail>
    <header>Re: Edge repeating with CI Gaussian Blur</header>
    <body>You need to chain a couple filters together to blur with clamped edges.  The second filter you're looking for is CIAffineClamp: /apple_ref/doc/filter/ci/CIAffineClamp</body>
  </mail>
  <mail>
    <header>Re: Massive contention in multiple threads, in ripc_DrawImage</header>
    <body>I got a private reply that indicated that I might not have been entirely clear in my question. I am  drawing in the background threads. The only "drawing" I'm doing in the background threads is called by CGImageSourceCreateThumbnailAtIndex. Does this contention mean that I can't use Quartz in background threads? Avi I'm writing an app that generates and displays thumbnails of photos. Being the daring guy I am, I'm using ImageKit on the front end, Core Data in the middle, and Quartz on the backend scheduled with NSOperation. Lots of stuff that's new to me. When I unleashed my app on a thousand photos or so over AFP, it locked up hard. Spinning beachball, load averages in the 40s. The machine was barely responsive, but I could get a sample. And what I found was surprising. A few snippets: &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 580 CGImageSourceCreateThumbnailAtIndex &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 548 CGImageCreateCopyWithParameters &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 428 CGContextDrawImage &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 428 ripc_DrawImage &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 428 __spin_lock &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 428 __spin_lock</body>
  </mail>
  <mail>
    <header>Re: Edge repeating with CI Gaussian Blur</header>
    <body>Yep, and you can't get at it because the CISamplers are defined inside the CIFilter. I've met this one a couple of times. What's really nice is when you start working on CoreVideo YUV images and the edges go green (YUV 0-0-0). I've not come up with a nice way of doing that, but I've done the opposite i.e. cropping a pixel off the result, but that's not right in a lot of situations. I think there must be a way of adjusting the samplers (Quartz Composer does it), but I have no idea how.</body>
  </mail>
  <mail>
    <header>Edge repeating with CI Gaussian Blur</header>
    <body>Hi all, Right now when I apply a Core Image Gaussian Blur, the edges go dark/transparent because the filter is not repeating the edge pixels. Is there an built-in way to repeat edges (I saw some edge wrap options in CISampler, but not CIImage or CIGaussianBlur)? Or do I need to do it manually by compositing my CIImage into a larger image, stretching the edges, applying the blur, and then cropping? Thanks! - ben</body>
  </mail>
  <mail>
    <header>kCGTextClip and Rotation</header>
    <body>Hi, I'm trying to follow some of the examples in Programming with Quartz and am trying to draw rotated text with a shading. The gist of the code I am trying out is I would expect the text to be rotated by 45 degrees (the example image in the book is). However, the text ends up being weirdly italicized to the left. As a test I changed the text Drawing mode to kCGTextFillClip. This results in two pieces of text being rendered. One which is filled with the context color(set to red) correctly rotated at 45 degrees. And one with the shading in the weirdly italicized manner. The 'S' in the two pieces of text exactly overlap. Is this a change in behavior with Leopard or am I missing something with the usage of kCGTextClip? Any pointers much appreciated. Thanks Regards George M.P.</body>
  </mail>
  <mail>
    <header>Re: Getting raw PNG pixels</header>
    <body>Its a Copy/Create API, therefore you own the object returned and thus must release it to avoid a memory leak. Copy implies a copy is made for your use =). How that copy is made is an implementation detail, but regardless it is your responsibility. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Getting raw PNG pixels</header>
    <body>By the way... is it necessary to release/free the CFDataRef handle returned by CGDataProviderCopyData() or is it destroyed by CGImageRelease()? The docs do not say anything about it but I'm a little bit confused because the name of the function implies &amp;quot;copy data&amp;quot; which sounds to me as if the function allocates a new memory block, and copies the CGImage's raw pixel data into that block. In that case, I think I'd've to release the data after I'm done with it. But of course, it could also just return a handle to the data inside the CGImageRef.... it's the name &amp;quot;CopyData&amp;quot; that confuses me... Andreas</body>
  </mail>
  <mail>
    <header>Re: Massive contention in multiple threads, in ripc_DrawImage</header>
    <body>AFAIK you should be fine using CGImageSource in background threads, and I haven't had problems with it.  There are a number of spin lock cases in the frameworks that can kill you, though (ATSServer is one that causes me problems with PDF in my thumbnailing).  If that's your problem, it's unlikely that anyone outside of Apple can help you. Have you tried creating the thumbnails yourself instead of letting CGImageSource do it for you?  Create the image, then draw it into an appropriately sized CGBitmapContext.  You might have to watch memory consumption, though; I switched to vImage after finding some images that caused me to run out of address space.  Alternately, you could just pass the URLs or CGImages directly to ImageKit; it's not entirely clear why you'd create a thumbnail, convert to JPEG, then return it. If you want to dumpster-dive some code, I wrote a thumbnail view (since everyone else was doing it :) that's 10.4 compatible, and demonstrates various ways to abuse Quartz from multiple threads.  Graphics people might find my tiling/vImage code good for a laugh, anyway.  Currently available at svn co regards, adam</body>
  </mail>
  <mail>
    <header>Re: Massive contention in multiple threads, in ripc_DrawImage</header>
    <body>I got a private reply that indicated that I might not have been entirely clear in my question. I am  drawing in the background threads. The only &amp;quot;drawing&amp;quot; I&amp;#39;m doing in the background threads is called by CGImageSourceCreateThumbnailAtIndex. Does this contention mean that I can&amp;#39;t use Quartz in background threads? Avi I&amp;#39;m writing an app that generates and displays thumbnails of photos. Being the daring guy I am, I&amp;#39;m using ImageKit on the front end, Core Data in the middle, and Quartz on the backend scheduled with NSOperation. Lots of stuff that&amp;#39;s new to me. When I unleashed my app on a thousand photos or so over AFP, it locked up hard. Spinning beachball, load averages in the 40s. The machine was barely responsive, but I could get a sample. And what I found was surprising. A few snippets: &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 580 CGImageSourceCreateThumbnailAtIndex &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 548 CGImageCreateCopyWithParameters &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 428 CGContextDrawImage &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 428 ripc_DrawImage &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 428 __spin_lock &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 428 __spin_lock</body>
  </mail>
  <mail>
    <header>One-shot CIImage?</header>
    <body>Is it possible to prevent the internal caching of a one-shot CIImage? Since it's only rendered once it shouldn't be cached by the CIContext.</body>
  </mail>
  <mail>
    <header>Re: Getting raw PNG pixels</header>
    <body>The API is exposed in the 10.5 SDK. I see the same thing, but I can't explain it! I can only imagine its a quirk as I can find it in the Xcode's documentation window but not the developer website. CGDataProviderCopyData() vends pixels according to the format of the image as defined by the associated CGImageRef. So whatever the various CGImageGet*() apis tell you the image is vending is what you should get. I think that means with an indexed color space that you should get the color indices, but I haven't tried it myself to verify one way or the other. -- David Duncan Apple DTS Animation and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Re: Getting raw PNG pixels</header>
    <body>Oops, forgot one really important thing: How do I find out the order of the colors in the pixel buffer returned by CGDataProviderCopyData()? I.e. how do I find out whether the pixels are coded as ARGB, BGRA, RGBA etc. I need to know this information in order to interpret the colors correctly... in 32bpp PNG it always seems to be RGBA, but I'm sure other image formats will use a different color encoding. Tks Andreas --</body>
  </mail>
  <mail>
    <header>Re: Getting raw PNG pixels</header>
    <body>Thanks, it's working now but I had to declare the prototype of CGDataProviderCopyData() by myself because it doesn't seem to be in the SDK. Also, the documentation of the function seems to have been removed!? Google still has it in the cache but on Apple's site it is no longer there AFAICS. Also not in the &amp;quot;deprecated&amp;quot; section. Anyway, CGDataProviderCopyData() looks pretty interesting. I'll have to see if I can even use this to solve another old standing problem: When using CGContextDrawImage() to draw a CLUT image to an RGB pixel buffer, I always get some inaccuracies in the pixels because of color conversion done automatically by Quartz. The current solution for this problem is to use the old GetGraphicsImporterForFile() and NewGWorld() stuff to convert the exact pixels of CLUT images into RGB pixel buffers. But CGDataProviderCopyData() probably doesn't help here... even if it returns the CLUT pixels I'd still require a way to get hold of the color lookup table. So I'll probably have to keep the old solution using Graphics Importer and GWorlds for CLUT images. Andreas</body>
  </mail>
  <mail>
    <header>Re: Getting raw PNG pixels</header>
    <body>You can't do it by drawing to a bitmap context, but you can get the pixels themselves (which I presume is what you really want) by using CGDataProviderCopyData() after getting the CGDataProviderRef from the image via CGImageGetDataProvider(). If it is not possible to return data from the data provider passed, it will return NULL (usually only occurs if the data is too large for memory). -- David Duncan Apple DTS Animation and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Getting raw PNG pixels</header>
    <body>Hi, how do I extract the raw RGB pixels out of a PNG picture that has an alpha channel? I'm doing something like this: ---snip--- ---snap--- This code works fine EXCEPT that some pre-alpha mixing seems to occur because I do not arrive at the original non-mixed RGB pixels of the PNG image. It seems that Quartz already does some mixing but I don't want that. I want CGContextDrawImage() to ignore the alpha channel completely and just copy the raw PNG pixels to my bitmap. How do I achieve this? Tks Andreas</body>
  </mail>
  <mail>
    <header>Image Orientation</header>
    <body>Thanks,</body>
  </mail>
  <mail>
    <header>Massive contention in multiple threads, in ripc_DrawImage</header>
    <body>I&amp;#39;m writing an app that generates and displays thumbnails of photos. Being the daring guy I am, I&amp;#39;m using ImageKit on the front end, Core Data in the middle, and Quartz on the backend scheduled with NSOperation. Lots of stuff that&amp;#39;s new to me. When I unleashed my app on a thousand photos or so over AFP, it locked up hard. Spinning beachball, load averages in the 40s. The machine was barely responsive, but I could get a sample. And what I found was surprising. A few snippets: &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 580 CGImageSourceCreateThumbnailAtIndex &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 548 CGImageCreateCopyWithParameters &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 428 CGContextDrawImage &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 428 ripc_DrawImage &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 428 __spin_lock &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 428 __spin_lock</body>
  </mail>
  <mail>
    <header>Re: getting RAW file with No Color treatment</header>
    <body>Thanks for your thoughts on this. We use DCRaw just to help us de- mosaic the 'dozens' of different formats fast. Then we apply our own gamma and color transformation on the 16-bit RGB pixel data. I just was reading more documentation on this and maybe there is solution by creating my own RGB color space where the white and blackpoint are 0. CG_EXTERN CGColorSpaceRef CGColorSpaceCreateCalibratedRGB(const float whitePoint[3], const float blackPoint[3], const float gamma[3], const On Apr 16, 2008, at 8:15 AM, Marc Van Olmen wrote:</body>
  </mail>
  <mail>
    <header>Re: CIFilter outputImage other than NSImage?</header>
    <body>)". &amp;nbsp;Looking deeper, I think that either the documentation is probably not correct. On Apr 16, 2008, at 8:04 PM, Nathan Duran wrote: Douglas,</body>
  </mail>
  <mail>
    <header>Re: CIFilter outputImage other than NSImage?</header>
    <body>Nothing I've seen suggests that it should be possible to pass a CIImage to IKImageView, but I haven't had much luck getting it to do anything it actually does advertise, either, so I'm personally staying away from it until it's ready for prime time. Writing your own view will probably be the simplest course of action.</body>
  </mail>
  <mail>
    <header>Re: getting RAW file with No Color treatment</header>
    <body>involved in displaying a raw image in a way that makes sense to human brains. The actual sensor data is virtually useless unless your intention is to de-mosaic and gamma encode it all yourself, in which case you wouldn't want to use a library/framework which does this for exactly?</body>
  </mail>
  <mail>
    <header>Re: CIFilter outputImage other than NSImage?</header>
    <body>Have you tried to use IKImageView to display a CIImage? I have and so far failure. I have a bug report filed with Apple for investigation of the problem. It appears that IKImageViiew's setImage: imageProperties:dict requires a CGImage dictionary and that dictionary isn't compatible with a CIImage image.</body>
  </mail>
  <mail>
    <header>getting RAW file with No Color treatment</header>
    <body>I'm used to using the DC-RAW library to open a RAW image. I use this library because you have to option to get the pixel data with no color treatment, so no turn off the white balance, etc. Is there an option in ImageIO to do the same? Is this maybe possible to play special tricks with the color profile?</body>
  </mail>
  <mail>
    <header>Re: CIFilter outputImage other than NSImage?</header>
    <body>Douglas, That's perfect. Using -initWithCIImage: I can get at and twiddle the bits. Question 2: What kind of view can I use to display the CIImage on the GUI? Thanks, Russ Russ,</body>
  </mail>
  <mail>
    <header>Re: CIFilter outputImage other than NSImage?</header>
    <body>That's perfect. Using -initWithCIImage: I can get at and twiddle the bits. Question 2: What kind of view can I use to display the CIImage on the GUI? Thanks, Russ</body>
  </mail>
  <mail>
    <header>Re: CIFilter outputImage other than NSImage?</header>
    <body>Have you tried using an NSBItmapImageRep object?  Using - initWithCIImage: should get you to a place where you can start.</body>
  </mail>
  <mail>
    <header>CIFilter outputImage other than NSImage?</header>
    <body>I'm using CIFilters in a 10.5 Cocoa app and things are working fine. But the end result is always a NSImage. Isn't there a new way with Leopard to get something other than NSImage? I need direct programmatic access the final RGB pixel values. Thanks, Russ</body>
  </mail>
  <mail>
    <header>Re: Outlining a path, expanding a path</header>
    <body>Assuming a filled path, the fastest way to do this with vectors is probably to: 1. Test if the point is inside the path (either using the winding number or crossing number algorithms, which correspond to the winding rules that you're used to from Quartz/Cocoa). 2. If it isn't, calculate the minimum distance from the point to the path.  If this is less than some threshold, treat it as if the click was inside the path. Whether that's better than the pixel approach or not I'm not sure.  It does have the disadvantage that it will only work for pure vector shapes, whereas the pixel based approach works even if you apply e.g. Core Image filters over the top. --</body>
  </mail>
  <mail>
    <header>Re: Outlining a path, expanding a path</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Outlining a path, expanding a path</header>
    <body>CGContextReplacePathWithStrokedPath() is probably what you're looking for here.  You can't copy the result out of the context without using unsupported APIs though, and it looks like it doesn't bother clipping the inner segments at line joins, which is fine if you want to fill the result with non-zero-winding, but might not be what you want otherwise. There isn't a function for that. What you would need to do is go through, element by element, generating a new path that was &amp;quot;parallel&amp;quot; to the existing one.  The problems with that are: 1. It's impossible (except in special cases) for a B√©zier curve of degree N to be parallel to another B√©zier curve of the same degree. Therefore you have to approximate, and that may mean generating more than one curve segment for each existing curve segment. 2. Line joins.  At corners, you may have to generate extra segments, and/or deal with clipping of segments where they hit each other. Another issue is that if you're doing that and want to support dashes, you'll have to dash the path yourself. Offsetting paths isn't a trivial problem, unfortunately; even CAD because CAD has much more stringent requirements than a graphics tool, which is I assume what you're working on). --</body>
  </mail>
  <mail>
    <header>Re: Outlining a path, expanding a path</header>
    <body>Hi Glen, -JanakiRam. &amp;nbsp;_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list &amp;nbsp; &amp;nbsp; &amp;nbsp;() Help/Unsubscribe/Update your Subscription: This email sent to</body>
  </mail>
  <mail>
    <header>Re: Strange clipping problem with CIKernel</header>
    <body>Hi Paul, Paul Sargent wrote .. Ha -- that gets me too. :) Yeah, it's weird. 1.0 works, and 0.8 and below all work fine... It's really a problem between about 0.9 and 0.999 There actually is some info in the R&amp;amp;B channels now that I look, but the green is shot to pure black. Hmm... that certainly sounds plausible, but I wonder how I could get around this. I'm not doing anything unusual to draw the images into the NSView, just drawInRect... FWIW, I also tried getting the CIContext explicitly, and then drawing into it, like this: That gave me the same problem... I'll keep poking at it! - ben</body>
  </mail>
  <mail>
    <header>Re: Strange clipping problem with CIKernel</header>
    <body>... but it's fine if alpha = 1.0? Also, are those areas pure black on your system? There's some info in there on the mailed version, but I expect that's jp2 artefacts. I'm wondering if there's a unpremultiply (or something) on the way into the NSView, and that with the premultiply introduces an precision error and overflows one of the channels. It wouldn't take much to wrap that green to black. If that was the case I'd expect to see some remnant of the red and blue channels in the &amp;quot;black&amp;quot; colour values.</body>
  </mail>
  <mail>
    <header>Re: Strange clipping problem with CIKernel</header>
    <body>Whoops -- here's the image</body>
  </mail>
  <mail>
    <header>Strange clipping problem with CIKernel</header>
    <body>Hi all, I'm having a very strange problem with output from my CIKernel. When the alpha is near 1.0 (such as 0.99), I get clipped colors and areas that suddenly drop to black (see attached image). I think I'm doing everything correctly on the CIKernel side, because the kernel works fine in QC. Here it is: kernel vec4 dvmatte(sampler src, float mul, float add) I'm using this in an NSView's drawRect method like so: - (void)drawRect:(NSRect)rect It also happens if I draw an image into the NSView before overlaying the filtered image. Does this sound familiar to anyone? Hopefully I'm just a bonehead and forgot something simple... - ben</body>
  </mail>
  <mail>
    <header>Re: Saving CGContext as png</header>
    <body>On Apr 11, 2008, at 10:33 PM, Roger Herikstad wrote: CGLayer is faster for some things.  The advantage of CGLayer is that you create it with another context as a guide.  The computer will set up that layer so that copying information from the layer to the guide context will be as optimized as it can make it.  As an example the drawing context of the CGLayer might use the same color space as the guide context so that no color conversion need be done when drawing the CGLayer in the guide context.  Or, perhaps the guide context is a PDF context, in which case the CGLayer can store off a kind of display list that will end up being written into the PDF context once regardless of how many times you draw that layer. The exact nature of the optimization offered by a CGLayer is opaque to your application. CGLayers are very handy if you have a &amp;quot;rubber stamp&amp;quot; type image that you plan to draw repeatedly in the same context.  A brush image in a drawing application is a good example.  As the user drags the mouse across the screen, the same brush image may be stamped repeatedly into the drawing.  A CGLayer would help make sure that that stamping is done efficiently. Where CGLayers can be a problem is if you need to draw the graphic in different contexts.  For example, if you create the CGLayer to draw into a bitmap context, then try and use that same layer in a PDF context, you may incur quite a lot of overhead. You may not even get the results you expect.  For example the CGLayer could, potentially, draw into the PDF as a bitmap, rather than as a nice vector image which may not be desirable. In your case, the background sounds like a good choice for a layer because you plan to draw the background (unchanged) repeatedly.  You could create your bitmap context, create a layer for that bitmap context, draw the background into the layer.  Then, every time you want to redraw the background, draw the layer into the bitmap context. (then draw your foreground image on top of it again). What a Layer doesn't give you is access to the backing store... the drawing surface of the layer.  That surface may be a bitmap in main memory, a pixel buffer in your video card's VRAM, a PDF or OpenGL display list... any of a number of things.  You wouldn't have the opportunity to grab that gizmo and create an image from it.</body>
  </mail>
  <mail>
    <header>Re: Copying a JPEG image with CGImage changes file size</header>
    <body>On Fri, 11 Apr 2008 08:20:44 -0700, Actually, libexiv2 is dual-licensed (GPL/commercial) and the commercial license fee is very reasonably priced. I know since I use libexiv2 extensively in a Mac-only commercial application. Libexiv2 does not support as many file formats as, say, exiftool, but at least it's a C++ library, while exiftool is a collection of Perl modules.  Both packages can edit metadata (Exif/IPTC/XMP) in a JPEG file without disturbing the image data. -- marco -- It's not the data universe only, it's human conversation. They want to turn it into a one-way flow that they have entirely monetized. I look at the collective human mind as a kind of ecosystem. They want to clear cut it. They want to go into the rainforest of human thought and mow the thing down.</body>
  </mail>
  <mail>
    <header>Re: Saving CGContext as png</header>
    <body>Hi, Thanks a heap for that explanation. I love the metaphor! Just shows I have lots to learn yet. I was considering the CGBitmatContext as my main drawing surface as I set out on this project, but after reading some advice in Apple's Quartz 2D drawing guide about how you should consider CGLayer instead, as this would be faster, I decided to go with that. Now, I'm thinking that if I have the save operation run in a different thread, maybe I can get by with running the drawing commands twice. I don't know how much slower using CGBitmapContext will be, though, compared to CGLayer in my situation. I guess I'll have to try it out first. Thanks again! ~ Roger</body>
  </mail>
  <mail>
    <header>Re: Copying a JPEG image with CGImage changes file size</header>
    <body>It is, but being generous with the feedback helps everyone involved. -nvw</body>
  </mail>
  <mail>
    <header>Re: Copying a JPEG image with CGImage changes file size</header>
    <body>Thanks for the confirmation.  I can give up trying now... and thanks for the suggestions. Have done. That makes one bug report, one enhanhancement request and feedback on example errors in documention, all in my first week of mac programming :-)  I hope that is not a portent of things to come... Hugo</body>
  </mail>
  <mail>
    <header>Re: Any way to get number of entries for	CGGetDisplayTransferByTable()?</header>
    <body>-- David Duncan</body>
  </mail>
  <mail>
    <header>Re: CGGetDisplayTransferByFormula() fails first time on	MBP	built-in display</header>
    <body>There's also CGDisplayRestoreColorSyncSettings that sets all displays back to their normal gamma tables. Another way you might achieve something like this is with a transparent overlay window over each monitor that is entirely red with alpha faded from 0 to the desired opaqueness, then back to 0 when you are finished. Set the window level to something like kCGAssistiveTechHighWindowLevelKey so it's above everything else and make it ignoreMouseEvents.</body>
  </mail>
  <mail>
    <header>Any way to get number of entries for CGGetDisplayTransferByTable()?</header>
    <body>Other CG routines that take a pointer to a buffer let you pass NULL to get the required size of the buffer. CGGetDisplayTransferByTable() does not seem to do this. Is there a way to get the size of the table so one may know how big a buffer to supply? TIA, -- Rick</body>
  </mail>
  <mail>
    <header>Re: CGGetDisplayTransferByFormula() fails first time on MBP	built-in display</header>
    <body>On May 1, 2008, at 18:13:32, David Duncan wrote: I'm not sure what you mean. Just not bother reading the default parameters or restoring them? That might work, but would prevent me from restoring them with a click of the button... However, I can store the table values if the formula fails. I think I'll try that. Thanks! -- Rick</body>
  </mail>
  <mail>
    <header>Re: CGGetDisplayTransferByFormula() fails first time on MBP built-in display</header>
    <body>I would have to check, but I think that TransferByFormula will fail if the previous gamma table was set via TransferByTable. However, I would probably instead have your agent launch, set the transfer parameters, and then quit to let the system restore them automatically instead. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>CGGetDisplayTransferByFormula() fails first time on MBP built-in	display</header>
    <body>I'm noticing something interesting when calling CGGetDisplayTransferByFormula(). It works fine on the external monitor, but the first time I attempt to call it on the built-in monitor on my MacBook Pro 15&amp;quot; Penryn-based computer, it returns kCGErrorNoneAvailable. If I then Set the values on the display (with some rather arbitrary data), subsequent attempts to get the data succeed. -- Rick</body>
  </mail>
  <mail>
    <header>LED backlight control?</header>
    <body>Hi. I was posting questions on Carbon Dev, but this is probably more appropriate. I'm interested in developing a utility that allows the user to dim the screen and change the output to red, for very dark area use (i.e. astronomy). I've figured out how to use CGSetDisplayTransferByFormula() to make the screen red, and I can even make it fairly dark, but it's still far too bright. While CCFL backlights can only dim so far, LED backlights can theoretically dim all the way to &amp;quot;off.&amp;quot; However, the standard backlight controls don't go anywhere near &amp;quot;off&amp;quot; before turning the backlight all the way off. Is there any way (perhaps even low-level communications with the hardware, such as I've seen to get the light sensor and accelerometer readings) to programmatically set the backlight level very low? -- Rick</body>
  </mail>
  <mail>
    <header>Re: Core Animation performance</header>
    <body>It may not fit the bill for you, but I recently discovered CATiledLayer (through a DTS event), and does it work great! If you can live with equally sized tiles, you can use a CALayer delegate to draw whatever you want with the confines of the tile. The beauty of this solution is that CATiledLayer does not cache the whole layer; it only caches what is visible. I have a 2G image that I constructed for my test, threw the CATiled Layer in a NSScrollView, and watched it work! What you see when you scroll is that it first draws background, then calls your delegate to fill tiles in. After they go out of view (its a bit more complicated) they get tossed. I watched the memory usage when I scrolled and it hardly moved, even with this virtual 2G image! I have been waiting for something like this for years, to get away from having to manage my own pool of views or layers. David</body>
  </mail>
  <mail>
    <header>Re: Core Animation performance</header>
    <body>I'd like to avoid having to go to some kind of layer-paging mechanism if I can. Perhaps I'm doing something silly. Does anyone else have any data on trying to draw ~1500 layers?</body>
  </mail>
  <mail>
    <header>Re: CGImageCreateWithMask returns nil</header>
    <body>On Sun, Jul 31, 2011 at 9:33 PM, Michael Crawford &amp;quot;If the mask is an image, it must be in the DeviceGray color space, must not have an alpha component, and may not itself be masked by an Since you're loading the image from a file, it's probably not going to be in DeviceGray color space. So to use this function you'll need to create a DeviceGray bitmap context and draw your rounded rect into it. But there are better and easier ways described in the WWDC video I mentioned. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: CGImageCreateWithMask returns nil</header>
    <body>On Sun, Jul 31, 2011 at 9:08 PM, Michael Crawford I'm trying to mask one image using another on the iPhone.</body>
  </mail>
  <mail>
    <header>Re: CGImageCreateWithMask returns nil</header>
    <body>On Sun, Jul 31, 2011 at 9:08 PM, Michael Crawford Watch the WWDC 2011 video for Session 121 ‚Äî Understanding UIKit Rendering, and consider if you could take a better approach. What you've described requires a lot of CPU-based rendering. It's impossible to help with questions like this without seeing code. So please post your exact source code. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>CGImageCreateWithMask returns nil</header>
    <body>I'm trying to mask one image using another on the iPhone. I'm using a UITableView with the grouped style.  I want to substitute the background of a custom cell with one containing an image in the far left with the top and bottom left corners of the image rounded off to match the shape of the table-view cell.  In order to accomplish this I took an image of the rounded table-view cell with a size of 300x76 points and cropped it to a size of 68x76 points, starting from 0x0 in the upper-right corner.  I want resulting cropped image to be my mask. I then take a colored image with four channels (RGBA), which is also a sized to 68x76 but without rounded corners, and I pass both this image and my cropped mask image to CGImageCreateMaskWith.  The result is always nil and I'm not sure why.  Actually I've never tried this before so I'm not even sure my approach is sound. How can I create an image that is a combination of the rounded table-view cell and a non-rouned image clipped to have is left corners rounded to match the table-view cell? If I simply add my 68x76 image to table-view cell as a subview the corners on the left are not clipped and rounded.  if a simply draw a bitmap with the rounded table-view cell and then draw the 68x76 image on top of it, again there is no clipping and the left side of the cell is no longer rounded. -Michael</body>
  </mail>
  <mail>
    <header>HIViewDrawCGImage crash on 10.7</header>
    <body>I know this is a difficult to impossible issue to debug with this amount of information but I was wondering if any Apple guys know what would cause HIViewDrawCGImage to crash on 10.7 &amp;quot;sometimes&amp;quot; when drawing .png's. Is there any thing I could look for or other vague suggestions even? Thanks. Thread 0:: Dispatch queue: com.apple.main-thread 0   libsystem_c.dylib             	0x90b43c1c malloc_make_nonpurgeable + 1 1   com.apple.ImageIO.framework   	0x92ff0eb5 imageio_make_nonpurgeable + 17 2   libcache.dylib                	0x9003617e cache_get_and_retain + 346 3   com.apple.ImageIO.framework   	0x92ff0069 CGImageReadGetCachedImageBlockData + 110 4   com.apple.ImageIO.framework   	0x92fefff3 CGImageReadSessionGetCachedImageBlockData + 235 5   com.apple.ImageIO.framework   	0x92fefd29 ImageIOGetCachedBlocks + 132 6   com.apple.ImageIO.framework   	0x92ff4573 copyImageBlockSetPNG + 676 7   com.apple.ImageIO.framework   	0x92fee3e6 ImageProviderCopyImageBlockSetCallback + 440 8   com.apple.CoreGraphics        	0x9b781c4c CGImageProviderCopyImageBlockSetWithOptions + 173 9   com.apple.CoreGraphics        	0x9b781b99 CGImageProviderCopyImageBlockSet + 69 10  com.apple.CoreGraphics        	0x9b7816fc img_blocks_create + 332 11  com.apple.CoreGraphics        	0x9b7afcb6 img_blocks_extent + 73 12  com.apple.CoreGraphics        	0x9b7afc61 img_extent + 28 13  com.apple.CoreGraphics        	0x9b7afc61 img_extent + 28 14  com.apple.CoreGraphics        	0x9b6f8923 img_data_lock + 7949 15  com.apple.CoreGraphics        	0x9b780b10 CGSImageDataLock + 165 16  libRIP.A.dylib                	0x94222989 ripc_AcquireImage + 1901 17  libRIP.A.dylib                	0x942211db ripc_DrawImage + 951 18  com.apple.CoreGraphics        	0x9b780775 CGContextDrawImage + 437 19  com.apple.HIToolbox           	0x94ed9952 HIViewDrawCGImage + 177 Regards, Ryan Joseph</body>
  </mail>
  <mail>
    <header>Re: Computing bounds</header>
    <body>Oh! I stupidly didn't notice that call doesn't need a context!</body>
  </mail>
  <mail>
    <header>Re: Computing bounds</header>
    <body>Micheal's recommendation is to use an API that doesn't require a CGContext at all, but gives you a reasonable approximation of size. That said, if you want a better approximation one solution is to create  1x1 bitmap context, set all of your style attributes correctly, then use CGContextReplacePathWithStrokedPath and you'll get a new path that if filled would match the stroke of that path. If you get the bounds of that, it should closely match the area needed to draw the path (regardless of stroke or fill). -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: Computing bounds</header>
    <body>Since I'm not being provided a context by the OS for drawing, I have to create one, right? I think (although I haven't double-checked) that I need to specify a size when doing so. But since I don't know the bounds before hand, I have to choose some arbitrary size (already kinda gross). If I draw a path into a context that's not big enough for that path, and then ask for the bounding box, will that box include the full path, or will it only go as far as the bounds of the context? This discussion may be moot, because the process of drawing my parts creates and finishes lots of paths, as well as text, and I don't think there's any easy way to add those all to a single path. I'd have to create paths for each, get their bounds, and accumulate those; doesn't sound any easier than what I was doing. Thanks, though. -- Rick</body>
  </mail>
  <mail>
    <header>Re: Computing bounds</header>
    <body>Sent from my iPhone CGPathGetBoundingBox ( CGPathRef path</body>
  </mail>
  <mail>
    <header>Re: Computing bounds</header>
    <body>Sent from my iPhone CGPathGetBoundingBox ( CGPathRef path Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Computing bounds</header>
    <body>CGPathGetBoundingBox ( CGPathRef path Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Computing bounds</header>
    <body>Without only as much as you've given us it's a hard question to answer definitely.  My opinion, however, is that you should be able to compute the bounds without having a drawing context. The context is a view-level entity (in the Model-View-Controller sense).  As such, it is responsible for giving you one way of looking at your model objects... the &amp;quot;parts&amp;quot; you are drawing.  Your parts should live in a model-level &amp;quot;world&amp;quot; and your model should store the location and size of all the objects in that world.  You should be able to compute the bounds of those objects in that world space and those bounds should be valid irrespective of how they are drawn. This is, of course, based on a LARGE number of assumptions on my part about what your data looks like, and how you are treating it. In general, however, my opinion is that you should be able to tell where things exist in your model and later convert them to whatever view you like. Scott</body>
  </mail>
  <mail>
    <header>Computing bounds</header>
    <body>I'm working on a CAD app of sorts. It draws objects that consist of simple geometric primitives, like a rectangle, some short line segments, and some text labels. The user can build up these parts in a part editor mode. The part definition is stored as a set of objects representing the geometric primitives. Each has a CGPoint position which is relative to the part's origin. A separate class called a PartRenderer takes a part definition object graph, and a position, and uses Quartz and Core Text to draw it all. One of the things I need to do is compute the bounds of the part in part-local coordinates. I use these bounds to draw an icon for the part for display in the part library. I also use the bounds to compute which parts are in view and which are outside the canvas. It's hard to compute the bounds, though, without a graphics context. This suggests deferring computing bounds until the part has been drawn once, but I'm not sure I can draw without knowing the bounds. An additional complication is that I want to draw the parts in a selected state with a bit of glow, which means pixels outside the non-selected bounds. Ideally, my bounds computation would include all that. Right now, I've modified my PartRenderer to have two modes: drawing or computing bounds. In the latter mode, it avoids any drawing it doesn't have to do as part of the bounds computation. But it tends to do things like translating the current context (supplied in the drawing call) to the location of sub-geometry. What recommendations can the list give me on solving this problem? Should it be possible for me to compute the bounds without a graphics context, or should I just create one for the computation? Thanks! -- Rick</body>
  </mail>
  <mail>
    <header>Re: filters that are not isomorphic</header>
    <body>BTW, I have read the docs on core image filters. I think what I need to do is set the DoD. But, the docs are not very clear about whether a DoD should be used for this purpose. I haven't found sample code for any isomorphic filters.</body>
  </mail>
  <mail>
    <header>filters that are not isomorphic</header>
    <body>Where would I find docs on creating a CIFilter (wrapped as an Image Unit) which outputs a CIImage that is a different size that it's input CIImage ? Note, this is not a simple scaling issue. So, CIAffineTransform or CICrop are not suitable. The algorithm actually generates a result that has a different size than it's input. I need to write an image unit that implements someone else's specific algorithm. -- Brian</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>David, Chris, Thanks a lot ÂèëËá™ÊàëÁöÑ iPad Âú® 2011-7-9Ôºå11:56ÔºåDavid Duncan &amp;lt;email@hidden&amp;gt; ÂÜôÈÅìÔºö</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>Its part of the API. Only the front most application can use OpenGL ES resources, because the GPU powers the UI on iOS (via Core Animation). -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>Sean, Sorry, but your app will not be able to access OpenGL resources while it is in the background. Doing so will cause your app to crash. The actual reason may be due to thread contention or it may be resource contention - both with regard to other OpenGL activity from the active app. I have experienced this directly in iOS. Cheers, Chris 2011/7/8 Sean &amp;lt;email@hidden&amp;gt;:</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>Thanks Kyle, Chris. I just wanna capture the screen when my application is in the background. Can OpenGL handle it in iOS? Best regards, Sean ÂèëËá™ÊàëÁöÑ iPad Âú® 2011-7-9Ôºå2:27ÔºåKyle Sluder &amp;lt;email@hidden&amp;gt; ÂÜôÈÅìÔºö</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>That's a cool example! I'll have to try that and see if it captures from windows such as KeyNote, which is an OpenGL implementation itself. Thanks for the link.</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>I'm pretty sure there are better ways to do this, like OpenGLCaptureToMovie: --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>On iOS, there is nothing stopping an app from sharing content while backgrounded. There are some things that you can't do while backgrounded, such as make any OpenGL calls (because the foreground app might be using OpenGL in which case your app will get terminated). I still not sure what you mean by &amp;quot;share&amp;quot;, but if your app is in charge of the content, then it can definitely encode and transmit that content via the network, even while backgrounded. -Chris</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>On the Mac, you can do some elaborate things to get most of the content of a display. You can write your own IOFrameBuffer kernel extension and create a virtual display. Then you register for changes on that display so that you know when it's updated (and what rectangles changed). Next you grab the changed rectangles directly from the frame buffer memory that your IOFrameBuffer allocated. This is all non-trivial, but it works very well. The only problem is that OpenGL rendered views will not be captured this way because the IOFrameBuffer is not an accelerated driver. There is a call in the display system to get an image of a display's view. It is public API. See // CGDisplayCreateImage - Returns an image containing the contents of the specified display. CGImageRef CGDisplayCreateImage( CGDirectDisplayID displayID Since I use the IOFrameBuffer method, I have not tried this API, but it looks promising. It probably implements DRM as well and also may not capture the OpenGL views (but it might!). -Chris</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>Hi David, Thanks for detail explanation. Rumor has it that skype for iphone 4 can share the screen content when it is in the background. I  don't use it so far, so I am not sure if it is real or not. Just wonder if you've tried that? Best regards, Sean</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>It was also a confusion between Mac OS X and iOS. On iOS only one application can be onscreen at a time (and typically the applications that are offscreen don't even have content and don't execute). Mac OS X is quite obviously different. I know of nothing different on Mac OS X. My comments are restricted to iOS, which is also the only place where UIGetScreenImage exists. -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>Hi Glenn, I don't know how VNC, I mean the remote control software, works. I just guess without the API to let app capture the screen &amp;quot;held by another application&amp;quot;, it is impossible to implement VNC functionalities in iPad. Best regards, Sean</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>I just asked: I got it now.  (Clearly I'm not a morning person.) --Glenn</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>I'm confused. David wrote that the app cannot capture the screen *held by another application* while the app is in the background.  Presumably that's for privacy/security reasons.  (I sure wouldn't want an app to capture my screen while I'm doing online banking; the recipient of that image would bust a gut laughing. ;) So why would that eliminate VNC/podcast apps? --Glenn</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>David, thanks. Then it is obviously that there should be NO VNC like application for iPad which can share the iPad screen to other devices. Right?</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>This is an iOS question. AFAIK nothing has changed with respect to this on Mac OS X. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: Is UIGetScreenImage banned by apple?</header>
    <body>David, I presume this is for DRM reasons. Does this mean applications for generating pod casts and other screen capture features are going to disappear? Or will Apple put the hooks in place to only block DRM protected elements in the final composited display when being captured? Greg</body>
  </mail>
  <mail>
    <header>Re: Goofy behavior with CICheckerboardGenerator filter and	transparency</header>
    <body>Why don't you use the opacity from the color panel ?</body>
  </mail>
  <mail>
    <header>Goofy behavior with CICheckerboardGenerator filter and transparency</header>
    <body>Didn't have much luck with this question over on cocoa-dev, but this seems like a more logical place to ask anyway, so here's hoping! ----- I have an NSImageView subclass that I'm filling with a checkerboard pattern, using this method: - (void)drawRect:(NSRect)aRect nsColor = [[[prefsWdw gridColor1] color] inputColor0 = [CIColor colorWithRed:[nsColor redComponent] green:[nsColor greenComponent] blue:[nsColor blueComponent] nsColor = [[[prefsWdw gridColor2] color] inputColor1 = [CIColor colorWithRed:[nsColor redComponent] green:[nsColor greenComponent] blue:[nsColor blueComponent] inputWidth = [NSNumber numberWithFloat:[[prefsWdw gridSlider] CIFilter *theFilter = [CIFilter [context drawImage:transparentBG atPoint:CGPointZero fromRect:CGRectMake(aRect.origin.x, aRect.origin.y, When I adjust the alpha value of the color of the checkerboard pattern, I get really weird behavior; it's kind of hard to describe, computing is done in this routine, there's no outside stuff being done beyond adjusting the sliders and colors.  I also tried NSCalibratedRGBColorSpace, and it behaved the same way. Thanks! randy</body>
  </mail>
  <mail>
    <header>Help using CISpotlight</header>
    <body>Hi, Does anybody know if there is any sample code to using the CIFilter CISpotlight. When I use the filter in Quartz Composer, I get a nice spotlight effect. But, setting the same values in code results in a totally washed out image with some patches of bright yellow, blue etc. It's a very strange effect. [spotlightFilter setValue: inputLightPointsAt [spotlightFilter setValue: inputConcentration CIImage *spotlightOutput = [spotlightFilter I guess I am not passing the parameters correctly, but, I'm not sure what's wrong. In CIFilter documentation for CISpotlight I see inputBrightness An NSNumber class whose attribute type is CIAttributeTypeDistance and whose display name is Brightness. Default value: 3.00 Minimum: 0.00 Maximum: 0.00 Slider minimum: 0.00 Slider maximum: 10.00 Identity: 1.00 Why is minimum and maximum both 0.0? Does slider values refer to the slider in QuartzComposer? And what does identity mean? Do I multiply/divide slider value I set in Quartz Composer with the Identity to arrive at the value I should set in code. It probably doesn't matter for inputBrightness but inputConcentration has identity 20 and others null. I have tried various combinations of number, multiplying and dividing values in QuartzComposer, to no avail. I'm a  little confused with how to use these params and would appreciate any help/pointers. Thanks George M.P.</body>
  </mail>
  <mail>
    <header>Re: Clear a path?</header>
    <body>There is, but to use it effectively you need to set the blend mode to Copy. Of course you can do that even more easily by setting the blend mode to Clear and then drawing in any color. But these modes (and other Porter-Duff modes) are only available on 10.5 or later and only work reliably in a bitmapped context. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Clear a path?</header>
    <body>You would call CGContextClearRect(). And yes, it would only work reliably in bitmapped contexts (it will likely work in a window context, but probably not have the effect you expect in a PDF or Printing context for example). -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Clear a path?</header>
    <body>On Jul 10, 2008, at 8:44 AM, James W. Walker wrote: Isn't there a clearColor in CoreGraphics as there is with Cocoa?</body>
  </mail>
  <mail>
    <header>Re: QTMovie memory usage</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Clear a path?</header>
    <body>On Jul 9, 2008, at 8:12 PM, Nick Nallick wrote: What would I fill it with that would clear the alpha channel?</body>
  </mail>
  <mail>
    <header>Re: Clear a path?</header>
    <body>On Jul 9, 2008, at 8:06 PM, James Walker wrote: You could clip to the path then fill the path's bounding box. Nick</body>
  </mail>
  <mail>
    <header>Clear a path?</header>
    <body>Is there any way to clear the interior of a path?  Like CGContextClearRect, but with a shape other than a rectangle.  It would be fine if it only worked for bitmap contexts. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>ImageIO writes JPEG2000 images in JP2 but not JPX flavor?</header>
    <body>Can ImageIO output images in the JPX flavor of JPEG2000? I can only seem to get JP2 streams out of my bitmaps. Prior to the fragment below I created a CFMutableDictionaryRef and added the color profile type kCGImagePropertyProfileName thinking that might cause ImageIO to generate the extended JPX flavor of JPEG2000. It did not. Thanks, Brian Hewitt</body>
  </mail>
  <mail>
    <header>Re: Getting rid of gamma correction in a custom CoreImage kernel</header>
    <body>I now see that I was wrongly assuming only the mantissa would be used in the 0...1 range. Thanks for the pointer. On 09.07.2008, at 15:46, Graeme Gill wrote:</body>
  </mail>
  <mail>
    <header>Re: Getting rid of gamma correction in a custom CoreImage kernel</header>
    <body>Think about it though. It's linear light, so you need extra precision at the dark end, which is what gamma encoding gives you, and floating point is effectively logarithmic encoding, so it's perfect for this purpose, giving very fine steps near black, and coarser steps near white where it matters less. In fact, it's even better than that, it's high dynamic range capable, making it superior to 10 bit gamma encoding. for the EXR format, which uses 3 16 bit &amp;quot;half&amp;quot; float numbers. Graeme Gill.</body>
  </mail>
  <mail>
    <header>Re: Getting rid of gamma correction in a custom CoreImage kernel</header>
    <body>Let me clarify a few problems I see with the current CI implementation - one last time:) 1) What format is CI using for intermediary renders: int 16, float 16, float 32? Last time I checked it was float 16, but I see no way to configure this. Float 16 only has 11 bits of precision, so working on a linear (gamma 1.0) space at float 16 looks like a very bad idea to me. How can I output 10bit gamma encoded images from a linear 11bit source? And what about using CI to edit a 16 bit (gamma encoded!) Tiff? You see where I'm heading... 2) For the matrix color transforms I assume CI is doing all right, but what about CLUT profiles? I once profiled my app in and saw that a 16bit CLUT profile was being uploaded as a 8bit 3D texture for the transforms. This may be fast and good enough for presentation, not for rendering in high quality. How is CI interpolating, trilinear or higher quality tetrahedral? The CLUT transforms made by CI are not documented at all. 3) I guess this would be true if all filters that ship with CI where able to handle colors outside the 0...1 range. Do the filters? If not, we not only do not have infinite gamut, but using the standard CI working space are limited to the rather small gamut of GenericRGB (roughly equivalent to sRGB). 4) At least one filter I used to use is not really color managed: CISharpenLuminance. The matrix to get Luminance from RGB is defined by the working RGB space, but looking at Quartz Debug I always read luminance [0.299 0.587 0.114] regardless of the working color space. Was this hardcoded inside the filter? If so, we shouldn't even be given the option to set the working space. What about other filters? As nice as CI is compared to GLSL, the uncertainty about how things are done in CI makes it (in my opinion) useless for high quality work. What I would like to see: 1) complete documentation 2) filters should actually read parameters form the current color space, not assume GenericRGBLinear 3) more flexible design in which color transforms are filters themselves so one can easily use the ones supplied or new ones 4) not constrained to RGB color spaces: there's a lot that can be done in YCbCr, Lab,... also, not every operation looks best at a linear space 5) some GLSL goodies like multiple rendering targets, more flexible branching Mark</body>
  </mail>
  <mail>
    <header>Re: Getting rid of gamma correction in a custom CoreImage kernel</header>
    <body>Don't get me wrong guys. I've certainly had my moments of being completely frustrated with this stuff too, and what I'd really like is paths in and out of CI that can have colour management turned on or off. Also, the whole black box nature of CI has made me scream at times and just throw it out. That said, I do think that having colour management be part of the path, and having the shaders defined as being in linear space to be good and &amp;quot;the right thing&amp;quot;. It's just that the right thing can get in the way when it's part of a system that isn't entirely consistent and transparent. I was answering a question that (as it appeared to me) was &amp;quot;0.5 != 0.5. WTF! Switch this off!&amp;quot;, and I was trying to say that maybe that wasn't the right way of looking at it.</body>
  </mail>
  <mail>
    <header>Re: Getting rid of gamma correction in a custom CoreImage kernel</header>
    <body>I agree here too. This uncertainty of what/how CI is doing color conversions and other transformation makes it quite useless for many professional developers. At some point I decided I would rather re- implement all the color management stuff I needed in GLSL than use CI, this is what I've done. Maybe CI is just intended to add eye-candy to apps, like the nice effect when you add a dashboard widget to the desktop. Ridiculous. This is a trend that can be seen on other Apple apis, eg QTKit: it allows everyone to add basic video capture to any app, but is useless to anybody needing a bit more. And also, its buggy. On 08.07.2008, at 04:48, ben syverson wrote:</body>
  </mail>
  <mail>
    <header>Re: Getting rid of gamma correction in a custom CoreImage kernel</header>
    <body>I agree completely. God only knows how this double color conversion is implemented, and whether or not it's how I would do it (probably not). Is it done using texture LUTs or procedurally? How does it handle over- range (beyond 0-1) values? Oh well, you're not supposed to think about such things. Because it is so ill-defined, and because there are no exposed ways of controlling the color management behavior, you wind up having to do ridiculously convoluted things to get the simple results you expect. Example? I have an application that has a color picker, which is passed to a CI Kernel. However, it's not passed directly as a color, but as a vector, since I need to do some simple math to the color before it gets to the Kernel, and the values wind up going over-range. Sounds simple enough ‚Äî just apply whatever color transformation happens to images/colors to the kernel (in other words, convert it to the reference RGB Linear profile). Amazingly, there is NO color conversion method to call. I literally had to take a whole day to really, fully believe this. The only solution is to create a 1x1 pixel bitmap context, assign it the correct Linear profile, draw my color (with the display profile associated) into the context, and then read it back out again manually. I am still amazed. THEN I can pass this as a vector to my CI Kernel and get (very very close, but never exactly) the same color as I see from the picker. In other words, in order to display the color from a color picker via a CI Kernel (passed in as a vector): 1. The picked color needs to be drawn into a 1x1 bitmap context with the right Linear Profile 2. The value is read back from the context 3. This value is passed to the Kernel as a vector 4. The input is converted (somehow...) to Linear RGB 5. The converted color picker value is used in the kernel 6. The output is converted back (somehow...) to an output colorspace. This is in stark contrast to how I would do this with an ARB Fragment Program, which would look something like this: 1. The picked color is passed to the ARB 2. The color value is used in the ARB It becomes even more ridiculous if you have multiple inputs, multiple Kernels to run, etc. For example, if you have to apply two Kernels in a row which each have 8 image inputs, it means you're doing 32 color conversions (16 in, 16 out). If the Kernels don't care about color management (which is often in my case), those 32 unnecessary color management routines really drag the whole thing down. Now, don't get me wrong. I'm all for doing compositing operations in Linear RGB, and was an early adopter of the technique, lobbying along with Stu Maschwitz for Adobe to support it in After Effects, long before his &amp;quot;ProLost&amp;quot; revolution got its footing. However, I am not a clueless end-user. I am a software developer that has spent literally months poring over the pages of documents from the ITU, IEC, and CIE. I have implemented back-and-forth conversion routines for XYZ, xyY, sRGB, L*a*b*, HSV, and Cineon/DPX, all in very highly optimized ARBfp. I know better than anyone when and where I want these conversions applied within my applications. The fact that I can't choose when or how the conversion happens (even if it means overriding a default) is TOTALLY ridiculous, and means CI is not viable for any of my video software, which is very speed-sensitive. It's like having a copyeditor who INSISTS on translating everything I write into French while he edits, and then finally translates it all back to English for publication. There is obviously some way to apply CI Kernels without color transformation, since, as vade writes, it's done in QC. Why it's not exposed to developers is beyond me (unless it's a quadruple conversion‚Äî input-&amp;gt;inverseInputTransform-&amp;gt;inputTranform-&amp;gt;CIKernel-&amp;gt;outputTranform- On 7 juil. 08, at 20:04, vade wrote:</body>
  </mail>
  <mail>
    <header>Re: Getting rid of gamma correction in a custom CoreImage kernel</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Getting rid of gamma correction in a custom CoreImage kernel</header>
    <body>I&amp;#8217;ve found that sampled pixels in a custom CoreImage kernels have their color values shifted on a curve. They&amp;#8217;re raised to the power of 1.8 when entering the kernel, and then getting the 1.8-root at the end of the kernel.&amp;nbsp; For example, if I have a pixel with a blue value of 127 in 8-bit color, I expect that the blue value in the kernel will be 0.5, but instead it&amp;#8217;s 0.287 inside the kernel.&amp;nbsp; Likewise, if I hardcode the kernel to always use 0.5 for the blue value of returned pixels, I expect &amp;nbsp;the resulting image to have blue values of 127, but instead they are 173. &amp;nbsp; I assume this comes from gamma correction, and it is different when I run with my monitor set to a different gamma.&amp;nbsp; How can I prevent I&amp;#8217;ve tried creating the CIImage I use as the inputImage using the color space in hopes of setting the gamma to 1.0 and thus preventing gamma correction, but it still happens.&amp;nbsp; I&amp;#8217;ve found that I can undo the correction by doing the inverse of the correction myself, but I need a way to get the current gamma rather than hardcode it at 1.8.&amp;nbsp; I&amp;#8217;ve tried &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;span style='font-family: where buffer is a byte array allocated according to the size returned by a previous call the CMGetProfileElement, but I just end up getting 48 bytes that I don&amp;#8217;t know what to do with.&amp;nbsp; The ICC Profile Format Spec is no help. &amp;nbsp;How can I either turn off that automatic gamma correction in CoreImage, or get the current gamma programmatically so that I can manually undo the gamma correction in the kernel?&amp;nbsp; Thanks for your help.</body>
  </mail>
  <mail>
    <header>Re: Extracting text objects from PDF page</header>
    <body>Thanks David, that gets me the stream's length and filter name. What I was looking for though is the string of characters that are the contents of the Tj operator, and I just realised how to extract them from the scanner. Really simple, now that I begin to get an inkling of how it works: void operator_Tj(CGPDFScannerRef scanner, void *info) // The Tj operator takes a string. Pop the string off the stack. // Convert the PDF string to an NSString and pass it back up. ((ScannedPageData *)info)-&amp;gt;string = (NSString ----------------------------------------------------------- And could you keep your heart in wonder at the daily miracles of your life, your pain would not seem less wondrous than your joy. --Kahlil Gibran -----------------------------------------------------------</body>
  </mail>
  <mail>
    <header>Re: Extracting text objects from PDF page</header>
    <body>It looks like you want to write a CGPDFDictionaryApplierFunction and call CGPDFDictionaryApplyFunction with it. This will iterate the dictionary calling the applier function for each key &amp;amp; object in it. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Extracting text objects from PDF page</header>
    <body>I am trying to parse text on a CGPDF page. I am for the moment mainly interested in retrieving the characters of individual text objects. For that, I catch the PDF operator Tj in a callback. I know how to get the content stream for that callback, but then I do not know to access the contents of the stream. When I try getting the component streams I get an array with 1 stream so I retrieve it, and try to extract some data from it: void operator_Tj(CGPDFScannerRef scanner, void *info) CGPDFContentStreamRef contentStream = This gets me a dictionary with two objects, but I don't know how to access them. A CGPDFDictionary don't allow examining its contents. I can't check what keys it contains. Maybe the above is not correct to start with anyway. I don't know. Anything to help me along the way much appreciated. ---------------------------------------------------- Energy is like a muscle, it grows stronger through being used. ----------------------------------------------------</body>
  </mail>
  <mail>
    <header>IKImageView and filters</header>
    <body>I am trying to write a basic image diff-ing tool consisting of two overlaid IKImageViews, with the top view having a Difference Blend Mode compositing filter applied to it in IB's Effects palette. So far so good... until I call -setImageWithURL: to put images into these views, at which point the images appear (good) and the filter effects disappear (not good). I've tried the same thing with other background and compositing filters just to check, and in each case the applied effects can be seen on an empty IKImageView but vanish as soon as I call - setImageWithURL:. Hamish -- Hamish Sanderson Production Workflow Developer Sun Branding Solutions Ltd Tel: +44(0)1274 200 700 www.s-brandingsolutions.com</body>
  </mail>
  <mail>
    <header>Re: Crash while using NSBitmapImageRep TIFFRepresentation</header>
    <body>On Jul 3, 2008, at 3:44 PM, Chris Meyer wrote: In practice (as you note), there are a bunch of known (at least to me - I reported them :-)) bugs still in the latest version of Leopard. Apple seems very interested in fixing them, though, so keep reporting them :-) Having said this, I have a little test app I wrote to stress NSOperations on my 8-core, and I use TIFFRepresentation in it (call it twice in one of my NSOperation subclasses).  Even in runs where I have many thousands of NSOperations queued up to run in parallel, I've never seen it crash on any of my 2, 4, or 8 core machines. So empirically, I think TIFFRepresentation is safe.  XMP reading out of TIFs, drawing color managed images into that bitmap rep... well, there are still issues. --&amp;gt; Michael B. Johnson, PhD --&amp;gt;  (personal) --&amp;gt;  (alum) --&amp;gt; MPG Lead --&amp;gt; Pixar Animation Studios</body>
  </mail>
  <mail>
    <header>Re: Crash while using NSBitmapImageRep TIFFRepresentation</header>
    <body>I suspect something similar, but I couldn&amp;#39;t find any official documentation or mail message that confirms this. Do you know of some documentation officially talking about the thread safety of TIFFRepresentation? Here&amp;#39;s what I found: the web page &amp;quot;Threading and Graphics Contexts&amp;quot; (link below) states: Creating bitmaps on secondary threads is one way to thread your drawing code. Because bitmaps are self-contained entities, they can be created safely on secondary threads. From your thread, you would need to create the graphics context object explicitly (as described in&amp;nbsp;"Creating a Screen-Based Context") and then issue drawing calls to draw into the bitmap buffer. For more information on how to create bitmaps, including sample code, see&amp;nbsp;"Creating a Bitmap."</body>
  </mail>
  <mail>
    <header>[ANN] PreFab Event Taps Testbench 1.1</header>
    <body>I've posted PreFab Event Taps Testbench 1.1 at software developers. Use it to explore what your applications can do with Quartz event taps. In addition, you should find it to be a useful tool to test drivers for hardware input devices such as trackballs and tablets, to test virtual software input devices including assistive applications for persons with disabilities, and to test remote control software. These are the new features that I like the most: 1. It now includes complete support for tablet proximity and pointer events. (I've only tested this on a high-end professional Wacom tablet. I'd be interested in feedback from users of Wacom consumer tablets, and non-Wacom tablets.) 2. It includes a new Current Event window where you can examine the most recent event in real time in an easy-to-read layout showing all available event information at a glance. The old Events window is still available as the Event History window, showing a configurable number of recent events in text format. 3. It allows users to make Event Taps Testbench a trusted accessibility process without turning on global access for assistive devices. I'll add this feature to UI Browser and UI Actions in due course. 4. It enables users to turn on global access for assistive devices, if desired, without opening System Preferences. 5. There were altogether too many bugs in 1.0. I think I've fixed all of them. ;) The most important new feature is that the PFEventTaps framework is now installed separately as a shared framework in /Library/Frameworks, where it can be used (under license) by any developer. The framework wraps Apple's Quartz event taps C API, including many transparent workarounds for Apple bugs. It's an Objective-C 2.0 universal binary supporting 32-bit and 64-bit architectures and retain/release and garbage collection memory management. It enables a Cocoa application to respond to both local and remote user input events from a keyboard, mouse, scroll wheel or tablet using Cocoa delegate methods Will Post Event, Should Post Event, May Post Modified Event For Event, and Did Post Event, regardless of the application at which the events were targeted. It also helps you build remote control applications, assistive applications for persons with disabilities, and other tools that modify user input events on the fly before they are posted to their target. I'm using this as an opportunity to try an experiment in commercialization of Objective-C frameworks. The PFEventTaps framework may be licensed without charge for distribution with a free software product. A flat one-time per-product license fee of $250 is required to distribute the framework with a product for which you request or require payment, such as donationware, shareware and commercial software. Source code is available for a separate fee. Different terms apply to large or established commercial software developers. I'll be interested in hearing what people think of this approach. It seems to me that developers of Mac frameworks and similar code should be able to share in any income that they help other developers to generate. As the Mac platform grows, I believe this will encourage more and better software for all of us. At the same time, I think developers of free software should be able to use my frameworks without charge, to the same end. So the framework is installed, with headers, in /Library/Frameworks, where everybody can see it. The headers include copyright notices and information about how to obtain a distribution license, and the distribution license form is embedded in the framework and available for download from our Web site. -- Bill Cheeseman - email@hidden Quechee Software, Quechee, Vermont, USA www.quecheesoftware.com PreFab Software - www.prefabsoftware.com</body>
  </mail>
  <mail>
    <header>Re: Crash while using NSBitmapImageRep TIFFRepresentation</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Crash while using NSBitmapImageRep TIFFRepresentation</header>
    <body>Hi all, I have code like this to convert a CI Image to TIFF data: - (NSData *)convertCIImageToTIFF:(CIImage *)ci_image I run this code simultaneously in many threads. On some video cards (GeForce) I get crashes (see below, thread 28). Again, there are also other calls to TIFFRepresentation happening simultaneously while this is crashing (stack traces omitted for brevity). The crash looks like it is a problem with threading access to an OpenGL context. My question: Why is this crashing? Perhaps there is an implied use of the current OpenGL context while running the code above? Any help would be appreciated. Regards, Chris Meyer Stack trace:</body>
  </mail>
  <mail>
    <header>Re: CALayer property values resetting after CAAnimation finishes</header>
    <body>On Jul 2, 2008, at 2:32 AM, David Duncan wrote: Thanks! That did the trick. I feel stupid now, because I've used the fill mode before and completely forgot about it. I'm also running an animation on a background filter property, from the docs I read I need an explicit animation to do that. Thanks again.</body>
  </mail>
  <mail>
    <header>Re: CALayer property values resetting after CAAnimation finishes</header>
    <body>You also need to set the fill mode to kCAFillModeForwards or kCAFillModeBoth. The default is kCAFillModeRemoved. However, it is better to set the real value of the property (you can do this before or after adding the animation) as animations added explicitly like this only affect the presentation layer. For something like this, you may be better off either using CATransaction to set the duration of implicit animations or to use the -actionForKey: mechanism (see the docs for that for a full discussion of how this works). This allows you to override the default implicit animations to do much of what you can do with explicit animation. -- David Duncan Apple DTS Animation and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Re: CALayer property values resetting after CAAnimation finishes</header>
    <body>You'll want to either set the property value to its final state when you add the animation, or add a delegate to the animation and respond to its animationDidStop: by setting the final value.</body>
  </mail>
  <mail>
    <header>Re: CALayer property values resetting after CAAnimation finishes</header>
    <body>Hi Scott, thanks I did try this, but the same results, as soon as the animation completes the values reset. I set a break on [animationDidStop:finished:] and verified that finished was YES (indicating the animation finished without being removed).</body>
  </mail>
  <mail>
    <header>Re: CALayer property values resetting after CAAnimation finishes</header>
    <body>On Jul 1, 2008, at 8:32 AM, Brian Bergstrand wrote:</body>
  </mail>
  <mail>
    <header>Re: Starting and stopping a QCCompositionLayer</header>
    <body>Make sure you are at the developer.apple.com site before searching. There isn't much connection between the main Apple site and the developer site. In this case it should at least pull up In regards to your questions, you would use the existing CoreAnimation functionality to interact with the composition, and I think that's what you have described in your various methods. Another one that people use is setting the &amp;quot;speed&amp;quot; property to zero to pause it without stopping it (by removing the layer) completely. There is no direct API to start and stop a composition layer as there is for QCView.</body>
  </mail>
  <mail>
    <header>Starting and stopping a QCCompositionLayer</header>
    <body>Hi, A search for QCCompositionLayer on  turns up nothing, which is funny because I know this has been discussed. How do I start and stop a composition I am using via this mechanism (as opposed to a QCView)? I ask because I can think of a few somewhat lengthy ways to do it, but I'm not sure any of them are worth pursuing. This seems like a problem there should be a simple solution for. 1) Go through the QCLayer's -animation list looking for the right one, then get its timingFunction, and set that to something. This seems like a long, long route to do a simple thing. 2) Use the view delegate method for the QCLayer to set its view delegate and pass the startRendering call to that. That sounds like a bad hack. 3) Something simpler? Perhaps even obvious? -Chilton</body>
  </mail>
  <mail>
    <header>Re: Text drawing crooked</header>
    <body>In case this helps, below is the relevant piece of code that does the drawing. CGContextTranslateCTM(myContext, tagNameOrigin.x + columnRect.size.width, [labelCell drawInteriorWithFrame:NSMakeRect(0.0f, 0.0f, labelWidth - 2.0f, tagNameSize.height) Thanks, Doug Penny</body>
  </mail>
  <mail>
    <header>CVOpenGLBuffer Question</header>
    <body>I&amp;#39;m completely new to Quartz Composer, so bare with me ! I&amp;#39;d like to know if my opengl app can be modified to render to an offscreen CVOpenGLBuffer, then is it possible to have another application grab the contents of this Opengl buffer somehow by using Quartz Composer? My goal is to use hardware accelerated opengl to render to an offscreen buffer, and have another application read/display this buffer, but have the pixels stay on the GPU. If this is possible then can anyone provide some direction/help on how this could be implemented ? thanks Jonathan</body>
  </mail>
  <mail>
    <header>PDFView print Problem ?</header>
    <body>Hi , I am trying to print a pdf file thru my application using the below code. Above code is working perfectly on Tiger, but when it come's to leopard it's not working. Print panel is coming and hanging . any one has idea on wher the things going wrong. Thanks, Naresh K Share files, take polls, and make new friends - all under one roof. Go to</body>
  </mail>
  <mail>
    <header>CALayer property values resetting after CAAnimation finishes</header>
    <body>I'm running an animation on a layer and while the animation runs and changes the property values correctly, as soon as the animation finishes, the property values reset to their starting values. However, if I directly manipulate the layer property which invokes the implied animation, the property value remains set to the new value: superLayer.layoutManager = [CAConstraintLayoutManager superLayer.needsDisplayOnBoundsChange = YES // this works, leaving the opacity at 0 when the implied animation completes // this resets the opacity to 1.0 when the animation finishes CABasicAnimation *a1 = [CABasicAnimation [a1 setValue:layer forKey:@&amp;quot;layerContext&amp;quot;]; // store a reference to the layer The only thing the delegate implements is [animationDidStop:finished:] so I get notified when the animation completes. p.s. It's not just the opacity property, but all properties and sub- properties (such as those of filters).</body>
  </mail>
  <mail>
    <header>Re: Best (easiest fasterest most efficient) Means to Load an image and extract a subimage</header>
    <body>Answering my own question here: I'm conflating Automator actions with Automator workflows. I don't actually know if you can bundle an executable as part of a workflow. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: What happened to the Python Bindings to Quartz 2D</header>
    <body>Yes, it is. David originally meant to refer to BridgeSupport, which is the &amp;quot;objc bridge&amp;quot; your'e talking about. It actually bridges more than just ObjC. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: Best (easiest fasterest most efficient) Means to Load an image and extract a subimage</header>
    <body>The last thread was starting to go all over the place, so thought I'd refocus it on my specific task. My goal is to create a script based Service in Automator that I can post for non-developers - so everything I use has to be on a base Snow Leopard or even Lion machine sans Xcode. The script will at the end crop an image to get just a portion of the left side of the image. sips cannot do this - it only crops from the middle</body>
  </mail>
  <mail>
    <header>Re: Best (easiest fasterest most efficient) Means to Load an image and extract a subimage</header>
    <body>Is there a reason you can't just build an executable and bundle it in your action? What's the real goal here: to have a service that does your cropping, or to have a service whose implementation can be read and understood by non-developers? Quartz.framework lives in /System/Library/Frameworks. It is a core component of Mac OS X that is installed on all machines. /Developer might contain symlinks to it in the current platform SDK, or it can contain shell copies of the header files from previous OS's versions of Quartz.framework in their SDKs to facilitate writing backwards-compatible code. There is no Quartz plugin, per se. `import Quartz` loads Quartz.framework and creates a module containing functions for all the functions found in Quartz.framework. There is some glue code that makes this possible, and this glue code relies on BridgeSupport. But conceptually speaking, you should think of `import Quartz` as the Python equivalent of `dlopen(&amp;quot;/System/Library/Frameworks/Quartz.framework/Quartz&amp;quot;)`. Ruby was mentioned previously. --Kyle Sluder _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Best (easiest fasterest most efficient) Means to Load an image and extract a subimage</header>
    <body>The last thread was starting to go all over the place, so thought I'd refocus it on my specific task. My goal is to create a script based Service in Automator that I can post for non-developers - so everything I use has to be on a base Snow Leopard or even Lion machine sans Xcode. The script will at the end crop an image to get just a portion of the left side of the image. sips cannot do this - it only crops from the middle. It appears that at least one scripting language, Python, can load Quartz, and then let me access the functionality from that language. However, the only &amp;quot;Quartz&amp;quot; I can find on my system in a Python folder in /Developer (using Spotlight)? I did see that the load took a few seconds (from Quartz import * ), and so I suppose that the script, having to load Quartz at each invocation, is going to be slower than it would be using an executable (like sips) - but at least it would be possible! 2) What other scripting language options do I have, and are there example scripts anywhere to be found (or readmes etc) on using Quartz?</body>
  </mail>
  <mail>
    <header>Re: What happened to the Python Bindings to Quartz 2D</header>
    <body>That's what I was really trying to get at - there must be some documentation on this somewhere, and example scripts. For instance, how would I ever know I needed to import Quartz?</body>
  </mail>
  <mail>
    <header>Re: What happened to the Python Bindings to Quartz 2D</header>
    <body>The python bindings were deprecated in favor of general Scripting Bridge support that was introduced in Mac OS X 10.5 (iirc). Through the Scripting Bridge you have access to all of Mac OS X, as such you can use the Quartz C API directly rather than via the bridge.</body>
  </mail>
  <mail>
    <header>Re: What happened to the Python Bindings to Quartz 2D</header>
    <body>Mark - OK - now we're cookin! So you knew the magic incantation - &amp;quot;from Quartz import *&amp;quot;. How is a mere mortal such as me suppose to know these magic commands? That's what I was really trying to get at - there must be some documentation on this somewhere, and example scripts. For instance, how would I ever know I needed to import Quartz? David</body>
  </mail>
  <mail>
    <header>Re: Calculating length of bezier CGPath?</header>
    <body>I agree with the other poster.  Just get the straight-line approximation and sum.  In other Bezier implementations I've seen accessors for length and to let you, e.g. create a point at a certain parameter, distance from the original end, along the length of the curve, but you'd have to have access to the internals to do a good approximation, and that's all encapsulated the last I checked.</body>
  </mail>
  <mail>
    <header>Re: Calculating length of bezier CGPath?</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Calculating length of bezier CGPath?</header>
    <body>Dear list, I have a few simple CGMutablePathRefs that I'd like to calculate the (approximate) length of. Any ideas as to how this can be achieved? In my case, each path has only the 2 end points and 2 control points. Thanks in advance, Joachim</body>
  </mail>
  <mail>
    <header>problems with Keyboard Events</header>
    <body>Nick</body>
  </mail>
  <mail>
    <header>Triggering an animation during a call to resizeSubviewsWithOldSize</header>
    <body>I have some NSViews that I'm putting in one of two layouts depending on the size of my main window. I'm adjusting the layout when the relevant superview receives the resizeSubviewsWithOldSize method. This works, but I'd like to animate the change.  So naturally I tried calling the animator proxy when I set the new frames, but the animation won't run while the user is still dragging.  If I release the mouse before the animation is scheduled to be done I can see the tail end of the animation, but nothing until then.   I tried making sure kCATransactionDisableActions was set to NO, but that didn't help. Is it possible to start a new animation and actually have it run during the resize? Thanks much, Chris</body>
  </mail>
  <mail>
    <header>Fade transitions not working on order in / out</header>
    <body>I'm trying to get some pretty simple Core Animation transitions working. I would appreciate it if someone could take a look before I submit a bug report. I want to provide custom transitions for a layer when it's added to / removed from it's superlayer, i.e. for the kCAOnOrderIn and kCAOnOrderOut actions. I'm doing this via a delegate which serves up a simple CATransition for the appropriate actions: However, this isn't working despite the delegate's actionForLayer:forKey: method being called and returning an appropriate CATransition. It looks as though Core Animation is using the standard fade in animation and is not using the specified duration of 5 seconds. I have a Xcode project showing the problem at: The code is pretty straightforward: a layer-backed NSView (TransitionView) and a delegate object which handles all the CALayer logic (TransitionDelegate). Clicking on the view will add or remove a single CALayer (via the NSView's mouseDown event). The most interesting thing is that if I change the CATransition type from kCATransitionFade to kCATransitionReveal then the code works correctly for kCAOnOrderIn (i.e. it does a fade in of the specified duration). However, this doesn't work for kCAOnOrderOut. You can see this by changing ‚Äò#if 1‚Äô to ‚Äò#if 0‚Äô on line 13 of TransitionDelegate.m. r i c k _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Get CGColorSpace for NSScreen (not working since 10.5.5)</header>
    <body>weird. It is now working and I did nothing besides restarting OS X. Anyway, good that it is working. Best regards Mark</body>
  </mail>
  <mail>
    <header>Re: Get CGColorSpace for NSScreen (not working since 10.5.5)</header>
    <body>Where does it break ? Raphael</body>
  </mail>
  <mail>
    <header>Re: drawing PDF with annotations</header>
    <body>On Sep 12, 2008, at 1:13 AM, Ken Ferry wrote: Thanks, I got this approach working with no hassle. Attachment:</body>
  </mail>
  <mail>
    <header>IKImageView zooming</header>
    <body>I've been working through the Image Kit Programming Guide and it seems that the zooming is misbehaving.  Once the image is loaded, displayed, and panned, any change in zoom (via setZoomFactor) seems to pan the image back to (0,0).  There was a discussion about this at Cocoabuilder back in November but there didn't seem to be a good solution.  Is this a bug?  Workaround?  Am I missing somehting?</body>
  </mail>
  <mail>
    <header>Occasional crash in ripr_path_fill called by CGContextDrawPath</header>
    <body>Hi Quartz folks, I'm working on some Dock animation code using Quartz.  The code draws a circle in the CGContext of the Dock filled with a specified color. This code runs on an NSAnimation thread, and does the following: CGContextAddArc(context, DOCK_PROGRESS_CENTER_X, DOCK_PROGRESS_CENTER_Y, DOCK_PROGRESS_RADIUS, 0, 2 * M_PI,  When I run this overnight, it occasionally crashes (after about 10-12 hours of running), with the following backtrace (non-Apple code snipped out): OS Version:      Mac OS X 10.5.4 (9E17) Exception Type:  EXC_CRASH (SIGABRT) Thread 0 crashed with X86 Thread State (32-bit): eax: 0x00000000  ebx: 0x004ffb23  ecx: 0xb0a9fa6c  edx: 0x9077fb9e edi: 0x00000000  esi: 0x00000006  ebp: 0xb0a9fa88  esp: 0xb0a9fa6c ss: 0x0000001f  efl: 0x00000282  eip: 0x9077fb9e   cs: 0x00000007 ds: 0x0000001f   es: 0x0000001f   fs: 0x00000000   gs: 0x00000037 cr2: 0x00813000 Thread 0 Crashed: (snip) 4   libSystem.B.dylib             	0x9077e09b _sigtramp + 43 5   ???                           	0xffffffff 0 + 4294967295 6   libRIP.A.dylib                	0x900488e4 ripr_path_fill + 198 7   com.apple.CoreGraphics        	0x9232a23e CGPathApply + 267 8   libRIP.A.dylib                	0x900491a3 ripr_Path + 1333 9   libRIP.A.dylib                	0x900402b7 ripc_DrawPath + 315 10  com.apple.CoreGraphics        	0x92357c27 CGContextDrawPath + 176 (snip) 14  com.apple.AppKit              	0x96439aa7 -[NSAnimation(NSInternal) _advanceTime] + 311 15  com.apple.Foundation          	0x955fa5d7 __NSFireTimer + 279 16  com.apple.CoreFoundation      	0x94a31b45 CFRunLoopRunSpecific + 4469 17  com.apple.CoreFoundation      	0x94a31cf8 CFRunLoopRunInMode + 88 18  com.apple.Foundation          	0x955fa4a5 -[NSRunLoop(NSRunLoop) runMode:beforeDate:] + 213 19  com.apple.AppKit              	0x965d2c54 -[NSAnimation(NSInternal) _runBlocking] + 255 20  com.apple.AppKit              	0x965d2b3a -[NSAnimation(NSInternal) _animationThread] + 86 21  com.apple.Foundation          	0x955c5f1d -[NSThread main] + 45 22  com.apple.Foundation          	0x955c5ac4 __NSThread__main__ + 308 23  libSystem.B.dylib             	0x907426f5 _pthread_start + 321 24  libSystem.B.dylib             	0x907425b2 thread_start + 34 I have filed this bug as rdar://6217252 . Does anyone have any suggestions or thoughts, or ways to diagnose or avoid this crash in ripr_path_fill? I understand that BeginCGContextForApplicationDockTile() isn't explicitly thread-safe, but I only ever call it from a single thread. I don't know if AppKit APIs also call this internally. Thanks, Ben</body>
  </mail>
  <mail>
    <header>Re: Hit detection using CGContextPathContainsPoint</header>
    <body>It really depends on the drawing parameters and the desired result. In particular CGContextPathContainsPoint() takes stroke width, dash patterns, caps, and joins into account, while CGPathContainsPoint cannot (since it doesn't have that information). If you leave all those parameters at default, then the two techniques will likely get similar results, but if you change any of them you can miss edge cases. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Hit detection using CGContextPathContainsPoint</header>
    <body>David may know something I don't, but there is CGPathContainsPoint. That lets you operate without a context. -Ken Cocoa Frameworks</body>
  </mail>
  <mail>
    <header>Re: drawing PDF with annotations</header>
    <body>I haven't tried this but have a look at: The main idea is that you derive a bitmap image representation from the NSImage. Once you have that you can get a CGImageRef through NSBitmapImageRep's CGImage method, provided you are targeting Leopard or later. If not you'll need to go through CGImageCreate after obtaining the bitmap data from NSBitmapImageRep's bitmapData method. (I think, I'm not quite versed in this stuff.) But, just as I was readying this message I saw Ken's much better solution, so you probably want to go that way. ---------------------------------------------------- Disapprove of sin but not of the sinner ----------------------------------------------------</body>
  </mail>
  <mail>
    <header>Re: drawing PDF with annotations</header>
    <body>If you need to get a CGImage, then you don't need to go to NSImage as an intermediary step.  Make a CGBitmapContextRef, draw in it with PDFKit, and use CGBitmapContextCreateImage.  To direct Cocoa drawing to a CGContext, do this: [NSGraphicsContext setCurrentContext:[NSGraphicsContext // do drawing As far as getting a CGImage from NSImage and friends, NSBitmapImageRep added the method -CGImage in Leopard.  NSImage itself can not in general be represented as a single CGImage.  For example, an NSImage may be backed by vector data, or by multiple bitmaps appropriate for different sizes, or by Quartz drawing commands, or what have you.  A CGImage, on the other hand, is a single bitmap.  So, to get a CGImage from an NSImage you need to specify how you want it flattened and rasterized.  Making a CGBitmapContext and drawing the NSImage in it is reasonable way to do it.  SnowLeopard makes this easier. -Ken Cocoa Frameworks</body>
  </mail>
  <mail>
    <header>Re: drawing PDF with annotations</header>
    <body>On Sep 11, 2008, at 10:35 PM, Antonio Nunes wrote: Thanks.  Then I just have to figure out how to go from an NSImage to a CGImage or CGBitmapContext or something.  I found a couple of past threads where people were asking about that task, but both ended without any claim of success.Attachment:</body>
  </mail>
  <mail>
    <header>Re: drawing PDF with annotations</header>
    <body>Use PDFKit. The drawWithBox: method of a PDFPage draws the page including any annotations (at least the ones that it handles, which is most of the basic ones, but caret and cloud annotations and a number of others are not supported unfortunately). If you draw the PDFPage into an NSImage you should be able to get what you want. ----------------------------------------------------------- Some things have to be believed to be seen. --Ralph Hodgson -----------------------------------------------------------</body>
  </mail>
  <mail>
    <header>Re: drawing PDF with annotations</header>
    <body>After further experimentation, it appears that it's not the import routines that changed.  Rather, the Leopard version of Preview saves annotated PDFs differently than the Tiger version did. I'd still like to know whether it's possible to take a PDF that was annotated with the Leopard version of Preview, and import the image *with annotations* without jumping through too many hoops. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>drawing PDF with annotations</header>
    <body>I want to take the first page of a PDF file and turn it into a pixmap that looks just like what I would see in Preview.  I used to be able to do that with a QuickTime graphic importer, but at some point it stopped showing annotations.  CGContextDrawPDFPage also omits annotations. Surely there is some way to do it? -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Squeeze parameter on CIDiscBlur filter</header>
    <body>Does anyone have any details about the inputSqueeze parameter of the CIDiscBlur filter? Our software presents a GUI which allows users to apply many of the built-in Core Image filters. It builds the UI for the input parameters dynamically, based on information from the filter attributes. In the case of the CIDiscBlur filter, the input parameter includes these attributes for inputSqueeze: ...which suggests that it is a normal scalar parameter. However, neither CIAttributeClass nor CIAttributeDisplayName are present, so our code flounders trying to present this parameter to the user. Surprisingly, the Core Image Filter Reference section on the CIDiscBlur does not mention of this parameter. CIAttributeDisplayName == nil) then ignore this parameter&amp;quot;, but since this is the only such parameter I have encountered, I am worried about generalizing. Thanks, Brian SmileOnMyMac</body>
  </mail>
  <mail>
    <header>Re: cocoa/quartz clipping problem</header>
    <body>Why do you feel the need to cache this context? Compared to drawing its fairly cheap to refetch it every time your -drawRect: is called. As an expression of the API, there is no guarantee that the context you get is the same for every invocation of -drawRect:. The drawing should already be clipped to the area of aRect, so you shouldn't have to clip it yourself. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>cocoa/quartz clipping problem</header>
    <body>In my app, I use Cocoa as application framework, Quartz as drawing library and C++ for my general infrastructure and the app's specific functionality. The three come together in this class : NSGraphicsContext* nativeContext = [NSGraphicsContext In the -(void)drawRect:(NSRect)aRect function of my NSViews, I get this stored nativecontext and draw on it, doing a CGContextSaveGState first, and a CGContextRestoreGState when I'm done. Inside the drawRect, I clip to the aRect, using CGContextClipToRect. This works very well, except for the case where the NSView's drawRect is called by the system, for example when the app is activated. In this case, the NSView's clipping area is too large, so background NSViews are partly drawn over front-laying ones. The problem appears to be that the cached CGContext that I use is not Cocoa's currentContext, and therefore Cocoa clips only its currentContext, and not my cached one. I tried to work around this problem in two ways: - attempting to get the currentContext's clip area and imposing it onto mine. This doesn't work, as I can only get the clip's boundingRect, and not the individual areas that constitute the real clip (so I get a large rect which is the bounds of all clip areas combined) - cache [NSGraphicsContext currentContext] in my mNativeContext variable (which I set during the first drawRect, to make sure it's the same context). This leads to strange drawing artifacts (mirrored text, missing background, etc.), so probably this is not the way to go either. Again, when the drawing is triggered by me, or when the app is already in the foreground (like when resizing the window), all works well, no matter which approach I take. But when the system runs the update, as when the app becomes activated, the clipping issues occur. A fine solution would seem to be able to grab cocoa's clip and apply it onto my cached one, obtained via [NSGraphicsContext graphicsContextWithWindow:myNSWindow];. However there does not seem to be a way to get or copy the current clipping path. Niels</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContext-backed CIContext: Large memory allocations in CGContextDrawShading?</header>
    <body>Thanks Ralph, that's excellent advice! However, I noticed that the supported pixel formats (CIFormat) are limited to 32/64-bit fixed point RGBA and floating point RGBA, but I need the pixel data in an 8-bit mono format. That's achievable by using drawImage with a CIContext made from a CGBitmapContext with that format, but is there also a way I could still use render:, while getting the pixel format I need? Thanks again, Georg</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContext-backed CIContext: Large memory allocations in	CGContextDrawShading?</header>
    <body>- (void)render:(CIImage *)im toBitmap:(void *)data rowBytes:(ptrdiff_t) rb method instead of calling drawImage:... to a CGBitmapContext to get bits out. This has some advantages for your case: - the output of the render: call is not cached, so the memory allocations you're seeing should go away - better performance because it avoids one buffer copy -  you pass a pointer to the target bitmap at render time, thus you could implement your own buffer pool management suitable for your application Hope this helps, - Ralph</body>
  </mail>
  <mail>
    <header>CGBitmapContext-backed CIContext: Large memory allocations in	CGContextDrawShading?</header>
    <body>I'm capturing video via QuickTime, then I need to apply some CIFilters and finally access the pixel data to do some image processing on it. In order to get the pixel data from the CIImages, I'm creating a CIContext backed by a CGBitmapContext, draw the image into it, flush the CGBitmapContext and access the pixel data in the buffer. It's working great, but when looking at my app in the &amp;quot;Object Allocations&amp;quot; view in Instruments, I notice that during the CIContext drawImage call, a large chunk of memory is allocated during a call to CGContextDrawShading. It is properly freed then, but why is this chunk of memory even allocated for every frame I receive from the camera? First, I have no idea where the CGContextDrawShading call is even coming from (I don't use any shadings in my filters, they're all custom filters). More importantly, though, how can I get the CIContext to re-use this chunk of memory, rather than allocate/free it on every pass (I'm re-using the same CIContext and CGContext for every frame)? It's not a leaking problem, but I want to improve performance by getting rid of this large allocate/free cycle in the critical path of my processing pipeline. Any help would be appreciated! Thanks, Georg</body>
  </mail>
  <mail>
    <header>Re: CIColor with values above 1.0?</header>
    <body>The problem lies in the model of color you'll be using. The usual picture you have are defined relative to an illuminant, that is the value are not amount of light but amount of light relative to the amount of light delivered by the illuminant. the radiance you have access in an HDR image is different : it measures the radiance you can measure of a particular scene. Then the interpretation of color is difficult because the eye and the brain make global and local (both chemical and cognitive) adaptation of the radiance emitted by a scene. The problem you're facing looks for me more like on how to interpret the radiance than a more general coding problem. What I would do personally considering your goal is in fact to try interpret the radiance as relative to a source to fallback into some more &amp;quot;simple&amp;quot; problem. To do so, I would calculate the max of the radiance vector. Then I would convert to XYZ, then finally project this point on the line you choose as the white point. This would give you some XYZ vector with a certain luminosity on Y which would serve as the general scalar to normalize the entire image if Y is greater than 1. Then you would fall approximately on an image which would have all its color in your source RGB space *if you're RGB space is large enough*. You may try to refine this model by also considering the corners (e.g. max R min G/B, max G min R/B, etc.) to ensure that the scalar is small enough to both fit your hottest point, and those color. If this latter scalar is too much different from the one calculated with the hottest point, I would then try to have a Lab approach to try to tighten the space more by modifying the &amp;quot;saturation&amp;quot; of the colors. If you still need to export a radiance format, I would then do the inverse transformation with this scalar and/or lab approach. Raphael</body>
  </mail>
  <mail>
    <header>CIColor with values above 1.0?</header>
    <body>) I get values for color components that are above 1.0 (in the white areas).  Once I have my image loaded I put in in a CIImageacculator and then I draw it to a context and when I get the float value, it is still, as expected is above 1.0. So reading seems fine, but how to I  use this color in a CIFilter ? If I create a CIColor the values will all max out at 1.0. I see 10.5 added a kCGColorSpaceGenericRGBHDR colorspace. If I create a ColorRef with this colorspace and then a CIColor from it, will it retain the correct value and be right way to do it? But then would this mean I can't do it in Tiger? -------------------------------------------- Santiago (Jacques) Lema - link-u</body>
  </mail>
  <mail>
    <header>Draw to an FxTexture using Core Image</header>
    <body>Quartz Dev, I am converting a real time Core Image filter into an FxPlug, but in order to that I need to be able to convert in FxTexture into a core image, run filters, and then write it back out again.  I got the first part, but am not sure how to configure the context to correctly and efficiently draw to the output texture.  can anyone show me the basic bare bones example of how to simply render the input texture onto the output texture using Core Image as the intermediary.  I can do all the filtering myself. Here is what I have so far, which doesn't work.  Any assistance would be much appreciated!! NSOpenGLPFAAccelerated, NSOpenGLPFANoRecovery, NSOpenGLPFAColorSize, 32, NSOpenGLPixelFormat* pixelFormat = [[[NSOpenGLPixelFormat alloc] CGLCreatePBuffer( [outTex width], [outTex height], [outTex target], GL_RGBA, 0, &amp;amp;pbuffer myCIContext = [CIContext contextWithCGLContext:renderInfo.sharedContext CIImage *imageBuffer = [[CIImage alloc] initWithTexture:[inTex [myCIContext drawImage:imageBuffer atPoint:rect.origin // Should probably be rect.origin -- - Jim</body>
  </mail>
  <mail>
    <header>Creating a 1-D Temperature CIFilter</header>
    <body>Attachment:</body>
  </mail>
  <mail>
    <header>Re: CGPathElementType</header>
    <body>Thanks for that.  I did read that in the documentation.  I miss- understood.  I thought the original specified Arc Points would remain and the Bezier approximation stayed behind the scenes.  By that I mean, I was using the Applier function to get my CGPath points back and I figured that even though the Arc was approximated as a Bezier on screen the Applier Functions ElementType would show me the original specified Arc data.  I guess not.  Oh well, not  a big problem. Best Regards, Sean On 04/09/2008, at 11:43 PM, Steve Mills wrote:</body>
  </mail>
  <mail>
    <header>Re: CGPathElementType</header>
    <body>An arc isn't a path segment type, it's a carry-over from PostScript's drawing commands. Read the docs for CGPathAddArc and you'll see that it adds actual path type segments: &amp;quot;‚Ä¶then approximates the new arc _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>CGPathElementType</header>
    <body>, , , ,</body>
  </mail>
  <mail>
    <header>Looking for help from an Experienced QC Developer</header>
    <body>font-family:Arial'&gt;Hi, font-family:Arial'&gt; font-family:Arial'&gt;We&amp;#8217;re looking for an experienced Quartz Composer Developer to assist with a paid project. font-family:Arial'&gt; font-family:Arial'&gt;The application may require linking to a RSS, XML, or S&lt;font color=black&gt;AP based information font-family:Arial'&gt;which will cue updated text displays, visuals, and music. font-family:Arial'&gt; &lt;span style='font-size: 10.0pt;font-family:Arial;color:black'&gt;Any assistance or &lt;font size=2 face=Arial&gt;if you might be able to point me in the right direction, it would be greatly appreciated. font-family:Arial'&gt; font-family:Arial'&gt;-- font-family:Arial'&gt;Ken font-family:Arial'&gt;</body>
  </mail>
  <mail>
    <header>Re: Core Image and &amp;quot;kCISamplerFilterMode&amp;quot;</header>
    <body>___ Andrew Lindesay www.lindesay.co.nz</body>
  </mail>
  <mail>
    <header>Core Image and &amp;quot;kCISamplerFilterMode&amp;quot;</header>
    <body>kCISamplerFilterLinear kCISamplerFilterNearest ___ Andrew Lindesay www.lindesay.co.nz</body>
  </mail>
  <mail>
    <header>CIFilter Kernel Pixel Position</header>
    <body>I have a filter/kernel that is used to calculate a median pixel value from the surrounding pixels.  The problem I encounter is image edge detection.  If I try to grab pixel values outside of the image, it corrupts my median calculation.  Has anyone come up with a solution to determine if a coordinate value is within the bounds of the image? Thanks Andy Brace</body>
  </mail>
  <mail>
    <header>Re: ColorSync Manager deprecated</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Image Unit template broken in Xcode 4.2?</header>
    <body>The message &amp;quot;Automatic Reference Counting Issue&amp;quot; should be a clear indicator that the problem is that the template hasn't been updated to comply with ARC. If you turn off ARC in the compiler settings, or fix the code so it's ARC-compliant, the error will go away. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Image Unit template broken in Xcode 4.2?</header>
    <body>Hi all, I just opened up Xcode 4.2 in Lion and created a new Image Unit from the template. Compiling it fails with errors in runtime.h: Automatic Reference Counting Issue Pointer to non-const type 'Protocol *' with no explicit ownership Switching the compiler from Apple LLVM 3.0 to LLVM GCC 4.2 solves the problem. Obviously a bug, but does anyone know what causes it? Ben</body>
  </mail>
  <mail>
    <header>ColorSync Manager deprecated</header>
    <body>I have an app that is color managed. I haven't done a new version of it in a few years. I'm starting to work on a new version. I brought all the code into XCode 4 on Lion - and found out that the entire ColorSync Manager library has been deprecated. Unfortunately there does not seem to be a single word anywhere on what one is supposed to to instead. Does anyone know anything about this? Thanks... Robert _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: trapping on errors in Core Animation code?</header>
    <body>On Oct 31, 2008, at 1:48 PM, Michael B Johnson wrote: It's objc_exception_throw on 10.5, but I don't think any of those are exceptions.  Try breaking on asl_send or write, which will show you which CG logging function is used. -- Adam Attachment:</body>
  </mail>
  <mail>
    <header>Re: CIPageCurlTransition oddity with large view area</header>
    <body>No, there are no Console errors and no errors returned to my app (which is exhibiting the same problem as Core Image Fun House). Just the odd break right down the middle of the screen.</body>
  </mail>
  <mail>
    <header>Re: CIPageCurlTransition oddity with large view area</header>
    <body>Coincidently, On Oct 31, 2008, at 1:48 PM, Michael B Johnson wrote: Brian, Are any of those types of messages appearing on the console when you see the problem? Michael, Those are not NSExceptions. The console (and odd image results) is the only way to know that a problem occurred, so far as I know. Michael &amp;amp; Brian, I sent a message to this list back in May asking if there is any way to know the maximum &amp;quot;safe&amp;quot; number of pixels to send to various CoreImage filters. I never received a response. In my implementation I chain CIAffineTransform filters in front and behind the &amp;quot;desired&amp;quot; filter if the input seems like &amp;quot;too many&amp;quot; pixels. Unfortunately, &amp;quot;too many&amp;quot; is guesswork. For many distortion filters, more than 1 mega-pixel seems to cause problems. Other filters seem to have essentially no limits. Some blow up over about 6 million pixels. It sucks. My guess is that it depends on a combination of the video card on the system and the particular implementation of the filter. I really wish there was documentation of the limits and/or error reporting. Cheers, Brian</body>
  </mail>
  <mail>
    <header>Re: CGImageRef with alpha drawing wrong?</header>
    <body>ps: I don't know, but you're right, all that HIViewDrawCGImage does is flip the context. Nothing else.</body>
  </mail>
  <mail>
    <header>PDF parsing and image replacement...</header>
    <body>I have been trying to use CGPDF functions to help me write a PDF viewer that can special case bitmap images. I have written a table of operations with which I can scan a PDF page and find any images I would like to special case. All is well and good. What I would like to do is replace those images at or before render time of CGContextDrawPDFPage(). I can't seem to find any way to read an operation table (only calls to set are available). My first thought was to do what CGContextDrawPDFPage does internally and set the scanner up to call my &amp;quot;Do&amp;quot; &amp;amp; &amp;quot;EI&amp;quot; handlers. So far how to do this has eluded me. Not sure where to turn to at this point. The CGPDF seems rather lacking in ways to hook in to rendering and data providing (or more likely I can't find what I am looking for because I am not looking in the right place). Any suggestions would be good. One terrible hack I am considering is to scan a page before rendering, if it has the images I want to replace, then get the CFMutableDataRef() of the page then parse it myself (how, not sure yet) and manually replace it in memory, then draw that edited version. Pointers, sample code, ideas are all welcome. W.</body>
  </mail>
  <mail>
    <header>Re: CGImageRef with alpha drawing wrong?</header>
    <body>Hi Dave, It rather sounds like you're hitting a special cased fast-path in the code that isn't behaving correctly in this case.  Perhaps, for example, the fast path is inappropriate for images with an associated mask. So, you should file a bug with a test app that demonstrates the problem. As far as working around it- try doing something weirder to get you off the fast path. :-) You've already found that drawing the image upside down does it.  So, at the cost of a couple extra buffer copies, you could draw your image upside down into a new context, then extract that result to CGImage and draw it upside down into a second context to produce the original desired image, but without the associated mask.  That one would probably draw correctly. -ken</body>
  </mail>
  <mail>
    <header>CGImageRef with alpha drawing wrong?</header>
    <body>ps: I don't know, but you're right, all that HIViewDrawCGImage does is flip the context. Nothing else.</body>
  </mail>
  <mail>
    <header>Re: black point compensation ...</header>
    <body>What are you trying to achieve ? There is one method to do Relative Colorimetry and Black Point Compensation which is documented as far as I remember in the appendixes of the ICC specifications. As far as I remember this is meant as a way to describe how adobe does this internally in their CMM.</body>
  </mail>
  <mail>
    <header>Re: black point compensation ...</header>
    <body>is there a way to specify &amp;quot;adapting  the black points &amp;quot; with a quartz context ?</body>
  </mail>
  <mail>
    <header>Re: black point compensation ...</header>
    <body>Do you mean is there is a way to specify the media black point ('bkpt') when specifying the colorspace in quartz ? If yes, if you need to create a color space dynamically, and if this color space is of class 'RGB ' you can do this with CGColorSpaceCreateCalibratedRGB. Or do you mean if there is a way to specify that the transform between one colorspace to another one should be made by adapting the black points ?</body>
  </mail>
  <mail>
    <header>black point compensation ...</header>
    <body>is there a way to specify &amp;quot;black point compensation&amp;quot; in quartz when specifying a colorspace for a quartz context ?</body>
  </mail>
  <mail>
    <header>Number of inputs to Core Image filter</header>
    <body>Hi, I'm trying to find out if having a large number of inputs to a Core Image filter is likely to cause software rendering fallback. I have a CIFilter that requires 90 float inputs, divided into 15x vec2 and 15x vec4s (90 floats in total). I'd really like a definitive answer as to if this is likely to cause software-fallback. Also, I'd like to know if splitting the filter into 3 discrete passes (all of which will be executed each frame), will reduce the risk of software fallback. Cheers, a|x</body>
  </mail>
  <mail>
    <header>Re: Core Image and wrong pixel color values</header>
    <body>I have also been VERY curious what that checkbox in Quartz Composer does. One way I think will *actually* turn off behind the scenes color correction is to make your input color space, working space and output space for your CIContext all kRGBlinear, and also if you use QCRenderers make sure they are inited with the same linear RGB colorspace. If you are using QT content you can also do this on your QTVisualContext. This speeds things up quite a bit, and you can use CIKernels as one might expect to use them without &amp;quot;somewhat behind the scenes&amp;quot; color correction. Note, there is also a hidden &amp;quot;Image Color Matching&amp;quot; patch in QC, which does something similar, but as far as I can discern different to the 'Uncorrect Core Image' hidden preference. Id love if there was an (un)documented way of globally disabling color correction in procedurally loaded QCRenderers and friends, or perhaps a more human readable document explaining how to set things up for different scenarios, etc.</body>
  </mail>
  <mail>
    <header>Re: Core Image and wrong pixel color values</header>
    <body>There is one way at least : it is to provide the image directly in the destination color space. Of if you have already an image in some over color space that you wish to keep, while using CI for the values with no color profiling is to create an image copy while *assigning* it a new color space (the one for the destination). There is one function to do the latter which is : CGImageCreateCopyWithColorSpace in the CG framework.</body>
  </mail>
  <mail>
    <header>Re: Core Image and wrong pixel color values</header>
    <body>Sadly, there is no exposed way to disable color correction. I'm not sure how QC is doing it, but it's almost certainly unsupported by Apple. Ah, sort of like the Overlay mode. Just keep in mind, when working on the transformation itself, that the image itself is in linear space, and you should be fine!</body>
  </mail>
  <mail>
    <header>Re: Core Image and wrong pixel color values</header>
    <body>That's right, that makes sense. Regards, Thomas</body>
  </mail>
  <mail>
    <header>Re: Core Image and wrong pixel color values</header>
    <body>I'm guessing this is because most image formats store color values as integers rather than floats, and at some step on its way to a floating point value the halfway *index* (127) is divided by the max value (255): I'm not sure of the usual way of dealing with this, but to get 50% you'd have to add one to both numbers before doing the division. hth, -natevw</body>
  </mail>
  <mail>
    <header>Re: Core Image and wrong pixel color values</header>
    <body>It does help! I suspected it was related to color correction. Thanks for the clarifications. A few remarks: - As you mentioned, passing a gray CIColor as input applies the correction to the color which can then be used as the key value. - The settings of the &amp;quot;Image Importer&amp;quot; patch from Quartz Composer contain a checkbox called 'Disable Color Correction'. When checked, the value passed to the code is not corrected and therefore I can hard- code 0.5 as the key value (no need to pass in a CIColor). However when calling the filter from Objective-C, I'm not sure how to disable color correction (I am creating a CIImage using a URL that points to the file on disk). That being said, passing the CIColor may just be the better solution... - Building a gray image (50 %) using Seashore or Gimp (I haven't tried with Photoshop yet) does not produce a perfect gray, the retrieved value in the kernel code is a little bit under 0.5. The 'DigitalColor Meter' utility also shows a difference: 49.8% instead of 50%. Weird. About my filter: there are two input images, a source image and a grayscale image. The grayscale image is used to apply some transformations to the source image. A gray pixel (0.5) in the grayscale image means no transformation on the source image for that particular pixel. Regards, Thomas</body>
  </mail>
  <mail>
    <header>Re: Core Image and wrong pixel color values</header>
    <body>CI operates in linear colorspace (kCGColorSpaceGenericRGBLinear), so any hardcoded values you output will change depending on your output (display) colorspace. For instance, your Mac is probably set to a Gamma of ~1.8. So 0.5 ^ (1.0 / 1.8) = ~0,6804 I'm not sure how your algorithm works, but you can get a color- transform the CIColor from the source colorspace to kCGColorSpaceGenericRGBLinear, giving you your 50% gray in linear space. kCGColorSpaceSRGB (gamma ~= 2.2), you should get something close to: It's a bit confusing, and not very well documented in the Core Image docs, but once you have a handle on it, it does make some sense. I only wish there was a Core Image (or CG) method to convert individual color values from one colorspace to another... On 11 d√©c. 08, at 13:04, Thomas Clement wrote:</body>
  </mail>
  <mail>
    <header>Core Image and wrong pixel color values</header>
    <body>I've started playing with Core Image (using Quartz Composer) and noticed something quiet strange. Writing this kernel code: kernel vec4 myKernel(sampler src) does not produce a pure gray image (50% white) but a lighter gray (around 68.2% white). Likewise pixel values retrieved from an gray input image are not equal to 0.5 (rather something like 0.2). I want to use pure gray value as a key value in my algorithm (I was using 0.5 which does not work as expected). What am I missing? Thanks, Thomas</body>
  </mail>
  <mail>
    <header>Re: Optimized image drawing (currently using CGImageCreate -&amp;gt;	CGContextDrawImage)</header>
    <body>Hi Raphael, Since most of our devs are Windows based, your 3rd tip will be particularly useful. Generally, we try to avoid platform specific code, but we'll likely make an exception when we update our code - our drawing code is becoming quite heavy. We'd really love to offload it, where possible.</body>
  </mail>
  <mail>
    <header>Re: Optimized image drawing (currently using CGImageCreate -&amp;gt;	CGContextDrawImage)</header>
    <body>Well we encountered some problems too while developing it. What I remember from it : - always display image aligned to pixels if you can, and remember that they are a lot of pixel blending and modulation modes to match the maths in the OpenGL API. - don't use polygon antialiasing, i.e. if you need to have quality, do it on the CPU and upload a texture - don't use OpenGL on Windows (use DirectX)</body>
  </mail>
  <mail>
    <header>Re: Optimized image drawing (currently using CGImageCreate -&amp;gt;	CGContextDrawImage)</header>
    <body>Thank you all for your responses. (Three chronological responses in 1 reply) Hi Andr√©, We've done a bit in our code to lighten rendering for pixels which do not change. The unfortunate part is that there *is* a lot that does change in our UIs (more than typical plugins, IMO). Using CGLayers could help - using them in our current drawing code will be difficult to implement in a short time. We're developing for multiple environments and platforms; using CGLayers would affect much of our graphics code, for multiple projects - and we are avoiding making such significant changes at this phase. I expect we will look into this option when we update our graphics code, but that will not happen this year. Hi Rapha√´l, That's quite a difference! We have looked into OpenGL a few times over the years. Historically, OpenGL has provided some problems for us in development and testing. When we reevaluate our graphics code, we'll reevaluate OpenGL's state for the platforms that we support -- like you say, it will not be a small change and we will not have time to implement it in the near future (very busy month), but it looks like it will be worth the change if it works when and where we need it to. Though I would expect it to be a lengthy transition, I think it will be a worthwhile improvement. Hi David, As I mentioned to Rapha√´l, we'll give OpenGL another try. In comparison to a few years ago, we're doing much more graphics processing these days -- hopefully it does well enough in development and testing to adopt. Since we have worked with OpenGL, we realize that it will require a lot of time to update our code, but I believe it is worth the time. The ability to offload most of our processing would be excellent since we're using doing a lot with graphics, for audio developers.</body>
  </mail>
  <mail>
    <header>Re: quartz and colorsync ...</header>
    <body>Tag your data with the correct profile (Generic RGB is a reasonable default for RGB color) and Quartz will automatically color match to the drawing destination. There is no need to set the color profile of the destination context, as the OS obtains the the correct color profile for you automatically. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Optimized image drawing (currently using CGImageCreate	-&amp;gt;	CGContextDrawImage)</header>
    <body>I would have to agree with those saying to go to OpenGL. It won't trivial, and you may want to change what you draw entirely in the name of performance, but you should be able to offload nearly all of the processing time to the GPU by doing so. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>quartz and colorsync ...</header>
    <body>I am trying to do some color management using colorsync ... I don't really understand how to do things like : color matching, proofing, and so on under quartz ... I have find how to access some of the icc profiles, but I didn't understand what are the related quartz call needed to for eg: color match a CIImage and or a CGimage. Nor how ot set the colorprofile of a NSviewfor eg: printing ...</body>
  </mail>
  <mail>
    <header>Re: accessing PDF content</header>
    <body>Wikipedia has some entries you might find useful ATSUI_Concepts/atsui_chap2/chapter_2_section_3.html And the old GX Typography manual (though 100% deprecated) has a lot of general purpose information that is still excellent -2.html I don't have any links to hand but Microsoft has some really good information about fonts in general. Also Adobe have some good information. It is not too bad once you get in the water - there is just a lot that is new. For me the key thing was the realisation that there was a difference between a character-code and a glyph. Once you have got that everything else falls into place.</body>
  </mail>
  <mail>
    <header>Re: Optimized image drawing (currently using CGImageCreate -&amp;gt;	CGContextDrawImage)</header>
    <body>&amp;gt; You may want to try using CGLayers instead, see Well personally (I'm also an audio plug-in developer) I would stay away from CGLayers at least for now. If you take the performance sampling project for the CGLayer from Apple, you'll see clearly that the CGLayer method is always disapointing. I've tested the project with a wide variety of computers, from mac mini to mac pro. As far as I remember, you can get something like 1000 512x512 image draw per sec on a mac pro (not a very good card, but not bad also). If you use OpenGL instead of CoreGraphics, this is more like 7000 1024x768 image per sec. As far as I would have understand, I would have think that CGLayers would have been implemented with OpenGL framebuffers (which basically makes a rendering context being a possible texture source), but it seems that this is not the case. If you really want some more speed, I would explore the OpenGL way, but this is not a 5 minute change. Raphael</body>
  </mail>
  <mail>
    <header>Re: Optimized image drawing (currently using CGImageCreate -&amp;gt;	CGContextDrawImage)</header>
    <body>You may want to try using CGLayers instead, see for more information about them.  If drawing to a CGLayer context instead of a CGImage's graphics context doesn't speed things up, perhaps you can cache a lot of the graphics elements as CGLayers so that the GPU can composite them quickly.  (You mentioned that you're writing an audio plugin, so I'm assuming you have a lot of little images of e.g. sliders or knobs that are likely to be be the same and can thus be cached.) -- Attachment:</body>
  </mail>
  <mail>
    <header>Re: Optimized image drawing (currently using CGImageCreate	-&amp;gt;	CGContextDrawImage)</header>
    <body>1) The users may not have the overhead to spare, especially when multiple instances are visible. It is a audio plugin, many users manage/push their system's limits as they work - offloading CPU takes time, so it is a performance critical context where audio is the priority. 5% is a large number, considering everything that is happening. If we can reduce it without a major rewrite, we will. 2) Because we would also like to increase the frame rate, or offer it as an option.</body>
  </mail>
  <mail>
    <header>Re: Printing doesn't work with layer backed views (still!)?</header>
    <body>On Dec 8, 2008, at 3:17 PM, David Duncan wrote: This seems like a pretty common case, so hopefully there will be automated tests in place after this. In my case, I don't want pixels most times.  My CALayers draw using Quartz and as such should be able to emit vector representations (unless I have an attached image). I don't need the CIFilters, borders, mask layer or much complicated. A lot of utility would be had by supporting printing with the following properties: - hidden - backgroundColor - opacity - masksToBounds - shadows - various transforms, even if just affine. - maybe corner radius and border, but that's getting iffy I'm pretty sure most people sending CALayers to PDF would not want zillions of bytes of bitmaps unless they explicitly asked for it, so a vector-only printing path with support for calling out to a pixel- accurate renderer is what I'd expect to exist. I guess it's even possible that the choice of rendering path could be based on the properties set on the layers. I've managed to get my test case to emit actual PDF with some subclass methods shown below.  These aren't great, and make some assumptions they shouldn't.  On the other hand, this should really work out of the box, IMO. -- On my layer: // TODO: Transforms // TODO: Clipping via masksToBounds if (![gc isDrawingToScreen]) // We get called on behalf of the layer; don't recurse infinitely // With this, -drawRect: gets called again (need to get print panel up and stuff with more complicated code, but this yields actual PDF). NSPrintOperation *op = [NSPrintOperation [pdfData writeToURL:[NSURL fileURLWithPath:@&amp;quot;/tmp/test.pdf&amp;quot;]</body>
  </mail>
  <mail>
    <header>Re: Optimized image drawing (currently using CGImageCreate	-&amp;gt;	CGContextDrawImage)</header>
    <body>To be honest, your draw is taking less than 5% of the CPU (presumably while running at 30Hz?) so I'm not certain what target your trying to reach. If you could say why your current performance is a problem that might help, but it looks like your on a pretty optimal path as it is... -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Optimized image drawing (currently using CGImageCreate	-&amp;gt;	CGContextDrawImage)</header>
    <body>We are working on updating graphics code. We're drawing images that we render in realtime to screen. We're using CGImageCreate then calling CGContextDrawImage - for each frame that we need to draw. The images are opaque, and we're using kCGImageAlphaNoneSkipFirst. This approach is taking more time than I hoped it would (yes, I profiled with Shark, CGContextDrawImage in particular) and I was figuring there would be a faster way that is safe and would not require too many changes to our code. Our executable is a bundle, and we are subject to render in Carbon and Cocoa environments. Our target is 10.4 and up, and we do not currently support 64 bit. + 5.6%, MyFoo | + 4.6%, CGContextDrawImage, CoreGraphics | | + 4.6%, ripc_DrawImage, libRIP.A.dylib | | | 4.3%, argb32_image, CoreGraphics | | | 0.0%, CGSDeviceUnlock, CoreGraphics | | | 0.0%, CGSDeviceLock, CoreGraphics | | | - 0.0%, CGSImageDataLock, CoreGraphics | - 0.2%, QDBeginCGContext, QD | - 0.2%, _CFRelease, CoreFoundation | - 0.2%, CGContextSynchronize, CoreGraphics | - 0.2%, (anonymous namespace)::createImage() | 0.1%, GetGWorld, QD Info: - Our Pixels are stored as UInt32 (8 bits/component). - The image that is being drawn is opaque, and we've configured it to skip alpha. - The image is about 750 x 550 - it animates and has several layers which we're rendering to a single image. - QD calls are necessary - This sample was taken earlier this week, we don't call CGContextSynchronize anymore.</body>
  </mail>
  <mail>
    <header>Re: accessing PDF content</header>
    <body>What happens is that only some glyphs from a complete font's glyph set are actually required from the font in any given PDF document.  So only those glyphs actually make it into the target PDF's embedded font.  For this reason, the encoding on those embedded fonts may not correlate with any standard even if the original text was entered for a latinate language. ___ Andrew Lindesay www.lindesay.co.nz</body>
  </mail>
  <mail>
    <header>Re: Printing doesn't work with layer backed views (still!)?</header>
    <body>I believe the bug resulted in fixes to AppKit's NSView printing path that was causing the contents of NSView owned layers to not be rendered properly for printing. Since your sample creates and adds another CALayer as a sublayer to the NSView in question, it isn't participating in the printing path. In general, getting that to work arbitrarily is a bit harder. The only reliable way to correctly print a Core Animation layer tree is to use the CARenderer to render into an OpenGL context and readback the pixels. -renderInContext: is not appropriate for an arbitrary render as it does not implement the full Core Animation rendering model, although it might suffice in some situations (although probably not in yours as you note). -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: accessing PDF content</header>
    <body>&amp;gt; Because in the encoded font probably the characters represented by most Guess I need to read up glyphs and encoding. But I find it awkward as I never expected glyphs in that sense in western languages like English. I would have expected this problem to only appear if the the text entered is a little more exotic. Any good pointers to read up on this appreciated. Well, I would have thought that for English a mapping should always be -&amp;gt; f,i ...and IIUC that might not be possible to extract? Well, sure ...if it's not there - it's not there. Hm ...right :) *nod* I've just tried the PDF document that I was using. And in fact it does give the same result. It extracts the characters as a glyph and makes it one character when pasted into Textmate. Interesting. That PDF has been created by Pages. A PDF created via the print dialog from TextEdit gave a similar result. A PDF generated from Open Office behaves totally different - the Tj is actually nothing but a single space. Man - this is scary :) cheers -- Torsten</body>
  </mail>
  <mail>
    <header>Printing doesn't work with layer backed views (still!)?</header>
    <body>I logged this in #5671982 ages ago, which was returned to me as fixed in 10.5.3. Stupidly, I didn't force myself to take time out to test that it fixed the problem, assuming that it'd be obvious. It seems it isn't, or the fix requires a non-obvious change in my test case. I'd expect that when printing a layer-backed view, any cached backing store images would be ignored and instead the layers would receive requests to draw into a PDF-based CGContextRef (sans CIFilters, or maybe emitting bitmaps if filters are enabled). But, it seems like this still doesn't work and nothing is emitted at all when attempting to print such views. This seems amazingly obvious to me, so maybe my test case is just that no calls are made to the layer-drawing. I suppose I can hack together my own printing path based off the layer tree, transforms and such, but that would be a bunch of work that should already be done.  From my testing -renderInContext: also seems to use cached backing store, sadly.</body>
  </mail>
  <mail>
    <header>No way to get CAMediaTimingFunction used for implicit animations?</header>
    <body>When adding an implicit animation, the CAMediaTimingFunction has the control points: timing:&amp;lt;CAMediaTimingFunction: 0x1b036a40&amp;gt; (0,0), (0.25,0.1), (0.25,1), (1,1) timing:&amp;lt;CAMediaTimingFunction: 0x1875e10&amp;gt; linear, (0,0), (0,0), (1,1), (1,1) timing:&amp;lt;CAMediaTimingFunction: 0x1871ab0&amp;gt; ease-in, (0,0), (0.42,0), (1,1), (1,1) timing:&amp;lt;CAMediaTimingFunction: 0x186b450&amp;gt; ease-out, (0,0), (0,0), (0.58,1), (1,1) timing:&amp;lt;CAMediaTimingFunction: 0x186bf80&amp;gt; ease-both, (0,0), (0.42,0), (0.58,1), (1,1) So, it seems like there is no public way to access the timing function used for implicit animations, unless I'm missing it somewhere.  Without this, mixing explicitly and implicitly animations will result in visual glitches.  Of course, I can hard-code the constants I've found empirically, but they might change in the future.  Is there public API for this that I'm not seeing?</body>
  </mail>
  <mail>
    <header>Re: accessing PDF content</header>
    <body>Because in the encoded font probably the characters represented by most glyphs map orthogonally to an ASCII/Unicode equivalent, but glyphs like the ligature &amp;quot;fi&amp;quot;  are represented by a single glyph encoding, which does not map back to its represented multiple characters. There are several issues that may prevent successful reduction from the text drawing commands back to the represented text in the form character streams: ‚Ä¢ As seen above, not all glyphs map to a single character ‚Ä¢ Glyph encodings in fonts do not always map to standard (ASCII/ Unicode) character codes. Because of the latter a font object may contain a ToUnicode entry which provides a translation table of glyph codes to 'Unicodes'. But this is of no help if a glyph represents multiple characters, and a font may simply not contain a ToUnicode mapping, or the mapping table may be faulty or incomplete. There is also an &amp;quot;ActualText&amp;quot; element that may be used to represent the visible text as a sequence of character codes (See Adobe PDF Reference 1.7, paragraph 10.8.3), but I have yet actually to actually see such an entry in any PDF document. In short, reducing the textual elements of a page stream back to character codes can be quite a bit of work, and will frequently not be fully possible. You can see this in Preview: test the text extraction of a few PDF documents by copying text and pasting it into a text editor, then look for places where words are not quite right. In my experience this happens mostly where the visual text contains ligatures and also because of faulty ToUnicode maps. (The latter is an issue with the Mac OS X PDF producing frameworks, which can also produce other articfacts. I have submitted bug reports on two such issues.) ---------------------------------------------------- It is better to light a candle than to curse the darkness ----------------------------------------------------</body>
  </mail>
  <mail>
    <header>Re: accessing PDF content</header>
    <body>&amp;gt; The actual text may represent offsets in a custom encoding in a sub-setted Oh my ...anyone got some further pointers to documentation on that? Or dare I ask - even an examples? ..or open source to look at? But I am a little surprised as it's the most simple PDF. In fact I am not sure I understand why only this character pair (of &amp;quot;fi&amp;quot;) is affected ...it's even from the same text line. Same formatting and everything. And it's in the ASCII range. Anyway - looking at the spec it seems I indeed to run this through the font. And I assume the font is not on the page level. Just FYI - here is the page stream: Q	Restore graphics state q	Save graphics state re	Append rectangle to path W	Set clipping path using nonzero winding number rule n	End path without filling or stroking cs	Set color space for nonstroking operations sc	Set color for nonstroking operations re	Append rectangle to path f	Fill path using nonzero winding number rule i	Set flatness tolerance re	Append rectangle to path f	Fill path using nonzero winding number rule sc	Set color for nonstroking operations i	Set flatness tolerance q	Save graphics state cm	Concatenate matrix to current transformation matrix BT	Begin text object Tm	Set text matrix and text line matrix Tf	Set text font and size Tj	Show text ET	End text object Q	Restore graphics state q	Save graphics state cm	Concatenate matrix to current transformation matrix BT	Begin text object Tm	Set text matrix and text line matrix Tf	Set text font and size Tj	Show text ET	End text object Q	Restore graphics state q	Save graphics state cm	Concatenate matrix to current transformation matrix BT	Begin text object Tm	Set text matrix and text line matrix Tf	Set text font and size Tj	Show text ET	End text object Q	Restore graphics state q	Save graphics state cm	Concatenate matrix to current transformation matrix BT	Begin text object Tm	Set text matrix and text line matrix Tf	Set text font and size Tj	Show text ET	End text object Q	Restore graphics state q	Save graphics state cm	Concatenate matrix to current transformation matrix BT	Begin text object Tm	Set text matrix and text line matrix Tf	Set text font and size Tj	Show text ET	End text object Q	Restore graphics state q	Save graphics state cm	Concatenate matrix to current transformation matrix BT	Begin text object Tm	Set text matrix and text line matrix Tf	Set text font and size TJ	Show text, allowing individual glyph positioning ET	End text object Q	Restore graphics state q	Save graphics state cm	Concatenate matrix to current transformation matrix BT	Begin text object Tm	Set text matrix and text line matrix Tf	Set text font and size Tj	Show text ET	End text object Q	Restore graphics state q	Save graphics state cm	Concatenate matrix to current transformation matrix BT	Begin text object Tm	Set text matrix and text line matrix Tf	Set text font and size Tj	Show text ET	End text object Q	Restore graphics state q	Save graphics state cm	Concatenate matrix to current transformation matrix BT	Begin text object Tm	Set text matrix and text line matrix Tf	Set text font and size Tj	Show text ET	End text object Q	Restore graphics state q	Save graphics state cm	Concatenate matrix to current transformation matrix BT	Begin text object Tm	Set text matrix and text line matrix Tf	Set text font and size Tj	Show text ET	End text object Q	Restore graphics state q	Save graphics state cm	Concatenate matrix to current transformation matrix BT	Begin text object Tm	Set text matrix and text line matrix Tf	Set text font and size Tj	Show text (failed) ET	End text object Q	Restore graphics state q	Save graphics state cm	Concatenate matrix to current transformation matrix BT	Begin text object Tm	Set text matrix and text line matrix Tf	Set text font and size Tj	Show text (failed) ET	End text object Q	Restore graphics state q	Save graphics state cm	Concatenate matrix to current transformation matrix BT	Begin text object Tm	Set text matrix and text line matrix Tf	Set text font and size Tj	Show text ET	End text object Q	Restore graphics state q	Save graphics state cm	Concatenate matrix to current transformation matrix BT	Begin text object Tm	Set text matrix and text line matrix Tf	Set text font and size Tj	Show text ET	End text object Q	Restore graphics state q	Save graphics state cm	Concatenate matrix to current transformation matrix BT	Begin text object Tm	Set text matrix and text line matrix Tf	Set text font and size Tj	Show text (failed) ET	End text object Q	Restore graphics state Q	Restore graphics state cheers -- Torsten</body>
  </mail>
  <mail>
    <header>Re: accessing PDF content</header>
    <body>The actual text may represent offsets in a custom encoding in a sub- setted font.  If this is the case then you need to create a mapping from the character codes in the text object ---&amp;gt; characters in the font encoding ---&amp;gt; font glyph names ---&amp;gt; target encoding. ___ Andrew Lindesay www.lindesay.co.nz</body>
  </mail>
  <mail>
    <header>Re: accessing PDF content</header>
    <body>&amp;gt;&amp;gt; But how can I access the actual content of the text object? Awesome, thanks Antonio. That was the last puzzle missing. Unfortunately it is giving me mixed results that I don't quite understand. The above is working fine for most strings (except that it is leaking). But if there is a  &amp;quot;fi&amp;quot; in the String it gets printed as U+00DE. Is this some kind of encoding problem? cheers -- Torsten</body>
  </mail>
  <mail>
    <header>Re: accessing PDF content</header>
    <body>You need to check the PDf reference on how the contents are represented for each operator. E.g. a matrix is represented by an array of six numbers, so those six number will be on the stack. You can pop them off the stack (which means you obtain them in reverse order): - (void)op_Tm:(CGPDFScannerRef s, void *info) // Now do something with the data and pop the next number. Repeat until you've popped all six numbers. Of course you may actually implement this differently, but it paints the picture. The popping functions return a bool indicating whether the value was successfully popped of the stack. The op_Tf function should pop a number and a name. For the Tj operator you would pop a string. ----------------------------------------------------------- Don't believe everything you think -----------------------------------------------------------</body>
  </mail>
  <mail>
    <header>accessing PDF content</header>
    <body>Through the CGPDFOperatorTableSetCallbacks I get the following list of operators that are called: .. BT	Begin text object Tm	Set text matrix and text line matrix Tf	Set text font and size Tj	Show text ET	End text object .. .. .. But how can I access the actual content of the text object? Any pointers? cheers -- Torsten</body>
  </mail>
  <mail>
    <header>Re: Core Animation problems -o-n--P-P-C- in certain configurations</header>
    <body>It turns out this problem isn't PPC in general, which seemed like it had to be unlikely but for the test case. However, I was testing on my PPC mini via Screen Sharing, while it was headless. Once I plug a monitor in, it works on screen and over Screen Sharing too, until I unplug the monitor. Then the layer-hosting view stops displaying completely! So for the record, as of 10.5.5 there seems to be serious problems with Core Animation views under Rosetta, and in headless configurations (maybe only PPC). I've updated rdar://problem/6402982 with this info. If anyone has an Intel machine without a monitor they can try the code on ( ), I'd love to know if we can expect problems on Intel too in this situation. Does anyone know offhand if I could expect Quartz Layers (CGLayerRef) to behave better in this situation, even when they might be hosted on the graphics card? Given that they could be drawn directly in drawRect, rather than getting drawn on Core Animation's whim, it seems things might work more reliably if I convert my view to use CGLayers instead. thanks, -natevw</body>
  </mail>
  <mail>
    <header>Re: Confusion with kCAOnOrderOut/kCAOnOrderIn</header>
    <body>On Nov 26, 2008, at 4:57 PM, John Harper wrote: Ouch. Logged as 6408602. Thanks for the hint!  I've made some progress using the approach you describe, but now I'm hitting two new issues shown in this updated This version of the sample uses a CISwipeTransition to eliminate the possibility of these being due to my custom CIFilter.  The swipe is configured so that there is a white bar transitioning across the layer as it is animated.  Since the layer content isn't changing, the two input images to the swipe are the same (which makes swipe useless for the real case) and this is the only indication of the animation. When we want to remove the layer, a transition animation is started w/o actually removing it.  On completion, we remove the layer.  The order-out animation is turned off via the delegate even though it should have no effect due to the Radar above. - The animating content is offset by the origin of the parent layer's frame origin.  I've tried manually poking the CIFilter's 'inputExtent' w/o luck. The documentation for CATransition.filter says that kCIInputExtentKey will be set if supported on the filter, but I haven't seen it get set when using my custom filter. - The timing of the transition is strange; I'm setting the kCATransactionAnimationDuration to 2.0, but the swipe of the white bar across the entire span of the animating region takes place in less time than that.  I'd expect inputTime to get set to [0..1], normalized across the animation's duration and adjusted for the timing curve. Maybe there's something else I need to do to get this (though it seems like it would be the default behavior). Even if I can get these two issues working, it is going to be a pain to track whether I'm in the midst of removing a layer and then &amp;quot;re-add&amp;quot; it by inverting the animation in progress, etc.  Hopefully future CA versions will make this basic case work.  This was the sort of cruft that CA promised to eliminate =)</body>
  </mail>
  <mail>
    <header>How to draw 1-bit B&amp;amp;W image on NSView?</header>
    <body>Hi all I want to show B&amp;amp;W image on my application image view. I had the raw data of 1-bit depth B&amp;amp;W image. How to draw it on the NSView? I had try that: CGImageCreate( imageWidth, imageHeight, 1, 1, bytesPerRow, colorSpace, bmpInfo, dataProvider, NULL, false, It failed. Anyone can give me some hint? - yang</body>
  </mail>
  <mail>
    <header>CGRegisterScreenRefreshCallback vs. Keynote</header>
    <body>Hello, I'm doing some work on an app that records the screen, and I've run into an issue with CGRegisterScreenRefreshCallback and Keynote. When Keynote presentations are being 'presented' full-screen, all callbacks registered with CGRegisterScreenRefreshCallback stop being called, and only resume again when the presentation stops.  If I change Keynote's &amp;quot;Allow Expos√©, Dashboard and others to use screen&amp;quot; (in the &amp;quot;Slideshow&amp;quot; panel), the callbacks are called as expected.  I'd rather not have to ask users to change their keynote prefs, however. I have noticed that if I just grab the full screen periodically anyway (I'm using CGWindowListCreateImage with kCGWindowListOptionAll and kCGNullWindowID), I do get the pixels from the Keynote presentation, so it would appear that I am &amp;quot;allowed&amp;quot; to see it.  Changing to capture the entire screen all the time, though, is very inefficient. Is there anything I can do to get CGRegisterScreenRefreshCallback callbacks to work in this situation?  Is there another method I could be using to get dirty rects?  Alternatively, is there a way I can know if the updates are going to stop, so that I can switch to just doing periodic grabs while the updates are not working? Thanks, Jamie. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Can I output PNG files that don't depend on my monitor's colour profile?</header>
    <body>On 10 January 2012 18:23, James Montgomerie If you're using a script to do that bytewise comparison, you could have it run pngcrush first: I'd be interested to hear an answer at the Quartz level if there is one, though. H</body>
  </mail>
  <mail>
    <header>Can I output PNG files that don't depend on my monitor's colour profile?</header>
    <body>I'd like to create PNG files using an sRGB color space.  In my imagination, I'll be able to bytewise-compare files output by different runs of my program for testing purposes, to check the rendering doesn't change.  This works mainly, but if I change monitors, the files start to be different even with no changes in the program. Looking at them visually, it looks like the color profiles are different (colors are slightly different shades). Am I doing something wrong?   Code below.  This is being run on Lion. CGContextRef renderingContext = CGBitmapContextCreate(NULL, pageBounds.size.width, pageBounds.size.height, 8, pageBounds.size.width * 4, colorSpace, // Do some rendering, occasionally calling CGContextSetFillColor or CGContextSetStrokeColor, // but not changing the color space (unless it can happen implicitly?) CGImageDestinationRef pngDestination = CGImageDestinationCreateWithURL( (__bridge CFURLRef)[NSURL fileURLWithPath:testOutputPDFPath], kUTTypePNG, 1, Thanks! Jamie. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Confusing redraw error in CIFilter related to mod()</header>
    <body>I've never written any core image code before -- I've read a fair bit of documentation, but I might be missing something obvious so please bear with me. I've written a bump map to normalmap converter as a CI filter. It works, and it's fast and that's cool. However, I need the sampler to *tile* the image, not clamp it. The CISampler docs specify only two wrap modes, kCISamplerWrapBlack, and kCISamplerWrapClamp, so I used mod() in the CIKernel to wrap across the sampler dimensions. kernel vec4 normalMap( sampler src, float strength, float r ) float alpha = sample( src, samplerCoord(src)).a, n = sample( src, mod( samplerCoord(src) + vec2(  0, +r ), size)).r, s = sample( src, mod( samplerCoord(src) + vec2(  0, -r ), size)).r, e = sample( src, mod( samplerCoord(src) + vec2( +r,  0 ), size)).r, w = sample( src, mod( samplerCoord(src) + vec2( -r,  0 ), // flat goes from 0 to 1, as strength goes from 0 to 0.5 //steep goes from 0 to 1 as strength goes from 0.5 to 1 // generate the normal, scale it to 0-&amp;gt;1 vec3 normal = normalize( vec3( flat*(w-e), flat*(s-n), Now, this produces the expected results. Except there's an odd redraw bug which looks like a wrapping error. The bug shows up somewhat randomly. Usually when I first run my app ( which is a simple app, derived from the haze demo ), but sometimes when changing the input parameters. Resizing the window *usually* fixes it. Here's two screenshots. The first shows the error -- the horizontal 1- pixel cyan line across the top and the purple line across the bottom. Those lines correspond exactly to the sample radius ( the third param to the kernel ). This second screenshot shows the window after resizing it slightly, and you can see the lines are gone. I thought it might be a result of drawing a subset of the image in my drawRect call, but forcing a draw of the whole view rect didn't fix it. Here's my -drawRect: - (void)drawRect:(NSRect)rect CGContextRef cgContext = (CGContextRef)[[NSGraphicsContext CIContext *context = [CIContext contextWithCGContext: cgContext options: [NSDictionary dictionaryWithObjectsAndKeys: (id)workingColorSpace, kCIContextWorkingColorSpace, if ( context ) if ( filter ) CGRect  cg = CGRectMake(NSMinX(rect), NSMinY(rect), NSWidth(rect), [context drawImage: [filter valueForKey: kCIOutputImageKey] atPoint: cg.origin Note, the color space stuff is in there to guarantee that the output values are numerically -- not optically -- correct. I'm guessing that this has to do with my use of mod(), but since CISampler doesn't support tiling, I don't see a way around it. shamyl zakariya - squamous and rugose</body>
  </mail>
  <mail>
    <header>Resize a Suffix</header>
    <body>Hello, I would like to resize a suffix before it reaches argument 2 in a string printer patch. Is there a way to achieve this? Thank's</body>
  </mail>
  <mail>
    <header>Re: How does one access the CoreGraphics framework and CGDirectDisplay.h ?</header>
    <body>-Michael ---------------------- The united stand.  The divided get played. On Feb 25, 2009, at 1:38 AM, Michael Vannorsdel wrote: Attachment:</body>
  </mail>
  <mail>
    <header>Re: How does one access the CoreGraphics framework and	CGDirectDisplay.h ?</header>
    <body>Could you include the error you're getting?  You really shouldn't need to include any of those if you're linked to Cocoa. On Feb 25, 2009, at 12:03 AM, Michael A. Crawford wrote:</body>
  </mail>
  <mail>
    <header>Re: How does one access the CoreGraphics framework and	CGDirectDisplay.h ?</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>How does one access the CoreGraphics framework and	CGDirectDisplay.h ?</header>
    <body>Attachment:</body>
  </mail>
  <mail>
    <header>I need to draw in millimeters; how can I determine mm to graphical unit ratio?</header>
    <body>Attachment:</body>
  </mail>
  <mail>
    <header>Re: Gradients along a circular path</header>
    <body>On Feb 18, 2009, at 9:14 AM, Nick Nallick wrote: A bitmap image, or a series of pie-shaped wedges in the gradient colors.  The trick in that case will be choosing the right number of wedges to make the graident &amp;quot;smooth enough&amp;quot;.</body>
  </mail>
  <mail>
    <header>Re: Gradients along a Bezier path</header>
    <body>You would have to flatten the path and then apply a series of gradients along the flattened segments... or something along those lines (the joints where segments overlap would be an interesting challenge). Even if you did that, you might not end up with the smooth gradient you might like... depending on the distances between the control points, the color blended along the parametric variable might travel very quickly in some places, and much more slowly in others.  If you want a smooth gradient, you'd have to advance the gradient's color at a rate unrelated to either geometry or the parametric variable.  That would be... challenging.</body>
  </mail>
  <mail>
    <header>Re: Gradients along a Bezier path</header>
    <body>... Just a thought. I have a more generalized and possibly slightly different question which may shed some light. (1) If one has an open Bezier curve composed of line segments and arcs (ovoids, whatever), and the beginning of the first line is at parameter 0.0 and the end of the last arc (for example) is at parameter 1.0 (these are not coordinates, but proportional distance along the aggregate Bezier curve, where 1.0 represents the whole length, and 0.5 would represent the mid-point), how does one apply a gradient to the edge color? Since it's open, I'm not talking about fill gradient, but the next exercise (2) could be to close it by dropping a couple lines down from the above-mentioned Bezier curve and joining the bottom ends of those 2 line segments with a horizontal line segment, and do the fill gradient, while letting the edge color be black, for example. &amp;nbsp;Hmmm, now that I think of it, the desired result could be a couple differen things. It could be, e.g. a gradient running straight across from left to right, or it could be a gradient that follows the Bezier curve around from beginning to closed end, in which case you have the difficulty of deciding what should appear at the, uh, collision, along the middle of the shape. &amp;nbsp;I suppose one could draw another invisible curve along the &amp;quot;middle&amp;quot; and somehow use that, but don't quite see how that could be made to work based on the examples in the Quartz doc. &amp;nbsp;_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;(email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Gradients along a Bezier path</header>
    <body>I have a more generalized and possibly slightly different question which may shed some light. (1) If one has an open Bezier curve composed of line segments and arcs (ovoids, whatever), and the beginning of the first line is at parameter 0.0 and the end of the last arc (for example) is at parameter 1.0 (these are not coordinates, but proportional distance along the aggregate Bezier curve, where 1.0 represents the whole length, and 0.5 would represent the mid-point), how does one apply a gradient to the edge color? Since it's open, I'm not talking about fill gradient, but the next exercise (2) could be to close it by dropping a couple lines down from the above-mentioned Bezier curve and joining the bottom ends of those 2 line segments with a horizontal line segment, and do the fill gradient, while letting the edge color be black, for example.  Hmmm, now that I think of it, the desired result could be a couple differen things. It could be, e.g. a gradient running straight across from left to right, or it could be a gradient that follows the Bezier curve around from beginning to closed end, in which case you have the difficulty of deciding what should appear at the, uh, collision, along the middle of the shape.  I suppose one could draw another invisible curve along the &amp;quot;middle&amp;quot; and somehow use that, but don't quite see how that could be made to work based on the examples in the Quartz doc.</body>
  </mail>
  <mail>
    <header>Re: Gradients along a circular path</header>
    <body>On Feb 18, 2009, at 6:06 AM, Luigi Castelli wrote: I think you're on your own.  CoreGraphics supports the two forms of gradients supported by PDF.  For others you'll need to create a bitmap image. Nick</body>
  </mail>
  <mail>
    <header>Re: Gradients along a circular path</header>
    <body>Let's imagine a circle and let's say that the beginning of the circle is the topmost point located at an angle of 0 degrees. Then the rightmost point would be located at an angle of 90 degrees. The bottommost point at an angle of 180 degrees and the leftmost point at 270 degrees. I want to be able to create a gradient whose color changes according to the angle. So for instance at 0 degrees is white and at 90 degrees black, and so on and so forth... I hope I made myself clear. I think photoshop refers to these kind of gradients as angular gradients. Ideas? Thanks. - Luigi</body>
  </mail>
  <mail>
    <header>Re: Gradients along a circular path</header>
    <body>Yes, of course... I am very familiar with routines for both creating axial and radial gradients. My question is about creating gradients that follow arbitrary paths. Specifically for my case, I need an axial gradient around a circular path. I am sure I am not the first one with this problem... Any solutions? Thanks - Luigi</body>
  </mail>
  <mail>
    <header>Re: Gradients along a circular path</header>
    <body>Have you looked at CGShadingCreateRadial() ?  That answers the circular issue, now if only there were a way to do it along an arbitrary path... Cheers, Brian</body>
  </mail>
  <mail>
    <header>Gradients along a circular path</header>
    <body>Hi there, is there a way in Quartz to draw a linear gradient along a path other than straight? Along a circular path, for instance... It doesn't seem possible, looking at the current API. I tried to stich together a bunch of small wedges - each with a different linear gradient the previous - but the antialiasing doesn't allow that to look nice. Also turning antialiasing off doesn't help expecially when transparency is involved. I am kind of stuck... Will the possibility of creating circular gradients be implemented in the future? Does anybody know of any workarounds for the time being? Thanks for any advice/suggestion. - Luigi</body>
  </mail>
  <mail>
    <header>Re: Mouse interaction with NSViews that have transformed backing	layers?</header>
    <body>AppKit and Core Animation each keep separate knowledge of where a view and its corresponding layer are. AppKit does hit testing on its own geometry, not on the geometry from Core Animation, so implementing this is going to be non-trivial. I would honestly recommend against doing it. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Mouse interaction with NSViews that have transformed backing layers?</header>
    <body>I have a bunch of standard Cocoa views (buttons, labels etc) that are layer-backed. What I want to do is apply 3D-transformations on the layers of these views and still be able to interact with them. And that is my problem, whenever I transform a layer it's frame is obviously altered, however, the area that receives mouse events remains the same, i.e. the unchanged frame of the NSView instance. What I want is everything that NSView and it's subviews provides but the ability to transform the backing layer without loosing the ability to interact with the view. I have tried to come up with a generic solution to this in a couple of different ways without getting it right. First I tried to override the method - (NSView*)hitTest:(NSPoint)aPoint in NSView and perform the actual hit test on the view's backing layer instead of the default implementation, and return the view that corresponds to the layer being hit. I don't know if this is the way to go but I didn't manage to convert the incoming NSPoint to the layers coordinate space correctly. Another way I have tried to solve it is to create a subclass of NSView that I then ask to pose as NSView at application launch. In this subclass I override the method - (NSRect)frame and return the frame of the view's layer instead of the frame of the actual view. This didn't seem to have the effect I was after, the &amp;quot;clickable&amp;quot; area remained the same even though the returned value changes when the layer has been transformed. The method is called when I click the controls so I assumed that the mouse events uses the returned NSRect of that method to determine which view to send the event to. Maybe this assumption is incorrect? Is there some other method I should modify instead? Have anyone else dealt with this problem? Is one of these attempts the way to go or should I try something different? Any thoughts on the matter are welcome! Thanks! William</body>
  </mail>
  <mail>
    <header>Event taps: Varying results with CGEventPost, kCGSessionEventTap,	kCGAnnotatedSessionEventTap, CGEventTapPostEvent</header>
    <body>I'm running into a thorny problem with posting an event from an event tap. I'm tapping for NSSystemDefined at kCGHIDEventTap, then replacing the event with a new one. The problem I'm running in to is that depending on how I post the event, it's being seen only by some applications. My test applications are Opera, Firefox, Quicksilver, and Xcode. Here are the different techniques I've tried within my event tap callback, with results. I'm expecting an action (the &amp;quot;correct response&amp;quot;) from each app; &amp;quot;system beep&amp;quot; means the nothing-is- bound-to-that-key system sound. 1. Create a new event, and return it from the callback. Opera: no response/system beep, Firefox: no response/system beep, Quicksilver: correct response, Xcode: no response/system beep 2. Create a new event, post to kCGSessionEventTap with CGEventPost, return null. Opera: no response/system beep, Firefox: no response/system beep, Quicksilver: correct response, Xcode: no response/system beep 3. Create a new event, post to kCGAnnotatedSessionEventTap with CGEventPost, return null. Opera: correct response, Firefox: correct response, Quicksilver: no response/system beep, Xcode: no response/system beep 4. Create a new event, post with CGEventTapPostEvent, return null. Opera: no response/system beep, Firefox: no response/system beep, Quicksilver: correct response, Xcode: no response/system beep 5. Create a new event, post to kCGSessionEventTap with CGEventPost, and return new event. Opera: no response/system beep, Firefox: no response/system beep, Quicksilver: correct response, Xcode: no response/system beep 6. Create a new event, post to kCGAnnotatedSessionEventTap with CGEventPost, and return new event. Opera: correct response and system beep, Firefox: correct response and system beep, Quicksilver: correct response and system beep, Xcode: no response/double system beep 7. Create a new event, post with CGEventTapPostEvent, and return new event. Opera: no response/system beep, Firefox: no response/system beep, Quicksilver: correct response, Xcode: no response/system beep (6) is the best, but users are complaining about the extra system beep on correct responses, which I'm guessing is coming from the double- posting of the event. I'm not sure of other combinations to try, or where else to look. Can anyone offer any guidance? Is there any way to get the results of both returning the event from my callback and posting to the annotated tap without doing both? Thanks in advance -- Kevin Kevin Gessner email@hidden</body>
  </mail>
  <mail>
    <header>Re: Running out of memory when adding CGImageRef-to-NSImage frames	to QTMovie in NSOperation subclass</header>
    <body>A follow-up for posterity. I use [NSImage -addRepresentation:] instead of -TIFFRepresentation and made the object life cycle local. The result in Instruments was that the object allocation graph(all objects still living), drew like how the stock market has been recently(up and down, indicating objects being released as expected) rather than how the stock market used to be a two years ago(steadily increasing). Thanks Ken. Side note, in Instruments it seems generally more useful to use Leaks instead of just ObjectAllocations alone. Michael</body>
  </mail>
  <mail>
    <header>Synchronization &amp;amp; timing issue</header>
    <body>Hi, &amp;nbsp; I&amp;#39;ve run into a problem of synchronization and timing with Quartz 2D. Simplifying, my application has a main thread that put on a queue some requests for displaying pictures with a certain duration. This requests are picked up by my display link callback and rendered. In the worst scenario, my application should display full screen pictures with frequency equal to the screen refresh rate and each picture should persist only for the time of a screen refresh. The display link callback is called precisely every 1/60 seconds (no matter what the screen refresh rate is), provided that it doesn&amp;#39;t take more than 1/60 seconds to process the rendering. Also, these calls occur asynchronously respect to the screen refresh. I know, moreover that Quartz uses double buffering and during each flush the drawing operations are blocked. Considered all these premises, how can I fit the requirements of the case I&amp;#39;ve described into the constraints imposed by Quartz? For what I could understand, there&amp;#39;s no a really a precise way to know when a flush has just occurred: I could set a screen refresh callback, but it would depend on the main events cycle of my application and so it turns out to be not very accurate). What I did then, is setting a timer that fires with frequency equal to the refresh rate and set a flag. From inside the&amp;nbsp; display link callback, each time a picture is drawn I call CGContextSynchronize(), then I check the flag and if it&amp;#39;s on I call QDFlushPortBuffer() (that defers the flush of the context to the next avaiable chance) and I reset it. I keep running a loop in the display callback till all the pictures in the queue have been rendered, which may takes several By this approach, I got something that works acceptably, but for the worst scenario I see I miss some frames... I could delay the drawing of a picture till I&amp;#39;m sure the previous one has been drawn but this would break the timing costraint (1 picture every screen refresh rate period). In another case, I have to draw more (2 or 3) smaller in size pictures on the screen with screen refresh frequency: I mean, for example, 3 pictures the same time, lasting for 1 screen refresh interval. This is even more tough, and I miss more frames. Does anyone know a better way to get what I need? Many thanks in advance Luca Filippin</body>
  </mail>
  <mail>
    <header>Re:Image Unit using lookup table</header>
    <body>I think I've solved it, changed variable lookupTable to inputTable throughout and now it passes. -Carl</body>
  </mail>
  <mail>
    <header>Image Unit using lookup table</header>
    <body>I am trying to create an Image Unit that takes two images and a float value.  The first image is the source image, the second is a lookup table.  The kernel works fine in Quartz Composer but I am having difficulty creating the image unit.  Specifically, it appears I am not defining it correctly in the customAttributes function.  Here is how it is currently defined: [NSDictionary dictionaryWithObjectsAndKeys: [CIImage emptyImage], kCIAttributeDefault, nil],    @&amp;quot;lookupTable&amp;quot;, This does not pass but if I add the statement: to the outputImage function, it passes the ImageUnitAnalyzer. Any suggestions on how to define a CIImage in customAttributes?  I can post the code in it's entirety if it would help. -Carl</body>
  </mail>
  <mail>
    <header>Re: Running out of memory when adding CGImageRef-to-NSImage frames	to QTMovie in NSOperation subclass</header>
    <body>Hi Michael, anImage = [[NSImage alloc] initWithData:[bitmapRep &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;NSBitmapImageRep *bitmapRep = [[[NSBitmapImageRep alloc] I have an NSOperation that opens a CGImageRef from a digital camera(large file). It creates a full-size TIFFRepresentation in RAM, then scales it down to a smaller CGContextRef. I create an NSImage out of this CGContextRef. This same NSOperation then adds it to a QTMovie with the -addImage:forDuration:withAttributes: method. Once the image is added, I send off an NSNotification and the thread/NSOperation dequeues(I&amp;#39;m assuming). Once the frame-adding operation is complete, the movie (originally created with a tempfile on the main thread) that is generated is subsequently added to another NSOperationQueue which just writes it to a file. However, I find that I run out of RAM very quickly during the image-creation phase. When analysing it in Instruments, objects don&amp;#39;t appear to be getting deallocated(not sure which objects). After about 120 images, my 4-gig macbook pro runs out RAM ... mmap error stating that it couldn&amp;#39;t allocate RAM. I thought encapsulating the image-creation and movie-creation together in a separate thread would guarantee me that once that operation got dequeued at least the NSImage objects associated with that autorelease pool would be sent a -release message and be freed. It appears the NSImage is being properly disposed-of because when I try: the second call to [anImage retainCount] causes a crash: objc[4573]: FREED(id): message retainCount sent to freed object=0x24114470 so the NSImage must be gone? Why does my RAM fill-up then? I keep the full movie in memory because I need it to save to disk later, but it shouldn&amp;#39;t hog all the system resources after 120 1/2 second frames, should it? So here is the main() method to the NSOperation subclass that takes a CGImageRef, converts it to an NSImage, the adds it as a frame to a QTMovie: -(void)main &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;if (![self isCancelled]) &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;NSBitmapImageRep *bitmapRep = [[[NSBitmapImageRep alloc] &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;anImage = [[NSImage alloc] initWithData:[bitmapRep I don&amp;#39;t think I have enough room in this email to include -scaledImageForURL: nor -setImageAsMPEG4: but can supply it in a subsequent posting. Michael</body>
  </mail>
  <mail>
    <header>Running out of memory when adding CGImageRef-to-NSImage frames to 	QTMovie in NSOperation subclass</header>
    <body>I have an NSOperation that opens a CGImageRef from a digital camera(large file). It creates a full-size TIFFRepresentation in RAM, then scales it down to a smaller CGContextRef. I create an NSImage out of this CGContextRef. This same NSOperation then adds it to a QTMovie with the -addImage:forDuration:withAttributes: method. Once the image is added, I send off an NSNotification and the thread/NSOperation dequeues(I'm assuming). Once the frame-adding operation is complete, the movie (originally created with a tempfile on the main thread) that is generated is subsequently added to another NSOperationQueue which just writes it to a file. However, I find that I run out of RAM very quickly during the image-creation phase. When analysing it in Instruments, objects don't appear to be getting deallocated(not sure which objects). After about 120 images, my 4-gig macbook pro runs out RAM ... mmap error stating that it couldn't allocate RAM. I thought encapsulating the image-creation and movie-creation together in a separate thread would guarantee me that once that operation got dequeued at least the NSImage objects associated with that autorelease pool would be sent a -release message and be freed. It appears the NSImage is being properly disposed-of because when I try: the second call to [anImage retainCount] causes a crash: objc[4573]: FREED(id): message retainCount sent to freed object=0x24114470 so the NSImage must be gone? Why does my RAM fill-up then? I keep the full movie in memory because I need it to save to disk later, but it shouldn't hog all the system resources after 120 1/2 second frames, should it? So here is the main() method to the NSOperation subclass that takes a CGImageRef, converts it to an NSImage, the adds it as a frame to a QTMovie: -(void)main if (![self isCancelled]) NSBitmapImageRep *bitmapRep = [[[NSBitmapImageRep alloc] anImage = [[NSImage alloc] initWith I don't think I have enough room in this email to include -scaledImageForURL: nor -setImageAsMPEG4: but can supply it in a subsequent posting. Michael</body>
  </mail>
  <mail>
    <header>Quartz or Open GL for playing an Image Sequence</header>
    <body>I have been looking over the mailing lists for image sequence related info, and some posts are from 2005 or 2003, dunno if their approaches fully apply to Leopard or not Anyway, so I am starting to mess around with Quartz and Open GL, by playing around I literally mean I am just starting to mess with them. Anyway, is Quartz or Open GL going to be better for creating/playing back a PNG image sequence?  That should at least point me where to start doing some more reading. My next question from there (if Quartz), is would it be best to use NSTimer to act as my timekeeping/event object for handling when to change images (again, this assumes Quartz, as I believe Open GL has some frame rate related functions)  ? Or is there another resource someone can point me to that would be better suited to generating a frame rate.</body>
  </mail>
  <mail>
    <header>Re: filterWithName for unknown filter</header>
    <body>Yes that would be one way. Or you use the filterNamesInCategories:nil API and check against the returned array of filter names.</body>
  </mail>
  <mail>
    <header>filterWithName for unknown filter</header>
    <body>If I want to know whether a Core Image filter with a particular name is registered, is it safe to just test whether -[CIFilter filterWithName: name] returns nil?  The docs don't say what happens if the filter does not exist. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: compatibility check in Quartz Composer</header>
    <body>The compatibility information is a little over-zealous when it checks. It's basically saying that there's a possibility that a composition that uses this will not work in Tiger. In this case, there are extended options that allow you to use Javascript to run multipass filters. If you don't use this option, it should work fine on Tiger.</body>
  </mail>
  <mail>
    <header>Re: disabling color correction in quartz 2d</header>
    <body>You don't create one from a CIImage, you create one from a rendering target. So you can create a CIContext from a rendering destination, a Core Graphics context, an OpenGL Context or NSGraphicsContext. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: disabling color correction in quartz 2d</header>
    <body>Thanks, David. I spent some time in the documentation to figure out how to make a CIContext from CIImage, but no luck. Let me know if you have a code sample. -lajos</body>
  </mail>
  <mail>
    <header>Re: CIAreaAverage filter vs. CIColorTracking sample</header>
    <body>I suspect there may be a bug in the part of the area mean filter in CIColorTracking that deals with non-power-of-2 images. I find that I get an excellent match between that filter and CIAreaAverage (for opaque images) if I make 2 changes: 2. Follow up with another filter that just forces the alpha channel to 1. Darn if I know why these changes would be needed.  For a few seconds I thought I saw a reason for the square root, but not now. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: CIAreaAverage filter vs. CIColorTracking sample</header>
    <body>for CIAreaAverage, it sounds like you're looking at the result after color correction. GenericRGB has a gamma of 1.8, and 0.68^1.8 = 0.5 ... the 50% gray you probably expect to see. I don't know why you get a different result in CIColorTracking...</body>
  </mail>
  <mail>
    <header>CIAreaAverage filter vs. CIColorTracking sample</header>
    <body>I need CIAreaAverage on Tiger, which led me to the CIColorTracking sample code (by way of GPU Gems 3).  But I'm puzzled at the results.  If I start with an image that's exactly half black and half white, CIAreaAverage produces a pixel that is 68% gray, whereas the Color Tracking Area Mean Filter included in CIColorTracking produces a pixel that is 89% gray.  What's up with that?  If I feed in pure black, I get black, so I can't say it's always lighter than it should be. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: disabling color correction in quartz 2d</header>
    <body>You need to create a CIContext to extract a CGImage from a CIImage. One is created implicitly when you create an NSBitmapImageRep from a CIImage, but in general you have to do this yourself. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: writing CGLayer to a TIFF file</header>
    <body>NSBitmapImageRed conversion is where the color shift occurs. To correctly write a TIFF from a CGLlayer, draw the CGLayer onto an CGBitmapContext, get a CGImage from the CGBitmapContext and write the CGImage to a TIFF. See the email thread &amp;quot;disabling color correction in quartz2d&amp;quot; for actual code. -lajos</body>
  </mail>
  <mail>
    <header>Re: disabling color correction in quartz 2d</header>
    <body>OK, so the answer to the question is not &amp;quot;don't use CGLayers&amp;quot;;) I took the CGLayer and composited it into a bitmap context. Then got a CGImage from the bitmap context and wrote it to a TIFF file (by the way, the documentation is a terrible place to look for information on how to write a TIFF or JPEG from a CGImage.) The TIFF file written from the CGLayer has the correct RGB value. So the bug is either with the CGLayer to CIImage conversion or the CIImage to NSBitmapImageRep conversion. I can't figure out how to get a CGImage from a CIImage without going to NSBitmapImageRep, so I can't be sure. If you know, let me know. Anyway, filing a bug about it. Here's the final test code. I added comments where rgb values were written correctly and incorrectly. BTW, writing a JPEG from CGImage also modifies the RGB values (1-2 points in this case, not nearly as bad as the CIImage problem.) Bugging that also. -lajos CGImageDestinationRef dr = CGImageDestinationCreateWithURL CGColorSpaceRef colorSpace = CGContextRef drawContext = CGBitmapContextCreate(NULL, drawingSize.width, drawingSize.height, 8, 4 * drawingSize.width, CGLayerRef drawLayer = CGLayerCreateWithContext(drawContext, drawingSize, CGContextFillRect(context, CGRectMake(0, 0, drawingSize.width, CGContextDrawLayerInRect(drawContext, CGRectMake(0, 0, drawingSize.width, saveImageTiff(cgImage,@&amp;quot;/tmp/testFromCGLayer.tiff&amp;quot;);				// CORRECT rgb color NSBitmapImageRep *bitmapImage = [[NSBitmapImageRep alloc] [[bitmapImage TIFFRepresentation] saveImageTiff(cgImage,@&amp;quot;/tmp/testFromBitmap.tiff&amp;quot;);        // CORRECT rgb color NSDictionary *ciImageColorSpace = [NSDictionary dictionaryWithObject:(id)colorSpace CIImage *image = [CIImage imageWithCGLayer:drawLayer [[bitmapImage TIFFRepresentation] writeToFile:@&amp;quot;/tmp/testFromCIImageBitmap.tiff&amp;quot; atomically:YES];      // INCORRECT rgb color saveImageTiff(cgImage,@&amp;quot;/tmp/testFromCIImageBitmapByCGImage.tiff&amp;quot;);  // INCORRECT rgb color</body>
  </mail>
  <mail>
    <header>Re: Image unit infinite bounds</header>
    <body>I'm aware that I can use the crop patch, but it bugs me that you can avoid it with executable filters, and not (unless I'm missing something) with nonexecutable filters. My image unit ultimately not going to be used as a patch, but as a Core Image filter called programmatically.  I'm just using Quartz Composer as a way of testing my filters.  I suppose that the programmatic thing corresponding to the crop patch would be the CICrop image filter. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: Image unit infinite bounds</header>
    <body>You just need to run the output of your patch through an Image Crop patch before sending it to the renderer. The Image Crop patch is in the Modifier category, or you can just search for &amp;quot;crop&amp;quot; and it will show up. You'll need to set the bounds to crop the image to, but other than that it should just work. Hope this helps, Andy On Feb 4, 2009, at 6:34 PM, James Walker wrote:</body>
  </mail>
  <mail>
    <header>Image unit infinite bounds</header>
    <body>In my first attempt at making an image unit, Quartz Composer showed the output image as &amp;quot;infinite bounds (needs cropping to render)&amp;quot;.  I don't really understand this, but when I made my outputImage method say this, - (CIImage *)outputImage return [self apply: _Average6FilterKernel, src, kCIApplyOptionDefinition, [src definition], the addition of the kCIApplyOptionDefinition part fixed the infinite bounds.  So far, so good. For my second attempt at an image unit, I tried to make a nonexecutable filter.  The filter takes 2 images of the same size as input, and produces one image of the same size as output.  Again, Quartz Composer says the output has infinite bounds, but this time I have no idea how to fix it.  Must I make it an executable image unit? The CIDemoImageUnit example project contains a nonexecutable image unit, and it also has the property of producing an output image with infinite bounds. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: help! coregraphics+python+webserver now fails</header>
    <body>many thanks for taking an interest in this. Please see my comments inline below: i believe your assumptions are correct. The webserver is running *as* the &amp;quot;www&amp;quot; user, but it is started via a sudo command. i tried simply logging out and got no different results. I'll try rebooting the server when i can and test your theory. this is completely out of my realm of experience. I'll do some reading, but i'd welcome any tips to expedite my ramping up on this.</body>
  </mail>
  <mail>
    <header>Re: help! coregraphics+python+webserver now fails</header>
    <body>I assume this script is being run by the web-server and that web-server is running under the root launchd process? If so the script is being run from the root session and that session doesn't have reliable access to various services normally associated with a user session (aka a user logged into the console). It may start working for you if you restart the server and leave it at the login screen. In the long run you likely will need to have this run as a launch agent (pre-session or aqua) unless someone else can suggest a better solution to this problem. covers this type of thing (look at the stuff about what is daemon safe and window server life cycle). -Shawn</body>
  </mail>
  <mail>
    <header>help! coregraphics+python+webserver now fails</header>
    <body>i have very little experience with anything relating to cocoa, and not much more with quartz, but i'm an experienced web app developer, and have used python quite a bit. This list is the closest thing i can find to a forum where i can discuss this issue. I have a script that i've been using for a long time to create jpeg previews of PDF documents, and it recently started failing. Here is the essence of the script: ----------------- pdfData = CGPDFDocumentCreateWithProvider(CGDataProviderCreateWithFilename (pdfPath)) origWidth, origHeight = (originalRect.size.width,originalRect.size.height) width, height = (originalRect.size.width,originalRect.size.height) ## set the color space (rgb): cs = CGColorSpaceCreateWithName(kCGColorSpaceUserRGB) ## set the context (whatever that is). Background color is hardcoded to white for now: ctx = CGBitmapContextCreateWithColor(int(newRect.size.width), int(newRect.size.height),cs, (1, 1, 1, 1)) ## draw the image ctx.drawPDFDocument(newRect, pdfData, page) ## write out the jpeg ctx.writeToFile(jpegPath, kCGImageFormatJPEG) The problem i'm now suddenly getting (i'm on 10.5.6) is on the drawPDFDocument() call. When i hit that, the following gets logged to the system log: /System/Library/Frameworks/Python.framework/Versions/2.5/Resources/ Python.app/Contents/MacOS/Python[402]: ATSClient: can't make connection to ATSServer. bootstrap_look_up status = 268435459 /System/Library/Frameworks/Python.framework/Versions/2.5/Resources/ Python.app/Contents/MacOS/Python[402]: ATSClient: can't re-connect with ATSServer   status = -3182</body>
  </mail>
  <mail>
    <header>Re: disabling color correction in quartz 2d</header>
    <body>&amp;gt; You're misinterpreting &amp;quot;device independent&amp;quot;. In a color context this You are absolutely correct. RGB, CMYK, HLS, HSB and friends are device dependent. However by forcing color operations to use the same predetermined color conversion matrix we should be able to make them independent of the current device (in this case they are going to be dependent on an imaginary device described by the color conversion matrix.) This is exactly what I'm trying to do. I'm specifying RGB values with a specific color conversion matrix and then expect to get the same exact RGB values back by again specifying the same matrix. It's not happening, so the options are: - I'm doing something wrong because I didn't read the manual - there is missing or misinformation in the manual - there is a bug in the API -lajos</body>
  </mail>
  <mail>
    <header>Re: disabling color correction in quartz 2d</header>
    <body>Hi lajos, Still here for your signature cookies. Actually I find that the CGLayer documentation is not that well. I don't exactly what it gives as for patternization, but the CGBitmapContext -&amp;gt; CGImage leads to far better performances. There is even a sample code from apple that demonstrate how Bitmap contexts are superior to CGLayer. This project is called QuartzCache. On the architecture part, you should probably change this code so that it accepts a CGContext. One of the first operation of those parts of code is to extract the context right ? Even in the case where it is using it into another context to patternize, I think you'll get far more better performance using CGImage extracted from a CGBitmapContext (at least this is somehow what shows the QuartzCache project). Hope this will help, Raphael</body>
  </mail>
  <mail>
    <header>Re: disabling color correction in quartz 2d</header>
    <body>I see your point. According to the CGLayer documentation however: CGLayer objects are useful for offscreen drawing and can be used in much the same way that a bitmap context can be used. In fact, a CGLayer object is a much better representation than a bitmap context. .... ... Any CG object that you draw repeatedly¬óincluding CGPath, CGShading, and CGPDFPage¬óbenefit from improved performance if you draw it to a CGLayer object. Another reason for drawing on the CGLayer is that other parts of the code expect a CGLayer. Drawing on a bitmap context and converting it to a CGLayer after every drawing operation would be expensive. The test I sent is the most compressed way for me to show you the data flow. The actual code is a lot more complicated. I'll rewrite the test to render straight into the CGContext to see if that helps. That could in fact pinpoint where the colors are going wrong. -lajos</body>
  </mail>
  <mail>
    <header>Re: Advice needed on moving to CALayers from a dirty rect model of	drawing</header>
    <body>Hi Hamish, Thanks for your reply, and thanks for taking the time to poke around and take a look at our app.  You're definitely right that there would be reasonable solutions to get those example documents performing well.  The problem is that these are just example documents, and that users can create virtually any construction using our desktop software. In my experiments, below are some of the more curious behaviors I've discovered with CALayer that prevent a pure CALayer solution from being viable for us: - Trying to maintain a CALayer the size of the screen and only updating portions of it doesn't work.  Sending a -setNeedsDisplayInRect: message to the CALayer will result in the layer's draw method being called with a bounding box the size of the view.  If the CALayer is the root layer of a UIView, then setNeedsDisplayInRect: works as expected, where CGContextGetBoundingBox returns the rect that was requested needing display.  Clearly, UIView has got some special type of CALayer. - I tried having each object be a CALayer that implements its own -drawInContext: method.  The app exits without warning (or even raising an exception) when I hit some kind of limit of CALayers.  It doesn't blow up when I create the layers, but when the layers are actually being rendered (which prevents any kind of detection that trouble is brewing).  I found this limit to be fairly low, as low as 50 layers (and maybe even lower).  This limit is not present with other kinds of CALayers.  I haven't hit a limit with CAShapeLayer or with CGImageRef-backed CALayers (the latter worked up to 10000!).   Is there an explanation for this?   Is there any way to detect that I'm going to hit the wall before I hit it? To get around my first point above, we plan to just push up to UIView and layer a few UIViews, one of which draws portions of itself using quartz, and another UIView which contains a bunch of CALayer objects.  Is there any downside to using UIView rather than CALayer for this?  It seems like the wrong tool for the job since I don't care about touch handling and really just want a more robust CALayer. Thanks again for your reply. -Jon</body>
  </mail>
  <mail>
    <header>Re: Advice needed on moving to CALayers from a dirty rect model of	drawing</header>
    <body>Hi Jon, I think &amp;quot;essentially limitless&amp;quot; is the part that's going to have to give. Would it be so bad if, for instance, the blue lines in your Folded Circle Construction of ellipses faded in the same way your green dots do in the Tangent Circles Construction? Taking the latter as an example, you can see that during animation, when the red circle is small the frame rate is much better than when the circle is large. I'd say this is a good candidate for CAShapeLayers. Each &amp;quot;green dot&amp;quot; object could be a layer (probably raster rather than shape for fixed sizes, but you should profile) from a pool to which dots whose alpha has reached zero is returned (with the decrease in alpha not just timed but bounded by the size of the pool: you may have to speed up the animation). Admittedly I've only looked at the examples in your iPad app, so you may have other cases that don't translate so well. However, I'd expect your Pythagorean Tree to work well with each square a CALayer of equal contents -indeed, you could even structure the layer tree accordingly :) Hope this helps, H</body>
  </mail>
  <mail>
    <header>Advice needed on moving to CALayers from a dirty rect model of drawing</header>
    <body>Our iPad application has run into debilitating performance issues with the retina-display iPad, and after research, we see that many apps encountered similar problems.  The recommended solution in most cases is a move to CALayers, especially for content that doesn't change between frames.  Unfortunately, we are finding it difficult to make the CALayer approach work as a general solution in our case. Our application renders frame-by-frame animation of many (hundreds to low-thousands) of layered graphical objects of various sizes.  Some of these (&amp;quot;static objects&amp;quot;) have constant contents (though perhaps changing locations) across consecutive frames, others (&amp;quot;dynamic objects&amp;quot;) have slowly changing appearances in whole or part on each frame.  (Most of these dynamic objects represent geometric primitives whose fundamental properties may change on any frame. Some may be effectively full-screen though most are &amp;quot;low ink,&amp;quot; like a diagonal line extending from screen-edge to screen-edge. Thus representing each as a bitmap is prohibitively expensive in memory, but drawing each with Quartz as needed has been usually straightforward.) Our pre-retina rendering engine maintained a single dirty rectangle in the overall frame that included all changed content since the last frame, and redraw the subset of the full stack of objects that intersected that dirty rect into the (clipped) dirty rect, thereby cleaning it for the next frame. This worked great until retina, which slowed us down tremendously with its 4x pixel cost on bit-blits and anti-aliasing operations. Our goal for our retina-display optimization is to treat each of our static graphical objects as a CALayer of fixed contents, which can be handled entirely by CoreAnimation and the GPU, and then only re-render the actually-changed-in-this-frame parts of our dynamic content.  But this dynamic content then presumably lives in other CALayers sandwiched between our static (fixed content) CALayers, and CALayers don't appear to give us any reliable way to &amp;quot;reuse&amp;quot; their previous contents in ways that permit us to use dirtyRects and clipping-to-changed-areas. Thus to change a single changed visual component of, say, 2,000 potentially changing visual elements in a dynamic layer requires redrawing all 2,000 of them (to rebuild that layer from scratch) --- each frame, where before we only needed to redraw 1. Obviously this kills any benefit we see from avoiding redrawing of the static objects!   And we can't say that our dynamic objects are themselves represented by 2,000 separately-addressable CALayers(eg. CAShapeLayer) in any efficient way, since these elements are essentially limitless in number, and produced as the result of an ongoing calculation that takes into effect ongoing user interactions (think dynamic fractals, or live music visualization, or similar). Can anyone give us advice about how to proceed? We're aware of the OpenGL option, and that would be a huge cost for us, and break various other user features concerning typography, etc.  We're very interested in fixing a Quartz rendering pipeline that has worked great---and better with each new OS or hardware update---since early Mac OS X right up to the retina, where it's become almost completely useless. How can we leverage the GPU's ability on the fixed content layers of our dynamic/fixed/dynamic/fixed/dynamic/fixed layer cake, without having to treat each dynamic layer as a fullscreen CGBitmapContext() that we update each frame and then repush, in its entirety, down into a CALayer? Thanks in advance for any help. We're really stymied here.</body>
  </mail>
  <mail>
    <header>Re: adding random rectangles</header>
    <body>If you select the patch and look at it's settings, there should be one called something like &amp;quot;enable feedback rendering&amp;quot;. That should be on, and the clear should be off (if there is one), to keep the color buffer from clearing.</body>
  </mail>
  <mail>
    <header>adding random rectangles</header>
    <body>I&amp;#39;m trying to whip up a little test app, and I ran into a wall. Hopefully this is an easy question and I just missed something obvious.</body>
  </mail>
  <mail>
    <header>Core Animation-related Compositing Woes</header>
    <body>Hi all! I've got a setup that involves a custom NSView as a content view for my window and then a layer-hosting subview of the same size.  The content view never has any subviews besides the layer-hosting view.  Instead, it does all its drawing directly to the window's CGContext via the content view's drawRect: (though there is also occasional direct drawing via lockFocus/unlockFocus in modal loops or on timers).  The layer-hosting subview hosts a completely transparent layer I never draw into.  I use sublayers to provide overlays on top of my content view's drawing. For the most part this works fine, but some users have been encountering strange flickering while updating the layer's properties.  It is as if the compositing of the CALayer onto the window's backing store happened mid-draw for the window, after the content view's area had been erased but before the drawing had been flushed to the context.  The result is that the updated regions sometimes show the window's background color behind the layer instead of the contents of the window. In trying to debug this issue, I turned on CA_COLOR_FLUSH to see what was getting updated.  The areas that flash with that debug flag on are identical to the areas that exhibit these compositing glitches.  In other words, what showed up as a yellow flash while debugging appears as the window background color behind the layer for a handful of users. Has anyone seen any behavior like this?  I know that the layer's rendering is handled from a background thread with a CVDisplayLink to synchronize it to the refresh rate of the monitor.  Is it possible that for some reason the drawing on the main thread is getting interleaved in some bad way with the CALayer's compositing?  Is there anything that seems fishy about my particular setup? Thanks so much for your time! Michael Miller</body>
  </mail>
  <mail>
    <header>Re: Can I output PNG files that don't depend on my monitor's colour	profile?</header>
    <body>Well, the answer that the PNG files differ even with the color information removed. However, it does seem like I misinterpreted what was going on before.  If the files contain only things drawn with raw Quartz calls, they are in fact identical - it's only files in which I've drawn _images_ into, with CGContextDrawImage, that differ. It's still confusing though - if the context was created with kCGColorSpaceSRGB, and drawing into it with Quartz calls produces identical results, why is the result of doing a CGContextDrawImage into it apparently dependent on my monitor's color space? What seems to be happening is that when I load an image from disk (code in copyImageForURL: method, below), it's automatically assigned, because it doesn't have a color space embedded, a space of kCGColorSpaceDeviceRGB.  Output that shows this is below the code.  Presumably the rationale here is that, for images that a user's saved, they'll at least look identical when loaded back in.  However, it becomes a strange choice when reading files created by others - everyone will see different output depending on their monitor's color space. In order to gain consistency, I've implemented colorspace-swapping code (quoted at the bottom) that will check whether device-RGB has been assigned to an image, and replace it with sRGB.  This does indeed leave me with identical PNG output (hurray!).  My 'solution' seems rather fragile though, dependent as it is on equality comparison of color spaces and their names. Is there a way to create an image specifying the fallback color space rather than just having device-RGB used automatically? Jamie. -- - (CGImageRef)copyImageForURL:(NSURL *)url -- 2012-05-01 17:47:45.422 PageLayoutTest[9568:403] Colorspace: &amp;lt;CGColorSpace 0x7fb43260b0e0&amp;gt; (kCGColorSpaceDeviceRGB) -- ... ...</body>
  </mail>
  <mail>
    <header>Re: Can I output PNG files that don't depend on my monitor's colour	profile?</header>
    <body>Please excuse the brevity -- sent from my phone I'd say a good first step would be to find out the answer to this question manually using pngcrush. If the answer is yes, then it seems likely to me that it's just a matter of the API assuming the common case. I'm not at my machine right now but pngcrush may be able to tell you this too. H</body>
  </mail>
  <mail>
    <header>Re: Can I output PNG files that don't depend on my monitor's colour	profile?</header>
    <body>Resurrecting an old thread that didn't end up with a definitive answer.  I feel like I must be missing something here, but coming back to this, I'm still unable to render and output a PNG file that isn't dependent on my monitors colour profile. If the answer is &amp;quot;it's not possible&amp;quot;, that would still be good to know.  I realise that my bitwise comparison test isn't /guaranteed/ to work even if the PNG files represent identical pictures - I wouldn't be complaining if the pictures at least looked identical. Jamie. I'm doing the comparison inside my Obj-C code, unfortunately, but yes, point taken.  I wonder though if the bitmap data is actually the same? I do feel like there should be a way to get a PNG file outputted where I control the color space, and it doesn't depend on the monitor.  Besides the output, the actual drawing I'm doing is specified to be in the sRGB color space, so I'd like to know that it is, at least. Jamie.</body>
  </mail>
  <mail>
    <header>CIFilter bugs</header>
    <body>When I run this filter with the commented out line compiled in, I get odd yellow, blue or green areas in my image. When I run it the way it is, it works fine. This filter is ONLY being run on grayscale images so the coloring is very noticeable. Is there any known filter coding that will cause this issue that I should look for in my code? Or is this a bug that I just have to work around? / *---------------------------------------------------------------------------------------------------- Threshold filter Pixel values &amp;lt;= threshold = 0, otherwise 1.0 ----------------------------------------------------------------------------------------------------*/ kernel vec4 main(sampler inputA, float threshold) // no need to concern ourselves with premultiplying //	return step(threshold, pixel); this does not work</body>
  </mail>
  <mail>
    <header>CIFilter oddity when filter a cropped image</header>
    <body>I have a simple filter that averages 2 adjacent pixels. It works fine when the source image is actually an image. However, if I create a random noise filter and then apply a crop filter, and then use my filter, I get a terrible looking result. The result looks like the image was broken into 256x256 tiles and the edges of the tiles are all white. Also, it looks like the result is zoomed in. Also, I am not clear if I should use the destCoord, and then offset that, or just use the sampler coord and offset that. Neither approach make any difference on the result, but I would like to do this the proper way. kernel vec4 main(sampler inputA) vec4 pixel = unpremultiply(sample(inputA, samplerTransform(inputA, pixel += unpremultiply(sample(inputA, samplerTransform(inputA,</body>
  </mail>
  <mail>
    <header>Core Animation optimization - hiring experts</header>
    <body>We are developing an interactive Mac OS X application for a museum exhibition using Core Animation. So far developing has been straightforward, but we are having some performance issues that need to be resolved. We are looking for someone who eats, sleeps and breathes Core Animation to help us. If you: * have extensive experience in developing applications with Core Animation, * have a knack for making things fast, * can work as part of a global distributed agile team, * are thrilled by the chance to deploy software on 14 52&amp;quot; touch screens, Please send an email to email@hidden with a bit of information about you and your work, your availability and your sought hourly rate. Regards, Orestis Markou -- email@hidden</body>
  </mail>
  <mail>
    <header>Re: Core Animation: Hosted Layer v. Layer Backed View</header>
    <body>Carmen What is the best approach for designing this setup, a layer backed view or a hosted layer?</body>
  </mail>
  <mail>
    <header>Re: Persistence of the CFData of CGImage</header>
    <body>My apologies for the late comment</body>
  </mail>
  <mail>
    <header>Re: Core Animation: Hosted Layer v. Layer Backed View</header>
    <body>David, Carmen What is the best approach for designing this setup, a layer backed view or a hosted layer?</body>
  </mail>
  <mail>
    <header>Re: Core Animation: Hosted Layer v. Layer Backed View</header>
    <body>Layer Backing: This declares that you want to use CALayers to back your NSViews, but otherwise want to ignore Core Animation. Layer Hosting: This declares that you want AppKit to get out of your way and you want to manage the layer tree yourself via Core Animation. Doing either requires you to call -setWantsLayer:YES or turn on Core Animation in Interface Builder for the root view. Doing the latter requires that you additionally set a layer for the view that will host your layer tree. It is not recommended that you mix the approaches (that is, that you set a CALayer on an NSView, then add subviews and arbitrarily mix AppKit and Core Animation). Hope this helps you make your decision. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Core Animation: Hosted Layer v. Layer Backed View</header>
    <body>Howdy, Thanks, Carmen</body>
  </mail>
  <mail>
    <header>CIFilter - making a rank filter</header>
    <body>Does anyone have any tips or pointers that can help me implement a general rank filter as a CIFilter? I have seen a few GPU related articles on this, but no actual implementations.</body>
  </mail>
  <mail>
    <header>CIFilter CPU processing</header>
    <body>I have a filter that needs to process the pixels on the cpu. To do this, inside of the outputImage method I am converting the inputImage into  and NSBitmap. This is very slow but it works. The question I have is about the lazy evaluation of a filter. The filter does not normally do any processing until it is drawn. It seems to me that just accessing the outputImage value will cause processing to happen with the method I am using. Is this the proper way to do this, or is there some other way to process the pixels on the CPU that works better?</body>
  </mail>
  <mail>
    <header>CIFilter bug with edges of image</header>
    <body>I have a simple filter that operates on a 5x5 window of pixels around the current pixel. I simply add up all the pixels and average them. I am finding that on the iMac that the edges of my image have ugly artifacts. I am guessing this is due to how the pixel locations beyond the edge of the source image are being handled. This works fine on the Mac Pro. I tried adding the kCISamplerWrapMode to the sampler settings but it did not help. Is this a known problem? Is this my bug in my filter with something I am not doing correctly? Does anyone have any ideas?</body>
  </mail>
  <mail>
    <header>Re: CGDataProviderReleaseInfoCallback not being called when using	CGDataProviderCreateSequential()</header>
    <body>On Apr 26, 2009, at 6:53 PM, Ken Ferry wrote: Hi Ken, This is exactly the problem I was encountering. Somehow I totally overlooked not releasing the CGImage I created. Thank you for pointing it out. Sincerely, MichaelAttachment:</body>
  </mail>
  <mail>
    <header>Re: CGDataProviderReleaseInfoCallback not being called when using 	CGDataProviderCreateSequential()</header>
    <body>Hi Michael, Hello all, I&amp;#39;m implementing the following on 10.5. I have created an obj-c object to be a self contained data provider that I hand off to CGDataProviderCreateSequential() as the info pointer. A class method of that class returns the callback struct filled with the static C functions of the callback functions I have implemented. It looks like this: + (CGDataProviderSequentialCallbacks)sequentialDataProviderCallbacks I&amp;#39;m creating an image using the data provider as follows (more or less): CGImageRef image = CGImageCreate(x, ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† y, ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 8, ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 32, ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† (x*4), ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† generic_rgb_space, ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† kCGImageAlphaPremultipliedFirst|kCGBitmapByteOrder32Little, ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† sequentialDataProvider, ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† NULL, ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† NO, I get callbacks for getBytes and rewind allowing the image to display fine. However, I have never seen callbacks for skipForward or releaseInfo. I can understand not having skipForward being called, but it would be nice to know what would cause it. My main concern is that I have yet to see a releaseInfo callback that would release my OptimizedImageSequentialCGDataProviderInfo object and the image data that it holds. What am I missing... Is this a problem? What prompts the releaseInfo callback to be called? How can I force the callback to be called? Thank you, Michael ¬†_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list ¬† ¬† ¬†() Help/Unsubscribe/Update your Subscription: This email sent to</body>
  </mail>
  <mail>
    <header>CGDataProviderReleaseInfoCallback not being called when using	CGDataProviderCreateSequential()</header>
    <body>I'm implementing the following on 10.5. I have created an obj-c object to be a self contained data provider that I hand off to CGDataProviderCreateSequential() as the info pointer. A class method of that class returns the callback struct filled with the static C functions of the callback functions I have implemented. It looks like this: + (CGDataProviderSequentialCallbacks)sequentialDataProviderCallbacks ImageSequentialCGDataProviderInfo *dataProviderInfo = [[OptimizedImageSequentialCGDataProviderInfo alloc] CGDataProviderSequentialCallbacks callbacks = CGDataProviderRef sequentialDataProvider = CGImageRef image = CGImageCreate(x, y, 8, 32, (x*4), generic_rgb_space, kCGImageAlphaPremultipliedFirst| kCGBitmapByteOrder32Little, sequentialDataProvider, NULL, NO, NSBitmapImageRep *bitmapImageRep = [[[NSBitmapImageRep alloc] I get callbacks for getBytes and rewind allowing the image to display fine. However, I have never seen callbacks for skipForward or releaseInfo. I can understand not having skipForward being called, but it would be nice to know what would cause it. My main concern is that I have yet to see a releaseInfo callback that would release my OptimizedImageSequentialCGDataProviderInfo object and the image data that it holds. What am I missing... Is this a problem? What prompts the releaseInfo callback to be called? How can I force the callback to be called? Thank you, MichaelAttachment:</body>
  </mail>
  <mail>
    <header>CGContextDrawImage memory usage and threads</header>
    <body>I'm working on some iPhone code (OS 2.2.1) and noticing some unexpected behavior from CGContextDrawImage. I've got a 1600 x 1200 UIImage from the built in camera (or the photo picker) which I display in a UIImageView on the main thread. This appears to use up about 5.7MB according to ObjectAlloc, which is expected (the image is being loaded and displayed, though I expected the memory usage to be closer to 7.8MB). For testing purposes I'm displaying the whole image, but in production code I'll downsample it to a more reasonable size for display... I then pass that same UIImage object off to a background thread which calls CGContextDrawImage to copy the image into a context I created with CGBitmapContextCreate (using memory I have already allocated with mmap). The reason for this is that I'm going to do some color processing on the image in my buffer. After much trial and error, what I've found is that if I call CGContextDrawImage on a secondary thread, it allocates an additional 5.7MB buffer that appears to linger for the life of the UIImage (which is already consuming 5.7MB from the initial load). Below is what I see from running ObjectAlloc in Instruments: #	Object Address	Category	Creation Time	Size	Responsible Library Responsible Caller 1	0x7f77000	GeneralBlock-5763072	00:39.943	5763072	CoreGraphics img_data_lock 5 CoreGraphics CGContextDrawImage 4 libRIP.A.dylib ripc_DrawImage 3 libRIP.A.dylib ripc_AcquireImage 2 CoreGraphics CGSImageDataLock 1 CoreGraphics img_data_lock 0 libSystem.B.dylib malloc If I either perform CGContextDrawImage on the main thread, or don't display the UIImage in the UIImageView, there is only a single 5.7MB buffer allocated. With the limited memory on the iPhone, using 13+MB of RAM at this point causes my application to be terminated by Springboard, so I'm trying to figure out what my options are. Is CGContextDrawImage trying to enforce some sort of thread safety by allocating a new buffer for the image? If so, can someone elaborate on what the trigger conditions are? Is there some way to make it not do this (other than performing the CGContextDrawImage on the main thread)? I can move the call, but it's less convenient in my code... Thanks, Dave</body>
  </mail>
  <mail>
    <header>Re: Current context when not in drawRect:</header>
    <body>Duncan www.VelOCRaptor.com - Simple Mac OCR</body>
  </mail>
  <mail>
    <header>Re: Current context when not in drawRect:</header>
    <body>Hi I&amp;#39;m sure this must have come up, but I&amp;#39;m having trouble phrasing the question for Google, so I&amp;#39;d really appreciate some pointers. I&amp;#39;m writing various functions at the moment that render pdf or images on the command-line. It&amp;#39;s RubyCocoa, but the gist is code like pdf_context = CGPDFContextCreateWithURL(pdf_url, rect, nil) ¬† ¬† ¬† ¬† ¬† ¬† CGPDFContextBeginPage(pdf_context, nil) CGContextDrawImage(pdf_context, rect, image) Now I&amp;#39;d like to be able to use, for example, NSAttributedString to paint text aString = NSAttributedString.alloc.initWithString(&amp;quot;Hello World&amp;quot;) aString.drawInRect(CGRectMake(100, 100, 100, 100)) If I do this, then all hell breaks loose, viz: Wed Apr 22 09:48:09 Longjohn-2.local ruby[74313] &amp;lt;Error&amp;gt;: CGContextSetFillColorWithColor: invalid context Wed Apr 22 09:48:09 Longjohn-2.local ruby[74313] &amp;lt;Error&amp;gt;: CGContextSetStrokeColorWithColor: invalid context ... Wed Apr 22 09:48:09 Longjohn-2.local ruby[74313] &amp;lt;Error&amp;gt;: CGContextShowGlyphsWithAdvances: invalid context I&amp;#39;m guessing that somewhere upstream of drawRect: in NSView et al, something sets a thread-local CGContext for NSAttributedString, NSImage etc to catch hold of when they want to draw. So how can I set my pdf_context for them to see? Or have I got this completely wrong? Thanks in anticipation Duncan McGregor - Simple Mac OCR</body>
  </mail>
  <mail>
    <header>Current context when not in drawRect:</header>
    <body>I'm sure this must have come up, but I'm having trouble phrasing the question for Google, so I'd really appreciate some pointers. I'm writing various functions at the moment that render pdf or images on the command-line. It's RubyCocoa, but the gist is code like CGPDFContextBeginPage(pdf_context, nil) CGContextDrawImage(pdf_context, rect, image) Now I'd like to be able to use, for example, NSAttributedString to paint text aString = NSAttributedString.alloc.initWithString(&amp;quot;Hello World&amp;quot;) aString.drawInRect(CGRectMake(100, 100, 100, 100)) Wed Apr 22 09:48:09 Longjohn-2.local ruby[74313] &amp;lt;Error&amp;gt;: CGContextSetFillColorWithColor: invalid context Wed Apr 22 09:48:09 Longjohn-2.local ruby[74313] &amp;lt;Error&amp;gt;: CGContextSetStrokeColorWithColor: invalid context ... Wed Apr 22 09:48:09 Longjohn-2.local ruby[74313] &amp;lt;Error&amp;gt;: CGContextShowGlyphsWithAdvances: invalid context I'm guessing that somewhere upstream of drawRect: in NSView et al, something sets a thread-local CGContext for NSAttributedString, NSImage etc to catch hold of when they want to draw. So how can I set my pdf_context for them to see? Duncan McGregor www.VelOCRaptor.com - Simple Mac OCR</body>
  </mail>
  <mail>
    <header>CVImageBufferRef color depth strangeness</header>
    <body>Hello, I am working  with the QTCaptureSessions to do some real time video processing I have come across a strange color depth quality anomaly that I need to resolve. If I create a CIImage straight from the incoming CVImageBufferRef by using imageWithCVImageBuffer and then draw that it looks fine but if i copy the bits into a buffer and create a new CIImage from that data using imageWithBitmapData the color quality is significantly less, and it looks a lot whiter and noisier, like the dynamic range is compressed. here is the relevant code: //set up the pixel format for the capture session delegate [self setPixelBufferAttributes: [NSDictionary dictionaryWithObjectsAndKeys: [NSNumber numberWithInt:k32ARGBPixelFormat], kCVPixelBufferPixelFormatTypeKey, [NSNumber numberWithFloat:HEIGHT], kCVPixelBufferHeightKey, [NSNumber numberWithFloat:WIDTH], kCVPixelBufferWidthKey, [NSNumber numberWithBool:NO], kCVPixelBufferOpenGLCompatibilityKey, //in this method, one of two things - (void) outputVideoFrame:(CVImageBufferRef)videoFrame withSampleBuffer:(QTSampleBuffer *)sampleBuffer fromConnection:(QTCaptureConnection *)connection //This looks good: //This is all washed out and bad looking memcpy( buffer, pixels, CVPixelBufferGetBytesPerRow(videoFrame) * CIImage* image = [CIImage imageWithBitmap bytesPerRow:CVPixelBufferGetBytesPerRow(videoFrame) size:CGSizeMake(CVPixelBufferGetWidth(videoFrame), CVPixelBufferGetHeight(videoFrame)) Can someone perhaps help indicate with the results would be different at all, and what i can do to get the bitmap version looking good (because I ultimately need bitmap pixels to do the type of processing I am doing) one last thing, In other projects I have done I have seen this same problem, but it was for drawing frames to CGBitmapContexts.  For those occasions, having the context use float values solved the problem, but i don't see here how to get float values out of the image buffer without drawing to a context (which i don't want to do because I want to avoid the overhead of using contexts entirely, as I am only interested in working with raw bytes) Thanks! -- - Jim</body>
  </mail>
  <mail>
    <header>Re: CGDirectDisplay type</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: No file from CGPDFContextCreateWithURL</header>
    <body>I'll try it. Sounds like you are building the URL the hard way and getting it wrong in the process... :-) -- If this email is spam, report it here:</body>
  </mail>
  <mail>
    <header>Solved: No file from CGPDFContextCreateWithURL</header>
    <body>I had been using CFURLCreateWithBytes to make the url from the path in NSSavePanel. Seemed to be good, and worked for a different purpose elsewhere. But for CGPDFContextCreateWithURL that doesn't work; this does: CFURLCreateFromFileSystemRepresentation. Ah... this tends to confirm my suspicion that the url must have the wrong form. But I cannot find any specifics about the form it should take. I see that the url I'm supplying is absolute: CFURLGetBaseURL returns zero. The path portion of the URL as revealed with CFURLGetPath is /Users/myName/Documents/Untitled.pdf. Of course, there are other arguments, too - the media rect and the dictionary. The rect shows valid dimensions in the debugger and I was passing NULL for the dict, Just tried passing a real dict value there, but still got the -15. So there must be something about the format required for the url that I'm just not getting... Jeff This error is coming from CFURLWriteDataAndPropertiesToResource, and is kCFURLImproperArgumentsError. The most likely culprit here is that the URL is not in a form that is expected by the call to CFURLWriteDataAndPropertiesToResource. -- David Duncan Apple DTS Animation and Printing -- If this email is spam, report it here:</body>
  </mail>
  <mail>
    <header>Re: No file from CGPDFContextCreateWithURL</header>
    <body>Sounds like you are building the URL the hard way and getting it wrong in the process... :-) Dave</body>
  </mail>
  <mail>
    <header>Re: No file from CGPDFContextCreateWithURL</header>
    <body>Ah... this tends to confirm my suspicion that the url must have the wrong form. But I cannot find any specifics about the form it should take. I see that the url I'm supplying is absolute: CFURLGetBaseURL returns zero. The path portion of the URL as revealed with CFURLGetPath is /Users/myName/Documents/Untitled.pdf. Of course, there are other arguments, too - the media rect and the dictionary. The rect shows valid dimensions in the debugger and I was passing NULL for the dict, Just tried passing a real dict value there, but still got the -15. So there must be something about the format required for the url that I'm just not getting... Jeff This error is coming from CFURLWriteDataAndPropertiesToResource, and is kCFURLImproperArgumentsError. The most likely culprit here is that the URL is not in a form that is expected by the call to CFURLWriteDataAndPropertiesToResource. -- David Duncan Apple DTS Animation and Printing -- If this email is spam, report it here:</body>
  </mail>
  <mail>
    <header>Re: No file from CGPDFContextCreateWithURL</header>
    <body>This error is coming from CFURLWriteDataAndPropertiesToResource, and is kCFURLImproperArgumentsError. The most likely culprit here is that the URL is not in a form that is expected by the call to CFURLWriteDataAndPropertiesToResource. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: No file from CGPDFContextCreateWithURL</header>
    <body>Does anyone know if a particular style of path is required in the url argument? I'm getting the url by calling NSSavePanel, getting &amp;nbsp;[panel filename], then getting the bytes of that filename path and converting to a url with CFURLCreateWithBytes().</body>
  </mail>
  <mail>
    <header>Re: CGDirectDisplay type</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: No file from CGPDFContextCreateWithURL</header>
    <body>Have you tried +fileURLWithPath:? NSURL and CFURLRef are tollfree bridged, so you should be able to just cast one to the other. The CF equivalent should be CFURLCreateWithFileSystemPath() which would allow you to avoid using the CreateWithBytes() call (again, using bridging to cast the NSString to a CFStringRef). It should create a file, but may not do so immediately. Have you called CGPDFContextEndPage, CGContextEndPage, CGPDFContextClose or CFRelease on the context returned to you by the time you are looking for the file? -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Animating CALayer Position and Size</header>
    <body>Hopefully I am understanding your reply correctly. First, I am not setting the layer to the new size anywhere in my code. I tried both the fromValue and the byValue and got the same results. My goal with this animation is to have the layer grow from its original size to a new larger size, shrink back down to its original size, and repeat the process.</body>
  </mail>
  <mail>
    <header>Re: Animating CALayer Position and Size</header>
    <body>When I try your code (using bounds.size) and don't set the value for bounds.size separately it works fine in my test harness. I would try getting the value for bounds.size and verifying that it is what you expect it to be (the original, not the new size). -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: CGImageCreateWithImageInRect() performance</header>
    <body>I got it working (thanks for the pointers) albeit currently only with one zoom level, as I haven't yet written the support code to create multiple copies of the original image with varying degrees of detail so, just as you pointed out, it's pretty slow when you're fully zoomed out. I think I now understand why UIScrollView's zoom is designed the way it is: I'm guessing it's assumed that all zoom scaling will be handled using a CATiledLayer! I have a couple of questions about CATiledLayer's operation: 1) When my test app starts, it's fully zoomed into the top left hand corner of the image. If I scroll to the right, my delegate's drawLayer:inContext: is called almost immediately, but if I scroll downwards, it's often a second or two before it's called. Why is this, and can I do anything about it? (It doesn't help if I explicitly invalidating the newly-visible rect from -[UIScrollView scrollViewDidScroll:].) 2) CATiledLayer seems to operate a queue of requests for tiles, adding tiles to the queue when they come into view and removing them if they go out of view before the drawing method is called (and of course when the drawing method completes). Is this the case? Is the delegate callback guaranteed only to be called from a single thread? Thanks again, Hamish</body>
  </mail>
  <mail>
    <header>Re: When is PDF v1.4 Created?</header>
    <body>I'm getting the version by calling CGPDFDocumentGetVersion(), does that provide the true version?  In any event it's somewhat beside the point, I get transparency when I draw to the screen but not when I draw to a PDF context.  This means I don't have WYSIWYG through the print path. I'm trying to figure out if I'm doing something wrong, or if this is a bug and/or limitation in CG. FWIW, I'm having the same problem with shaders.  They're imaged to the screen, but not to a PDF context. Nick</body>
  </mail>
  <mail>
    <header>Re: When is PDF v1.4 Created?</header>
    <body>Are you looking at the %PDF-1.3 at the start of the file to determine the version?  If so, that's not sufficient to determine the version. You need to look for a /Version key in the /Root entry of the document to determine the true version.  See section 3.4.1+ of the PDF 1.4 spec. Derek</body>
  </mail>
  <mail>
    <header>Re: When is PDF v1.4 Created?</header>
    <body>Derek, I guess I don't understand how to set transparency then.  How do I set the transparency of a PDF to make this happen? For example, if I create a create a PDF context, set the context alpha to 0.5 and draw a couple paths I get a v1.3 PDF with two paths correctly shown at 50%.  So far so good, however if I create the same PDF context with 50% alpha and instead draw an embedded v1.3 PDF (from a CGPDFDocumentRef) into it I also get a v1.3 PDF and the transparency is ignored, although doing the same thing into a screen context I do see the transparency in the embedded PDF.  There doesn't seem to be any way to get a v1.4 PDF. Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: Basic forms with NSBezierPath</header>
    <body>None that I'm aware of besides the basic forms defined in NSBezierPath.h.  If there are other common shapes that you think would be widely useful feel free to file an enhancement request at The &amp;quot;ovalteen&amp;quot; Graphics 2D code sample contains functions for drawing rounded rects.  It's a Carbon app that calls CG functions directly, but the general idea is the same: ovalteen.htm &amp;quot;This sample shows how to implement simple Quartz 2D replacements for the QuickDraw rounded rect and oval drawing functions. strokeOval, fillOval, strokeRoundedRect, and fillRoundedRect implement Quartz 2D equivalents to the QuickDraw oval drawing and rounded rect APIs, using CGRects instead of Rects. Troy Stephens Cocoa frameworks, Apple</body>
  </mail>
  <mail>
    <header>Re: When is PDF v1.4 Created?</header>
    <body>Typically, the version is bumped up to 1.4 only for 1.4-only features (such as transparency). Derek</body>
  </mail>
  <mail>
    <header>RE: Printing With PDF's...</header>
    <body>What a difference moving a line of code makes. Thanks very much. Andy -----Original Message----- From: David Gelphman [] Sent: Friday, January 30, 2004 12:30 PM To: 'email@hidden' Subject: Re: Printing With PDF's... If you want to use PMSessionGetGraphicsContext to get a CGContextRef, you need to call PMSessionSetDocumentFormatGeneration BEFORE you call PMSessionBeginDocument or PMSessionBeginDocumentNoDialog. David</body>
  </mail>
  <mail>
    <header>Re: Printing With PDF's...</header>
    <body>If you want to use PMSessionGetGraphicsContext to get a CGContextRef, you need to call PMSessionSetDocumentFormatGeneration BEFORE you call PMSessionBeginDocument or PMSessionBeginDocumentNoDialog. David</body>
  </mail>
  <mail>
    <header>RE: Printing With PDF's...</header>
    <body>I pulled the WWDC DVD and looked at the same slide to generate my code. Andy -----Original Message----- From: Joseph Maurer [] Sent: Friday, January 30, 2004 12:00 PM To: Brace, Andy Cc: 'email@hidden' Subject: Re: Printing With PDF's... You didn't tell how you are passing kPMGraphicsContextCoreGraphics to PMSessionSetDocumentFormatGeneration. But because I ran into a similar problem just yesterday (in my work on future Quartz sample code), I should show you what works for me (thanks to David Gelphman who pointed me at a slide from his 2001 WWDC presentation, which contained the code): ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Now, tell the printing system that we promise never to use any Quickdraw calls: CFArrayRef  graphicsContextsArray = CFArrayCreate(NULL, (const PMSessionSetDocumentFormatGeneration(printSession, ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ --(jm)</body>
  </mail>
  <mail>
    <header>Re: Printing With PDF's...</header>
    <body>You didn't tell how you are passing kPMGraphicsContextCoreGraphics to PMSessionSetDocumentFormatGeneration. But because I ran into a similar problem just yesterday (in my work on future Quartz sample code), I should show you what works for me (thanks to David Gelphman who pointed me at a slide from his 2001 WWDC presentation, which contained the code): ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Now, tell the printing system that we promise never to use any Quickdraw calls: CFArrayRef  graphicsContextsArray = CFArrayCreate(NULL, (const PMSessionSetDocumentFormatGeneration(printSession, ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ --(jm)</body>
  </mail>
  <mail>
    <header>Re: Printing With PDF's...</header>
    <body>Doesn't the printing system already take care of this for you? I believe it will rasterize postscript automatically for printers that don't support it. If not automatically the tools to do this are already in the printing system IIRC. -Shawn</body>
  </mail>
  <mail>
    <header>RE: Printing With PDF's...</header>
    <body>Didn't work. Also should state that the PDF I generated can be saved out to disk and Acrobat opens it just fine. Andy -----Original Message----- From: Haroon Sheikh [] Sent: Friday, January 30, 2004 11:43 AM To: Brace, Andy Cc: 'email@hidden' Subject: Re: Printing With PDF's... Give that a try and see if the problem goes away.</body>
  </mail>
  <mail>
    <header>Re: Printing With PDF's...</header>
    <body>Give that a try and see if the problem goes away.</body>
  </mail>
  <mail>
    <header>RE: Printing With PDF's...</header>
    <body>No I am not. Andy -----Original Message----- From: Haroon Sheikh [] Sent: Friday, January 30, 2004 11:36 AM To: Brace, Andy Cc: 'email@hidden' Subject: Re: Printing With PDF's... Out of curiosity, are you performing a CGContextBeginPage before calling CGContextDrawPDFDocument? haroon</body>
  </mail>
  <mail>
    <header>Re: Printing With PDF's...</header>
    <body>Out of curiosity, are you performing a CGContextBeginPage before calling CGContextDrawPDFDocument? haroon</body>
  </mail>
  <mail>
    <header>Printing With PDF's...</header>
    <body>I am developing an application that uses Postscript to output to postscript printers and presses.  On occasion however, I need to output to a non-postscript printer.  The solution I am trying to come up with involves taking the postscript I generate and convert it to a CGPDFDocumentRef using the CoreGraphic API's.  I then want to take the CGPDFDocumentRef and output this to a CGContextRef that I retrieved from a call to PMSessionGetGraphicsContext which I had first initialized the PMPrintSession with a call to PMSessionSetDocumentFormatGeneration passing kPMDocumentFormatPDF and kPMGraphicsContextCoreGraphics.    When I call CGContextDrawPDFDocument passing the context and PDFDocument I crash with an &amp;quot;unknown PowerPC exception&amp;quot;. What exactly am I doing wrong, and is this a valid way of doing what I need to do. Thanks Andy Brace Dex Media</body>
  </mail>
  <mail>
    <header>When is PDF v1.4 Created?</header>
    <body>I recall hearing at WWDC that Quartz would write PDF as version 1.3 for greater compatibility unless the graphics to be written required PDF version 1.4.  Can anybody tell me if this is correct in Panther and if so, which graphic features cause Quartz to write version 1.4? Thanks, Nick Nallick</body>
  </mail>
  <mail>
    <header>vImage Questions?</header>
    <body>Is this a good place to ask questions about vImage?  If not, is there a better place? I know it's not part of Quartz, but it's a graphics technology and this is probably the closest thing to a general graphics mailing list. Thanks, Nick Nallick</body>
  </mail>
  <mail>
    <header>Re: Insideness testing</header>
    <body>Sorry, I didn't read your method carefully the first time -- you are making a bitmap with exactly one pixel, and seeing whether it got painted by the path. Actually that's quite clever -- it solves the issue where I might need a bit of slop around the shape, simply set the stroke larger... when I get around to testing, I'll see if it actually is faster. If Quartz images row by row anyway, the algorithm is essentially the same. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com</body>
  </mail>
  <mail>
    <header>Re: Insideness testing</header>
    <body>If there is a Quartz function it's not exported in the public API. Assuming you have decent clipping logic, I don't see any reason why imaging a single pixel in a path would be appreciably slower than determining if an arbitrary point will be painted by a path or not.  In both cases you have to construct a table to trace a ray through the point.  Imaging one pixel from that table isn't going to make much difference. Nick</body>
  </mail>
  <mail>
    <header>Basic forms with NSBezierPath</header>
    <body>Hi there, is there any convenience functions/macros to initialize NSBezierPath instances with basic forms, for example a rectangle with rounded corners, and the like ? Not that I can't do it by myself but I could make me save some time... Thanks in advance Pierre DOUCY</body>
  </mail>
  <mail>
    <header>PDF generation and spot colors</header>
    <body>Hi! I ask this question already in cocoa-dev without any replies. Maybe someone here can give an answer: As PDF format supports spot colors within a PDF document, I am wondering if COCOA is able to handle spot colors, that are saved as real spot colors to a PDF and shown as CYMK or RGB color on screen. NSColor does not support this directly, i think. Any documentation seen or ideas or experiences on this issue? MiMo -- ======================================================= And refashioning the fashioned lest it stiffen into iron is work of endless vital activity. [Goethe] All about CALAMUS DTP suite: =======================================================</body>
  </mail>
  <mail>
    <header>Re: Insideness testing</header>
    <body>containsPath: is implemented in NSBezierPath. Ali</body>
  </mail>
  <mail>
    <header>Re: Insideness testing</header>
    <body>The Cocoa NSBezierPath has a method containsPoint: that does this, and I presume it must be implemented by the underlying CoreGraphics function -- but which one? Mathematically, I can shoot a vertical or horizontal ray from the point and count how many intersections with the path to determine insideness, but I'd rather not do that if there already exists such a Quartz function. I suppose using a bitmap context to test is doable -- filling the shape -- but I doubt it's going to be fast... Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com</body>
  </mail>
  <mail>
    <header>Re: Insideness testing</header>
    <body>You can &amp;quot;hit test&amp;quot; a specific pixel but I don't know of any way to do an abstract &amp;quot;insidedness&amp;quot; test.  To hit test a pixel you have to draw the path and test the pixel of interest.  What I do for this is to create a 1x1 gray bitmap context so the bitmap data is only one byte, translate the context to test the point I'm interested in, draw the path, and test for a non-zero value in my bitmap byte. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Insideness testing</header>
    <body>There seem quite a lot of functions in the Quartz dylib that aren't documented. In particular, how does one do insideness or hit testing? If I have a path, how do I determine whether a particular point is &amp;quot;inside&amp;quot; it? (Cocoa has such a function, but I'm interesting in a CoreGraphics only solution.) Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com</body>
  </mail>
  <mail>
    <header>Draw PDF inverted</header>
    <body>I need to draw PDF with inverted colors. How can I do this? Thanks Manfred</body>
  </mail>
  <mail>
    <header>How does Quartz vector graphics internally works ?</header>
    <body>Hello, I'm currently having a look at several moderns 2D Graphic Engines, and have been playing a little with Quartz 2D. From my tests (drawing a couple of polygonal shapes, and timing the rendering), I have deduced a couple of things about how Quartz vector drawing internally works, and I would like to get some confirmations of my deductions... - Quartz 2D rasterization is based on a Scanline algorithm. - It's using a Gordon-like (also called &amp;quot;Critical Points&amp;quot; or &amp;quot;Local Minimums&amp;quot; on the litterature) approach for its Active Edge Table. Can anybody with access to Quartz sources confirms (or deny) this ? Regards, Raphael email@hidden</body>
  </mail>
  <mail>
    <header>CGContextShowGlyphs -- not for drawing strings?</header>
    <body>I wanted to draw some text with CGContextShowGlyphs, but if e.g. I use combining glyphs then these are not overlayed onto the character to which they bind. Am I to assume that this function is really only for drawing the actual glyph, and layout should be done by a higher level system (like ATSUI), which will draw glyphs one at a time (if necessary)? One of the free ADC TV videos showed a slide with international text (arabic, hebrew, etc.) and the speaker said that &amp;quot;this was all drawn with Quartz&amp;quot; -- but either I have overlooked something, or the speaker literally meant drawn by Quartz, not positioned by Quartz. Regards Allan --</body>
  </mail>
  <mail>
    <header>PDF Marked Contexts</header>
    <body>I'm looking for a way to insert marked-context points and/or sequences into a PDF graphics stream while recording it with a CG PDF context. Is there any way to do this currently, or should I file an enhancement request? Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: How expensive is CGContextSave/RestoreGState?</header>
    <body>To be honest, even/odd fill is so rarely used I don't think anyone's ever made this measurement. I'd probably stick with winding rule simply because it gets the lion's share of the attention. Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: How expensive is CGContextSave/RestoreGState?</header>
    <body>Here's one I wonder about from time to time.  Is there much performance difference between using a winding rule vs. even/odd fill?  For example, if I'm just filling something simple like a rectangle or circle should I prefer one over the other?  It seems like they could be using essentially the same algorithm, or completely different algorithms. Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: How expensive is CGContextSave/RestoreGState?</header>
    <body>Just to add to this: save/restore is very cheap, so you shouldn't worry about calling them.  On the other hand, what you do _after_ saving the gstate is what you should focus on:  for example, clipping, depending on the path, may be cheap or expensive. As Haroon said, the best thing is to profile your app.  That way you'll know what the impact is in your particular case. Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: Minimum Font Smoothing Size</header>
    <body>If there's no kosher way to do it (and if there's not, that would be a good subject for an ATSUI enhancement request), then maybe you can draw text at a large size (large enough that it's greater than the antialias minimum) and then apply a downscaling affine transform. It's worth a try anyway. Dave</body>
  </mail>
  <mail>
    <header>Re: Jaguar: Is the result of CGColorSpaceCreateDeviceRGB() a   cached object ?</header>
    <body>If you call CGColorSpaceCreateDeviceRGB(), you should always call CGColorSpaceRelease when you are done with it. haroon [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: Is CGContextDrawPDFDocument() multi-thread safe?</header>
    <body>Do you have a backtrace for your crash? [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: Quartz-dev Digest, Vol 2, Issue 16</header>
    <body>I tried this with OmniGraffle. For 15% and 29%, which is light grey, there is no visible banding and for 85 to 71%, which is dark grey, I imagine there might be just a trace of banding. There is no visible dithering. This is on a PowerBook with LCD display tilted over a range of angles. So _if_  OmniGraffle is using core graphics without dithering it would seem that there is no reason to believe that 256 greys cannot produce the gradient without banding which is what I would expect. This result also suggests that core graphics can do the job. On Saturday, January 29, 2005, at 07:36  AM, email@hidden wrote: Dr Richard Rothwell Faraday R&amp;amp;D Google: Macintosh contractor Australia email@hidden Educational software developer for Mac/Win/Web</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics Shading Quality</header>
    <body>On Jan 28, 2005, at 12:07 AM, Andrew Kimpton wrote: Don't know if the following will help, but it may be worth a shot... In the QuickDraw days, whenever I needed to fill a rectangle with a linear gradient (only for angles of 0, 90, 180 and 270), I would create an 32-bit offscreen GWorld. The size of the GWorld would always be 1 pixel in one dimension and the &amp;quot;length&amp;quot; of the gradient in the other.  For example, if the rect to fill was 800 x 600 and the gradient went from left-to-right, my GWorld would be 800 x 1. I'd then do the appropriate math and draw into the GWorld.  Finally, I used CopyBits with a mode of ditherCopy to take that smaller GWorld and render it over the larger area.  This gave beautiful dithering regardless of bit-depth of the destination device. I have no idea if CGImage will provide you the same. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics Shading Quality</header>
    <body>I'm not sure exactly how what you're doing works. Can you explain in more detail? Here's one suggestion, not knowing anything more about what you're doing. Can you convert your percentages to 8 bit integers, the convert back to floating point percentages and propagate the difference between that and the original to the next pixel? This would give you a very simple sort of dithering that might help eliminate the banding. Alternatively, you can add a very small amount of monochromatic noise to the gradient. For 8-bit per channel pixels, a random value of + or - 1 or 2 often gives good results. Darrin -- Darrin Cardani - email@hidden President, Buena Software, Inc. Video, Image and Audio Processing Development</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics Shading Quality</header>
    <body>On Jan 27, 2005, at 11:07 PM, Andrew Kimpton wrote: Perhaps there's nothing you can do in CG short of going to higher precision pixels (in Tiger).  However if it was me, I'd be interested to know how many times my callback is being called to see how many steps I should expect in the final result.  Then I'd examine the result pixel by pixel to see how my callback values were interpreted in the result.  Once you know what the I/O is to the CG shader you may be able to improve your results.</body>
  </mail>
  <mail>
    <header>Re: CoreGraphics Shading Quality</header>
    <body>On Jan 28, 2005, at 12:07 AM, Andrew Kimpton wrote: If I had to guess (which I do... sorry) I would say that Photoshop is probably using dithering to reduce banding.  There's not really a good way to replicate that with Quartz 2D without doing something like drawing the shading into an offscreen, dithering it yourself, and returning the results using something like CGContextDrawImage.  If you're going to go through all that trouble... you may as well just skip Quartz 2D altogether and draw your own gradient with the dithering in the offscreen.</body>
  </mail>
  <mail>
    <header>CoreGraphics Shading Quality</header>
    <body>I'm trying to replicate a shading effect from a photoshop file 'in code' using CGShading etc. The shading is subtle - an axial shading gradation from 15% grey to 29% grey and back again over a span of approx 800 pixels with the 29% grey at the midpoint. My CoreGraphics drawn shading has much more pronounced banding in it than the photoshop example I'm trying to replicate. Is there anything I can do to control the amount of banding ? The headers don't seem to reveal a quality setting - my CGFunction callback just interpolates between the two shades according to the passed in position value. I'm using single precision floats for my calculations since CoreGraphics using single precision too I don't think changing that will help.</body>
  </mail>
  <mail>
    <header>Re: Bitmap context using CreateCGContextForPort</header>
    <body>On Jan 26, 2005, at 5:17 AM, james roy wrote: Quartz 2D, in general, doesn't support indexed colors.  Your best bet is to draw what you want in a 16 or 32 bit bitmap and then use CopyBits or some other tool that understands indexed colors to move the drawing into your port.</body>
  </mail>
  <mail>
    <header>Bitmap context using CreateCGContextForPort</header>
    <body>Hi All, Can somebody tell me regarding creating context by passing the bitmap port to CreateCGContextForPort. I have a offscreen graphics world created by NewGWorld with a pixel depth of 8. According to specifications for CreateCGContextForPort offscreen graphics worlds with pixel depths of 1, 2, 4, and 8 are not supported. What should we do in case we need to create a context for the bitmap port with pixel depth of 8. Your suggestions are welcomed. Thanks in advance. Regards, James. __________________________________ Do you Yahoo!? Yahoo! Mail - 250MB free storage. Do more. Manage less.</body>
  </mail>
  <mail>
    <header>Re: Quartz speed</header>
    <body>On Jan 24, 2005, at 9:46 AM, John Kerr wrote: If you have run numbers yourself, as it appears you have, what more are you looking for? If you want others to run numbers it may be best to provide your &amp;quot;benchmark&amp;quot; code. As said before if you are hitting a particular issue in regards to performance it is best to outline that issue so folks on this list (including Apple folks that frequent) can assist otherwise file specific defects with Apple. The simple fact is that Quartz2D will nearly always be at a disadvantage in terms of performance to QD simply because of the model and fidelity used in Quartz2D (not factoring in Tiger) when running on similar hardware.</body>
  </mail>
  <mail>
    <header>Re: Quartz speed</header>
    <body>The QDSetDirtyRegion, CGContextAddLine(Path accumulation) tricks are used in my tests. Path accumulation sometimes slows down the drawing speed! Antialiasing, Line End-Caps, Line Join, On/Off screen, variations are all included in my tests. Antialiasing sometimes makes things faster! I am familiar with the test results shown at WWDC.  Interestingly there was not much in there with &amp;gt;1 pixel thick lines.  I have tested with the new stuff in Tiger, and found some caveats. But, back to my original question.  Surely someone has performed similar tests? ________________________________ From: Scott Thompson [] Sent: Monday, January 24, 2005 10:51 AM To: John Kerr Cc: 'Shawn Erickson'; email@hidden Subject: Re: Quartz speed FYI in Quartz 2D I bet you are talking about a 2 point thick line not a 2 pixel thick line since you work in points in user space and those are mapped to pixels in the output device as needed after applying any transformation. It's actually drawing in pixels. Yes... but when you specify the size of the line in user space, you specify it in points. If you scaled the context by 2x, those same two points would draw at 4 pixels The problem is that Quartz is 3x slower at drawing thicker lines. It would be nice to know if anyone has done any kind of timing tests with Quartz, to see if there is some consistancy in these results, or if there is a realistic expectation of anything better. You can rest assured that the process of drawing lines with Quartz has been studied extensively by Apple. There has been quite a lot of work put into comparing the performance of QuickDraw and Quartz 2D in respect to drawing lots of single lines. At WWDC, Apple demonstrated the performance of Quartz 2D vs. QuickDraw with regards to this very problem. They discussed some optimizations that can improve the drawing speed of Quartz 2D when using lots of single lines. They also demonstrated some improvements that are forthcoming that will improve the performance of Quartz 2D DRAMATICALLY (so long as you are not also trying to draw in the window with QuickDraw).. I can remember some of the optimization techniques they described. Turning off Antialiasing is one acceleration technique. Using CGContextAddLines to draw a bunch of lines at once is another one acceleration technique. The last one I remember is to use QDSetDirtyRegion to mark the entire window's port as dirty before drawing a bunch of lines. This is because each line has to add it's pixel representation to the dirty region. If you've pre-dirtied the region, the region union calculations become trivial enough to simply drop out. Scott</body>
  </mail>
  <mail>
    <header>RE: Quartz speed</header>
    <body>&lt;SPAN class=796431016-24012005&gt;The QDSetDirtyRegion, CGContextAddLine(Path accumulation) tricks are used&amp;nbsp;in my tests. &lt;SPAN class=796431016-24012005&gt;Path accumulation&amp;nbsp;sometimes slows down the drawing speed! &lt;SPAN class=796431016-24012005&gt; &lt;SPAN class=796431016-24012005&gt;Antialiasing, Line End-Caps, Line Join, On/Off screen,&amp;nbsp;variations are all&amp;nbsp;included in my tests. &lt;SPAN class=796431016-24012005&gt;Antialiasing sometimes makes things faster! &lt;SPAN class=796431016-24012005&gt; &lt;SPAN class=796431016-24012005&gt;I am familiar with the test results shown at WWDC.&amp;nbsp; Interestingly there was not much in there with &amp;gt;1 pixel thick lines.&amp;nbsp; I have tested with the new stuff in Tiger, and found some caveats. &lt;FONT face=Arial color=#0000ff size=2&gt;But, back to my original question.&amp;nbsp; Surely someone has performed similar tests? Scott Thompson [mailto:email@hidden] John email@hidden Re: Quartz speed FYI in Quartz 2D I bet you are talking about a 2 point thick line not a2 pixel thick line since you work in points in user space and those aremapped to pixels in the output device as needed after applying anyIt's actually drawing in pixels.Yes... but when you specify the size of the line in user space, you specify it in points. If you scaled the context by 2x, those same two points would draw at 4 pixels The problem is that Quartz is 3x slower at drawing thicker lines. It wouldbe nice to know if anyone has done any kind of timing tests with Quartz, tosee if there is some consistancy in these results, or if there is aYou can rest assured that the process of drawing lines with Quartz has been studied extensively by Apple. There has been quite a lot of work put into comparing the performance of QuickDraw and Quartz 2D in respect to drawing lots of single lines. At WWDC, Apple demonstrated the performance of Quartz 2D vs. QuickDraw with regards to this very problem. They discussed some optimizations that can improve the drawing speed of Quartz 2D when using lots of single lines. They also demonstrated some improvements that are forthcoming that will improve the performance of Quartz 2D DRAMATICALLY (so long as you are not also trying to draw in the window with QuickDraw)..I can remember some of the optimization techniques they described. Turning off Antialiasing is one acceleration technique. to draw a bunch of lines at once is another one acceleration technique. The last one I remember is to use to mark the entire window's port as dirty before drawing a bunch of lines. This is because each line has to add it's pixel representation to the dirty region. If you've pre-dirtied the region, the region union calculations become trivial enough to simply drop out.Scott</body>
  </mail>
  <mail>
    <header>Re: Quartz speed</header>
    <body>FYI in Quartz 2D I bet you are talking about a 2 point thick line not a 2 pixel thick line since you work in points in user space and those are mapped to pixels in the output device as needed after applying any transformation.</body>
  </mail>
  <mail>
    <header>Re: Quartz speed</header>
    <body>On Jan 24, 2005, at 9:22 AM, John Kerr wrote: Well, Apple has announced publicly that, in tiger, 2D operations will likely be GPU accelerated. (a) apple probably did timing tests with Quartz (b) drawing via the GPU is probably going to be just a tad bit faster than with software ;) And even without GPU acceleration, every release of OS X, from Public Beta to 10.3 has seen performance improvements in graphics. No reason to think Tiger's software renderers won't be faster once again.</body>
  </mail>
  <mail>
    <header>RE: Quartz speed</header>
    <body>It's actually drawing in pixels. The problem is that Quartz is 3x slower at drawing thicker lines.  It would be nice to know if anyone has done any kind of timing tests with Quartz, to see if there is some consistancy in these results, or if there is a realistic expectation of anything better. The fact that Quartz is more capable in the long run doesn't really help a customer when they are waiting for their redraw to finish. -----Original Message----- From: Shawn Erickson [] Sent: Friday, January 21, 2005 10:31 PM To: John Kerr Cc: email@hidden Subject: Re: Quartz speed Quartz2D does a lot more work (for one having to map between user space and device space) then the primitives in QuickDraw so I would expect it to be slower for some/all classes of options that have a peer in QD (at least for current Mac OS X versions, wink, wink, nudge, nudge, say no more) but Q2D is more capable and flexible in the long run. If you find an area that is problematic for something you are trying to do folks on this list can likely outline some optimizations that you may want to try if you outline what issues you are having. Also file defects as needed to let Apple know about issues you are hitting. FYI in Quartz 2D I bet you are talking about a 2 point thick line not a 2 pixel thick line since you work in points in user space and those are mapped to pixels in the output device as needed after applying any transformation. -Shawn</body>
  </mail>
  <mail>
    <header>Re: 16-bit per [channel] drawing</header>
    <body>I think what you're referring to above in terms of 16-bit is &amp;quot;Thousands of colors&amp;quot; (i.e. 16 bits total for all components and/or alpha).  This thread is about 16-bits per channel/component (e.g. 64K levels each of red, green, blue).  The original subject was misleading. -- Rick Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: 16-bit per pixel drawing</header>
    <body>That's one of the reasons why I'm still sticking with Quickdraw for some things. In one particular app I need to render things to pixel-precision (simulating the form drawing on a Palm device) and the only reliable solution I found was to use a Quickdraw view into which I could appropriately draw in 16 bit and control the pixel-level rendering, especially when drawing rounded rectangles and such. -- Florent Pillet Freelance software developer/consultant - Palm OS &amp;amp; Mac OS X ICQ: 117292463                Skype: callto://florent.pillet</body>
  </mail>
  <mail>
    <header>Re: pattern</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Quartz speed</header>
    <body>On Jan 19, 2005, at 6:46 AM, John Kerr wrote: Quartz2D does a lot more work (for one having to map between user space and device space) then the primitives in QuickDraw so I would expect it to be slower for some/all classes of options that have a peer in QD (at least for current Mac OS X versions, wink, wink, nudge, nudge, say no more) but Q2D is more capable and flexible in the long run. If you find an area that is problematic for something you are trying to do folks on this list can likely outline some optimizations that you may want to try if you outline what issues you are having. Also file defects as needed to let Apple know about issues you are hitting. FYI in Quartz 2D I bet you are talking about a 2 point thick line not a 2 pixel thick line since you work in points in user space and those are mapped to pixels in the output device as needed after applying any transformation.</body>
  </mail>
  <mail>
    <header>Re: NSImage from encrypted PDF</header>
    <body>On Jan 21, 2005, at 2:53 PM, Clark Cox wrote: You know, that might just do it :) Thanks for pointing this out!  Guess I was too deep in the APIs to see obvious solutions such as this. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: NSImage from encrypted PDF</header>
    <body>Would something like this be appropriate?: -- Clark S. Cox III email@hidden</body>
  </mail>
  <mail>
    <header>NSImage from encrypted PDF</header>
    <body>I'm exploring the possibility of having my apps PDF images all be encrypted.  This is basically to add at least some protection to my artwork. I'm hoping to have my app baselined against 10.3 and so it looks like I need to use APIs such as CGPDFDocumentUnlockWithPassword. What I'm having trouble with is coming up with a &amp;quot;pipeline&amp;quot; that will take the original encrypted PDF and ultimately give me an NSImage. I can figure out how to create a CGPDFDocumentRef and unlock it with the appropriate password.  However, I cannot seem to find how to obtain the (decrypted) PDF data to ultimately build an NSData object which in turn can be used to initialize an NSImage. About the only thing I can get from the document is a CGPDFPageRef (in my case, I'll only ever have single page documents, so this would be easy).  But from there, I still cannot find a function to get any data.  There are some hints that the page's dictionary could contain a PDF data stream, but that's where everything is getting blurry :P Any help greatly appreciated, -- Rick Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: Drawing on top of Sequence Grabber video</header>
    <body>Scott Thompson: Consider using a grouped overlay window. This way, Quartz Extreme will be used for compositing, which is likely to be faster on most systems. On the other hand, it's likely to be slower on older systems. -- Jens Ayton</body>
  </mail>
  <mail>
    <header>DVI to HDMI video cable on G5</header>
    <body>I realize this list may not be the most appropriate place to ask this &amp;quot;hardware&amp;quot; question, but nothing else on lists.apple.com seemed more appropriate... Does anyone have any experience or knowledge concerning the use of a DVI-to-HDMI cable for connecting a G5 powermac (NVIDIA GeForce FX 5200 Ultra) to an HD TV that has an HDMI input? Should it work?  I would have assumed that it would work, but recent experience with one such cable and a Sony HD TV seemed to prove otherwise*. I have read a few things stating that cable quality can make a difference between success and failure, and perhaps that is the case here. Any kind of help would be appreciated. * No other display was connected at the time owing to the lack of an ADC-to-VGA adapter.</body>
  </mail>
  <mail>
    <header>Re: Providing shade colors to Calculate CGFunctionCallback</header>
    <body>On Jan 20, 2005, at 12:45 PM, Toine Heuvelmans wrote: The first parameter of CGFunctionCreate is a pointer to anything you want.  This pointer is passed back to you as the first parameter of your function callback.  Generally you would pass a pointer to a structure containing your colors and whatever else you need here.  Just make sure the data pointed to still exists when your callback is actually called.</body>
  </mail>
  <mail>
    <header>Re: Drawing on top of Sequence Grabber video</header>
    <body>On Jan 20, 2005, at 1:34 PM, Toine Heuvelmans wrote: The routine QDBeginCGContext and QDEndCGContext allow you to create a CGContext on a QuickDraw port.  Theoretically you will be able to set up a callback on the QuickTime content and can use the calls to create a context, draw your content, and destroy that context.</body>
  </mail>
  <mail>
    <header>Providing shade colors to Calculate CGFunctionCallback</header>
    <body>I tried implementing the functions from the Quartz documentation, concerning shading. that is, the: MyDrawEventHandler myPaintAxialShading myGetFunction myCalculateShadingValues It worked great, I was drawing rectangles the entire day! I provided a color within the Calculate... function, and calculated a darker and brighter shade, which were the two colors for my shade. But the thing I want to do, is to let the user pick a color, and put this into the Calculate... function (and then the darker and brighter calculus...). I could not find a way to provide any extra info to this function. How do I do this?</body>
  </mail>
  <mail>
    <header>Drawing on top of Sequence Grabber video</header>
    <body>I'm totally new to Quartz, just drew my first line today, but before I go any further, I just want to know if what I want is possible with Quartz. So... Is it possible to draw on top of QuickTime video output, generated by a Sequence Grabber? (it uses the window as a port, and draws directly on top of it, or at least that's what I believe happens) My application is Carbon based, and uses a HIView for drawing Quartz stuff. Thank you Toine Heuvelmans</body>
  </mail>
  <mail>
    <header>Re: pattern</header>
    <body>If you are writing a CFM app you need to provide a MachO function pointer for the Callback.  See the CFM_MachO_CFM sample from Apple, and look for MachOFunctionPointerForCFMFunctionPointer.</body>
  </mail>
  <mail>
    <header>pattern</header>
    <body>I want to draw pattern. I din't succeed. Compilation and link are OK, but the fonction MyDrawColoredPattern is never called. Can someone tell me if what I've write is correct or what is not correct? Thanks Vincent #define H_PSIZE 16 #define V_PSIZE 18 void MyDrawColoredPattern (void */*info*/, CGContextRef myContext) float subunit = 5; // the pattern cell itself is 16 by 18 if(fCGContext) static const	CGPatternCallbacks	 callbacks = if(pattern != NULL) pattern = CGPatternCreate (NULL, CGRectMake (0, 0, H_PSIZE, V_PSIZE), CGAffineTransformMake (1, 0, 0, 1, 0, 0), H_PSIZE, V_PSIZE, kCGPatternTilingConstantSpacing, true,</body>
  </mail>
  <mail>
    <header>Quartz speed</header>
    <body>Has anyone compared the speed of drawing graphic primitives with Quartz and compared them with QuickDraw?  I am finding Quartz is often slower, especially when line thickness is greater than 1 pixel. Here's my timing results for drawing 10000 primitives. G5, GeForceFX5200, OS10.3.7 1 Pixel thick Line	Rect	RRect	Poly	Ellip	Arc QuickDraw	2.8	3.19	3.36	26.57	4.46	4.15 Quartz	2.51	3.54	18.02	8.63	17.42	14.04 2 Pixel thick Line	Rect	RRect	Poly	Ellip	Arc QuickDraw	2.86	3.25	3.33	27.53	4.06	3.88 Quartz	8.46	9.61	28.02	25.35	30.15	23.7 For QuickDraw, the numbers are for drawing off-screen, since that was the fastest case. For Quartz, the numbers for 1 pixel thick lines are for on-screen, antialiased, accumulated path drawing, since that was the fastest.  The numbers for 2 pixel thick lines are for off-screen, non-antialiased, non-accumulated paths, since they were the fastest. I am curious to know what kind of performance numbers are expected from Quartz.</body>
  </mail>
  <mail>
    <header>Flushing performance (was Re: tiger)</header>
    <body>Generally we can't discuss the Tiger changes on a public list like this because of the NDA. I probably broke protocol by mentioning the floating point support. While I can't go into the features, one important change that will affect performance (and more likely benchmarks) is flushing. For the past few years we've been telling developers not to flush faster than the display refresh rate as that is just wasteful, and in Tiger we will be enforcing this by making every flush operation vbl syned. This is important to reduce or eliminate visual tearing in the system and continue going forward with glitch free playback in the system. That means that when you have something like the following in your event loop: 1) Do some initital drawing (usually to erase the backgound) 2) Do some other drawing 3) Flush the drawing (either explicitly through CG or through Carbon/Cocoa's flush) Now while the flush operation is asynchronous, when you go back to the top of the event loop to draw things again, the first drawing operation (1) will block until the previous flush has completed. In Tiger, this flush will now wait at least 1/60th of as second for the vbl sync. If you profile your application through shark you will notice that most of the time will be spent on the first drawing operation (not because it is expensive) because it waiting for the previous flush to complete. Benchmarks that test the drawing performance of the system will definitely be affected.  Please note that different graphics cards have had different vbl syncing behavior. With the Tiger change things will be consistent across the graphics hardware. So recommendations (which are still valid for 10.3 and earlier) - Minimize the flush operations in every frame. i.e. don't draw, flush, draw, flush, ... for every update. Instead draw, draw, draw, flush - Consolidate your drawing operations together and issue a single flush. Defer your drawing until necessary. i.e. don't draw as the first thing in your event loop before you go off and do other work in your application. Because the flush is asynchronous, you can avoid blocking by delaying your drawing. - Don't flush faster than 60 Hz. In many cases 30 Hz is more than enough. - Use Quartz Debug to detect cases where you are overflushing or performing unnecessary drawing. These recommendation should not be new but I thought I'm mention them again.</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <body>On Jan 17, 2005, at 7:58 PM, Eric Schlegel wrote: Thanks Eric, that worked well.  I didn't really understand what SetWindowGroup did since the documentation for that function doesn't say much.  The header file says a little more but I still didn't quite catch what its purpose was.  Now it makes sense.  I'm now just using a simple kPlainWindowClass window for my color palette. I will probably be back using a kOverlayWindowClass window as soon as I start working on drawing to the screen using Quartz instead of QuickDraw.  The overlay window will be important for  our graphic cursor/crosshairs and for our temporary objects.   But that's a multi-month project that's going to have to wait until after the rest of the Carbon work.</body>
  </mail>
  <mail>
    <header>tiger</header>
    <body>what are the most significant changes in quartz that come with tiger? render to float was mentioned earlier.  Are there others?</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <body>On Jan 17, 2005, at 6:30 PM, Mike Lazear wrote: If that's the only reason, and you'd prefer to use a different window type, you have an easy solution: just use SetWindowGroup to move whatever window you like into the window group for the overlay window class. You can freely make any window you like appear above floating windows using SetWindowGroup( window, GetWindowGroupOfClass( kOverlayWindowClass ) ).</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <body>On Jan 15, 2005, at 4:41 PM, Nick Nallick wrote: Nick - you probably are giving much more credit than I deserve.  =) We have a color palette that our users can pick a color from.  This consists of 256 colors.  We don't use the standard Apple color picking because at the moment our database only supports one byte to select a color index as opposed to 6 bytes (2 bytes per color) which are required for RGB.  So in our property window you can either type in a color index or click down on a icon which pops up the color palette. This palette is only visually available until you let go of the button. This color palette is created in an overlay window. So why an overlay window?  Well it happens to be the only window type that will show up in front of my floating properties window.  Movable modals and the rest of the window types show up behind the window where the color icon is located.  The overlay window type shows up in front. So it may be an odd use but it works well and the window only lives for a few seconds and is relatively small.</body>
  </mail>
  <mail>
    <header>Re: 16-bit per pixel drawing</header>
    <body>Drawing to a 16 bit per channel context isn't available. Tiger will introduce rendering to 32 bit floating point image formats - maybe overkill for your purpose, but it will be the only high precision format to render to we will support for now. Attachment:</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <body>On Jan 15, 2005, at 1:01 PM, Mike Lazear wrote: I'm sure you know what you're doing, but in general there can't be too many cases where you'd want to use an overlay that wasn't transparent.</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <body>On Jan 15, 2005, at 6:51 AM, Nick Nallick wrote: Notice that the &amp;quot;must use&amp;quot; is connected with &amp;quot;to preserve the transparency of the window&amp;quot;.  I've had no problem with QuickDraw in an overlay window and in my case I don't want transparency.  That being said the way I'm currently drawing is not ideal but I'm in the middle of converting an application from QuickDraw to Quartz and it's going to take months.  Meanwhile I need to keep the program runable.  I tried pretty much every window type and the overlay window was the only one that would work for my particulars needs. I was not trying to suggest to Ricky that QuickDraw in an overlay window was ideal but rather to explain why I thought he was having the problem that he was seeing.</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <body>On Jan 14, 2005, at 7:55 PM, Ricky Sharp wrote: I would say that's it's a bad idea to try to use QuickDraw in an overlay window. Note the following from MacWindows.h (emphasis added): Nick</body>
  </mail>
  <mail>
    <header>Re: 16-bit per pixel drawing</header>
    <body>I was able to create a 16-bit greyscale NSBitmapImageRep, and I could lockFocus on it, but the drawing is never 16-bit because the context is still 8-bit. I went down to the CGContext level but was unable to create a 16-bit per channel context, so I suspect the support is not there. I think the 16-bit per channel CGImageCreate support is probably there to make it possible to view 16-bit per channel images, but I think they</body>
  </mail>
  <mail>
    <header>Re: 16-bit per pixel drawing</header>
    <body>On Jan 14, 2005, at 8:51 PM, Jamie Cho wrote: I too could not find any Quartz docs that mentioned 16-bit component support.  Forgot where that one table was, but it was something to do with &amp;quot;supported pixel formats&amp;quot; and they only had up to 8 bits per component. mentions that for the initWithDataPlanes:pixelsWide:... API, that the bits per sample (component) parameter can have the values 1, 2, 4, 8, 12 or 16. I would wager a guess that you should be able to pass in these same values for the bitsPerComponent parameter of CGImageCreate. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <body>Good catch.  Upon closer inspection, I see that CarbonSketch is using the Quartz APIs for drawing into the overlay whereas GrabBag uses QuickDraw. This would also explain why no issues cropped up in the Cocoa examples (since they ultimately use Quartz). Workaround definitely seems to be to use Quartz.  Although, &amp;quot;workaround&amp;quot; is a bad term here since QuickDraw is deprecated and one should migrate to Quartz anyhow. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>16-bit per pixel drawing</header>
    <body>There is a reference at this page: that claims that Quartz can draw to 16-bit per-channel bitmap contexts. All the documentation that I found indicates that the maximum number of bits per channel is 8. Is there anyway to create a 16-bit per channel context, or is this reference in error? I am particularly interested in creating  a 16-bit grayscale  context.</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <body>Greetings All, I am currently working with overlay windows under MacOS 10.3.7. The overlay windows, however, are completely invisible when the display is set to 16-bit color (&amp;quot;Thousands of Colors&amp;quot;) resolution. Other bit resolutions (8-bit and 24/32-bit) operate properly. Although this issue appears to be a bug of some sort, (it operates fine in MacOS 10.4 build 8A323), I would like to know why the &amp;quot;CarbonSketch&amp;quot; overlay window sample appears to operate correctly. The following Apple code sample appears to exhibit the aforementioned problem (&amp;quot;Overlay&amp;quot; tab - Large Cursor): http://developer.apple.com/samplecode/GrabBag/listing6.html Does anybody know why this code (as well as my code - overlay window initialization portion available if necessary) does not produce a visible overlay window under MacOS 10.3.7?</body>
  </mail>
  <mail>
    <header>Re: Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit)	Color	Resolution?</header>
    <body>On Jan 14, 2005, at 4:17 PM, Ayodeji A. Oshinnaiye wrote: This is definitely a strange one.  I can reproduce this problem also under 10.3.7.  And, I can't see anything in the code that should cause it to fail with 16-bit color.  The CarbonSketch example does work for me also under 10.3.7. I know this won't help you since you are coding in Carbon, but Cocoa overlay windows do not have any problems in 16-bit color.  I ran the FunkyOverlayWindow example and it worked a-ok: ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Overlay Windows Not Visible in &amp;quot;Thousands&amp;quot; (16-bit) Color	Resolution?</header>
    <body>Greetings All, I am currently working with overlay windows under MacOS 10.3.7. The overlay windows, however, are completely invisible when the display is set to 16-bit color (&amp;quot;Thousands of Colors&amp;quot;) resolution. Other bit resolutions (8-bit and 24/32-bit) operate properly. Although this issue appears to be a bug of some sort, (it operates fine in MacOS 10.4 build 8A323), I would like to know why the &amp;quot;CarbonSketch&amp;quot; overlay window sample appears to operate correctly. The following Apple code sample appears to exhibit the aforementioned problem (&amp;quot;Overlay&amp;quot; tab - Large Cursor): Does anybody know why this code (as well as my code - overlay window initialization portion available if necessary) does not produce a visible overlay window under MacOS 10.3.7? Thanks in advance, Ayo</body>
  </mail>
  <mail>
    <header>Re: CGImage/CGBitmapContext, guaranteed to be in sync?</header>
    <body>That's why I asked :) Thanks! -Jon</body>
  </mail>
  <mail>
    <header>Re: CGImage/CGBitmapContext, guaranteed to be in sync?</header>
    <body>No, please do not rely on this behavior. A CGImageRef, once created is supposed to be immutable, including the bitmap data backing it. That's the model. The fact that you see what you see is just an implementation detail. This will break if CoreGraphics performs any intelligent caching on images, so better not to rely on this behavior. Attachment:</body>
  </mail>
  <mail>
    <header>CGImage/CGBitmapContext, guaranteed to be in sync?</header>
    <body>I have a question about something I've been noticing, and before deciding to rely on it, I wanted to ask if it's expected to be true :) In some cases I create a CGBitmapContext and a CGImage pointing to the same data. If I draw the CGImage, and then proceed to draw more things to the CGBitmapContext, will the CGImage know to regrab the data before being redrawn? My initial tests show &amp;quot;Yes,&amp;quot; but before relying on anything, I decided it would be better to ask others :) 1) Create the CGBitmapContext and CGImage 2) Fill the CGBitmapContext with blue 3) Draw the CGImage to the screen 4) Draw a red circle on the CGBitmapContext 5) Draw the CGImage to the screen At step 5, am I guaranteed to have a red circle on a blue background show up? Thanks in advance, Jon</body>
  </mail>
  <mail>
    <header>Re: How to avoid stretched screen mode?</header>
    <body>On Jan 13, 2005, at 11:43 AM, Shawn Erickson wrote: Ok I need to amend what I stated... first off on my system 1280 x 800 x 32 only has a non-stretched mode listed (again walking all display modes), I was getting data points screwed up when I was looking at things because of not ready the numbers correctly. Anyway if I ask for 1280 x 960 I get both a stretched and non stretched version and when I ask for that resolution on the game side of things I get back a non stretched form and (not using my remove hack) the display gets switched to a non-stretched mode. capacity = 11, pairs = ( capacity = 12, pairs = ( I note that the default resolution 1920 x 1200 for my display has width/heigh ratio of 1.6 and that 1280 x 960 has 1.33 while 1280 x 800 has 1.6. Anyway I think the fault is mine now that I look at it... the game is assuming a ratio of 1.33 and hence some images are getting stretched by OpenGL to fill the space when in a screen mode without that ratio. I guess I will work on displaying only ones with the ideal ratio for the game.</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <body>On Aug 7, 2006, at 6:45 AM, Jerry wrote: The other problem with this is there is no API to extract the stroke path from Quartz.  The best you can do is make the current path into the path equivalent to the stroke of the previous current path (another enhancement request I've filed).  However there's no way to extract the current path from a CG context. Nick</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <body>On 7 Aug 2006, at 08:05, Vinay Prabhu wrote: You don't want to use the bounding box in the calculation. What you need to do is walk the path, and at each point work out its direction, then work out a point which is N units &amp;quot;to the left&amp;quot;, i.e. if you have a line segment from x1,y1 to y2, y2, you need to work out dx=x2-x1,dy=y2-y1, normalize it, multiply by N (your line width), and produce a new point x1-dy,y1+dx. Do this at both ends of each segment. You'd have to flatten out Bezier curves to line segments first. This won't be too bad for paths which don't turn too quickly, but for ones which do, you'll have to work out the miter points which is more complicated. How about using the Quartz stroke engine to do the work for you? Take your path, then convert the stroke of that path to another path. You now have one path which has the inner and outer paths joined together. If you could find where the join is, you could separate it into the two paths you want. I suspect that working out the join might be tricky though.</body>
  </mail>
  <mail>
    <header>RE: PenAlignment(GDI+) alternative in Quartz</header>
    <body>Hi, I thought of a method to solve this problem, We know the bounding rectangle of the path. Now we keep the center of the rectangle (midX and midY) as reference. While constructing the CGPath, for each point we can draw a imaginary line from center to the point using line dda algorithm. The extrapolate the line with the offset we want. Use the extrapolated points to create a new path which will be parallel to the original path. I have not yet implemented this. Any idea's whether this works or not? Regards Vinay -----Original Message----- From: quartz-dev-bounces+vinayprabhu=email@hidden [ n Behalf Of Nick Nallick Sent: Saturday, August 05, 2006 8:20 PM To: Quartz-Dev (E-mail) Subject: Re: PenAlignment(GDI+) alternative in Quartz As I mentioned before, you can do this for many simple paths but it's not a complete solution.  For example, consider the classic five pointed star used to illustrate the non-zero winding vs. even/odd fill rules (i.e., a five pointed star drawn with five overlapping lines).  Trying to clip to this path won't give you satisfactory results for either the &amp;quot;inside&amp;quot; or &amp;quot;outside&amp;quot; problems. I've illustrated what you get here: stroke_example.jpg I filed an enhancement request for this in March (rdar://problem/ 4472716) which is now closed, so I'd guess the idea wasn't warmly received.  If it's something people want I'd suggest they do likewise. If the original path were only being stroked it wouldn't be a problem, you could just export the path used to create the adjusted stroke (which might be either filled or stroked itself depending on the algorithm).  If the original path was to be both filled and stroked, I would expect this to be handled by breaking the stroke out as a second path drawn over the top of the original filled path. Nick _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: .in This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <body>On Aug 5, 2006, at 7:47 AM, Jens Ayton wrote: As I mentioned before, you can do this for many simple paths but it's not a complete solution.  For example, consider the classic five pointed star used to illustrate the non-zero winding vs. even/odd fill rules (i.e., a five pointed star drawn with five overlapping lines).  Trying to clip to this path won't give you satisfactory results for either the &amp;quot;inside&amp;quot; or &amp;quot;outside&amp;quot; problems. stroke_example.jpg I filed an enhancement request for this in March (rdar://problem/ 4472716) which is now closed, so I'd guess the idea wasn't warmly received.  If it's something people want I'd suggest they do likewise. If the original path were only being stroked it wouldn't be a problem, you could just export the path used to create the adjusted stroke (which might be either filled or stroked itself depending on the algorithm).  If the original path was to be both filled and stroked, I would expect this to be handled by breaking the stroke out as a second path drawn over the top of the original filled path. Nick _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <body>Vinay Prabhu: If I understand your requirement correctly, I think this can be achieved using clipping. For starters, to draw an outline of thickness N entirely inside a path, first clip to the path and then stroke the path with thickness N + 2. To draw entirely outside the path is trickier, but I believe it can be done, if you‚Äôre using the even-odd winding rule, by adding an appropriate surrounding rectangle to the clipping path. Once you‚Äôve drawn the inner and outer outlines, the ‚Äúcenter outline‚Äù (oxymoron alert) is simply a matter of stroking the path normally. All this breaks down if you need transparency, although on a raster device you could render the colour and alpha channels separately and then composite. -- Jens Ayton</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <body>That seems reasonable. I guess you're stuck with using a enough segments to come &amp;quot;sufficiently close&amp;quot; to the parallel path.</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <body>There is no Bezier curve (4 control points) that is the exact offset of another Bezier curve. I don't remember the proof (and it being late Friday I'm too lazy to look it up), but it could be looked up. Nope. For this there is an easy proof. You need the above and the fact that a Bezier curve can be split at any point along it into two Bezier curves that together are the exact shape of the original. (de Casteljau's algorithm.) To do so at an arbitrary point requires finding the t value for the point which may be hard but it is possible in principal. Now assume that you could construct the exact offset of a Bezier curve using two Bezier curves. (The two is just for convenience - you can easily extend it to N.) Take the point where your two offset curves join and find the equivalent point on the original curve. (Construct the tangent to the offset at the joint point, take the perpendicular and find its intersection with the original curve.) You would then have *two* Bezier curves, each with an exact offset that is also a Bezier curve. Bob</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <body>On Aug 4, 2006, at 4:22 PM, Robert Clair wrote: Sorry... the mathematical geek in me just leaked out. Is it the case that there is no bezier &amp;quot;curve&amp;quot; exactly parallel to another, or is it the case that, given a single segment of a bezier curve, there is no way to create the parallel curve using a single segment. That is to say, given a particular bezier curve segment (with four points in it's control polygon), could I construct a bezier curve of more than one segment which does lie parallel to the original curve at all points? Offhand I would guess the answer is &amp;quot;yes&amp;quot; at least theoretically as you could simply define a bezier curve with an infinite number of segments.  Each of those individual segments is the degenerate case where each of the four control points (in the cubic case) lie on the same point.  I wonder if there is a curve with fewer segments that is also parallel... Hmmm. Of course in graphics you really don't need a curve that is EXACTLY parallel... &amp;quot;close enough&amp;quot; would be fine.  Certainly that is possible using multiple segments. Scott</body>
  </mail>
  <mail>
    <header>Re: CGGLContextCreate appears not to release</header>
    <body>Thanks for your reply. With regards to returning to the main event loop: yes I am. Cheers, -C</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <body>Absolutely. Actually there is no Bezier curve that is exactly parallel (offset) to a given given Bezier curve (except in the degenerate case where the Bezier curve is a straight line. But if you can confine yourself to paths that have no cusp, no self intersections, and &amp;quot;small&amp;quot;  offsets you can construct a reasonable approximation by offsetting the control polygon of the original curve and using the offset polygon as the control polygon for a new curve. Small, here, means small with respect to the smallest radius of curvature of the original curve. ....Bob Clair</body>
  </mail>
  <mail>
    <header>Re: CGGLContextCreate appears not to release</header>
    <body>On Aug 3, 2006, at 10:29 PM, Christopher Hunt wrote: Are you returning to the main event loop between calls? should be putting the context into the autorelease pool.  If that autorelease pool is not released itself (between calls to this code) then the context would still have a reference owned by that pool.</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <body>Actually there is no way to transform the path to achieve this for an arbitrary bezier shape. For simple shapes, like ovals and rectangles, you can achieve this effect, but even for simple polygons (particularly concave polygons, or those with self-intersections) a simple affine transform won't do what you want it to.  If you want to prove this to yourself, try creating a polygon that looks like a blocky letter &amp;quot;C&amp;quot; and see if you can find a transform that will create a path that lets you stroke only on the interior of the path. Your next attempt would be to try and use clipping to achieve the desired result, but you'lll eventually find that that won't work either. To solve this problem in Quartz 2D you have to construct a path that is &amp;quot;parallel&amp;quot; to the original path which you can stroke.  Creating a bezier path that is parallel to another bezier path is complicated... particularly if the original path has a cusp. Y also have to consider removing self intersections from the new path. and patching up any resulting discontinuities that that generates. As Nick pointed out, the only practical way to implement this functionality would be to add it to the imaging model, but doing so would put Quartz 2D out-of-joint with PostScript and PDF.  As a result, it would probably have to be handled by a library that is outside of Quartz 2D (be it an Apple addition, or a third-party library) and to my knowledge, no such thing exists. :-( I know this same functionality exists in the imaging models of several other graphics libraries (GDI+ and Java 2D come to mind immediately).  It would be interesting to see how those libraries translate the resulting graphics into PostScript and PDF.</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <body>On Aug 4, 2006, at 7:07 AM, Vinay Prabhu wrote: You can transform when you draw in various ways, so you can easily achieve what you want above. Of course if the path crosses itself this scheme wont work. It isn't clear if you are working at the Cocoa level or down at the Quartz 2S level (I will assume the later since you are on this list)... -Shawn</body>
  </mail>
  <mail>
    <header>RE: PenAlignment(GDI+) alternative in Quartz</header>
    <body>I agree with you, for the basic paths I could achieve all kind of outline. I need to apply outline for the path extracted from mask of an image. Which is a complex arbitrary path. Is it possible to shrink or stretch the path with respect to center? I have tried to scale the path along both x and y axis. But this will retain its origin constant, so the desired effect can't be achieved. Regards Vinay -----Original Message----- From: Nick Nallick [] Sent: Friday, August 04, 2006 7:13 PM To: Vinay Prabhu Cc: Quartz-Dev (E-mail) Subject: Re: PenAlignment(GDI+) alternative in Quartz There isn't an equivalent feature to this in Quartz.  For some basic paths you could probably find a way to do it fairly easily, but I suspect that to make it work with any arbitrary complex path you'd have to implement your own stroking engine. Nick</body>
  </mail>
  <mail>
    <header>Re: PenAlignment(GDI+) alternative in Quartz</header>
    <body>On Aug 4, 2006, at 7:30 AM, Vinay Prabhu wrote: There isn't an equivalent feature to this in Quartz.  For some basic paths you could probably find a way to do it fairly easily, but I suspect that to make it work with any arbitrary complex path you'd have to implement your own stroking engine. Nick</body>
  </mail>
  <mail>
    <header>PenAlignment(GDI+) alternative in Quartz</header>
    <body>Hi, I am having some trouble drawing the outer and inner outline. I have a path which I need to stroke so that, the stroke will draw  with center, outer and inner 3 different color's. The outline center can be obtained by simply calling the stoke method. But for inner outline the drawing should starting from the path and move inwards from the path line. For outer outline it should move outwards. These features can be seen in Photoshop. Using GDI+ this can be achieved by just setting the PenAlignment to center, outer or inner. How to achieve this in Quartz? Is there any alternatives for this feature in Quartz? I thought of a method, keep a copy of the original path. Then shrink or expand it and then add it to the original path. Now if we fill the path with odd even fill, the desired effect might be possible to achieve. Is this method proper? If yes, how to shrink or expand the path? Regards Vinay</body>
  </mail>
  <mail>
    <header>CGGLContextCreate appears not to release</header>
    <body>The following CGContextRelease does not appear to release memory associated the created CG GL context: cgGLContext = CGGLContextCreate( [theCurrentContext CGLContextObj], theSize, 0 I have run the above under MallocDebug and noted that my total allocation grew by ~80K each time the above was called. Commenting out the above code eliminates this growth. Cheers, -C</body>
  </mail>
  <mail>
    <header>Re: Quartz API Rendering Load</header>
    <body>I'd suggest that you investigate Apple's OpenGL Profiler.app and OpenGL Driver Monitor.app both of which are available in /Developer/ Applications/Graphics Tools/ (Driver Monitor's decoder ring is documented here -- ) Furthermore Apple's OpenGL Programming Guide for Mac OS X (http:// developer.apple.com/documentation/GraphicsImaging/Conceptual/OpenGL- MacProgGuide/index.html) contains useful information related to performance data gathering and analysis -- documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/ opengl_performance/chapter_13_section_3.html#//apple_ref/doc/uid/ TP40001987-CH213-SW14 Max Rupp OpenGL / Core Video Programming email@hidden</body>
  </mail>
  <mail>
    <header>Quartz API Rendering Load</header>
    <body>I am developing an application that uses OpenGL, Core Image and Quartz to do some live video processing.  What I am wondering is how I can determine the rendering load (which I am assuming is the GPU load), much like what Quartz Composer shows. My primary reason is I want to be able to work on optimization and it would be useful to know if my bottlenecks are due to hard disk, CPU or GPU.  But I also like being able to monitor so that if I see the rendering load spiking I can start turning off filters to ensure peak performance. Much appreciate it if anybody has any ideas on how to pull this off, Thanks, Daniel</body>
  </mail>
  <mail>
    <header>Re: Convert NSBezierPath to Quartz path</header>
    <body>Re: On Aug 2, 2006, at 9:09 AM, Scott Thompson wrote: I think what you meant to say is that you use Cocoa methods to clip to your NSBezierPath and then draw the CGShading.</body>
  </mail>
  <mail>
    <header>Re: Abstract images in Quartz</header>
    <body>Yes, but CGImageRef is fixed in pixel count and depth, so my data provider (however sophisticated it is) will not be able to provide image with better resolution when CGContextDrawImage operation is called with bigger rect. This particular aspect - the behavior of the image on draw operation (not how it is stored internally or where did it came from initially) is what actually matters, IMHO. [Compressed pixmaps and regions in QuickDraw are two examples of what I'd call &amp;quot;bitmapped&amp;quot; objects.] What type would you recommend for hypothetical internal API that assigns &amp;quot;abstract single-page image&amp;quot; to object? CGPDFDocumentRef? CGPDFPageRef? Maybe not. [However, windows are implemented as OGL textures - read patterns. Does anyone care? :-) ]. I thought that if I use pattern with step&amp;gt;=size, phase=0 and identity matrix it won't have a chance to tile. And the good thing it has drawing callback not found in other CG objects. IMHO Quartz would only benefit from having support for abstract image type. Something that would allow me to: - CreateWithDrawCallback - Draw itself The rest is optional but nice: - CreateFromFile (TIFF, PNG, PDF, PICT, etc) - CreateFromIcon - CreateFromCGImage - CreateFromQTImporterComponent Mike</body>
  </mail>
  <mail>
    <header>Re: Abstract images in Quartz</header>
    <body>On Aug 2, 2006, at 4:45 AM, Mike Kluev wrote: Well, no.  CGImages may be stored in a bitmap, or they may be stored in other ways. For example, a CGImage can draw from a block of compressed PNG data. You are accurate in that a CGImage represents a rectangular array of color samples from a particular data source (as well as some associated metadata).  However, it makes not guarantees as to how that those samples are stored (or generated).  You could, in theory, create a CGDataSource that generates the color samples of an image on- the-fly. It sounds like you are looking for a &amp;quot;retained mode&amp;quot; drawing API and Quartz 2D is not that kind of API. The metafile format for Quartz 2D is PDF.  That sounds closest to what you are looking for. A PDF can be comprised of an arbitrary combination of Images, Text, and Line Art and can be drawn on any CGContext you care to. Depending on your needs, another solution may be to use a CGLayer which is a cached set of Quartz 2D drawings that can be optimally reproduced on a given device.  So long as you are able to guarantee that you will always draw that layer on the given device, it can be a very helpful caching mechanism. I wouldn't.  CGPatterns have a very specific use (tiling an image into a context) and trying to use them for anything else is not likely to be what you want.</body>
  </mail>
  <mail>
    <header>Re: Convert NSBezierPath to Quartz path</header>
    <body>On Aug 2, 2006, at 7:46 AM, Vinay Prabhu wrote: I don't know of any way to do this, but it shouldn't be necessary. You should be able to set up the CGShading you want to draw with, obtain the CGContextRef from the current NSGraphicsContext and set up the shading into that context, then just use the [fill] method of the NSBezierPath.</body>
  </mail>
  <mail>
    <header>Re: Debugging CGContext</header>
    <body>Cool! I'll play with this. Thanks, Mike</body>
  </mail>
  <mail>
    <header>Re: Debugging CGContext</header>
    <body>On Aug 2, 2006, at 6:08 AM, Mike Kluev wrote: Assuming this is just for debugging purposes: otool -Tv /System/Library/Frameworks/ApplicationServices.framework/ Frameworks/CoreGraphics.framework/Versions/Current/CoreGraphics | grep _CGContextGet shows me some stuff I would consider playing with....but not for shippable code of course :) Ditto for otool -Tv /System/Library/Frameworks/ApplicationServices.framework/ Frameworks/CoreGraphics.framework/Versions/Current/CoreGraphics | grep State Cheers, Dave</body>
  </mail>
  <mail>
    <header>Re: Debugging CGContext</header>
    <body>But how?! See below. Mike</body>
  </mail>
  <mail>
    <header>Re: Debugging CGContext</header>
    <body>On Aug 2, 2006, at 8:09 AM, Mike Kluev wrote: None that I see other then writing a function of your own. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Debugging CGContext</header>
    <body>rdar://4664063 Meanwhile, is there a workaround? Mike</body>
  </mail>
  <mail>
    <header>Re: Debugging CGContext</header>
    <body>On Aug 2, 2006, at 2:31 AM, Mike Kluev wrote: File an enhancement request to have CGContext instances support CFShow with at least some level of debug information. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Debugging CGContext</header>
    <body>Looks like I'll have to implement such debugging facility manually. How to get CG context's blend mode? Or alpha? Or rendering intent? Or any other setting that has CGContextSetXXX but doesn't have the corresponding CGContextGetXXX counterpart? For debugging purposes, is it possible to view the stack of saved context states? Mike</body>
  </mail>
  <mail>
    <header>Convert NSBezierPath to Quartz path</header>
    <body>Hi, I am developing a application, where I need to convert NSBezierPath to Quartz path for filling the gradient color's. Any one can help me in this? Regards Vinay</body>
  </mail>
  <mail>
    <header>Re: &amp;lt;no subject&amp;gt;</header>
    <body>I suspected as much. Thank you. -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: &amp;lt;no subject&amp;gt;</header>
    <body>First of all, there are 2 different things. One typedef -- CGEventSourceStateID and another enum.  Type is unsigned, enum does not have to.</body>
  </mail>
  <mail>
    <header>&amp;lt;no subject&amp;gt;</header>
    <body>Please forgive my ignorance of the finer points of C. Is this declaration in CGEventTypes.h legitimate? -- kCGEventSourceStatePrivate = -1, kCGEventSourceStateCombinedSessionState = 0, kCGEventSourceStateHIDSystemState = 1 It is typed as an unsigned int, but one of the values is -1. -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Rectangle, Rotating and Clipping</header>
    <body>Hi There, First post - please be kind. :-) I program using REALbasic. I am writing classes that are declaring in to the CoreGraphic header files. For the most part things are moving along nicely. (The declares I write use the same names and prototypes as the header files.) Here is my problem. I draw a rectangle on a resizable window within a canvas. Collapsing my window left-right and up-down all is well, clipping is good within the boundaries of my canvas. Let me label the four edges of my rectangle one through four, clockwise. The horizontal top line is number one, right side vertical line is number two, etc. When I rotate my rectangle clockwise, say 45 degrees, then shrink my window from right to left, as soon as the window reaches the edge 1, edge 2 corner, edge 2 begins to shrink as if it is still unrotated. Can you give me some idea's how I might go about clipping this rectangle properly? Thank you. Thomas C.</body>
  </mail>
  <mail>
    <header>Re: How to embed a font in PDF</header>
    <body>Drill down into the XObject.  When we place a source PDF page into an output PDF document, we put it in a form (a type of XObject), for various arcane technical reasons.  In the XObject dictionary, you should find a form containing the contents of your source page with all of the fonts. Derek</body>
  </mail>
  <mail>
    <header>Re: How to embed a font in PDF</header>
    <body>SNIP So the font IS embedded in the file, but for some reason these Xerox printers do not see it when the PDF is sent directly to the printer or when printed from Acrobat 6. I sent the same file to a GMG rip which produced a perfect tiff. It also printed fine on a cheapo HP LaserJet 4250. But printing from Acrobat 6 and 7 (reader or full/pro version) ends up with the Xerox approximating the font. Acrobat reader 8 however printed the file perfectly on this Xerox. Is there anything I can switch around in my code to try producing a different PDF structure? I guess I'm tripping on bugs in the printer and these versions of Acrobat, but I'd like to see if they can be worked around. David.</body>
  </mail>
  <mail>
    <header>Re: How to embed a font in PDF</header>
    <body>Hmm, being new to programming and not familiar with PDF structure means I am probably making an ass of myself and your help is really appreciated. So... Inspecting the original and one of my new files with Voyeur, the structure of the original is something like this (page 4 in this case): /Pages /Count /Kids 3 /Contents /Type /Parent /Resources /ColorSpace /Font /F1 /F2 /F3 /Xobject /ProcSet /ExtGState /Properties ... And so on for each page. For my new file, the structure is: /Pages /Count /Kids 0 /Contents /Type /Parent /Resources /ProcSet /XObject /MediaBox Printing the original file from Adobe Acrobat 6 on 10.4.8 to a Xerox postscript printer works as expected, but printing the new file causes the printer to substitute Courier for all the text. (And now I've discovered that printing the new file from Preview _does_ work... the font prints as expected.) So that confuses me. I've put an example of a new file here: How will the font data appear in the new file? David.</body>
  </mail>
  <mail>
    <header>Problems parsing pdf with Form XObjects	(CGPDFContentStreamGetResource fails)</header>
    <body>I'm trying to parse some pdf files starting with the code from _Programming With Quartz_, but I'm having trouble with Form XObjects. If the XObject stream only has drawing commands, everything is fine. But if the XObject stream contains another Form XObject it doesn't work. On the recursive call to myOperator_Do, it finds the correct name for the secondary form but the line: Here's my modified code (most of the error checking removed for clarity): void myOperator_Do(CGPDFScannerRef s, void *info) CGPDFScannerPopName(s, &amp;amp;name) if(!xobject) CGPDFObjectGetValue(xobject, kCGPDFObjectTypeStream, &amp;amp;stream) CGPDFDictionaryGetName(dict, &amp;quot;Subtype&amp;quot;, &amp;amp;name) if(strcmp(name, &amp;quot;Form&amp;quot;) == 0) CGPDFContentStreamRef localContent = CGPDFScannerRef scanner = else // Images, etc. Form XObjects with Form XObjects are legal pdf and the file renders properly with a variety of programs. Any help much appreciated.</body>
  </mail>
  <mail>
    <header>Re: How to embed a font in PDF</header>
    <body>This should happen automatically.  Are you sure that the original fonts are not in the new PDF files?  They will not appear identically, of course, because we create our own font subsets, but the glyph data (the outlines, in particular) should be identical.</body>
  </mail>
  <mail>
    <header>How to embed a font in PDF</header>
    <body>Hello, My app reads accepts a PDF file and then writes individual PDF files for each page from the original, using the CGPDFDocument functions. This works well. My problem is that embedded fonts in the original file are not written to the new files. How can I embed fonts in my PDF files? I've looked through the CoreGraphics PDF references, and the newer PDF Kit reference and I don't see a way to write font resources back to a PDF file. There is the Voyeur sample application and the documentation for CGPDFObject, but these handle reading only (as far as I can tell). Pointers, suggestions much appreciated, David.</body>
  </mail>
  <mail>
    <header>'Above' tag does not work in Safari</header>
    <body>Hello, I was excited to see the 'ABOVE' tag as part of Quartz embed since nothing can go on top of a quicktime embed. Setting it to True does indeed prevent any html content appearing above it. However setting it to false simply makes it disappear on the web page. Please note I am trying to make the tag in a widget, but need it to work inside of Safari. So this shows up blank in safari : The documentation is here below figure 12 : Any ideas ? Amazing technology this quartz.</body>
  </mail>
  <mail>
    <header>Saving jpg file with embedded thumb</header>
    <body>I am processing an image using CIFilters. I can save the image with writeToFile, but the image is saved without a thumbnail. The second part of the code tries to save the image with an embedded thumbnail. The code copiles without error or warnings, but bails in GraphicsImportExportImageFile. I suspect the problem is that I am feeding the method a wrong presentation of the image. I have tried different approaches, but all fail at GraphicsImportExportImageFile. BTW, if GraphicsExportSetThumbnailEnabled is removed, the problem is still there. Thanks, James // crop image with vertical centre crop // more code here // convert CGImage to NSImage outputNSImage = [self CIImageToNSImage: outputCIImage usingRect: // convert into NSdata for saving imageProps = [NSDictionary dictionaryWithObject: [NSNumber numberWithFloat: [self jpgCompression]] forKey: imageData = [imageBitmapRep representationUsingType: NSJPEGFileType // Write to file. // code works to here - if writeToFile is enabled, it will save a JPG file without embedded thumb // I show the above code, to show how the image source file is created. // The code below aims to save the image as a jpg with an embedded thumb // Convert the picture handle into a PICT file (still in a handle) // by adding a 512-byte header to the start. err = OpenADefaultComponent( GraphicsImporterComponentType, kQTFileTypePicture, // create CFURL from path string CFURLRef cfURL = CFURLCreateWithFileSystemPath (NULL, (CFStringRef)pathToOutputFile, kCFURLHFSPathStyle, //Convert the CFURLRef to an FSRef //Convert the FSRef to an FSSpec err = FSGetCatalogInfo(&amp;amp;fsref, kFSCatInfoNone, NULL, NULL, &amp;amp;path, // specify thumbnail presence and size err =  GraphicsExportSetThumbnailEnabled (gi, TRUE,	// enable thumbnail 160.0,	// thumbnail width 120.0 );	// thumbnail height // save jpg with embedded thumb  // always bail here err = GraphicsImportExportImageFile(gi, kQTFileTypeJPEG, 0, &amp;amp;path, bail:</body>
  </mail>
  <mail>
    <header>Re: Weird CGDataConsumer error</header>
    <body>That error repeats, but what errors, if any, show just before that one appears the first time?  Maybe they will give us some hints. ____________________________________________________________________________________ Finding fabulous fares is fun. Let Yahoo! FareChase search your favorite travel sites to find flight and hotel bargains.</body>
  </mail>
  <mail>
    <header>Weird CGDataConsumer error</header>
    <body>I've got a user with an odd data consumer error. Note that all graphics code is written in Cocoa so this is happening underneath that. Anyway the error is (repeated a bunch of times):</body>
  </mail>
  <mail>
    <header>Re: How do Quartz event kCGEventSource... fields work?</header>
    <body>Ah, that's very clear, and helpful as I move forward with this project. Thank you. -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: How do Quartz event kCGEventSource... fields work?</header>
    <body>On Mar 25, 2007, at 11:10 AM, Bill Cheeseman wrote: An event originating with the hardware, or technically, with a device driver associated with hardware, will have a kCGEventSourceUnixProcessID of 0, and will have a kCGEventSourceUserID of root (UID 0) and kCGEventSourceGroupID  of wheel (GID 0).  There will normally be no kCGEventSourceUserData value, so that will return 0. An event produced by a user space process will  have kCGEventSourceUnixProcessID set to the process ID (a pid_t), and kCGEventSourceUserID and kCGEventSourceGroupID will reflect the process's user and group ID.  The process creating the event is free to set kCGEventSourceUserData to any value it likes.  This may be used, for example, for a program that acts as an event source to pass identifying or additional information along to a cooperating application or event tap process.</body>
  </mail>
  <mail>
    <header>Re: Caps lock in event taps doesn't send flags changed upon release</header>
    <body>On Jan 31, 2007, at 8:14 PM, Ben Gertzfield wrote: Could be.  I haven't designed any USB keyboards. NX_NUMERICPADMASK does not mean what you think it means.  It indicates the keystrokes originate from the numeric keypad area of a keyboard, and does not reflect the state of the Num Lock switch. Arrow keys are considered part of the numeric keypad area. This flag is present to allow applications to differentiate between key characters from the main keyboard matrix and ientical characters from the numeric keypad matrix, such as the number keys and math symbols.</body>
  </mail>
  <mail>
    <header>Re: Caps lock in event taps doesn't send flags changed upon release</header>
    <body>The Caps Lock, and on many keyboards, Num Lock, is electromechanically implemented as a toggling device.&amp;nbsp;&amp;nbsp;One press toggles it to a &amp;#39;down&amp;#39; state, and a second press toggles it to an</body>
  </mail>
  <mail>
    <header>Re: Caps lock in event taps doesn't send flags changed upon release</header>
    <body>On Jan 31, 2007, at 7:02 PM, Ben Gertzfield wrote: The Caps Lock, and on many keyboards, Num Lock, is electromechanically implemented as a toggling device.  One press toggles it to a 'down' state, and a second press toggles it to an 'up' state.  On many older keyboards (early IBM M series, for example) the Caps Lock used a mechanical toggle, locking the button in the down state until a second press released the mechanical lock and allowed the button to rise.   On newer keyboards the toggle behavior is implemented in the keyboard microcontroller. Event taps, as well as the modifier flags in Carbon events and NSEvents, are working correctly.  The hardware may not be behaving in the manner you were expecting.</body>
  </mail>
  <mail>
    <header>Re: Caps lock in event taps doesn't send flags changed upon release</header>
    <body>On 1/31/07, I wanted to email the list to check on this before I submit a bug. My application uses Event Taps to monitor and consume keyboard events at the session level.&amp;nbsp; For modifier keys like Caps Lock and Num Lock, we correctly get events of type kCGEventFlagsChanged.</body>
  </mail>
  <mail>
    <header>Re: Simple question about parameter sizes</header>
    <body>On Jan 31, 2007, at 3:13 PM, Rick Mann wrote: Normlly, a C enum is the same as an int (whatever size that is for that compiler). It can be different: enumerations-and-bit_002dfields-implementation.html but AFAIK Apple's ABI has enums being the same size as ints. There are language extensions (like gcc's &amp;quot;packed enums&amp;quot;) to make specific enums smaller, for use in structs and the like. (e.g., an enum with only 12 values can fit in a 'char'.) I don't think any of Apple's headers do that. (And probably it wouldn't affect function calls anyway, since function arguments tend to get promoted to int.)</body>
  </mail>
  <mail>
    <header>Simple question about parameter sizes</header>
    <body>Hi. I'm trying to call CG APIs from another language. So far it's working out fine, but I wanted to verify something. Some APIs use typedef'ed enums to declare parameters. When this stuff is built, is the compiler set to make all enums be 4-byte ints? In general, what's the size of these parameters (that CG is expecting)? kCGLineJoinMiter, kCGLineJoinRound, kCGLineJoinBevel TIA, -- Rick</body>
  </mail>
  <mail>
    <header>Re: QDPictRef to create PICTs?</header>
    <body>I know of a way which is simple and uses the (often underestimated) pasteBoard capabilities. I start with your -imageDataWithImporter: method: - (NSData *) imageDataWithImporter:(MovieImportComponent) inImporter NSData *imageData = [self TIFFRepresentation];  // use compression !!! // write the imageData to the pasteBoard // the pasteBoard now has 2 types: NSTIFFPboardType AND NSPICTPboardType // due to the docs for NSPictImageRep (see -PICTRepresentation) we create: pictData = [NSMutableData dataWithLength:512];  // 512 zero bytes [pictData append // for a test only (to see if it works): NSLog( @&amp;quot;class is %@\n&amp;quot;, [NSImageRep imageRepClassFor If you want to create a PICT image from an existing image file ( TIFF, jpg, PNG, .. ) you need not create TIFF data. This will do: (written in mail) - (NSData *) PICTDataFromFileAtPath:(NSString *)path [pb set forType:NSTIFFPboardType]; // works also for jpg, png, ... pictData = [NSMutableData dataWithLength:512];  // 512 zero bytes Heinrich -- Heinrich Giesen email@hidden</body>
  </mail>
  <mail>
    <header>Re: programatically exporting quartz composition to quicktime</header>
    <body>I'd recommend looking through the Archives of the QuickTime list. For example: Although on the QuickTime java list, outlines the what you need to do pretty well.  You'll have to make adjustments for the new mechanisms that let you avoid GWorlds, but by-in-large, I think the outline given in that link should work. IQ_InteractiveMovies/quicktimeandsmil/chapter_10_section_1.html to put together the slide show.  The down-side is that SMIL would require you to actually save the images to disk at some point.  The up-side is that you could write a simple XML description of the slide show in SMIL and let QuickTime do the heavy lifting of getting the media samples into a movie. Attachment:</body>
  </mail>
  <mail>
    <header>programatically exporting quartz composition to quicktime</header>
    <body>I'm working on a plug-in for Aperture that generates slideshows. I was able to use the Quartz Composer SlideShow example to figure out how to display the images that the user has selected full screen in the form of a slide show.  That works great. The next step is to be able to save what the QCRenderer and NSOpenGLContext are displaying on screen to disk in the form of a quicktime movie.  I've been looking for example code that demonstrates this but I haven't had much luck. I found this ADC article on how to turn a composition in to a quicktime movie QuartzComposer/qc_scr_qtmov/chapter_7_section_3.html but it focuses more on exercising functionality that Quartz Composer provides vs. going over what Quartz or Quicktime framework APIs can be invoked to do the same. that goes over how to render a Quartz composer composition offline and then save the frames to disk as separate compressed TIFF files. Seems to me my code needs to do the same but instead of saving the composition out as separate TIFF files, it needs to save it out as a .mov file instead. Apologies in the advance for the noob question - and I realize this may be more applicable to the quicktime mailing list - just thought I'd try here first.</body>
  </mail>
  <mail>
    <header>Re: QDPictRef to create PICTs?</header>
    <body>On Jan 30, 2007, at 7:06 PM, Wim Lewis wrote: Hmmm. ImageIO sounds promising, Thanks for the tip. Ken</body>
  </mail>
  <mail>
    <header>Re: QDPictRef to create PICTs?</header>
    <body>I think you can use the quicktime exporter API to get a PICT directly from a CGImageRef (along these lines: OpenADefaultComponent (GraphicsExporterComponentType, 'PICT', ...), GraphicsExportSetInputCGImage(), GraphicsExportSetOutputHandle(), GraphicsExportDoExport(), CloseComponent()). I'd expect that under 10.4+ you can use the ImageIO library (CGImageDestination*()) to produce a PICT by giving it the &amp;quot;com.apple.pict&amp;quot; UTI, but I haven't tried that. So in that case, you'd need to create a CGBitmapContext, draw into it (possibly wrapping it in an NSGraphicsContext if you want to use the AppKit-style APIs), then use CGBitmapContextCreateImage() to get the CGImageRef for export. Or possibly you can create the CGImage directly from the NSImage's bitmap data. I assume that calling lockFocus on an NSImage creates a CGBitmapContext under the hood.</body>
  </mail>
  <mail>
    <header>Re: QDPictRef to create PICTs?</header>
    <body>On Jan 30, 2007, at 6:05 PM, David Gelphman wrote: At runtime, the PICTs don't exist, I have to generate them from basic ad info such as width, height, client name, client number etc. Currently, I do the drawing into an NSImage using NSxxx draw commands , convert the image to a tiff and use a graphics converter to convert the tiff to a PICT. Is there a quicker way to convert an NSImage to a PICT than first generating a tiff? Unfortunately, when adding images to picture boxes, Quark 6.5 only accepts PictHandles. If I wanted to use, PDFs, I'd have to write them to a file and tell quark to import them. We're talking upwards of 50 ads or so for any one document and I suspect all that writing/ importing of PDFs would be far worse time-wise than my current method. P.S. I understand the need to modernize and all, but what would have been the harm in adding a method of NSImage that could convert to PICTs for those of us that still need them? Something like: Come to think of it, is there a hack I could use to just wrap a bitmap inside a PICT?</body>
  </mail>
  <mail>
    <header>Re: QDPictRef to create PICTs?</header>
    <body>One possibility is to use the CGImageSource APIs which can handle PICTs. When passed PICT data this API will create a CGImageRef that represents the bits for that picture. The reason the QDPict API is less performant is that it is converting the PICT data into Quartz line art, text, and image calls. You don't just get bits. The conversion of arbitrary pictures is time intensive and is performed on the fly each time you draw the QDPict. You can do this once and draw the QDPict into a Quartz PDF context to create PDF data that you can render much more quickly upon redraws than possible with a QDPict. Hope this helps, David</body>
  </mail>
  <mail>
    <header>Re: QDPictRef to create PICTs?</header>
    <body>On Jan 30, 2007, at 2:32 PM, Ken Tozier wrote: The short answer is no.  You can create a QDPictRef from a PicHandle, but you can't go back the other way.  Your best bet for performance is probably going to be to use QuickDraw. Nick</body>
  </mail>
  <mail>
    <header>QDPictRef to create PICTs?</header>
    <body>I'm writing an XTension for Quark 6.5 and one of the XTension's jobs is generating a placeholder image for ads. In an earlier incarnation of my XTension I used Quickdraw to generate the placeholders on the fly and used those PICTs directly. I found a kludgy way to generate picts in OS X but it's WAY slower than it was on OS 9. In OS 9, I could generate the ads for an entire publication and place them on a Quark document in about 3 seconds. With my kludge, it can take 15 to 20 seconds to just create a dozen or so placeholder images and generating the ads for an entire publication can easily take over a minute. Quark 6.5 only accepts pict handles when placing images directly in boxes so basically I'm wondering if there is some way to generate PICTs faster with QDPictRef? After reading the QDPictRef documentation I'm unclear on whether it's possible to create an image that, when returned, is a PICT. Below is my kludgy conversion method, a category on NSImage that uses a MovieImportComponent  to do the NSImage-&amp;gt; PICT conversion. Basically the workflow needs to be: Generate ad placeholder Convert to a PICT handle - (NSData *) imageDataWithImporter:(MovieImportComponent) inImporter NSData			*imageData		= nil, if (err == noErr) if (GraphicsImportGetAsPicture(inImporter, &amp;amp;picHandle) == noErr) result	= [[NSData alloc] initWithBytes: *picHandle length: else else if (picHandle != NULL) if (imageHandle != NULL)</body>
  </mail>
  <mail>
    <header>CoreImage filter package</header>
    <body>I have a standalone CoreImage filter, which takes a point (position) as one of its inputs. The filter works except that when I'm using Core Image Fun House, the description of the position parameter is Unfortunately it appears CoreImage is looking for a key that I haven't defined. On other filters, &amp;quot;(null)&amp;quot; is replaced with the name of the filter. I've defined CIAttributeFilterDisplayName in my Description.plist, but CoreImage doesn't appear to be looking for that in this case. Thanks, -andy</body>
  </mail>
  <mail>
    <header>Re: dynamically changing a color table</header>
    <body>On Jan 29, 2007, at 8:57 AM, Niels Bogaards wrote: A CGImageRef is an immutable object.  Quartz is allowed to optimize image handling based on the assumption this will never change. However it's not very expensive to make another image with the same backing data.  It means some extra overhead, but you can maintain your image data separately from your image and bitmap references and use that data repeatedly when you change color tables. Nick</body>
  </mail>
  <mail>
    <header>dynamically changing a color table</header>
    <body>I have an application that uses 8-bit bitmaps with a colortable. In QuickDraw, this colortable can be changed interactively, so I don't have to modify the bitmaps values to change their colors. I'm looking for a way to do this with Quartz, but it seems that the colortable can only be set upon creation of the bitmap or image. Am I missing something? As my app is in c++, it would be very inconvenient to have to resort to CoreImage to do this quite simple manipulation. Any ideas? Niels</body>
  </mail>
  <mail>
    <header>Re: Custom cursor icon</header>
    <body>On Jan 12, 2007, at 5:45 PM, Ricky Sharp wrote: The sample once again lives (just saw it in the what's new section of the developer main page).  Specific link: ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>video input record</header>
    <body>I'm having trouble searching the site so I hope i'm not going over a subject that's all ready been discussed, however my question is: Is it possible to set up a live video feed (i.e. the video input and billboard patch) that will also record or save direct to the harddrive when opened. I'm hoping to use the video input quicktime movie within macromedia, sorry, adobe director. Barret</body>
  </mail>
  <mail>
    <header>CoreImage gaussian blur</header>
    <body>I have different results on PPC and Intel with the CoreImage gaussian blur filter. The appended code works perfectly well on a PowerMac G5 with X800XT (10.4.8). However, on a MacPro with a 7300GT (10.4.8), on some images, the output image is &amp;quot;cropped&amp;quot;. I have a simple test case with a 1644x2544 image, and on the MacPro the output image is 1644x2050 only (actually I believe the missing 500 or so lines are alpha-masked). Is this a known problem? Is there a bug in the code below? I can email a testcase xcode project to anyone willing to help me. url   = [NSURL fileURLWithPath: [[NSBundle mainBundle] // Create a new NSBitmapImageRep. fondsImageRep = [[NSBitmapImageRep alloc] initWithBitmapDataPlanes:NULL pixelsWide:newSize.width pixelsHigh:newSize.height bitsPerSample:8 samplesPerPixel:4 hasAlpha:YES isPlanar:NO colorSpaceName:NSDeviceRGBColorSpace // Create an NSGraphicsContext that draws into the NSBitmapImageRep. NSGraphicsContext *nsContext = [NSGraphicsContext [[nsContext CIContext] drawImage:fondsImage atPoint:imageRect.origin</body>
  </mail>
  <mail>
    <header>Disable interpolation on a CIFilter</header>
    <body>I'm using the CIAffineTile and CICrop filters to tile a small pattern to a source image size, and then use CIMultiplyCompositing with the resulting image and a source image.  The problem is that the resulting tiled and cropped image has artifacts from the bilinear interpolation.  It seems that CISampler has a way to turn off the interpolation by using kCISamplerFilterNearest, but I cannot figure out how to use that with an existing CIFilter.  The filters create their own sampler, right?  However, it seems that it is possible, as Quartz Composer allows you to turn off interpolation on standard CIFilters in the &amp;quot;Settings&amp;quot; pane. Can anyone provide pointers to disabling bilinear interpolation on a CIFilter?</body>
  </mail>
  <mail>
    <header>Re: Runloop source for CGDisplayRegisterReconfigurationCallback?</header>
    <body>Hi Spencer, I'm pretty sure that's what's happening with this API, too. Unfortunately, I can't run a CFRunLoop in this thread, as it already has its own runloop.  (And as anyone who's ever tried to run two runloops in a single thread can tell you, it's always a mistake to try to run two runloops in a thread.) Anyway, I'll probably just spawn a separate thread whose sole purpose in life is to call CGDisplayRegisterReconfigurationCallback() and run a CFRunLoop waiting for the response. It's too bad that there are so many APIs that assume the thread that calls into them is running a CFRunLoop.  I wish every API that involved callbacks (Cocoa excepted, naturally) would provide a CFRunLoopSourceRef or CFMachPortRef. Yeah.  That's my main problem with this and other similar asynchronous APIs that Apple provides; they really mean &amp;quot;the thread that calls this API, assuming that thread also uses a CFRunLoop&amp;quot;. Ben</body>
  </mail>
  <mail>
    <header>Re: Runloop source for CGDisplayRegisterReconfigurationCallback?</header>
    <body>Hi Ben, This is mostly a shot in the dark, but Ive noticed that in one of Apple's other C APIs (IOBluetooth specifically), if the API didn't supply a CFRunLoopSourceRef or CFMachPortRef to work with, the callback would be called if I let the run loop run in the kCFRunLoopDefaultMode mode in the same thread.  So, in other words, the source associated with the callback was added to the run loop of the current thread in kCFRunLoopDefaultMode.  So it might be worth testing if this is the case for CGDisplayRegisterReconfigurationCallback().  If this is in fact the case, it would be sufficient to call CFRunLoopRun() in whichever thread registered the callback to have your callback called when appropriate. The header declaring that function states: &amp;quot;Callbacks are invoked when the app is listening for events, on the event processing thread, or from within the display reconfiguration function when in the program that is driving the reconfiguration.&amp;quot; (CGDisplayConfiguration.h) Unfortunately &amp;quot;event processing thread&amp;quot; isn't a well-defined notion in Mac OS X development, so far as I am aware. In general I agree that it would be nice to just get a CFRunLoopSourceRef or CFMachPortRef.  s hope this helps, spencer</body>
  </mail>
  <mail>
    <header>Re: Runloop source for CGDisplayRegisterReconfigurationCallback?</header>
    <body>Hmm.  Well, in a Cocoa application with a standard AppKit/Core Foundation runloop on the thread from which I call CGDisplayRegisterReconfigurationCallback(), my callback works when reconfiguring a display. In a standalone process with no AppKit/Core Foundation runloop running on the thread from which I call CGDisplayRegisterReconfigurationCallback(), I don't get any callbacks when reconfiguring a display. The standalone process definitely has a connection to the window server, because I can call other Core Graphics calls like CGWarpMouseCursorPosition(). Ben</body>
  </mail>
  <mail>
    <header>entry level developer needed</header>
    <body>I'm looking for an entry level developer to write some Core Image and OpenGL imaging plug-ins in Objective-C. If you're reading this list and have experience in Objective-C, Core Image, and OpenGL and want to do some programming, please contact me. You should have some Objective-C and Cocoa development experience, understand 3D matrix transformations, know a bit of Open GL (i.e. do you know what glMatrixMode(GL_MODELVIEW) means?), how to build Core Image objects, and how to use Xcode. This is a great opportunity to work on a fun project for an entry level programmer. If interested, please send me some examples of projects or details of projects that you've done that would be relevant to the job. If you've taken a graphics programming course at a university, you probably have enough experience and knowledge!</body>
  </mail>
  <mail>
    <header>Re: Runloop source for CGDisplayRegisterReconfigurationCallback?</header>
    <body>That's right.  Sorry for not being clear.  I have one thread whose sole purpose in life is to wait on a set of Mach ports retrieved from CFRunLoopSourceRefs and CFMachPortRefs and dispatch the callbacks from those ports to our pre-existing runloop mechanism. It's possible that this particular Core Grapics API actually spawns a thread which dispatches the callback.  I'm reading the disassembly in CoreGraphics now to see what CGDisplayRegisterReconfigurationCallback() does, and it doesn't deal with CFRunLoops at all, so I'm going to do some more experimentation.  As far as I know, there's no concept of a &amp;quot;main&amp;quot; thread in Core Foundation or Quartz outside of the documentation.  There's definitely no API to add a Mach port or callback to a particular thread's runloop, only the current thread's runloop. Our application definitely doesn't have a &amp;quot;main&amp;quot; thread runloop that is compatible with Core Foundation, but our approach works great with other Core Foundation and I/O Kit APIs that give us Mach ports or CFRunLoopSourceRefs. Ben</body>
  </mail>
  <mail>
    <header>Re: Runloop source for CGDisplayRegisterReconfigurationCallback?</header>
    <body>Every thread can have a run-loop and it will be created automatically as needed... I guess you mean that the thread doesn't actually run the run-loop. I have found that these callbacks (or related &amp;quot;wait&amp;quot; methods) depend on the main thread running a run-loop and of course they require a connection with the window server. -Shawn</body>
  </mail>
  <mail>
    <header>Runloop source for CGDisplayRegisterReconfigurationCallback?</header>
    <body>I'm working on a cross-platform application that has its own runloop, to which I dispatch events via a CFRunLoopSourceRef bridge I've developed. In this application, I'd like to be notified whenever the local displays are reconfigured. I found CGDisplayRegisterReconfigurationCallback(), which unlike many other Core Foundation and I/O Kit functions, does not provide a CFRunLoopSourceRef which I can add to my own runloop. Since I have my own runloop, I can't just call CGDisplayRegisterReconfigurationCallback(),  as I assume it will fail if the thread that calls this function has no runloop. How can I get a CFRunLoopSourceRef or, even better, a Mach port on which I can receive display reconfiguration notification? Ben</body>
  </mail>
  <mail>
    <header>Re: Quartz and daemons</header>
    <body>If you need to call only into CoreGraphics then you should be fine, but if you need AppKit or Carbon windowing functionality, there is strong possibility they will try and attach to the window server and you will run into problems. So it depends on what graphics functionality you need from Quartz and what components of quartz you require.</body>
  </mail>
  <mail>
    <header>Re: Quartz and daemons</header>
    <body>On Jan 13, 2007, at 4:48 PM, Mike wrote: Aside from using a non-Quartz framework (such as those used with web- scripting languages), you can get around the daemon limitations by not using a daemon. By running a process as the currently logged in user, you will be able to connect to the window server, which will satisfy Quartz's needs. HTH, Jon -- Jonathan Johnson email@hidden REAL Software, Inc.</body>
  </mail>
  <mail>
    <header>Quartz and daemons</header>
    <body>So if one wants to write a faceless background app with no UI that uses Quartz, how does one go about doing that? The Daemons &amp;amp; Agents technote says that Quartz is not a daemon-friendly framework. I need to write an app that has no UI and runs silently and stealthily in the background without the user's knowledge. This app has to be able to perform some graphics functionality using Quartz. Is there an alternate way to do this without using a daemon to do so that will fullfill the above requirements? Mike</body>
  </mail>
  <mail>
    <header>Re: Custom cursor icon</header>
    <body>That's the correct name.  It was actually mistakenly posted the first time. I have to make some changes to the sample that were requested by DTS. I'm not sure who to give feedback on this, but I'd suggest writing up bugs for the others if there's no other feedback path.</body>
  </mail>
  <mail>
    <header>Re: Custom cursor icon</header>
    <body>On Jan 11, 2007, at 6:36 PM, Chilton Webb wrote: I think it's the CarbonCocoa_PictureCursor sample as referenced on: But alas, that sample appears to not exist anymore.  It was just posted in July, 2006, so it's not that old.  Some of the other sample links on that page are also broken. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: Custom cursor icon</header>
    <body>I've been looking for this exact thing. Which example is it? -Chilton</body>
  </mail>
  <mail>
    <header>Re: Custom cursor icon</header>
    <body>On Jan 11, 2007, at 3:35 PM, Luigi Castelli wrote: Quartz does not support custom cursors, but AppKit does, and we recommend the use of NSCursor. There's sample code on developer.apple.com showing how to use NSCursor from a Carbon app.</body>
  </mail>
  <mail>
    <header>Custom cursor icon</header>
    <body>Hi there, I am developing on MacOS 10.4 in Carbon. I am porting all my Quickdraw code to Quartz and I am stuck on a small problem. In Quickdraw I used to include a CURS resource in my project, draw pixel by pixel the cursor icon with a resource editor (i.e. resEdit) and I was able to switch to that custom cursor pretty easily. Is there anything like it available in Quartz ? I basically need to change the standard arrow cursor to some custom types. From the Quartz documentation on line I see that there are indeed some predefined types I can use with the SetThemeCursor() call. It works well but I really do need a custom icon in this case. Is there anyway to do it in Quartz ? Any solution or tip highly appreciated. Thank you. - Luigi ------------------------------------------------------------ THIS E-MAIL MESSAGE IS FOR THE SOLE USE OF THE INTENDED RECIPIENT AND MAY CONTAIN CONFIDENTIAL AND/OR PRIVILEGED INFORMATION.  ANY UNAUTHORIZED REVIEW, USE, DISCLOSURE OR DISTRIBUTION IS PROHIBITED.  IF YOU ARE NOT THE INTENDED RECIPIENT, CONTACT THE SENDER BY E-MAIL AT email@hidden AND DESTROY ALL COPIES OF THE ORIGINAL MESSAGE. WITHOUT PREJUDICE UCC 1-207. ------------------------------------------------------------ ____________________________________________________________________________________ Do you Yahoo!? Everyone is raving about the all-new Yahoo! Mail beta.</body>
  </mail>
  <mail>
    <header>Re: Control Transparency of CIImage in OpenGL Context</header>
    <body>Would be very interesting to see benchmarks comparing the two techniques. Anyone have the expertise required to test it?</body>
  </mail>
  <mail>
    <header>Re: Control Transparency of CIImage in OpenGL Context</header>
    <body>kernel vec4 makeTransparent(sampler inputImage, float inputAlpha) But I guess that's not really simpler than using the color matrix filter. Cheers, Martin Norrkross Software</body>
  </mail>
  <mail>
    <header>Re: Adding A Virtual Display</header>
    <body>I&amp;#39;m not sure if this is exactly a Quartz related problem, but Quartz seems to be the closest thing to it. I&amp;#39;m trying to figure out how to add a &amp;quot;virtual&amp;quot; display to a I mean that I want my software to be able to make the system think it has an external monitor and then process the video image destined for that monitor to do something useful.&amp;nbsp;&amp;nbsp;For example, I could encode the image and transmit it to a remote location for display there.</body>
  </mail>
  <mail>
    <header>Control Transparency of CIImage in OpenGL Context</header>
    <body>I've been struggling to find a simple way to control the overall transparency of a CIImage when rendering it to a OpenGL Context. I am using the CIContext drawImage function to draw the CIImage. I've tried messing around with OpenGL stuff (like using setting GL_TEXTURE_ENV_MODE to GL_BLEND and setting the GL_TEXTURE_ENV_COLOR) but to no avail. I suppose I can use a Color Matrix filter to adjust the overall alpha, but I was hoping for a simpler/faster way to do this. Obviously the &amp;quot;Billboard&amp;quot; module of Quartz Composer can handle this -- adding in rotations, colorization, and transparency. Is that because it is converting the CIImage to an Open GL texture? If so, what is the fastest way to handle that conversion. (There's been various suggestions posted here and on the net, but no definitive solution.) Thanks in advance, Mark -- ======================================================================= Mark Coniglio             |  email@hidden TroikaTronix              | Isadora¬Æ                  | |  (interactive media processing environment) =======================================================================</body>
  </mail>
  <mail>
    <header>Adding A Virtual Display</header>
    <body>I'm not sure if this is exactly a Quartz related problem, but Quartz seems to be the closest thing to it. I'm trying to figure out how to add a &amp;quot;virtual&amp;quot; display to a I mean that I want my software to be able to make the system think it has an external monitor and then process the video image destined for that monitor to do something useful.  For example, I could encode the image and transmit it to a remote location for display there. To me this implies I want to write a custom display driver, but I can't find any information on this, other than a little bit for writing a window server for IOKit in Darwin (but not Mac OS X).  I think that what I want to do is hook into the system under the Mac OS X window server to let it think I'm an external monitor.  Is there any documentation available publicly on writing display drivers? Apart from this approach I suppose I could write some variant of a frame grabber that would take a screen snapshot every few milliseconds and use that.  The problem with this is that I'd really prefer to have the option to either extend the desktop or mirror it. A screen grabber would seem to be only able to mirror an existing display. Can anybody suggest a good approach for this and/or point me at any documentation? Thanks, Nick Nallick</body>
  </mail>
  <mail>
    <header>Re: Custom image formats</header>
    <body>On 03.01.2007, at 09:51, Rene Rebe wrote: If the only use case is TIFF (and not GIF, PSD and PDF which also support multiple subimages), then not necessary. your app will likely already multiple times write and read the images - uncompressed due to VM swapping and finally once compressed. Stitching together an TIFF container with multiple subimages is fairly trivial.  I'm not sure too many implementations will be happy with &amp;gt; 2 GB TIFF images (32 bit tag offsets, potentially interpreted as &amp;quot;signed&amp;quot;...), though. Regards, Tom_E</body>
  </mail>
  <mail>
    <header>Re: Custom image formats</header>
    <body>Hi, Which has two major limitations: - bad performance (writing data twice) - might get out of disk space creating the final file while buffering many high res pages in the GB range Yours, -- Ren√© Rebe - ExactCODE GmbH - Europe, Germany, Berlin +49 (0)30 / 255 897 45</body>
  </mail>
  <mail>
    <header>Re: Custom image formats</header>
    <body>Well, a simple work around I can think of for this is to simply collect all the images before you ask ImageIO to write the file. Have you filed a bug about it? Can you send me the radar number? -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>ImageIO Exif GPS</header>
    <body>doing a little research on the web and my own prototyping is seems as though you cannot write GPS Exif data using ImageIO. Is this true?</body>
  </mail>
  <mail>
    <header>Re: applying a filter recursively?</header>
    <body>Instead of drawing the image, you would directly pass it to the blurFilter and then draw the final result.</body>
  </mail>
  <mail>
    <header>Re: CGCaptureDisplay() / CGDisplayHideCursor() on auxiliary	displays messing with main display</header>
    <body>On Sep 9, 2006, at 4:39 AM, nick briggs wrote: That's a deliberate feature of display capture.  Most hot keys, including Command-Tab are disabled.  Many full screen applications, including game ports, have a large set of predefined and documented key combinations that otherwise conflict with system-defined hot keys.  In addition, switching applications while displays are captured can lead to an unusable system. If your app wants to capture displays, while supporting Command-Tab, the app should be coded to catch Command-Tab, release all displays, and then resign foreground, effectively switching to another app. When your app becomes foreground again, recapture the displays. That API is a little misleading.  There is a single global cursor and hide/show count, and calling CGDisplayHideCursor() hides the cursor on all displays. Mike Paquette email@hidden</body>
  </mail>
  <mail>
    <header>applying a filter recursively?</header>
    <body>i'm pretty new to core image, so bear with me. i want to apply a blur filter to a CIImage, then apply the filter again to the resulting image and so on. i noticed that doing something like is an especially bad idea because the intermediate steps are rendered again and again, so in the nth loop the blur filter is computed n times. whoops! what would be the correct way to achieve what i want? should i use an CIImageAccumulator and copy the results into it after each filter operation? i would like to get a fluent animation in real time, so i would like to execute this at least 10 times a second, so i would prefer to re-use the computed pixel data instead of copying it somewhere. is that possible? regards, sebastian mecklenburg</body>
  </mail>
  <mail>
    <header>CGImageDestinationCreate* and image count</header>
    <body>I have a question regarding the count specified to the CGImageDesintationCreate* methods: count The number of images (not including thumbnail images) that the image file will contain. I get the images from a device and thus do not know beforehand how many image I'll get. Maybe there is something supported like passing 0 or -1 to indicate the count is not known yet? The images I deal with are usually pretty large and thus forbid storing them into memory of disk to count them before the go into the final file. -- Ren√© Rebe - ExactCODE - Berlin (Europe / Germany) +49 (0)30 / 255 897 45</body>
  </mail>
  <mail>
    <header>CGCaptureDisplay() / CGDisplayHideCursor() on auxiliary displays	messing with main display</header>
    <body>We're calling CGCaptureDisplay() / CGDisplayHideCursor() on 3 auxiliary displays attached to our system and experiencing problems with the main display: 1) The Command-tab feature breaks, that is to say while an auxiliary display is captured you can no longer command-tab between applications. As soon as the auxiliary display is released with CGDisplayRelease() then Command-tab works again. 2) Calling  CGDisplayHideCursor() on any auxiliary display hides the cursor on all displays. CGDisplayErr err = CGGetActiveDisplayList(kMaxDisplays, _displays, ..... for(i=1; i&amp;lt;_numDisplays; i++)  /* i=1 skipping main display */ /* never happens */ if(_displaysCaptured ) else</body>
  </mail>
  <mail>
    <header>Re: Composite PDF pages transparently?</header>
    <body>On Sep 7, 2006, at 7:09 AM, Matthew Chartier wrote: CGContextDrawPDFPage doesn't draw a background so in effect, everything you draw with it is transparent.  If you're seeing a background it's probably because the PDF image itself contains a solid (e.g., white) rectangle behind its other content.  There's no easy way to remove this, apart from deconstructing the PDF and taking it out. Nick</body>
  </mail>
  <mail>
    <header>CoreVideo and YUV420</header>
    <body>CoreVideo provides great support for YUV422 color space data but it doesn't seem to provide any support for YUV420 (the dominant MPEG-1/2/4 distribution format). Is that correct ? Or have I missed something ? Certainly I can up-convert YUV420 to YUV422 before display but I was wondering if there is any way I can offload that work to the graphics card ? A custom CoreImage filter perhaps ?</body>
  </mail>
  <mail>
    <header>CGPath thread safe?</header>
    <body>Can CGPath be used in a different thread if all the neccessary locks and whatnot are used? The header and docs don't say anything. TIA Mark</body>
  </mail>
  <mail>
    <header>Clipping with NSOpenGLView</header>
    <body>Hi all, In my vector drawing app, i want to use clipping to exclude some regions when drawing 2 Bezier paths. I use apple's method as described here: (Setting the Clipping Region). Each one of my paths subtracts the other path from his clipping region (i want to draw the difference) : Here is my code /**************************************************************** [NSGraphicsContext saveGraphicsState];¬† [clipPath	appendBezierPath:[[self graphicForClipping] // Add the path to the clip shape. // Draw the image (( Add my graphic storked CGPathRef to the context )) ****************************************************************/ While it all works great when the view I draw into is a standard NSView, it fails when I draw in a CGLayer, create a CIImage from it and composite it in an NSOpenGLView (clipping doesn't work in my layer). My CIImage based code uses some standard drawing code from FunHouse (draw in a layer, create a CIImage and composite it with the background CIImage). I've tried CGContextClip and CGContextEOClip, it doesn't works like I want. Is clipping at all supported when drawing using such techniques. What am I doing wrong ? Any help would be much appreciated. Christophe p2.vert.ukl.yahoo.com uncompressed/chunked Mon Sep  4 12:13:57 GMT 2006 ___________________________________________________________________________ D√©couvrez un nouveau moyen de poser toutes vos questions quelque soit le sujet ! Yahoo! Questions/R√©ponses pour partager vos connaissances, vos opinions et vos exp√©riences.</body>
  </mail>
  <mail>
    <header>Quartz Event Services CGEventCreate Hanging?</header>
    <body>I think I've misunderstood something basic.  I'm trying to create a HID event for injection into the window system.  Any idea why CGEventCreate hangs in the following simple example? Thanks in advance, Mark Rahner // Create a reference to event source associated with HID system. eventSource = CGEventSourceCreate // Create a reference to an event.  Want to pass eventSource as // an argument but that causes app to hang.  Running as root // doesn't help so I don't think it's a permission related problem.</body>
  </mail>
  <mail>
    <header>Re: glGrab / CGLCreateContext leak?</header>
    <body>I have now followed your approach, which indeed leads to less allocations. While I still do not fully understand the allocation behavior i was seeing and whether it was a real issue or not, this does seem more efficient and gives piece of mind :-). This may not be the right list, but can anyone here by accident explain to me what Real Memory in Activity monitor really means. Is it actually used memory, or is it the total allocated space (including unused blocks that were released but not reused because of memory fragmentation)? At some point i see my real memory jump from 13 MB to 98 MB without me doing anything special in the app, not does MallocDebug report any major allocations of this size... david.</body>
  </mail>
  <mail>
    <header>Re: glGrab / CGLCreateContext leak?</header>
    <body>On Sep 1, 2006, at 2:37 PM, David Niemeijer wrote: I am not seeing leaks in code that I have setup for this type of task (similar to the glGrab sample code). Of course I only create contexts once at the beginning for each screen I will be capturing from and simply make the context I need the current context when I go to capture. I have let test applications run for days without issue. -Shawn</body>
  </mail>
  <mail>
    <header>Re: glGrab / CGLCreateContext leak?</header>
    <body>I have something similar where i grab the screen.  Here is the content from my radar bug -- maybe it will be helpful: 05-Apr-2006 04:28 PM Mike Schrag: Every call to setFullscreen on NSOpenGLContext leaks a small amount of memory.  You can see this by running Apple's NSOpenGL Fullscreen example and repeatedly go fullscreen and back.  The leaked memory leaks past the release and dealloc of the context.  This happened on 10.4.5 as well, so it's not specifically a 10.4.6 problem. 07-Apr-2006 08:55 PM Mike Schrag: 1) I should have attached the system profile when I logged -- sorry about that.  Attaching now. 2) Initially it was just watching the real size in activity monitor/ top grow.  I was calling setFullscreen and clearing/disposing many times a second and it just continued to grow.  I later ran the app through Shark, but I've read that Shark can give bogus information when you attempt to track down OpenGL issues because of symbol problems.  What appeared to be the leak was coming from (I need to look it up again, but I have to roll back to an older version of code to get it) a GetString method that was chained WAY deep in a stack trace in the ATI GL driver.  Basically, I'm not sure the best way to track the problem accurately -- I'd be happy to try whatever would be helpful to you.  What I DO know is that when I removed the setFullscreen call, memory stayed constant.  When I put it back in, memory increased without bound (albeit slowly) 3) I just tried this again with the NSOpenGL Fullscreen sample code on the Apple dev site.  It's a little hard to nail down in this app, because it's actually always increasing memory normally, but if you quickly flip back and forth between fullscreen and not (position your mouse over the button and click ESC click ESC click ESC like a crazy person :) ), I ran memory up by about a meg in 10-15 seconds.  My particular case where I ran into this issue was flipping back and forth to fullscreen mode 10-15 times a second, which was kind of ugly.</body>
  </mail>
  <mail>
    <header>glGrab / CGLCreateContext leak?</header>
    <body>I have a situation that is strange to me. I use the glGrab source that was discussed here in the past. It calls CGLCreateContext then does most of its work and finally calls CGLDestroyContext. So all seems ok, but when I look in Activity Monitor my Real Memory seems to grow and grow each time the grab takes place. So I ran with MallocDebug and found no leaks, but if I look at Allocations from mark i see that most of the growth is coming from CGLCreateContext. Now I am not an expert on these tools and unix memory management. but is the growth of real memory (starting at 20 MB and in some time grown to 150 MB) a reason for concern, or is this just inefficient allocation and will not cause infinite growth? Is there something wrong? an issue with the glGrab code? david.</body>
  </mail>
  <mail>
    <header>Re: Core Image leaking when using a Core Video OpenGL texture</header>
    <body>Okay, I think I solved my problem, though I'm not exactly sure why. Lemme postulate out loud here.  I think I was passing the wrong CGLContextObj to CVOpenGLTextureCacheCreate(). I have two contexts, let's call them SCREEN and PBUFFER.  I use SCREEN to draw the CIImage to the actual screen in the CV callback. And I use PBUFFER in the main thread to render the scene.  The problem was I used PBUFFER when I created the texture cache.  I believe the texture isn't actually created until -[CIImage drawRect:] is called.  Of course SCREEN is the current context by the the CIImage is rendered, which means the texture cache was using the wrong context.  Thus I needed to use SCREEN instead of PBUFFER, when I created the texture cache.</body>
  </mail>
  <mail>
    <header>Re: Core Image leaking when using a Core Video OpenGL texture</header>
    <body>Okay, it could be the buffer.  I am using a CVOpenGLBufferPool, as well.  As long as we're on the topic, how does one return a buffer created using CVOpenGLBufferPoolCreateOpenGLBuffer() back to the pool?  Does CVOpenGLBufferRelease() put it back in the pool, or does it actually release it? Also, what is the life cycle of an CVOpenGLBuffer?  I create it, draw to it, then create a texture from it.  Finally I draw the texture onto the main screen.  At what point can I release the buffer?  Do I have to wait until *after* I draw the texture?</body>
  </mail>
  <mail>
    <header>Re: Core Image leaking when using a Core Video OpenGL texture</header>
    <body>I have been running into this same problem (see the thread &amp;quot;Quicktime/ CoreImage memory leak&amp;quot;). As best I can tell, it is caused by associating the CV/quicktime buffer with one openGL context and then trying to draw it into another openGL context.  And it is exactly as you have said, if you don't actually draw the image, it is fine, but if you draw the image it leaks (though in my case MallocDebug does not show it as an actual leak, just allocated memory that has not been freed). I have tried various things, even creating a raw memory buffer that I draw the image into but that still leaks too. I still have not found a solution, but my next attempt will be to replace the image creation code.  Get rid of the CV stuff and simply have Quicktime render into an offscreen buffer and then create a CIImage directly from that offscreen buffer (see the quicktime ASCII movie player) and see if that solves the memory leak.  I am sure it will be slower, but at this point a program that is slower but works is better for me than one that crashes after about an hour because it hits the 2GB memory limit.</body>
  </mail>
  <mail>
    <header>Re: Core Image leaking when using a Core Video OpenGL texture</header>
    <body>On Sep 1, 2006, at 11:14 AM, Dave Dribin wrote: That is a housekeeping call you can call anytime. Most likely you are retaining the buffer or the image. If you can see that you really release everything and it still leaks, file a radar bug with a little sample app that demonstrates the problem.</body>
  </mail>
  <mail>
    <header>Re: Core Image leaking when using a Core Video OpenGL texture</header>
    <body>Yeah, I've got an autorelease pool.  This is my CV callback: I don't think it's the CIImage itself that is leaking.  From the stack trace at the end, it looks like it has something to do with a texture being allocated during drawing: CVOpenGLBufferTextureBacking::initWithOpenGLBackingContext() CVOpenGLBufferBacking::createTextureBackingForContext() CVImageBacking::provideCachedImageTextureForContext() I *am* using a CVOpenGLTextureCache to create frameTexture.  Maybe I'm not using the cache correctly?  One part that confused me was when to call CVOpenGLTextureCacheFlush().  The documentation just says &amp;quot;periodically&amp;quot;, so I do it in every CV display link callback. Is that too often?  Should I do it in the main thread, instead of the display link thread?</body>
  </mail>
  <mail>
    <header>Re: Core Image leaking when using a Core Video OpenGL texture</header>
    <body>Are you releasing the CIImage? In your case make sure you wrap an Autorelease pool around your render callback otherwise you accumulate images.</body>
  </mail>
  <mail>
    <header>Core Image leaking when using a Core Video OpenGL texture</header>
    <body>According to MallocDebug, I'm getting a memory leak when trying to draw a CIImage (one stack dump pasted at the end of the mail).  If I comment out the call to drawImage:, there is no leak, so it's definitely drawImage: that triggers it.  I'm assuming this is a leak in my own code in how I use Core Image/Core Video, but I'm stumped. I'm doing the fairly standard procedure in the CV display link: CVOpenGLTextureRef frameTexture = //.... CIImage * inputImage = [CIImage imageWithCVImageBuffer: [ciContext drawImage: inputImage atPoint: CGPointMake(0, 0) Now, it is my own code that is generating the frameTexture object. I'm trying to use a CV buffer pool and texture cache, and I could very well be doing it incorrectly.  I can provide more detail, if necessary. STACK Address /041fc010/size/00000b30 0x9b14f468  / MDmalloc / libMallocDebug.A.dylib 0x01789590  / 0x1789590 / (null) 0x04395e3c  / 0x4395e3c / (null) 0x043a7aa8  / 0x43a7aa8 / (null) 0x043a7a0c  / 0x43a7a0c / (null) 0x9429ceec  / CVOpenGLBufferTextureBacking::initWithOpenGLBackingContext (CVOpenGLBufferBacking*, CVOpenGLContext*, int*) / QuartzCore 0x9429cd6c  / CVOpenGLBufferBacking::createTextureBackingForContext (__CFAllocator const*, CVOpenGLContext*, __CFDictionary const*, int*) / QuartzCore 0x942ca4bc  / CVImageBacking::provideCachedImageTextureForContext (__CFAllocator const*, _CGLContextObject*, __CFDictionary const*, int*, __CFDictionary const*) / QuartzCore 0x942cd0dc  / CVTextureBacking::provideImageTexture (_CGLContextObject*, long, long, _CGLPixelFormatObject*, unsigned long, CGRect*, __CFDictionary const*) / QuartzCore 0x942e91e4  / provide_texture / QuartzCore 0x942c2b64  / fe_gl_texture_load / QuartzCore 0x942e906c  / fe_context_texture_load / QuartzCore 0x942e8da8  / image_buffer_texture_ref / QuartzCore 0x942e8ccc  / fe_image_texture_ref / QuartzCore 0x9434ea88  / texture_retain / QuartzCore 0x942e89bc  / fe_texture_new / QuartzCore 0x942e8670  / fe_tree_create_texture / QuartzCore 0x94304fa0  / fe_tree_render_unary / QuartzCore 0x942e7994  / fe_tree_render / QuartzCore 0x94354310  / fe_tree_render_image_ / QuartzCore 0x942e72f0  / fe_tree_render_image / QuartzCore 0x942e719c  / fe_image_render_ / QuartzCore 0x942e70d8  / fe_image_render / QuartzCore 0x942e7024  / -[CIOpenGLContextImpl renderAccel:matrix:bounds:] / QuartzCore 0x942e6220  / -[CIOpenGLContextImpl render:] / QuartzCore 0x942fbeec  / -[CIContext drawImage:inRect:fromRect:] / QuartzCore 0x942fe5f4  / -[CIContext drawImage:atPoint:fromRect:] / QuartzCore 0x000048cc  / -[MyController(Private) drawFrame] / myapp 0x000036b4  / myCVDisplayLinkOutputCallback / myapp 0x94279db0  / CVDisplayLink::performIO(CVTimeStamp*) / QuartzCore 0x9427d0bc  / CVDisplayLink::runIOThread() / QuartzCore 0x9002bc28  / _pthread_body / libSystem.B.dylib ENDSTACK</body>
  </mail>
  <mail>
    <header>Core Image/OGL problem: last image appears on desktop</header>
    <body>I'm having a weird problem with Core Image drawing to a custom NSOpenGLView: after I draw the last image to the view, I'm opening a NSSavePanel. As soon as the panel shows up, the last ciimage gets drawn on the top- left position of my desktop - above all other widows. As soon as I move around some window or open expos√©¬¥, the image is cleared and my desktop appears normally again. I have no idea what is going wrong and already tried a lot of things like calling clearGLContext, clearDrawable and other stuff before opening the panel. But nothing seems to resolve the problem. Please help. The NSOpenGLView is mostly implemented as the SampleCIView from the WhakedTV sample. The NSSavePanel is opened with ...runModalForDirectory... Best regards, Mark _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: toggle mouse drag with</header>
    <body>On 30 August 2006, at 18:30, Roland Torres wrote: Come on mate, you're turning into Microsoft :)</body>
  </mail>
  <mail>
    <header>Re: toggle mouse drag with</header>
    <body>On 29 August 2006, at 16:55, Mike Paquette wrote: I've had this problem with an app I'm using on my old 400mHz G4 box... and quitting everything didn't help. It's not an app I wrote, it's one of Skype, X-Chat Aqua (I'm betting on X-Chat causing it), Adium, TextEdit, Terminal, and (gasp) Finder. Logging out fixes it... for a minute.</body>
  </mail>
  <mail>
    <header>Re: Coordinate flipping a CGLayer not working</header>
    <body>On Apr 29, 2007, at 12:15 PM, Ken Tozier wrote: To flip the context, you need to scale it. but before you do that, you need to move the origin to the right place so the whole thing will look something like:</body>
  </mail>
  <mail>
    <header>Re: Coordinate flipping a CGLayer not working</header>
    <body>Adding CGContextScaleCTM did the trick. I've been having a terrible time with coordinate flipping (a mental block or something) and your explanation cleared out some of the fog. On Apr 29, 2007, at 1:47 PM, Nick Nallick wrote:</body>
  </mail>
  <mail>
    <header>Re: Coordinate flipping a CGLayer not working</header>
    <body>On Apr 29, 2007, at 11:15 AM, Ken Tozier wrote: A standard (i.e., right-hand) coordinate system has the origin in the coordinate system has the origin in the upper left corner and a positive y value increments down (i.e., it's a left-hand coordinate system).  Therefore to flip a transformation you have to move the origin from one corner to the other (i.e., translate) AND invert the y scale factor.  You're omitting the scale factor.  Your translate call will simply move the context's origin. In general you can invert the y scale factor by calling CGContextScaleCTM(context, 1.0, -1.0).  However you should be aware that this scales about the CTM's current origin, so it's important to consider your translation (to move the origin) and scale (to flip the y axis) in combination.  For example, if you scale first then translate your translation will use the newly inverted y scale so your y translation value must be the negative value of the one you'd use if you translated first then scaled. Nick</body>
  </mail>
  <mail>
    <header>Coordinate flipping a CGLayer not working</header>
    <body>I have a CGLayer, whose contents is a PDF, that I draw into a flipped view and can't get the layer to draw unflipped. I read somewhere in the Quartz drawing docs that setting the &amp;quot;y&amp;quot; value in CGContextTranslateCTM would draw flipped (can't find a link now) but, regardless of whether I set &amp;quot;y&amp;quot; to positive or negative, the layer is always upside down. Is there a way to flip a layer without doing a rotate/translate? if ([superView isFlipped]) else Ken</body>
  </mail>
  <mail>
    <header>Inconsistent non-coalescing flags</header>
    <body>CGEventGetFlags(eventRef) &amp;amp; kCGEventFlagMaskNonCoalesced) tells me that mouse moved events are being coalesced, as expected. kCGEventFlagMaskNonCoalesced says they are not. I created the event source using kCGEventSourceStateCombinedSessionState. What's the story? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Inconsistent keyboard types</header>
    <body>CGEventGetIntegerValueField(eventRef, kCGKeyboardEventKeyboardType) tells me that my keyboard is type 31. CGEventSourceGetKeyboardType(eventSourceRef) says it is type 2. What's the story? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>How to read event flags in an event source?</header>
    <body>When I execute this code fragment on a CGEventRef, I get a meaningful result: (CGEventGetFlags(eventRef) &amp;amp; kCGEventFlagMaskNonCoalesced) But when I execute this code fragment on a CGEventSourceRef, my program goes into some sort of infinite loop that I can't debug: kCGEventFlagMaskNonCoalesced) The same issue exists for the other flags, such as kCGEventFlagMaskNumericPad. I created the event source in question using the CGEventCreateSourceFromEvent() function in an event tap callback function, and the event source state ID is 1 (kCGEventSourceStateHIDSystemState). Will I be able to read these flags from the event source if I authenticate or run as root? Or am I completely misunderstanding how this works? -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: Disabling Core Image antialiasing?</header>
    <body>So assuming your filter is a subclass of CIFilter, it should have an outputImage method. In this you&amp;#39;ll be creating CISampler objects which Something like: This would become: I&amp;#39;ve not tried this, but according to the docs it should work. There&amp;#39;s a couple of other settings you can put on the CISampler. Look at the CISampler Class documentation for them. Paul Hi, I&amp;#39;m working on a Core Image-based application and I&amp;#39;ve run into a little problem. I&amp;#39;m trying to implement simple non-antialiased zoom with Core Image, but the problem is that CIContext ignores both GL_TEXTURE_MIN_FILTER and GL_TEXTURE_MAG_FILTER. According to OpenGL Profiler, CIContext always calls glTexParameter with GL_LINEAR while I need GL_NEAREST. Any workaround for this? I&amp;#39;ve been thinking about messing with frame/ renderbuffers (render CIImage, scale matrix and render again), but performance is really important and perhaps there is a easier way to do it? Even private API or some nasty hacks would be appreciated. Thanks, Aidas</body>
  </mail>
  <mail>
    <header>Which is better?</header>
    <body>I am currently in the process of making an application similar to that of Photo Booth, where a user chooses a Core Image filter to apply to live video.  Would it be appropriate, in terms of performance, to a have a Quartz composition rendered in a QCView handle my filters and video controller?  Or would I be better off doing my effects and/or video controller programmatically?  Of course doing it programmatically would give the user more customizability, but would I see a performance increase?</body>
  </mail>
  <mail>
    <header>Re: Disabling Core Image antialiasing?</header>
    <body>On 24 Apr 2007, at 16:36, Aidas Dailide wrote: Can you write a Core Image kernel which does the zoom? Something like.... kernel vec4 zoomImage(sampler image, float zoom)</body>
  </mail>
  <mail>
    <header>Disabling Core Image antialiasing?</header>
    <body>I'm working on a Core Image-based application and I've run into a little problem. I'm trying to implement simple non-antialiased zoom with Core Image, but the problem is that CIContext ignores both GL_TEXTURE_MIN_FILTER and GL_TEXTURE_MAG_FILTER. According to OpenGL Profiler, CIContext always calls glTexParameter with GL_LINEAR while I need GL_NEAREST. Any workaround for this? I've been thinking about messing with frame/ renderbuffers (render CIImage, scale matrix and render again), but performance is really important and perhaps there is a easier way to do it? Even private API or some nasty hacks would be appreciated. Thanks, Aidas</body>
  </mail>
  <mail>
    <header>Re: Question about nclc/colr image description extension</header>
    <body>Can someone give me some information on this? Mark</body>
  </mail>
  <mail>
    <header>Re: Software Rendered kicking in with WhackedTV on PPC</header>
    <body>Technical Q&amp;amp;A QA1416: Specifying if the CPU or the GPU should be used for rendering. Specifically: &amp;quot;NOTE: By default, Core Image uses the CPU for rendering on systems with a GeForce 5200 series card because, for most benchmarks, the 5200 can be slower than the CPU on currently -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Software Rendered kicking in with WhackedTV on PPC</header>
    <body>I posted this thread originally on the quicktime list, but Brad Ford suggested that this might be a better place to ask. I'm seeing very poor performance (100% CPU consumption) with the WhackedTV sample code on PPC machines (vs about 10% on an intel MPB). This is with video only preview. One machine is a dual G5@2GHz with a GeForce 5200 video card and 64 MB G4@1GHz Powerbook with a GeForce FX Go5200 Video card and 32MB of VRAM. The video input is an external isight (the MPB performance figure used the external as well). System Profiler reports both machine's displays as supporting Quarz Extreme and Core Image Profiling with Shark shows that the additional CPU on the PPC is being consumed by the calls pasted below, which seems to indicate that the software renderer is kicking in. If this is a genuinely abnormal case affecting a few machines, that's fine, but these machines, while a few years old, are Tiger/Leopard capable, and seem like they should be able to support hardware rendering with no problem. When I play with Quartz Composer on the Dual G5 machine, it seems fine, and cpu consumption is low (&amp;lt; 10%), and other video programs that aren't descended from WhackedTV give acceptable performance on both PPC machines. cheers, m. 67.1%	67.1%	libGLImage.dylib	glgProcessColor(GLDPixelMode const*, GLGOperation const*, unsigned long) 0.0% 	67.1%	libGLImage.dylib	glgProcessPixels 0.0%       67.1%	QuartzCore	fe_bitmap_copy 0.0%	       67.1%	QuartzCore	image_buffer_get_data 0.0%	       67.1%	QuartzCore	texture_retain 0.0%	       67.1%	QuartzCore	fe_texture_new 0.0%	       67.1%	QuartzCore	fe_tree_create_texture 0.0%	       67.1%	QuartzCore	fe_tree_render_apply 0.0%	       67.1%	QuartzCore	fe_tree_render 0.0%	       67.1%	QuartzCore	fe_tree_render_image_ 0.0%	       67.1%	QuartzCore	fe_tree_render_image 0.0%	       67.1%	QuartzCore	fe_image_render_ 0.0%	       67.1%	QuartzCore	fe_image_render 0.0%	       67.1%	QuartzCore	-[CIOpenGLContextImpl renderSoftware:matrix:bounds:] 0.0%	       67.1%	QuartzCore	-[CIOpenGLContextImpl render:] 0.0%	       67.1%	QuartzCore	-[CIContext drawImage:inRect:fromRect:] 0.0%	       67.1%	WhackedTV	-[SampleCIView drawRect:] 19.2%	19.2%	Unknown Library	0x58b9000 [3.5KB] 0.0%	       19.2%	QuartzCore	fe_span_iterate 0.0%	       19.2%	QuartzCore	jit_program_renderer 0.0%	         9.8%	QuartzCore	fe_fragment_thread 0.0%	         9.8%	libSystem.B.dylib	_pthread_body 0.0%	         9.5%	QuartzCore	fe_fragment_render_quad 0.0%	         9.5%	QuartzCore	fe_tree_render_apply_1 0.0%         9.5%	QuartzCore	fe_tree_render_apply 0.0%	         9.5%	QuartzCore	fe_tree_render 0.0%      	  9.5%	QuartzCore	fe_tree_render_image_ 0.0%          9.5%	QuartzCore	fe_tree_render_image 0.0%	          9.5%	QuartzCore	fe_image_render_ 0.0%	          9.5%	QuartzCore	fe_image_render 0.0%	          9.5%	QuartzCore	-[CIOpenGLContextImpl renderSoftware:matrix:bounds:] 0.0%	          9.5%	QuartzCore	-[CIOpenGLContextImpl render:] 0.0%	          9.5%	QuartzCore	-[CIContext drawImage:inRect:fromRect:] 0.0%	          9.5%	WhackedTV	-[SampleCIView drawRect:]</body>
  </mail>
  <mail>
    <header>Aliased arc drawing</header>
    <body>Hi, I'm getting some unexpected results when drawing arcs with anti-aliasing turned off. I've uploaded an image of the results here: As the image shows, the fill actually extends one pixel outside the stroked line in some places. I would expect the fill to be completely enclosed by the line, and this is also what I get if I turn anti-aliasing on. Does anybody know a way to get correct painting with anti-aliasing off? - Morten</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate/ImageIO weirdness</header>
    <body>Thanks Nick Adding an options dictionary with kCGImagePropertyDPIHeight and kCGImagePropertyDPIWidth fixed the problem. Ken -------------- Original message ----------------------</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate/ImageIO weirdness</header>
    <body>On Apr 16, 2007, at 11:24 AM, email@hidden wrote: You have to pass an options dictionary to CGImageDestinationAddImage.  There might be another way to add this dictionary, but that's how I do it.  It can contain keys such as kCGImagePropertyDPIWidth, kCGImagePropertyDPIHeight, and kCGImagePropertyTIFFDictionary.  A TIFF dictionary can contain the key kCGImagePropertyTIFFCompression. Nick</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate/ImageIO weirdness</header>
    <body>Am 16.04.2007 um 19:24 schrieb email@hidden: that dumps all tags... Regards, Tom_E</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate/ImageIO weirdness</header>
    <body>After more testing, I'm beginning to suspect that there is some info missing from the ImageIO generated tiffs. I imported them into every application I have, that allows image placement, and found that in BBEdit the tiffs were also distorted. I also imported them into a really old version of Quark (v3.3.2) and got the exact same distortion. I was thinking that it might be the tiff lines per inch field but see no way to set that in CGImageDestinationCreateWithURL. Since at least two applications have problems with these images, that seems to indicate something with the images that is slightly off. I couldn't find any lines per inch fields in CGImageDestinationCreateWithURL but does anyone know if there is a way to set this info? Thanks Ken</body>
  </mail>
  <mail>
    <header>Re: CGLayer performance problem</header>
    <body>My understanding is that the context that you pass to the CGLayer is not the only context you can draw to... it's simply an example, a sample if you will, of the type of context that you would like to draw into.  The CGLayer will pull information from that context (In the example of a bitmap context that would be color depth, pixel order, color space, etc...) and optimize whatever off-screen representation it keeps for that layer so that it can draw on other contexts with similar arrangements. What that means is that even though you could be drawing to a different individual  CGContext each time, most of the times when you draw to the screen, the characteristics of that context are going to be the same as every other screen context you've seen up to this point (including the one used to create the layer).  The layer's off- screen representation should still be optimized for those conditions. Where you run into a problem is when the destination context has a different environment than the one the layer was created with.  On a two monitor system where one is on thousands of colors, and one is on millions of colors, for example, drawing the same layer in each environment may be sub-optimal.  Or if the user changes the color depth of the monitor and you continue to use layers created with the old setup, you might run into problems.</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate/ImageIO weirdness</header>
    <body>On Apr 16, 2007, at 3:29 AM, Thomas Engelmeier wrote: So it does... The odd thing though, is that I intentionally did not apply any of the Quark import options (like size to fit, scale proportional etc...) so these register in Quark's measurements palette as all being scaled at 100% x 100%. I fiddled with high-res vs low res preview, different measurement systems etc to no effect and I'm not aware of any option in Quark which would translate to &amp;quot;import all Ken</body>
  </mail>
  <mail>
    <header>Re: Incremental image loading using CG ?</header>
    <body>On Nov 8, 2005, at 6:18 AM, Bjoern Kriews wrote: Wow. You are almost trying to do what I am doing right now. Here's what I do: My image loading thread loads the image data in segments. It draws the segments onto a CGLayer that is shared with the main thread. There are 2 ways of triggering display update. One is sent a custom Carbon event to the main thread after each segment is drawn to the layer. The main thread just draw the layer onto the custom HIView that is holding the image. The other way is setup a Carbon event timer that fires every some seconds. It triggers the routine that draws the layer. CGImage doesn't work for me because whenever the partly loaded image is drawn, it's cached as it is right then. Any redraw following that draws the cached version. Like you said, I need to destroy it and recreate a new one to get the new display. The recreation of CGImage can be expensive. CGLayer doesn't have this problem. It's always drawn with the up to date content. A side benefit is after the image is completely loaded, the layer can be reuse. My app overlay a ton of other user- controllable stuffs on the image. This way, I don't have to reload the image every time the use turns on or off some overlay marks. I can also draw some permanent marks to the layer with normal CG calls instead of messing with bits in CGImage. BTW. Are your image data format private? For the life of it, I can't figure out how to use CGImageSource and friends to load private data format. --</body>
  </mail>
  <mail>
    <header>Incremental image loading using CG ?</header>
    <body>I am working on an application that loads images in a secondary thread. Right now, I am using a customized libjpeg-based loader that sends messages to the main thread triggering display of partly-loaded images. While looking for a more general approach to image loading, I tried to feed slices of data to a source created with CGImageSourceCreateIncremental. I am under the impression that (even with caching enabled on the ImageSource), that every time I ask for a CGImage it is decoded from the start which is too slow for my application. I even have to release the previous Image to get a new one. I have looked at the WebKit sources and it seems like their image loader does the same. What I need is a way to receive progress messages and have access to a CGImage (seems unlikely because of its immutability) or the decode buffer.</body>
  </mail>
  <mail>
    <header>How to draw a CIImage to CVPixelBufferRef</header>
    <body>Can I use a CVPixelBufferRef as the context of a CIContext, and then use the CIContext's draw method to process a CIImage to the CVPixelBufferRef? Kind regards, Mark Munte</body>
  </mail>
  <mail>
    <header>CGGLContextCreate color strangeness</header>
    <body>I have a strange problem that I can't nail down. I'm trying to set up an OpenGL context which is shared by Quartz, mainly to be able to use Quartz font drawing with my OpenGL program, but also to maybe draw Quartz Composer backgrounds. The problem is that as soon as I create a CGGLContext, the OpenGL colors are all wrong. I don't even have to actually draw anything using Quartz, the mere CGGLContextCreate() call is sufficient. I'm attaching an example that draws a red Quad; if compiled without the - DDEMO flag, it works perfectly, if compiled with -DDEMO, the Quad is gone. Experimentation shows that it's not *not drawn*, but just not drawn in the correct color. In case this is a driver-specific problem: I'm on a 15&amp;quot; AlBook, Radeon 9600. Has anyone encountered this problem before and can give me some pointers? [snip] @interface MyOpenGLView: NSOpenGLView @end @implementation MyOpenGLView: NSOpenGLView - (id) initWithFrame: (NSRect) aRect NSOpenGLPFANoRecovery, NSOpenGLPFAColorSize, 24, NSOpenGLPFADepthSize, 16, NSOpenGLPFADoubleBuffer, NSOpenGLPFAAccelerated, 0 NSOpenGLPixelFormat *winFormat = [[[NSOpenGLPixelFormat alloc] initWithAttributes: winattrs] #ifdef DEMO // this is what makes the colors go wrong! #endif - (void) drawRect: (NSRect) aRect glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT | @end @interface MyController: NSObject @implementation MyController: NSObject - (void) applicationDidFinishLaunching: (NSNotification*) aNote NSWindow* win = [[NSWindow alloc] initWithContentRect: r styleMask: (NSClosableWindowMask | NSMiniaturizableWindowMask | NSTitledWindowMask) backing: NSBackingStoreBuffered int main(int argc, const char* argv[]) [snip] _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: QuickDraw into a PDF</header>
    <body>You can capture your QD drawing in a PicHandle and use QDPictDrawToCGContext. --(jm)</body>
  </mail>
  <mail>
    <header>QuickDraw into a PDF</header>
    <body>I'm in the midst of a project that will use legacy Quickdraw printing code to render into a PDF. The Quickdraw code currently renders into a printing port, which should be easy to convert to a GWorld. Is it possible to render Quickdraw directly into a CGContext or will I have to use a GWorld, CGImageCreate and CGContextDrawImage? -Mick</body>
  </mail>
  <mail>
    <header>Quartz shadings: how to stroke, how to extend beyond start and  end?</header>
    <body>Hi all, My first question: Filling a path with a shading can be obtained by using the path as a clip path and then filling it with CGContextDrawShading. But for stroking the path, the only two options I can think of are: - stroke the path with a black color in a bitmap context, fill another - provide a pattern callback that fills the reference tile with the shading, create a pattern with the path bounds as the reference tile rect, use the pattern to stroke the path (with CGContextSetFillPattern+CGContextStrokePath. Both methods require to know the actual bounds of the path, taking into account the line width, line join type, etc. Beside these methods, is there a simpler and recommended way of stroking a path comparable to the fill method? My second question: I'd like to extend beyond the start and end limits to get repeating shadings (like in the bottom part of the picture). The shading created with Quartz repeatedly passes a gradient offset to a gradient function that returns color component values. However, the gradient offset value is always cropped in the range [0,1] and it seems the only way to get repeating shadings with Quartz is to move the start and end points (requiring to count the stripes or circles) and handle the repeats in the gradient function. My feeling is that it's inefficient, since the actual gradient offset (and not the cropped one) must be computed by Quartz somewhere. There must be a way... [demime 0.98b removed an attachment of type image/tiff which had a name of pastedGraphic1.tiff] Regards, Nicolas Normand</body>
  </mail>
  <mail>
    <header>CGBitmapContext to draw only the mask in a 32bit image?</header>
    <body>Hi, I can draw using the CGBitmapContext stuff into a 32bit GWorld. But can I define in this ARGB buffer a CGContext to draw a grayscale image into the alpha part only? bits per component = 8 components per pixel = 1 but bytes per Pixel = 4 Any idea? Mfg Christian -- Seven thousand functions in one REALbasic plug-in. The MBS Plugin.</body>
  </mail>
  <mail>
    <header>Framebuffer image is poor, hardware display is clear</header>
    <body>We've received a report from the field that seems to indicate that the image we're getting from a display's framebuffer isn't the same as the image the hardware is displaying.  We get at the framebuffer by calling Carbon's GetPixBaseAddr on the gdPMap of a GDHandle.  It appears from the customer's screenshots that the image is scaled or anti-aliased poorly, yet the user reports that &amp;quot;When connected to a monitor, the display is crisp and clear as The machine is an Xserve G5 with an ATI Rage 128 that was pulled from a B&amp;amp;W G3.  It's running 10.3.4. Any ideas? -Marc</body>
  </mail>
  <mail>
    <header>Re: CGPSConverter and friends</header>
    <body>Regarding: You can pass the full data stream of such files to the converter and it finds the PostScript data inside of Windows style EPSF files. If you encounter any behavior that differs from this then it is a bug and please report it; to date we've not seen any bugs in this area. David</body>
  </mail>
  <mail>
    <header>Re: CGPSConverter and friends</header>
    <body>Yes it seems so. I used CGPSConverter to convert PS to PDF into a CFData, then used that CFData as a provider to the CGPDFDocumentCreateWithProvider. It seems to work OK, although a smidgen slower than with just PDF. Minor question: does CGPSConverter work with old-style MSDOS header EPSF files i.e those that start with the magic numbers C5D0D3C6 re: the EPSF Spec page 19, or do I have to skip over the TIFF preview section? It looks like that function is piping Postscript to a Postscript printer, as opposed to converting it into an arbitrary Quartz context. But thanks for the heads-up. OK, I'll be a good little Quartz citizen :-). Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com</body>
  </mail>
  <mail>
    <header>Virtual desktops of extra-large size - how to?</header>
    <body>I'm just poking around with the idea of implementing a virtual desktop, in the standard fashion where there the virtual desktop's dimensions are larger than those of the screen.  It would then be translated/scaled/rotated/whatever as necessary for display.  I've seen apps which do similar things already, although I suspect they perform some special trick to do it.  And in my case, I'm probably pushing the limits of plausibility... For example, if I want a hypothetical virtual desktop which is about 4 metres square (think &amp;quot;average desk size&amp;quot;, if you want a metaphor), then on my iBook that corresponds to a buffer that's about 202 MiB, assuming only 3 bytes per pixel.  Obviously not practical. :) So, I was thinking about ways to do this without requiring a buffer beyond the screen's normal one... now, perhaps it'd be possible to disable any buffers that would normally need to be the virtual size (e.g. the desktop window, or whatever), and just fill the background of the buffer with a flat colour prior to each redraw... but then of course it's a bit of a problem as to how to do that, from the point of view of someone wanting to coerce the window server into doing all this, rather than writing my own window manager. :) I also had the thought that I could perhaps manipulate the position of window's, translating them in real screen space to simulate translating the view through the virtual desktop.  It's a bit trickier to include the appropriate scaling and whatever, although I'm hoping I could get away with using the window's rendering transform matrix, which would absolve me from having to handle that sort of thing. Anyone have any suggestions for any part of this?  I know there are existing virtual desktop apps for OS X, from yonder years, but I can't dig them up now that I look.  And I suspect, if they use the window translation hack I just suggested, they won't work on 10.3.4 due to the additional window server security (i.e. having to be the universal owner, which you have to fight the Dock for). Wade Tregaskis (aim: wadetregaskis) -- Sed quis custodiet ipsos custodes?</body>
  </mail>
  <mail>
    <header>Re: Private APIs</header>
    <body>As this wannabe-flamewar is now entirely off topic, I've taken my response(s) to Anyone who wishes to add to that page may email me their comments. Wade Tregaskis (aim: wadetregaskis) -- Sed quis custodiet ipsos custodes?</body>
  </mail>
  <mail>
    <header>Re: Private APIs</header>
    <body>I'd say that the solution to this is for Apple to make these routines (are there only 2, or more?) part of some public API.  And quickly.. Making a window into a cube (I assume something similar to what the login switcher does) might seem frivolous, but it could very well lead to some very interesting alternative interfaces (windows as surfaces that you could manipulate in 3D space with a 6DOF controller?  I can see the possibilities now..) By keeping the functions public, Apple is not only giving themselves flexibility to change things (which is important), it may also be stifling innovation (or keeping that innovation to themselves in OSX 5.0 or 6.0) This is very reasonable. Jim Witte email@hidden Indiana University CS</body>
  </mail>
  <mail>
    <header>Re: CGPSConverter and friends</header>
    <body>I think you know the answer to these questions:  do not use private SPI. Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: CGPSConverter and friends</header>
    <body>Regarding: I believe you are better served using the CGPSConverter functionality if your goal is to draw EPS data to an arbitrary context. The function you are asking about is private to CG and should not be used directly. The function PMCGImageCreateWithEPSDataProvider is a public function available from the printing framework and is one you could consider if you should choose. Whether it is appropriate for your purposes depends on what your purposes are. If you are trying to print EPS data to PostScript printers only, then this function may be what you'd like; the EPS data is sent to a PostScript printer and the input CGImageRef you provide to the function is used when raster printing. Here is the function prototype from the header file PMCore.h in the PrintCore framework in Application Services: /* *  PMCGImageCreateWithEPSDataProvider() * *  Summary: *    Create an image reference that references both the PostScript *    contents of an EPS file and a preview (proxy) image for that EPS *    file. * *  Discussion: *    For OS X 10.1.0, this function ignores the passed in data *    provider. The passed in image reference is retained and then *    returned. For 10.1.1 and later, then the data provider is used *    and the returned image reference is different than the passed in *    image reference, so please be careful with your use of these *    references. It is likely that the data will not be read from the *    EPS data provider until well after this function returns. The *    caller should be careful not to free the underlying EPS data *    until the provider's release routine is invoked. Similarly the *    preview image's data may be needed long after you think it should *    be. Do not free the image data until the image data provider's *    release data function has been called. To make sure these data *    providers are properly reference counted, release your reference *    the EPS data provider and on the EPS image preview when they are *    no longer needed by your application. For Mac OS X 10.2 and *    later, the contents of the EPS provider at the time of this call *    can be dumped to a file if you first do the following, BEFORE *    running your application. From the command line in terminal: *    defaults write NSGlobalDomain com.apple.print.eps.testProvider *    /tmp/dump.eps causes a dump of the EPS data into a file *    /tmp/dump.eps. * *  Parameters: * *    epsDataProvider: *      A Core Graphics data provider that can supply the PostScript *      contents of the EPS file. Post OS X 10.1, there will be some *      checking done on the EPS data provided to the *      PMCGImageCreateWithEPSDataProvider() call. It is important that *      the EPS data begin with the EPSF required header and bounding *      box DSC comments. * *    epsPreview: *      A Core Graphics image reference to the proxy image for the EPS *      file. When the image reference result of this function is *      rendered on screen or printed to a printer that can not render *      PostScript this proxy image is drawn instead. * *  Result: *    an image reference capable of rendering either the EPS content or *    the proxy image depending upon the capabilities of the targeted *    context. * *  Availability: *    Mac OS X:         in version 10.1 and later in ApplicationServices.framework *    CarbonLib:        not available in CarbonLib 1.x, is available on Mac OS X version 10.1 and later *    Non-Carbon CFM:   not available */ extern CGImageRef PMCGImageCreateWithEPSDataProvider( CGDataProviderRef   epsDataProvider, CGImageRef          epsPreview) Again, the version in printing is public. No it does not use the CGPSConverter; it is basically a conduit for EPS data to PostScript printers. See the prototype above for PMCGImageCreateWithEPSDataProvider. Do not use private API. Whether the API for PMCGImageCreateWithEPSDataProvider suits your needs or you would be better served by using the CGPSConverter API to convert an EPS document to PDF totally depends on your needs and what you plan for the EPS data you are consuming. David</body>
  </mail>
  <mail>
    <header>Re: CGPSConverter and friends</header>
    <body>I know it's not Apple's policy to recommend the use of undocumented functions, but there's a function called CGImageCreateWithEPSDataProvider in 10.3 Quartz framework that I could use... 1. Is this stable and would not go away? (Perhaps Cocoa uses it for EPS work.) 2. Does this actually use the CGPSConverter route under the covers or something else? 3. Does it have the same params as the documented CGImageCreateWith PNGDataProvider etc. 4. Any caveats? Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com</body>
  </mail>
  <mail>
    <header>Re: CGPSConverter and friends</header>
    <body>Well, EGOS uses its own PS interpreter to directly create object structures (Objective-C) without converting the whole file.  But it isn't Quartz. Marcel -- Marcel Weiher				Metaobject Software Technologies email@hidden		www.metaobject.com Metaprogramming for the Graphic Arts.   HOM, IDEAs, MetaAd etc. 1d480c25f397c4786386135f8e8938e4</body>
  </mail>
  <mail>
    <header>Re: CGPSConverter and friends</header>
    <body>Effectively, yes. This is &amp;quot;in-process&amp;quot;, which means all of the code runs in your own app in the same thread you call the conversion functions from.  You may wish to run this in a separate thread or in a separate process (or just exec pstopdf from your app).  Some temp files may be used during the conversion. Yes. A CGPDFDocument needs the entire document available for random access. Consequently, the entire conversion from PS to PDF must complete before CGPDFDocumentCreate will succeed.  This is a limitation imposed by the architecture of PDF itself. Yes, that's a good approach. Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Lifetime of a CGPDFObject ?</header>
    <body>What is the lifetime of a CGPDFObject scanned from a stream and returned by CGPDFScannerPopObject()? That is, how long can I rely on the returned CGPDFObject being valid? The documentation makes a vague statement that the stream manages the allocation/deallocation of those objects, but does not actually say when the objects might be deallocated. op_foo(scanner, ...) and in some cases the memory pointed to by param1 becomes invalid (actually becomed unmapped) during the call to CGPDFScannerPopName().</body>
  </mail>
  <mail>
    <header>Re: Aliased line drawing trouble</header>
    <body>Quartz gets its rendering model from PostScript (via Display PostScript and PDF). PostScript didn't include the concept of antialiasing, since it was developed for 1-bit-deep output devices (laser printers), and its rule for coloring output pixels is that if the geometric path being stroked/filled intersects the pixel at all, then that pixel gets colored. So, if your line is wider than 1 pixel, it can easily touch three pixels --- one pixel fully, and a sliver of two more pixels. In the PS rendering model, all three pixels get filled. The recommended way to draw 1-pixel-wide lines in PS is to set your linewidth to 0. Of course, if you're drawing antialiased in Quartz, a 0-pixel-wide line won't even show up, but I'm guessing that if you turn antialiasing off then you get the old PostScript rendering model, in which case a CGContextSetLineWidth(..., 0) sounds like it might do what you want.</body>
  </mail>
  <mail>
    <header>Re: Aliased line drawing trouble</header>
    <body>On Feb 28, 2007, at 7:51 AM, Morten S√∏rvig wrote: You have anti-aliasing turned off so it can't simulate fractional pixels like it is optimized to do. If you draw things out on grid paper you usually can quickly see why it is doing what it is doing. -Shawn _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Aliased line drawing trouble</header>
    <body>To sum this up, adding a 0.5 offset to the x and y coordinates does solve the problem. I tried setting the quartz scaling to various values and this produced less than ideal results, the pen width seems to quickly jump to three pixels when increasing the scale factor beyond 1.1. Morten</body>
  </mail>
  <mail>
    <header>Re: Questions About Extracting Objects from PDFs</header>
    <body>On Feb 27, 2007, at 8:18 PM, Gordon Apple wrote: I've done what you propose and it's nontrivial.  This is an area where Quartz is lacking compared to QuickDraw.  There's no editable vector graphics exchange format.  I filed a bug long ago, but I don't think it's considered important. If you're just trying to extract paths you've simplified the problem quite a bit.  For a general solution the hardest parts are extracting text in meaningful chunks, shaders, embedded images with weird colorspaces, and probably a few other things that don't come to mind at the moment. Voyeur is based on the PDF component management functions introduced in Panther.  If you're going to dissect a PDF file these features will be very useful, but not until you've figured out how PDF files are structured.  One trick to Voyeur is to use File &amp;gt; Show Info on a stream object to display the stream's contents (vs. its properties). For example, a page's &amp;quot;Contents&amp;quot; stream contains a PostScript-like language you will have to parse.  In Tiger the PDF parser is a public API so that can also save you some time. PDF isn't really a tag based format.  It's closer to a stack based page description language such as PostScript without the general purpose language constructs.  I'd suggest the &amp;quot;PDF Reference&amp;quot; from the Adobe web site. I'm not aware of any available code to do what you want, but there are a number of functions in Quartz to help you along.  You'd have to write a function for each PDF operator to at least pull the appropriate number of objects off the stack.  You might also want to track the current graphics state and perhaps the clip path.  You'd definitely want to pay attention to the path generation operators (moveto, lineto, etc.). Good luck, Nick</body>
  </mail>
  <mail>
    <header>Re: Questions About Extracting Objects from PDFs</header>
    <body>On Feb 27, 2007, at 9:18 PM, Gordon Apple wrote: As a general rule, what you are trying to do is not that simple.  PDF files are designed to help you draw graphics, not edit them.  As a result, there is not really a good way to get individual graphic elements out of a PDF file unless that PDF has &amp;quot;editing&amp;quot; information embedded inside of it that you've put there. While PDF kit helps you explore files, you're going to have to concern yourself with a lot of drawing minutae from the PDF spec. The PDFKit will get you the large, coarse objects like pages, but then you'd have to parse the contents of the page yourself to identify paths and such. Attachment:</body>
  </mail>
  <mail>
    <header>Questions About Extracting Objects from PDFs</header>
    <body>I've done some reading on this in the Programming with Quartz book, but am currently in the exploratory phase and have not delved into the PDF specs yet.  I'm looking for ways to copy/paste or Drag/Drop simple draw objects between programs.  We can (and have for OS9) implemented formats for doing this internally.  However, since it appears that PDF is the standard OSX exchange format, I would like to be able to parse simple PDFs to extract draw shapes and their attributes.  The software is focused mainly on remote presentations will have only rudimentary drawing capabilities.  We would like to be able to draw and then move one or more shapes from other more draw-oriented programs (e.g., OmniGraffle) to ours and still be able to edit size and attributes, fill with images, QT movies, text, etc. I've tried running Voyeur, but gotten totally lost in nested triangles, even for a simple PDF.  It wasn't very instructive. I assume there is a standard set of attribute tags for PDF shapes.  The Programming with Quartz book has some code for extracting images from PDFs. My question is whether there is code already available to extract simple draw objects (shapes) and attributes from PDFs that I would do what I want without delving into the mass of PDF details and writing it myself?</body>
  </mail>
  <mail>
    <header>Re: Aliased line drawing trouble</header>
    <body>I should also note that Quartz's implementation is focused on drawing with quality (and fast) anti-aliasing and if you disable that Quartz is operating in a mode that it isn't generally optimized for. Also recall that you specify things to Quartz in terms of points and not pixels while many other drawing system work in terms of pixels more directly. If you must have pixel accuracy consider building a bitmap directly with the specific pixels you want (not hard to do for the simple graphics your are attempting). Finally note that point to pixel mapping will soon no longer always be 1 to 1 ... high DPI displays are getting close to being a reality and when they come on the market resolution independent UI capabilities of Mac OS X will be utilized. Review... &amp;lt;http:// developer.apple.com/releasenotes/GraphicsImaging/RN- ResolutionIndependentUI/&amp;gt; and most current versions of graphics related API docs also discuss this topic now. In the world on non-1 to 1 mapping you likely will want to draw with anti-aliasing enabled so that you can more fully leverage the pixel density available to you. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Aliased line drawing trouble</header>
    <body>On Feb 27, 2007, at 5:43 AM, Morten S√∏rvig wrote: Well if it affects you in a realworld situation then I would report a bug... but try shifting your coordinate grid by 0.5 in both X and Y since point centers at the center of the grid squares not at the corner point. -Shawn _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Aliased line drawing trouble</header>
    <body>On Feb 27, 2007, at 7:43 AM, Morten S√∏rvig wrote: It looks like expected behavior to me.  The &amp;quot;other paint engine implementations&amp;quot; are probably pixel-based where Quartz is not. In a default context, when Quartz maps your drawing onto the pixel grid, the mapping puts the pixel coordinates on the lower-left hand edge of the pixels, not the center of the pixel.  If you sketch out your graphic on a piece of graph paper using that information, then try and figure out which pixels should be filled in, you will find that Quartz is doing something reasonable. To &amp;quot;fix&amp;quot; your code you could add a half-pixel to each of your coordinates, i.e.: or you could offset the origin by a half a pixel so that integral coordinate values are in the center of the pixel: The result is that you would be drawing to the center of each pixel, not to the lower left hand corner. While this is fine for the default line setup in Quartz today, it may not do the &amp;quot;right thing&amp;quot; when we get to scaleable user interfaces. In that case each integral value in your coordinate system may corespond to 1.25 or 1.5 pixels on the screen.  After you try the CGContextTranslateCTM technique above, try setting your Quartz scaling to 1.25 or something and re-try your graphic.  You may be disappointed with the results. The upshot is that Quartz is not designed to be a pixel-oriented rendering engine.  If you need pixel-accurate graphics or images you will have to use other techniques.  A popular one is to create a pixel buffer and twiddle the bits in order to create a pixel-based graphic, then create a CGImage from that and draw the CGImage to your Quartz context.  Unfortunately, in the absence of QuickDraw, there is not a pixel-oriented rendering engine to twiddle the bits in your pixel buffer for you :-(.  I imagine it's a &amp;quot;third-party&amp;quot; opportunity. Scott _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Aliased line drawing trouble</header>
    <body>Hi, using painting code like this: seems to produce sub-optimal results compared to other paint engine implementations. I would expect code like this to produce a nice diamond shape, which it indeed does if I enable anti-aliasing. lines.png that shows the difference between Quartz (with the green background) and a reference drawing. Does this look like expected behavior from Quartz or is it something I should report as a bug? Morten</body>
  </mail>
  <mail>
    <header>Re: Endian Swap using Shader Program?</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: QCView Can Pause?</header>
    <body>I've never tried to grab a screen cap from a QCView but since it is a subclass of NSView you might be able to use dataWithPDFInsideRect On Feb 26, 2007, at 9:58 AM, Red XIII wrote:</body>
  </mail>
  <mail>
    <header>QCView Can Pause?</header>
    <body>hello everyone. This is a try to combine Quartz with Core Image. I am wondering whether when the QCView is playing a .qtz file, it can &amp;quot;pause&amp;quot; not stop with a complete black backgroud as it is showing the frame when it is required to pause. If it can pause, I can generate a bitmap from the instant image then apply some core filter on it. Can this be achieved ? Thank you in advance.</body>
  </mail>
  <mail>
    <header>Re: Endian Swap using Shader Program?</header>
    <body>kernel vec4 bgra2argb(sampler image) -- ======================================================================= Mark Coniglio             |  email@hidden TroikaTronix              | Isadora¬Æ                  | |  (interactive media processing environment) =======================================================================</body>
  </mail>
  <mail>
    <header>Endian Swap using Shader Program?</header>
    <body>It seems that Core Image really requires ARGB images -- I'm trying to convert BGRA into a CIImage via a CVPixelBufferRef on an Intel Mac, and the colors are all wrong because of the byte order. So, I'm wondering, as an absolute newbie to shader language programming, if there is a small bit of code that would allow me to do a byte swap to convert BGRA to ARGB. My thought is to make my own Core Image filter that would do this. Best Wishes, Mark -- ======================================================================= Mark Coniglio             |  email@hidden TroikaTronix              | Isadora¬Æ                  | |  (interactive media processing environment) =======================================================================</body>
  </mail>
  <mail>
    <header>Re: Why no release in all Core Image samples?</header>
    <body>On Feb 23, 2007, at 2:22 PM, Scott Thompson wrote: Ah yeah... that will soon make sample code, answering questions on list like this, etc. interesting. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Why no release in all Core Image samples?</header>
    <body>On Feb 23, 2007, at 3:52 PM, Shawn Erickson wrote: I was conjecturing that the sample code was developed under Objective C 2.0 under Garbage Collection and wasn't updated when it was posted back to the sample code site :-)</body>
  </mail>
  <mail>
    <header>Re: Why no release in all Core Image samples?</header>
    <body>I was more talking about HazeFilterView and its filter ivar (the only class I really looked at). Granted in this situation HazeFilterView is acting like a singleton given how it utilized in the sample code... why I implied it was a lazy style of programming (heck it is sample code). -Shawn</body>
  </mail>
  <mail>
    <header>Re: Why no release in all Core Image samples?</header>
    <body>It sure looks like that at first glance, but is in this case actually correct. If you follow the naming convention for filter parameters (essentially, input parameters are instance variables starting with 'input'), CIFilter will do some additional work for convenience ... like making sure that 'inputKeys' returns a properly populated array, the corresponding entries for key and class are in the attribute dictionary, and that all input keys get released in dealloc. You have a point that this is not well documented -- I'll make sure the documentation gets updated for the next dev tools release. Thanks! - Ralph</body>
  </mail>
  <mail>
    <header>Re: Why no release in all Core Image samples?</header>
    <body>For online documentation, this is the simplest and most direct way to get feedback to those responsible for the document. -- David Duncan Apple DTS Quartz and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Re: Why no release in all Core Image samples?</header>
    <body>After a quick look it looks like lazy programming that missed proper memory management. File defects with Apple about the issues you see in the code examples. If you are looking at an example that is available online you likely could also use the &amp;quot;Did this document help you?&amp;quot; box at the bottom of the page to report issues with the sample code. -Shawn</body>
  </mail>
  <mail>
    <header>Why no release in all Core Image samples?</header>
    <body>Looking at the ADC sample projects in the Core Image group, I noticed that they often lack the proper dealloc methods. In particular, since I write my own executable units, I was surprised to find out that the CIHazeFilterSample has no dealloc, and all the inputs set on the filter are leaking when the CIFilter is dealloc'd... or are they? Is this intentional or simply bad ObjC programming?</body>
  </mail>
  <mail>
    <header>CVPixelBufferCreateWithBytes &amp;amp; BGRA</header>
    <body>On an Intel Mac, I am creating a wrapper for a BGRA bitmap using CVPixelBufferCreateWithBytes -- the pixel format, when creating the CVPixelBufferRef is BGRA. I convert this to a CIImage, process it with a CoreImage plugin or two, and then display the result -- but it's clear from the resulting output that it _thinks_ the image is ARGB. Best, Mark -- ======================================================================= Mark Coniglio             |  email@hidden TroikaTronix              | Isadora¬Æ                  | |  (interactive media processing environment) =======================================================================</body>
  </mail>
  <mail>
    <header>Re: 3D rendering?</header>
    <body>Thanks for the link. The demo code gave a good walk through some examples. Thanks also for the reading references. Jeff</body>
  </mail>
  <mail>
    <header>Re: 3D rendering?</header>
    <body>Some of my personal favorites: Cheers, Dave</body>
  </mail>
  <mail>
    <header>CGImageDestinationFinalize</header>
    <body>Greetings. The following image formats return false for me using CGImageDestinationFinalize(....). * AppleICNS (.icns) * Microsoft (.ico) * QuicktimeIMage (.qtif) Using the same method I get True for the other formats as indicated in the header file. * .bmp * .gif * jpeg * .jpeg2 * .pict * 'png * .tiff This is on Mac desktop OS X 10.6.3. Is this expected? Also, in the same method, I am calling CGBitmapContextCreate(.....). The last parameter listed in the header file for this function indicates a type, CGBitmapInfo, but the example code I've found uses the enumeration, CGImageAlphaInfo, which I'm currently using. Is there a mapping of which CGImageAlphaInfo value to use and the type of file format I want to save? Thank you, Thomas C. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Find position of text inside of a CGPDFPage?</header>
    <body>Hi all, Is there any way to find the position of a string on a CGPDFPage short of pacing the whole document? I am able to draw the page using CGContextDrawPDFPage and I can extract and search text using CGPDFScanner, but I can&amp;#39;t figure out a way to combine the two to draw a simple highlight. Any suggestions would be much appreciated... Thanks, Carlo</body>
  </mail>
  <mail>
    <header>Re: modern support for PICT</header>
    <body>Actually, NSImage continues to read them more or less (by delegating to a background 32 bit process that rasterizes and returns a bitmap) and the pasteboard silently presents pict data as another format, TIFF if I recall. There is no support for writing. Yes, the message is that pict is entirely deprecated. -ken</body>
  </mail>
  <mail>
    <header>Does CVDisplayLinkStop wait for its thread to exit?</header>
    <body>When CVDisplayLinkStop() is called, does it wait for its thread to finish before returning? Is it guaranteed that its thread will exit by the time the function returns? Or do I still need to maintain locks even after calling that function? Kevin _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: injected mouse click in window brings up window,	does not update  main menu</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>[ANN] PFAssistive and PFEventTaps Frameworks Released</header>
    <body>&amp;gt;. The PFAssistive Framework was created in 2003 as the engine driving the UI Browser application for developers of assistive applications and for users of Apple‚Äôs AppleScript GUI Scripting technology. The framework supports and enhances Apple's Accessibility API. The PFEventTaps Framework was created in 2007 as the engine underlying the free Event Taps Testbench utility for developers. The framework supports and enhances Apple's Quartz Event Taps API. Both frameworks have been revised and updated over a period of years, and they have demonstrated their power and reliability in these commercial and free developer utilities. The frameworks are written using Objective-C 2.0. They are Universal Binaries supporting 32-bit and 64-bit architectures using reference counted memory management. Developers can license the frameworks for distribution or use with their own products that use Apple's Accessibility and Quartz Event Taps technologies. The frameworks may be licensed free of charge for personal use or for distribution or use with a free software product. A modest license fee is required to distribute or use the frameworks with a product for which payment is requested or required. Source code is available for a separate fee. Different terms apply to large or established commercial software developers. Both frameworks come with a 34-page PDF document titled &amp;quot;Assistive Application Programming Guide&amp;quot; and detailed framework references generated by HeaderDoc. The Programming Guide and the framework references can be downloaded separately at &amp;lt;&amp;gt;. -- Bill Cheeseman - email@hidden</body>
  </mail>
  <mail>
    <header>Re: injected mouse click in window brings up window,	does not update main menu</header>
    <body>Hi, Izidor - In the meantime I discovered that the problem is when mouse down and mouse up events are not close enough (using private eventsource or NULL when creating events, does not matter). Simple test is posting mouse down to some fixed location (where you arrange to have a window), and then sleep 1 second, and then post mouse up event to same location. Window will activate, but main menu will still show previous application and will not respond to mouse clicks. If you remove sleep(1), the application will activate correctly. - (IBAction)test:(id)sender Is there any way to make this work? izidor Hi, everybody, I have an application which receives remote mouse actions and posts them using CGEventCreateMouseEvent/CGEventPost at kCGSessionEventTap level with private event source. Weird thing is that when remote user clicks inside any window, that window comes in front, becomes active (changes title bar color), and everything works inside that window, but the main menu still shows the previous application and all clicks in main menu are ignored - do not cause any action (but remote user can click on Spotlight icon in main menu bar and use Spotlight just fine). Even weirder, when remote user clicks on title bar of any window, everything works fine (main menu updates to show appropriate application, and remote user can click inside main menu). Does anybody have any idea what to check and where to look? Thanks, izidor P.S. This is the code (which executes in separate thread, but even on main thread it was the same):</body>
  </mail>
  <mail>
    <header>[ANN] Event Taps Testbench 1.2.0 Released</header>
    <body>&amp;gt;. (It was formerly distributed by PreFab Software, Inc.) Version 1.2.0 corrects a couple of cosmetic bugs and one usability bug; it includes a revised Help book; and it has new contact information. In addition, a new version of the PFEventTaps Framework is installed separately as a shared framework in /Library/Frameworks, where it is available to any application that needs it. The framework supports and enhances Apple's Quartz Event Taps API, and it declares Cocoa delegate methods to make monitoring incoming events simple. For licensing information, go to &amp;lt;&amp;gt;. Source code for Event Taps Testbench is available for download at the same URL. -- Bill Cheeseman - email@hidden</body>
  </mail>
  <mail>
    <header>Re: injected mouse click in window brings up window,	does not update main menu</header>
    <body>In the meantime I discovered that the problem is when mouse down and mouse up events are not close enough (using private eventsource or NULL when creating events, does not matter). Simple test is posting mouse down to some fixed location (where you arrange to have a window), and then sleep 1 second, and then post mouse up event to same location. Window will activate, but main menu will still show previous application and will not respond to mouse clicks. - (IBAction)test:(id)sender event = CGEventCreateMouseEvent( NULL, mouseType, position, event = CGEventCreateMouseEvent( NULL, mouseType, position, Is there any way to make this work?</body>
  </mail>
  <mail>
    <header>injected mouse click in window brings up window,	does not update main menu</header>
    <body>I have an application which receives remote mouse actions and posts them using CGEventCreateMouseEvent/CGEventPost at kCGSessionEventTap level with private event source. Weird thing is that when remote user clicks inside any window, that window comes in front, becomes active (changes title bar color), and everything works inside that window, but the main menu still shows the previous application and all clicks in main menu are ignored - do not cause any action (but remote user can click on Spotlight icon in main menu bar and use Spotlight just fine). Even weirder, when remote user clicks on title bar of any window, everything works fine (main menu updates to show appropriate application, and remote user can click inside main menu). P.S. This is the code (which executes in separate thread, but even on main thread it was the same): CGEventType mouseType =  down ? kCGEventLeftMouseDown : event = CGEventCreateMouseEvent( mySource, mouseType, CGEventSetIntegerValueField( event, kCGMouseEventClickState, CGEventSetIntegerValueField( event, kCGEventSourceUserData,</body>
  </mail>
  <mail>
    <header>CGEventPost under 10.4</header>
    <body>This issue has been driving me a little nuts lately. We have code that allows a remote system to send events to the operating system.  For example, to send a mouse down event for the left click, we'll say: //Log error message There is additional processing work to figure out which event to send, but this is the general gist of it.   This works fine on 10.6 and 10.5 and we haven't had any problems with it, but under 10.4.11 it doesn't appear to work at all.  Simply, nothing happens. I can confirm that the events are being received and processed correctly outside of the CG calls (the right event is being produced, eventRef is not null, etc), but am not sure of a good way to see where the break is after these events get sent. Some potentially relevant compilation calls are: -isysroot /Developer/SDKs/MacOSX10.4u.sdk -framework Carbon -framework OpenGL -framework ApplicationServices GCC version is 3.3, the issue happens in both PPC and i386 applications.  Though I don't know why it would, creating an explicit event source doesn't seem to help matters. Any thoughts, ideas or help are appreciated. Sincerely, David _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>CGLayerCreateWithContext always returning NULL</header>
    <body>I'm trying to write a wrapper that builds an image using Quartz but am getting nothing but NULLs from CGLayerCreateWithContext even with apparently valid contexts and sizes. I check my CGContextRef for NULL immediately after creation and it appears to be OK, but when I try to make an image layer context from it, I get NULLs. Here's the relevant part of the code, anyone see the problem? - (void) initImageLayer CGRect				rect				= CGRectMake(0, 0, selfFrame.size.width, selfFrame.size.height), swatchRect          = CGRectMake(0, rect.size.height - SWATCH_HEIGHT, SWATCH_HEIGHT, SWATCH_HEIGHT), CFMutableDataRef	consumerDataRef		= CGDataConsumerRef	consumer			= CGContextRef		context				= CGPDFContextCreate(consumer, &amp;amp;rect, NSLog(@&amp;quot;imageLayer: %@&amp;quot;, (imageLayer == NULL) ? @&amp;quot;NULL&amp;quot; : @&amp;quot;NOT /* omitted rest of code */</body>
  </mail>
  <mail>
    <header>Starting point</header>
    <body>Hi,</body>
  </mail>
  <mail>
    <header>Re: Quartz text antialiasing and nearby NSRectFill box</header>
    <body>Yes, that was the reason.   Thanks.</body>
  </mail>
  <mail>
    <header>Re: Quartz text antialiasing and nearby NSRectFill box</header>
    <body>Sounds like you're double-drawing your text. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Quartz text antialiasing and nearby NSRectFill box</header>
    <body>I'm having an issue with some graphics - a blinking cursor that is drawn next to text.  What happens is that the antialiasing of the nearby letter changes whenever the cursor (NSRectFill) appears: the fringes of the glyph in question appears darker, as if the background color was dark.  If the text is redrawn without the box present, the antialiasing goes back to normal.  (The problem does not occur when antialiasing is turned off.)  The attached screenshot shows the problem.  This is annoying and distracting because I've not just a blinking cursor, but also a blinking glyph! The cursor drawing code just sets the color and does a NSRectFill.  I think (but I don't know for sure) the text is drawn in the same graphics context. We use CGContextShowGlyphsWithAdvances() for the text.  I have tried switching off antialiasing temporarily (or even changing compositingOperation) while the box is drawn - to no avail. Drawing another box in white (the text background color) just to the right of my red cursor prevents the &amp;quot;bold&amp;quot; antialiasing; drawing something in white somewhere else doesn't help. Thanks for your help.</body>
  </mail>
  <mail>
    <header>Re: CGDisplayRegisterReconfigurationCallback with event taps?</header>
    <body>Yeah I've found that to be the case. If I use RunApplicationEventLoop(), which is part of Carbon, it works. I'm sure that if I were to use NSApplicationLoad(), which is part of Cocoa, the same would happen. I would like to avoid using Carbon/Cocoa if possible. I've been looking over the documentation and I can't seem to find a way to do it... although I'm not that experienced with the Core frameworks. Even if I had to use Carbon with RunApplicationEventLoop, I'm not sure how to use that in conjunction with my CFRunloop. The two, so far, seem completely independent of each other. An ideal solution is to not use Carbon or Cocoa and stick with the Core frameworks. If there is a way to do this, I'd really appreciate the help. Luke</body>
  </mail>
  <mail>
    <header>Re: Output of stroking a path with many points per pixel?</header>
    <body>&amp;gt; I would like to do so such that the final output is identical to the I think Apple's QuartzLines sample code shows you how to do that. Jeff</body>
  </mail>
  <mail>
    <header>Re: Problem with font display</header>
    <body>When 64-bit HIToolbox was killed off, that pretty much did it for Carbon. Even if that weren't so, the original code is so dependant on the event and device polling architecture, not to mention ancient libraries like SAT that were never fully ported, that it would require a rewrite anyway. Somehow I don't think I could've directly ported a WaitNextEvent()-based loop that used Delay() to implement the game timing :). It made more sense to go with Cocoa. I used to be a Carbon enthusiast until I did enough Cocoa, so believe me, I considered it as an option. -- Gwynne Attachment:</body>
  </mail>
  <mail>
    <header>Re: Problem with font display</header>
    <body>Have you tried looking into Carbon and/or the free pascal cross compiler? I've heard that Carbon especially shares a lot of similarities between the Macintosh Toolbox, according to Wikipedia: If you haven't, it may be a good option, especially if you're trying to port the original. Luke</body>
  </mail>
  <mail>
    <header>Re: Problem with font display</header>
    <body>I know, it surprised me too, but somehow it's fully functional all the way from 7.1 to 9.1. The original coder did an amazing job of writing compliant code. No weird tricks at all. It's a complete rewrite from scratch in Cocoa and OpenGL. I'm using the original code as a reference for algorithms and numbers, but the architecture and drawing are entirely diferent. -- Gwynne Attachment:</body>
  </mail>
  <mail>
    <header>Determine if connected display supports HDCP?</header>
    <body>Hello Forgive this slightly off topic post, but I figure Quartz-Dev is the most appropriate place to ask. Is it possible either via Apple provided software, or a public API, to somehow determine if the OS detects a connected display as supporting HDCP, or being HDCP Complaint? I am trying to determine the source of a hiccup with a 3rd party DVI display/switch and it appears to be related to HDCP, and it would help tremendously if I could know what OS X is seeing. Forgive this off topic post, and apologies if this is obvious, I've yet to find any way to determine this. Thank you in advance. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Problem with font display</header>
    <body>Luke</body>
  </mail>
  <mail>
    <header>Re: Problem with font display</header>
    <body>Unfortunately, no combination of these seemed to work. But they DID work when I combined them with NSStringDrawingUsesDeviceMetrics, NSStringDrawingDisableScreenFontSubstitution, and NSKernAttributeName of 0.1! It's not a pixel-perfect recreation of the original appearance, but it's much, much closer than anything else I've tried. Thanks very much, Derek, you're a life-saver :). -- Gwynne Attachment:</body>
  </mail>
  <mail>
    <header>Re: Problem with font display</header>
    <body>Oh no, it's much older than that. Think System 7 Toolbox. Pre-Appearance Manager, WaitNextEvent(), InitDialogs(), needing to manually declare SetDialogDefaultItem(), etc. And it's all in Pascal. -- Gwynne Attachment:</body>
  </mail>
  <mail>
    <header>Re: Problem with font display</header>
    <body>&lt;blockquote cite="" type="cite"&gt; Can anyone here on quartz-dev comment? -- Gwynne</body>
  </mail>
  <mail>
    <header>Re: Problem with font display</header>
    <body>I sent this to cocoa-dev several days ago: I'm working on porting an old application from OS 9 to modern Cocoa (quite a jump). Over and over I've run into an issue where the text just doesn't draw the same way no matter what I do. Until now I've been able to ignore it because the cosmetic change wasn't as obvious, but now it's causing difficulty. I'm a bit of a newbie when it comes to font and text manipulation, so maybe I'm missing something obvious.</body>
  </mail>
  <mail>
    <header>Re: Problem with font display</header>
    <body>I sent this to cocoa-dev several days ago: And got these replies: Can anyone here on quartz-dev comment? -- Gwynne Attachment:</body>
  </mail>
  <mail>
    <header>Re: CGDisplayRegisterReconfigurationCallback with event taps?</header>
    <body>Your code looks like it does nothing to trigger a connection with the window server. Off the top of my head I cannot recall what of the CG API will trigger this but consider calling NSApplicationLoad() early in your main method. -Shawn</body>
  </mail>
  <mail>
    <header>Re: CALayers context question</header>
    <body>Thank you David.  You are always so helpful.  =)</body>
  </mail>
  <mail>
    <header>Re: CALayers context question</header>
    <body>You can, how that interacts depend somewhat on the settings of various layers (for example if you have clipToBounds set on a layer and it has sublayers that extend outside of its bounds, they will be clipped), but it is perfectly legal to have sublayers with larger bounds than parents. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>CALayers context question</header>
    <body>I have never used CALayers, but I have been reading up on them for the past few weeks.  There is one question that I have which I haven't found a direct answer to and wanted to post it to the list. It has to do with the CALayer bounds. I would like to make an empty CALayer that is used as a container for 3 sublayers.  My question is can a CALayer have a sublayer that is larger than the superlayer, or would a larger sublayer automatically increase the bounds of the superlayer?</body>
  </mail>
  <mail>
    <header>Re: CGImage Export</header>
    <body>A bit late, but thank you for the replies. They got me on the right path and except for some scaling/positioning issues, things are working as planned. Thank you! -- Thomas C. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Issues with CIColorInvert (and possibly others)</header>
    <body>Christoph, I'd think that for one thing there is a gamma issue there. Your NSBitmapRep is very likely in a gamma 2.2 space, but CIFilters by default operate in a gamma 1 space. Sandy</body>
  </mail>
  <mail>
    <header>Issues with CIColorInvert (and possibly others)</header>
    <body>Hello, I'm currently teaching myself into Core Image and ran into a problem with CIColorInvert - but possibly this is a more general problem I fail to understand. When I apply the CIColorInvert filter to a scan of a negative film, the result appears mucht to bright, compared to result I get when inverting the same image in Photoshop (please have a look at images 2.3 and 2.4 at ual/ImageUnitTutorial/WritingKernels/WritingKernels.html - inverting them in PS leads to darker images). That's what I did: [...] // apply filter // write to disk outputImage = [[[NSImage alloc] initWithSize:[imageRep size]] I get the same result as with CIColorInvert when using a CIColorMatrix filter negating all components and adding a bias of 1.0 to all components. I even programmed my own filter, and got the same results again: kernel vec4 brightenEffect (sampler src, float k) When I look at the components histogram of the original and the inverted image, they are not mirrored but the histogram of the inverted image is distorted and shiftet to the rights (more higher value). On the other hand, if I create a NSBitmapRep from the image and invert all pixels manually, I get the same results as I get from Photoshop. So, obviously I'm missing something fundamental about how CI treats the image data - but what? Thank you very much, Christoph</body>
  </mail>
  <mail>
    <header>Re: Output of stroking a path with many points per pixel?</header>
    <body>I would not draw such data via a simple path, considering there's several hundred or thousand points per pixel. Instead, i'd draw something like: For this, you'd have to compute min/max/average/etc for all datapoints that fall within a given pixel. From that, you could either draw pointwise, per horizontal pixel, or construct appropriately connected lines for min/max/avg, which would largely draw as wanted (probably). I don't know if it'd be faster, but to me it seems like a more appropriate visualization, as you would more clearly see erratic data.</body>
  </mail>
  <mail>
    <header>Re: Output of stroking a path with many points per pixel?</header>
    <body>I meant no disrespect to the excellent Apple engineers! I am not trying to reimplement antialiasing perse. My intention is to decide what filtering algorithm to apply to my data such that the rendered output of a CGPath generated from that data is &amp;quot;close&amp;quot; (see below) to the rendered output of a CGPath generated from the unfiltered data. In this application, the data is neurophysiology traces, the voltage output of a single neuron recorded using a microelectrode. Unlike sound-pressure data, there are very few zero crossings and it is possible to view the trace meaningfully at relatively low resolution. However, there are (meaningful) large deviations in the trace and (artifactual) deviations introduced by the recording equipment that a researcher would like to see. Thus, I want to preserve these large deviations. Decimating the data (taking every nth sample) is thus not appropriate as the deviations may span only a small number of samples yet still be meaningful (yes, I know this brings up several numerical issues in analyzing the waveform etc.; there are experimental limitations that make faster sampling impossible). So, the particular case that concerns me is the following original data, where the number pairs give index and value and the vertical bars denote pixel boundaries (at some resolution): * ...  *                            * ... (i,1)     (i+1,10)     (i+2,1) |                                        | In this case, if I create a CGPath from these points and stroke it, where is the *perceived* pixel drawn (I know anti-aliasing means that many pixels will be touched)? Is the pixel drawn at y=1 (the mode of the data), y=4 (the mean), etc. by Quartz. I know this is a deep academic literature, and the cold light of day makes it clear that Apple is unlikely to divulge the algorithms they use, so I'm just looking for a little guidance. Thanks for you help, Barry</body>
  </mail>
  <mail>
    <header>Re: CIFilter and CALayer</header>
    <body>Hi Dion, I ended up doing much the same workaround for a similar issue. Seems like a lame solution, but at least it works. Cheers, Brian</body>
  </mail>
  <mail>
    <header>Re: CIFilter and CALayer</header>
    <body>I found a workaround but it isn't that pretty. I have to pass the view height into the filter when the view frame changes like this: - (void)setFrame:(NSRect)frameRect [filter setValue:[NSNumber numberWithFloat:NSHeight(self.frame)] This seems pretty ugly but it's the only way I can get it to work. Can anyone comment on this? Thanks, Dion</body>
  </mail>
  <mail>
    <header>Re: CIFilter and CALayer</header>
    <body>Maybe I should clarify my question with an example: If I write a flip kernel like this it doesn't resize correctly in QC as you make the window smaller: kernel vec4 myFlip (sampler src) Turning on &amp;quot;Show Advanced Input Sample Options&amp;quot; fixes the problem. However if I try doing the same thing in my own app in my filter class like this it doesn't work: - (CIImage *)outputImage NSDictionary* options = [NSDictionary dictionaryWithObjectsAndKeys:kCISamplerWrapBlack, kCISamplerWrapMode, Any ideas? Thanks, Dion</body>
  </mail>
  <mail>
    <header>Re: CGDisplayRegisterReconfigurationCallback with event taps?</header>
    <body>Well the only documented way of monitoring display resolution change is by the CGDisplayRegisterReconfigurationCallback function. The CGEventTapCreate function would need a mask that represented display change, like kCGEventMouseMoved is for the mouse moving. As far as I know there isn't one, unless I missed it somewhere in the documentation. I can't seem to find a way to get CFMachPortRef for CGDisplayRegisterReconfigurationCallback. The documentation is a bit confusing. Luke</body>
  </mail>
  <mail>
    <header>Re: CGDisplayRegisterReconfigurationCallback with event taps?</header>
    <body>Usually you create a run loop source and add it to the run loop before calling CFRunLoopRun. In cAse of event tap it seems you need to create a CFMachPortRef wiith CGEventTapCreate then the source then add it to the Run loop. On May 4, 2010, at 11:38 PM, Luke Scott &amp;lt;email@hidden&amp;gt; wrote:</body>
  </mail>
  <mail>
    <header>Re: CGDisplayRegisterReconfigurationCallback with event taps?</header>
    <body>CGEventRef mouseEventCallback(CGEventTapProxy proxy, CGEventType type, CGEventRef event, void *refcon) if (type != kCGEventMouseMoved) int deltaX = (int) CGEventGetIntegerValueField(event, int deltaY = (int) CGEventGetIntegerValueField(event, void MyDisplayReconfigurationCallBack(CGDirectDisplayID display, CGDisplayChangeSummaryFlags flags, void *userInfo) int main(void) CGDisplayRegisterReconfigurationCallback(MyDisplayReconfigurationCallBack, mouseEventTap = CGEventTapCreate(kCGHIDEventTap, kCGHeadInsertEventTap, 0, (1 &amp;lt;&amp;lt; kCGEventMouseMoved), mouseEventCallback, if(!mouseEventTap) mouseRunloop = CFMachPortCreateRunLoopSource(kCFAllocatorDefault, CFRunLoopAddSource(CFRunLoopGetCurrent(), mouseRunloop, Luke</body>
  </mail>
  <mail>
    <header>Re: CGDisplayRegisterReconfigurationCallback with event taps?</header>
    <body>Is this runloop in the main thread?</body>
  </mail>
  <mail>
    <header>Re: CGDisplayRegisterReconfigurationCallback with event taps?</header>
    <body>I have a runloop running for my event taps and I'm not getting the notification. Is there some way that I need to add it to the runloop? Luke</body>
  </mail>
  <mail>
    <header>Re: CGDisplayRegisterReconfigurationCallback with event taps?</header>
    <body>I am fairly sure you need to have a runloop actually running in the main thread of the application to get these notifications. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Output of stroking a path with many points per pixel?</header>
    <body>It's not clear what advantage you're hoping for, unless you think you can write a more efficient implementation of this algorithm than the one Apple engineers have spent several years optimising! I'd say what you really want is something &amp;quot;similar enough&amp;quot;. Someone examining a plot of 1e6 datapoints at a width of 500 pixels isn't looking for fine detail, so &amp;quot;identical&amp;quot; doesn't really matter. The best approach really depends on the data. An audio waveform, for instance, is a graph -- 1e6 datapoints is about 23 seconds of monophonic 44.1KHz audio -- but it contains so many zero crossings that a simple plot would give almost no information when viewed at a width of 500 pixels. Conversely, a cumulative plot is well-behaved enough that you can trivially cut the number of data points by taking a running average. What kind of data are you looking to visualise? Best wishes, Hamish</body>
  </mail>
  <mail>
    <header>Output of stroking a path with many points per pixel?</header>
    <body>I'm rendering a line plot using the Core Plot framework () for a scientific application. Core Plot does its drawing via the Quartz2D API into a CALayer. To generate the line representing the data set, a CGPath is generated with a point for each record in the line (on the order of 1e6 points for this application) and the path is then stroked into the CALayer's CGContext. Obviously this has some perf issues (it takes on the order seconds to render the plot). For a plot that is, e.g. 500 points wide, 1e6 points is clearly overkill. I'd like to prune the number of points in the CGPath (e.g. plotting only each nth point in the original data set where n is the total dataset size divided by the plot size). I would like to do so such that the final output is identical to the plot generated from plotting the entire data set (i.e. all 1e6 points in the CGPath). Can anyone point me to a reference describing how Quartz will generate a pixel from a portion of a CGPath that contains multiple points? By matching this algorithm, I could &amp;quot;pre-prune&amp;quot; the data without changing the final output. The Quartz 2D programming guide did not appear to contain this information (but I would be happy to be told I just missed it). Thanks, Barry</body>
  </mail>
  <mail>
    <header>CGLayer reuse problem</header>
    <body>We are using CGLayers to fade between two drawing states (let‚Äôs assume for this discussion that we have good reason not to use Core Animation for this). We draw the first state into a layer called sourceLayer and the second state into a layer called targetLayer (all drawings are fully opaque). When redrawing during the animation, we draw sourceLayer with alpha = 1.0 and than targetLayer with alpha = current value of the animation. This works well. If a new, third state arrives while such an animation is running, we consolidate the current state in sourceLayer and update targetLayer with the new drawing state. That is, we draw the old targetLayer with the current old animation value into sourceLayer and then redraw the contents of targetLayer with the new drawing state and start a new animation. This works if we also drop the old targetLayer and create a new layer to draw the new state into it, which then becomes targetLayer. But following the docu of CGLayer, we tried to reuse the targetLayer object by simply drawing the new state in it. In this case the result looks like as if the new contents of targetLayer was consolidated into sourceLayer instead of the old contents. Is this expected? That is, if a layer is drawn into another layer (both based on a window context), can it happen that it is not composited to the bits of the target layer immediately, but instead drawn later when compositing to the window happens? In other words, should layers be treated as immutable once their initial contents is created? Thanks Kai</body>
  </mail>
  <mail>
    <header>CGLayer Drawing Fails at Large Sizes on 10.5</header>
    <body>I've run into an issue when using CGLayers with either size component It appears that anything drawn past that point wraps around and gets drawn at (destination - 65535) also.  This happens on 10.5 only, the issue seems to be fixed in 10.6.  However, we're still supporting 10.5 so I'm looking for some sort of workaround for this issue.  We're using a CGLayer to cache content we're drawing and because it can contain large images that have been run through some CIFilters, it needs to be this big. Here is a project reproducing the issue on 10.5: The code fills a view with red, draws a green box past the 65535 threshold in a CGLayer, then draws the layer in the view.  On 10.5, you can see that the green square is repeated towards the bottom of the view as well as where it should be at the top.  On 10.6, it renders fine.  Below is the code for anyone that wants to just slap it in a new project on their own (just put this view in a scrollview). Any advice would be appreciated. -(void) awakeFromNib CGLayerRef theLayer = CGLayerCreateWithContext([[NSGraphicsContext currentContext] graphicsPort], NSSizeToCGSize([self bounds].size), [NSGraphicsContext setCurrentContext:[NSGraphicsContext graphicsContextWithGraphicsPort:CGLayerGetContext(theLayer) CGContextDrawLayerAtPoint([[NSGraphicsContext currentContext]</body>
  </mail>
  <mail>
    <header>Re: [SOLVED] Hit testing a bezier curve</header>
    <body>This is where the technics I exposed to you earlier come into play. Raphael</body>
  </mail>
  <mail>
    <header>Re: Hit testing a bezier curve</header>
    <body>Thanks but I was hoping to get an answer to the method I want to try. It will be useful for when I want to change the curve as well.</body>
  </mail>
  <mail>
    <header>Re: Hit testing a bezier curve</header>
    <body>&amp;gt; I'm trying to do some hit testing on a bezier curve.  The method I plan on You may also draw to a bitmap context with size the radius of your hit, and draw in that buffer and test the buffer, or make a 1x1 bitmap context and draw your bezier path with the radius of your hit. Hope this will help, Raphael</body>
  </mail>
  <mail>
    <header>Hit testing a bezier curve</header>
    <body>I'm trying to do some hit testing on a bezier curve.  The method I plan on using is to get the line segments of the curve and do hit testing on the these line segement.  Does NSBezierPath curveToPoint: expose these somehow? I've reviewed the Quartz Pgm Guide?  If there are some other readings I missed, please let me know. Again, thanks -Tony</body>
  </mail>
  <mail>
    <header>Re: Mixing NSNSBezierPath calls with CGContext... calls</header>
    <body>NSBezierPath is (probably) constructing a CGPathRef internally.  That path is not added to the current context while you are building it. When you ask NSBezierPath to stroke it's path, it probably does something like: - (void) stroke In other words the path is added to the current context ONLY long enough to stroke it and this is all done internally by the stroke method.  Your code would have no way to jump into the middle of that and measure the path. You could either: 1)  Ask the NSBezierPath for the bounds of the path it has constructed 2) Use CGPathRef instead of NSBezierPath and take control of the whole operation. Scott</body>
  </mail>
  <mail>
    <header>Hit Testing within a view that has been transformed</header>
    <body>I would like some advice on the best approach to do hit testing when a image has been translated(translateXby: yBy:). I have a view and I draw a background the full bounds of the view.  Then, for example, I translateXby:10 yBy:10 and draw a square at the new coordinates.  I also assume my new bounds is (0,0) by (oldbounds-20, oldbounds-20).  Hopefully this is straight forward. Now, I if I do hit testing I need to account for the translations because the mouse position will be based on the view's coordinates. These are my thoughts on how to deal with this. 1. Add the offset to the point I am hit testing(this makes it relative to the actual view bound).  Works fine in a test app. 2. Don't do a transform, add a subview within the view to hold the square. Totally avoids the problem. Are there any other methods that I may have missed that are documented somewhere? Thank, -Tony</body>
  </mail>
  <mail>
    <header>Re: CIFilter and CALayer</header>
    <body>Hi Dion, First off, if you're flipping an image you may want to consider using the built-in method for doing this: - (CIImage *)imageByApplyingTransform:(CGAffineTransform)matrix but ... if you want to do this with a kernel of your own then you will need to specify an ROI function for your filter: - (CGRect) regionOf:(int)samplerIndex destRect:(CGRect)r userInfo:obj then in your outputImage method make sure to set the roi method: and when you call apply: To make a long story short, anytime you sample an image from a location != the current destCoord() then you will need to supply an ROI function otherwise you are likely to end up with an incorrect result. In QC you can do this as well via the &amp;quot;Edit Filter Function&amp;quot; dialog that shows up when you edit a Core Image Filter. Just look at the docs for how to create your own custom ROI function and pass in the appropriate data for the original image size. Hope this helps, Alex.</body>
  </mail>
  <mail>
    <header>Re: Mixing NSNSBezierPath calls with CGContext... calls</header>
    <body>Actually, it should not, as stroking, filling and clipping the current path clears it. You could do some work manually to generate a CGPathRef from the NSBezierPath and do your work based on that, but it probably isn't worth it. -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Mixing NSNSBezierPath calls with CGContext... calls</header>
    <body>Ah ok, so the call should succeed after the stroke command, which it does not.  The controlPointBounds works as expected. &amp;quot;Definitely take a look at the documentation, its all there :).&amp;quot;  Yep, it's all there, just like putting a document together that the NSA has shredded back together.  All kidding aside, I really appreciate the replies! -Tony</body>
  </mail>
  <mail>
    <header>Re: Mixing NSNSBezierPath calls with CGContext... calls</header>
    <body>Sorry, I thought I might have been unclear after I read what I wrote :). Attempts to stroke, fill or clip to the path ‚Äì basically anytime that the NSBezierPath needs to affect the current context. Definitely take a look at the documentation, its all there :). -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: Mixing NSNSBezierPath calls with CGContext... calls</header>
    <body>"until you attempt to use it'.&amp;nbsp; Does this mean post drawRect call(is it possible to get the same CGContext as the path was drawn into this late)? As to your other questions, I am coming from the Windows world so I don't know all the methods yet.&amp;nbsp; Those would certainly do the trick, thank you. Thanks so much, -Tony &lt;blockquote cite="" type="cite"&gt; I am trying to use CGContextGetPathBounds in my drawRect method after I draw a NSBezierPath. &amp;nbsp;The curve draws just fine, however, after the call to CGContextGetPathBounds, I get the CGRectNull returned. &amp;nbsp;The debug console prints an error message telling me 'no current point'. &amp;nbsp;Reading the documentation on stroke, it states it clears the current path. &amp;nbsp;So, I assumed you need to make the call to CGContextGetPathBounds prior to the stroke(however, this makes no sense to me because then the method is only good when drawing so I think I have an error in my assumption). &amp;nbsp;Calling it after the stroke also leads to the same error message. &amp;nbsp;Hence, I think there may be a problem mixing CG calls with object calls. &amp;nbsp;Maybe I don't have the right context? &amp;nbsp;Here is the snippet.&lt;font class="Apple-style-span" color="#000000"&gt;</body>
  </mail>
  <mail>
    <header>Mixing NSNSBezierPath calls with CGContext... calls</header>
    <body>I am trying to use CGContextGetPathBounds in my drawRect method after I draw a NSBezierPath.  The curve draws just fine, however, after the call to CGContextGetPathBounds, I get the CGRectNull returned.  The debug console prints an error message telling me 'no current point'.  Reading the documentation on stroke, it states it clears the current path.  So, I assumed you need to make the call to CGContextGetPathBounds prior to the stroke(however, this makes no sense to me because then the method is only good when drawing so I think I have an error in my assumption). Calling it after the stroke also leads to the same error message. Hence, I think there may be a problem mixing CG calls with object calls.  Maybe I don't have the right context?  Here is the snippet. ... [bp curveToPoint:NSMakePoint(1.0, 1.0) controlPoint1:NSMakePoint(cp1X, cp1Y) controlPoint2:NSMakePoint(cp2X, NSLog(@&amp;quot;Path Bounding Box: origin.x:%1.5f origin.y:%1.5f size.width:%1.5f size.height:%1.5f&amp;quot;, br.origin.x, br.origin.y, Thanks, -Tony</body>
  </mail>
  <mail>
    <header>NVIDIA Float Textures</header>
    <body>I'm having no luck at all rendering to a 128-bit floating point pbuffer (CGLCreatePBuffer) on NVIDIA GPUs.  All of the problem GPU's report renderer version &amp;quot;1.5 NVIDIA-1.4.18&amp;quot;. Most of them are NV34MAPs, but there's also a GeForce 7800 GT that's giving problems.  All of the NV34's have a render ID of 0x22402 while the 7800 has a render ID of 0x2240a. If it was just the NV34's that were problematic, I'd probably just ditch support for them, but the 7800 GT is a nice GPU, and it should work fine. I've tried every combination of target and internalFormat I can think of to create the pBuffer.  Most of the experimentation has centered around GL_TEXTURE_RECTANGLE_EXT and either GL_RGBA32F_ARB or GL_FLOAT_RGBA32_NV. My pixel format is: kCGLPFAAccelerated, kCGLPFANoRecovery, kCGLPFAColorFloat, kCGLPFAColorSize, 128, kCGLPFAPBuffer I create a CIContext from a CGLContextObj that is bound to my pBuffer and render a CIImage into it.  I then read the data back into system memory using I've tried playing around with other enumerants instead of GL_RGBA and had no luck. The resulting buffer is always filled with zeros on the problem GPUs.  Things work fine on all other GPUs that I've tested.  No errors are logged and I'm checking everything I can think to check. Attachment:</body>
  </mail>
  <mail>
    <header>Font rendering tool</header>
    <body>I've written a small tool (and .app) that renders text on demand and saves the result to an image file. Typical uses is rendering text from a database to use on the web. My questions is, does anybody know of any thing like this. I seem to remember a example tool from Apple that did something like this, anything else? Like this involves technologies like XML(-RPC) and font rendering using Mac OS X core functionality like ATSU, Quarts or a like. / Johan --- Johan Malmstrom www.johanmalmstrom.se</body>
  </mail>
  <mail>
    <header>Re: Offscreen bitmaps and colorspaces</header>
    <body>Well, I'm coding on 10.4.8, but I'd like to still support 10.3.x. James ---</body>
  </mail>
  <mail>
    <header>Re: Offscreen bitmaps and colorspaces</header>
    <body>On Oct 30, 2006, at 12:10 PM, James Thomson wrote: Which OS are you running on? In Tiger, you could probably use CGLayers instead of a CGBitmapContext/CGImage pair to help you side-step the issue. When you create the CGLayer, use a screen context as the model for the layer and all your offscreen contexts should be color matched.</body>
  </mail>
  <mail>
    <header>Offscreen bitmaps and colorspaces</header>
    <body>I'm currently working on an application that uses a lot of offscreen bitmaps. Typically, I'll create a bitmap context, draw my time- consuming content into it, then create an image from the context for fast drawing later on. Exact color matching isn't really important to my application. Previously, I've used CGColorSpaceCreateDeviceRGB to make a colorspace for the bitmap context and the image. I gather this is not recommended, and QA1396 suggests using CMGetSystemProfile and CGColorSpaceCreateWithPlatformColorSpace instead to create a colorspace that matches the main display. According to the Q&amp;amp;A, this should avoid unnecessary color matching when drawing and give the best performance. The problem is, if I call the CreateSystemColorSpace sample code in the Q&amp;amp;A instead of CGColorSpaceCreateDeviceRGB, CGContextDrawImage then spends over half of its time in &amp;quot;resolveICCBased&amp;quot; by way of CGColorTransformConvertNeedsCMS, which suggests the exact opposite is happening. If I just call CGColorSpaceCreateDeviceRGB, things are much faster. James ---</body>
  </mail>
  <mail>
    <header>Re: Converting a CGImageRef to a JPG</header>
    <body>On Oct 28, 2006, at 4:07 AM, Jerry wrote: A QuickTime Graphics exporter is fine on Mac OS X versions prior to Tiger.  On Tiger and Later you should use Image I/O. Conceptual/drawingwithquartz2d/dq_data_mgr/ Basically you want to create an image destination using one of the CGImageDestinationCreateWithXXX routines.  Then you can add one or more CGImages to it with CGImageDestinationAddImage.  You then call CGImageDestinationFinalize and Image I/O will write the images to the file. This mechanism supports the same image formats that QuickTime supports and more. dp/0123694736</body>
  </mail>
  <mail>
    <header>Re: Core Image ignoring antialiasing/interpolation settings?</header>
    <body>I have the same problem when drawing in an OpenGL view. Disabling GL_LINE_SMOOTH or setting shade mode to GL_FLAT doesn't help. The image is antialiased even at the 1:1 scale. I'm using the SampleCIView from devtools examples. And when the windows is being resized, sometimes the image is not antialiased.</body>
  </mail>
  <mail>
    <header>Re: Converting a CGImageRef to a JPG</header>
    <body>On 27 Oct 2006, at 20:13, R R Hornback wrote: This is very easy to do with a QuickTime graphics exporter and GraphicsExportSetInputCGImage.</body>
  </mail>
  <mail>
    <header>Re: Converting a CGImageRef to a JPG</header>
    <body>On Oct 27, 2006, at 12:13 PM, R R Hornback wrote:</body>
  </mail>
  <mail>
    <header>Converting a CGImageRef to a JPG</header>
    <body>I'm new to Quartz/Core Image and need to create a JPG from a CGImageRef / NSImage. R.R. email@hidden</body>
  </mail>
  <mail>
    <header>Re: trouble with CIImageProvider</header>
    <body>Ok thanks. Then it's harder than I thought to use CIImageProvider. I'll have to find a workaround.</body>
  </mail>
  <mail>
    <header>Access window title of any window on focus</header>
    <body>Hi, I am very new to apple development community; the problem I like to solve is the following: I would like to access the window title of any window, which is currently on focus. I solved this problem for X11 under Linux, but I do not have any idea how to implement it on mac os x. Any help/hint/comment is appreciated very much! Thanks a lot, raimund moser</body>
  </mail>
  <mail>
    <header>Re: API's for fast screen grabs</header>
    <body>'man screencatpure' in a Terminal window will also inform you about another option.</body>
  </mail>
  <mail>
    <header>Core Image ignoring antialiasing/interpolation settings?</header>
    <body>Could it be that Core Image is ignoring antialiasing/interpolation settings? Here is a snippet of my code: [[NSGraphicsContext currentContext] setImageInterpolation:NSImageInterpolationNone]; //disable interpolation [[NSGraphicsContext currentContext] setShouldAntialias:NO];							// disable antialiasing [output drawInRect:zoomedRect fromRect:outputRect operation:NSCompositeSourceOver fraction:1.0];	//draw image output is image from CIImageAccumulator. ZoomedRect is about 16 times bigger than outputRect and it scales fine, but the problem is that the zoomed image seems to be interpolated. Is there a workaround for this? I've tried to use NSAffineTransform instead, but it didn't seem to help. Anyone knows what's happening? Regards, Aidas</body>
  </mail>
  <mail>
    <header>Re: API's for fast screen grabs</header>
    <body>On Oct 26, 2006, at 8:31 AM, R R Hornback wrote: See the list archives:</body>
  </mail>
  <mail>
    <header>API's for fast screen grabs</header>
    <body>I know this topic has been visited in the past, but I haven't been able to find a definitive answer. I've seen different methods posted for capturing screen images but all seem to have their caveats: uses unpublished API's slow uses deprecated API's and mehods might break from one version of OS X to another etc... Basically I'd like to be able to very quickly capture screen images for functionality similar to VNC.  I've seen mentions of pure Cocoa ways to do this, using Quartz &amp;amp; Core Image, using Quicktime, using OpenGL and even QuickDraw. Is there a sanctioned way to do this from Apple?  I would need the code to be fast, using current API's and not be at risk from breaking from one release to the next. Can anyone point me in the right direction?  Any example code would be great as well. R.R. Hornback</body>
  </mail>
  <mail>
    <header>Re: trouble with CIImageProvider</header>
    <body>The bytes per row values are rounded up to an efficient value for the underlying hardware, so you can't assume that it will be the width multiplied by the bytes per pixel, On Oct 20, 2006, at 9:28 AM, Fabio wrote:</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreateImage on 10.3.9?</header>
    <body>If it's not in the headers for that release, you should consider it unsupported as the safest bet. I would take extra precautions to avoid using the API on 10.3.x. -- David Duncan Developer Technical Support WWDR email@hidden</body>
  </mail>
  <mail>
    <header>Re: trouble with CIImageProvider</header>
    <body>Has anyone ever succeeded in using CIImageProvider ? I couldn't find any sample code... help ! thanks Fabio</body>
  </mail>
  <mail>
    <header>CGBitmapContextCreateImage on 10.3.9?</header>
    <body>CG_EXTERN CGImageRef CGBitmapContextCreateImage(CGContextRef c) Rather than doing a system version test, my code tests to see if the function pointer is non-NULL before using it. To my surprise, when I tested my code on 10.3.9, it went through the &amp;quot;10.4 and later&amp;quot; code path. Was this API exported but unsupported on 10.3.9? Should I take additional steps to avoid using it, or is it usable on that OS release? Thanks, Jim</body>
  </mail>
  <mail>
    <header>re: Quartz optimizations for alpha value?</header>
    <body>I seriously doubt about it. The math here is very simple and imho that doesn't need any optimisation and it won't work any slower if you have alpha values different from 1.0. This is what I think, but of course I can be wrong. Fabio</body>
  </mail>
  <mail>
    <header>trouble with CIImageProvider</header>
    <body>I'm creating a CIImage like this: ******* CIImage      *ci=[CIImage imageWithImageProvider:myProvider size:w:h ******* ; function I have a printf: ******* //where &amp;quot;w&amp;quot; is the same as the one provided in the image creation ******* But the result is always different from 4 (= number of samples: A+R+G +B) except when the width and height are 16x10. Why ? Of course the created image is not the expected result, again except when the size is 16x10. 16x10 --&amp;gt; 4.00 (OK) 9x9 --&amp;gt; 7.111 (why??) 25x25 --&amp;gt; 5.12 Any help appreciated, thanks Fabio</body>
  </mail>
  <mail>
    <header>Re: drawing a shape with holes</header>
    <body>On Oct 19, 2006, at 7:24 AM, Niels Bogaards wrote: If the holes don't overlap and are completely contained in the outer shape, I'd expect an even/odd fill to work fine.  However, if the even/odd fill doesn't work for you, your other choice is the non-zero winding fill.  For this you'll want to draw your container object in one direction (e.g., clockwise) and the holes in the other direction (e.g., counter-clockwise). Nick</body>
  </mail>
  <mail>
    <header>drawing a shape with holes</header>
    <body>I want to draw a high-order polygon with many polygonal holes in it. The holes do not overlap. (so swiss cheese rather than donut). I tried creating paths for all the polygons, adding them all to the context and filling the total path with the CGContextEOFillPath and CGContextFillPath functions, but this seems to always keep some holes filled. I also tried the approach detailed by Matt Gough on July 12th, but can't get this to work either. Does anyone have an idea about what I should be doing? Niels</body>
  </mail>
  <mail>
    <header>Re: Bug mixing 16 bits images with other modes on Intel GMA chipset</header>
    <body>This may be because the Intel GMA 950 doesn't support floating point textures.  Which is sooooo 2002... On Oct 18, 2006, at 11:58 AM, Santiago (Jacques) Lema wrote: Attachment:</body>
  </mail>
  <mail>
    <header>Re: Re: CGImageRef to CIImage -options dictionary -single channel	images</header>
    <body>-- ---- Bruce Johnson email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGImageRef to CIImage -options dictionary -single channel images</header>
    <body>Yes it is very interesting that you mention vImage, because that is exactly what I'm doing. I use vImageConvert_16UToF for convertion so I have a single channel float image (PlanarF) It is very fast, but the CGBitmapContext is still an issue. -- ---- Bruce Johnson email@hidden</body>
  </mail>
  <mail>
    <header>Bug mixing 16 bits images with other modes on Intel GMA chipset</header>
    <body>I am not sure if I can report this bug to Apple because I don't have the hardware required to write a simple example to demonstrate it, but I'll report it here in the hope that someone will care. In my app (ChocoFlop) I paint on an accumulator several times rather fast. I have a target accumulator which then gets mixed in the appropriate mode. I always paint in black and white and use this as a mask to either fill with flat color or anything else. The importtant part: The brush itself always comes from a RGBA16 accumulator, even if the rest of the document is done in other modes (8 bits or 32 bits float). This works very well using either software rendering (including on Intel GMA of course) or hardware rendering on any other system (macbook pro, intel iMac). The only case where it works on the MacBook (non-pro) and the hardware-rendering on Intel GMA is when I paint on a 16 bit accumulator, Otherwise only software rendering is OK. Here is a link to the results users get when not using 16 bit+ 16 bit mixing (eg. 16 bits brush accumulator on an 8 bits document). rendering/ You can of course try this for yourself by getting Chocoflop on chocoflop.com (just create a new document and use the brush to paint. ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Re: Re: CGImageRef to CIImage -options dictionary -single channel	images</header>
    <body>I haven't tried what you are attempting to do so I cannot answer what you are asking but if you do need to pad things then consider using the conversion operations that vImage provides. They will do the data processing in the way best suited for the hardware you are running on. -Shawn</body>
  </mail>
  <mail>
    <header>Re: CGImageRef to CIImage -options dictionary -single channel images</header>
    <body>Re: On Oct 18, 2006, at 10:15 AM, Bruce Johnson wrote: There is a typo in that document for the grayscale destination. For floading point data you want 32 bits per pixel, 32 bits per component, as you would expect. David</body>
  </mail>
  <mail>
    <header>Need Simple Conversion: BGRA Bitmap -&amp;gt; CIImage</header>
    <body>CIImage Hi -- I would like to know the cheapest way to convert a BGRA bitmap to a CIImage. NSData* imageNSData = [NSData dataWithBytesNoCopy: mInputBuffer-&amp;gt;mBaseAddress &amp;nbsp;&amp;nbsp; length: imgSizeInBytes &lt;font color="#890051"&gt; NO CIImage* image = [CIImage &amp;nbsp;&amp;nbsp; imageWithBitmapData: imageNSData &amp;nbsp;&amp;nbsp; bytesPerRow: rowBytes &amp;nbsp;&amp;nbsp; format: kCIFormatARGB8 &amp;nbsp;&amp;nbsp; colorSpace: This works fine when running on G4 computers where my bitmaps are ARGB. But under intel machines, where the bitmap is BGRA it fails. This is to be expected because of the kCIFormatARGB8 parameter. CVReturn cvErr = ::CVPixelBufferCreateWithBytes( NULL height // image format // base address of image NULL&lt;font NULL&lt;font NULL&lt;font // pixel buffer attributes Assert_(pixels !=&lt;font But it actually gives the same result as above, even though CVPixelBufferCreateWithBytes accepts the image format parameter as BGRA. So is there a simple and fast way to convert from a BGRA bitmap to a CIImage? -- &lt;div &gt;====================================================================&lt;span &gt;=== &amp;nbsp;Mark Coniglio&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;span &gt;&amp;nbsp;&amp;nbsp; |&amp;nbsp; email@hidden &amp;nbsp;TroikaTronix&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;span http://www.troikatronix.com/ &amp;nbsp;Isadora¬Æ&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;span http://www.troikatronix.com/isadora.html &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;span &gt;&lt;/span &gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;span &gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; |&amp;nbsp; (interactive media processing environment) =====================================================================&lt;span &gt;</body>
  </mail>
  <mail>
    <header>Rendering fonts without activating them</header>
    <body>Greetings, first off excuse me if this is not the right mailing list for text and font questions. If so, please help me locate the right one. I need to render a string in a few thousands fonts. I don't want to install/activate them because that would slow down the system and show them in the Fonts window. I just need to use them once. Does ATS support on-the-fly font loading? Or should I use Freetype which allows me to do it? Thank you, - alessandro ranellucci.</body>
  </mail>
  <mail>
    <header>Re: How to set the alpha bit of 16-bit 1555 bitmap image to 0</header>
    <body>On Dec 14, 2006, at 8:41 PM, Cotton Chen wrote: I would actually capture the image in ARGB8888 then convert it to ARGB1555 using vImage:</body>
  </mail>
  <mail>
    <header>How to set the alpha bit of 16-bit 1555 bitmap image to 0</header>
    <body>I would like to send the captured 16-bit image to other machine, which could only recognize 16-bit color as 1555 format and the alpha(1) bit should be set to zero. The alpha bit of the image I get from Tiger is always '1', so I set the bit to zero by doing AND operation to the pixels and 0x7FFF, of course, it's not efficient. Is there any way to set the alpha bit of 16-bit, 1-5-5-5 format, image to zero more efficiently? Thanks. -- Nogoal</body>
  </mail>
  <mail>
    <header>ImageIO and alpha channel</header>
    <body>When I use ImageIO to convert from JPEG to TGA, I end up with an image with an alpha channel.  Is there any way to avoid that?  I tried passing an option dictionary to CGImageDestinationAddImageFromSource saying kCGImagePropertyHasAlpha = kCFBooleanFalse, but it didn't help. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: Quartz-dev Digest, Vol 3, Issue 263</header>
    <body>Thank you so much. I use timer to achive the purpose. reference : On 12/14/06, email@hidden -- Cotton Chen email@hidden Á≠âÂæÖÈ¢®Ëµ∑ÁöÑÊôÇÂÄô</body>
  </mail>
  <mail>
    <header>Re: HIViewSetVisible and HideControl/ShowControl</header>
    <body>The header Controls.h clearly states that HideControl and ShowControl are not thread safe. You must send a message to your main thread to make virtually all toolbox calls there. Also, this type of question is more appropriate for the carbon-dev mailing list than the quartz-dev one.</body>
  </mail>
  <mail>
    <header>HIViewSetVisible and HideControl/ShowControl</header>
    <body>I would like to make an image change the show and hide status every second, so I create a thread to do so. The result is very strange. The codes are appended. The image did not change it's status, hide and show. But, if I kept moving the mouse, the image hided and shown as I hoped. are printed out for every second. It seems that HideControl and ShowControl do not react immediately. How can I hide the control as I wish? =================================================== void * funUIThread( void *param) while (g_bUIThread) if ( show) else void SetUIThread() // create a thead to receive device return command if ( 0 != pthread_create(&amp;amp;g_hUIThread, &amp;amp;attr, funUIThread, NULL) ) void StopUIThread() -- Cotton Chen email@hidden Á≠âÂæÖÈ¢®Ëµ∑ÁöÑÊôÇÂÄô</body>
  </mail>
  <mail>
    <header>Re: Convert DPI of a Image (Already in memory)</header>
    <body>My first thought it to simply draw it into a new CGBitmapContext that is the size you want it to be. So if the original image is 100x100pixels at 300x200dpi, then create a bitmap that's 67x100 (pixel size / orig dpi * new dpi), set its CTM scale to the math right. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Convert DPI of a Image (Already in memory)</header>
    <body>On Dec 8, 2006, at 12:31 AM, KevinGu wrote: What are you trying to do?  If the image is already in memory, resolution really just an artificial concept used to determine how large a destination rectangle to use when drawing the image.  For example, to double the resolution just make the destination rectangle half the original size. If you're exporting the image to a file, you need to tell the exporter what resolution to save in the image header.  The procedure for this is different between ImageIO and QuickTime. Nick</body>
  </mail>
  <mail>
    <header>Convert DPI of a Image (Already in memory)</header>
    <body>I have an app which does some image processes, one of which is to convert the DPI of the image. For example, from 300DPI * 200DPI to 200DPI * 200DPI. Before this step, the image is already in the memory.(bitmap data without head).</body>
  </mail>
  <mail>
    <header>Re: Get PICT resource directly from NIB file by Quartz API ?</header>
    <body>On Dec 7, 2006, at 1:26 PM, Nick Nallick wrote: In terms of size, things had the opposite effect: Total PICT size: 5.1 MB Total PDF size : 3.8 MB Furthermore, the Cocoa flavor added quite a few new images.  Just a guess, but if I were to remove those, it would hover around the 3.0 MB mark. For associated images of equal size, I used 'tiled' images in both apps (i.e. a PICT or PDF with say 2 x 5 tiles).  All my PDFs are also single-page. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: Get PICT resource directly from NIB file by Quartz API ?</header>
    <body>this will read a .pict or .pct file and return a CGImageRef.   (error handling has been removed) Make sure you add the QuickTime.framework to your project. //---------------------------------------------------- CGImageRef LoadImageQT(const FSRef&amp;amp; imageFile) if (FSIsFSRefValid(&amp;amp;imageFile)) err = QTNewDataReferenceFromFSRef(&amp;amp;imageFile, 0, &amp;amp;dataRef, if (NULL != dataRef) There is a way to use PICT files with IB, but it's not recommended. I have since switched to png and pdf.</body>
  </mail>
  <mail>
    <header>Re: Get PICT resource directly from NIB file by Quartz API ?</header>
    <body>On Dec 7, 2006, at 12:03 PM, Ricky Sharp wrote: To me a few weeks seems like a lot of time not to be working on new features, but you know your situation better than I.  I may be the last of the &amp;quot;small is beautiful&amp;quot; types, but I'd have to imagine you increased the size of your app quite a bit.  PDF isn't typically that good for a lot of small images. I guess I'm more of a &amp;quot;don't fix what's not broken&amp;quot; kind of guy. PICT probably isn't the best choice for new work but I don't much believe in change for it's own sake. Nick</body>
  </mail>
  <mail>
    <header>Re: Get PICT resource directly from NIB file by Quartz API ?</header>
    <body>When moving off of Carbon (all PICT-based images), I went almost exclusively to PDF.  This move has paid off big-time.  It didn't take me too long to move approx 400 PICT resources to PDF.  I'd say maybe a few weeks total of work. I wouldn't read it as deprecating vector in favor of bitmaps.  In all of the res-ind articles I've read, it's actually suggested to use vector-based formats.  If you must use bitmaps, be prepared to offer either separate files, or better, multi-res files. I look at PICT is the old-style container.  It served its purpose, but now it's time to move to more modern containers. -- Rick Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: Get PICT resource directly from NIB file by Quartz API ?</header>
    <body>On Dec 7, 2006, at 10:17 AM, David Duncan wrote: Maybe, I guess it depends on the situation.  It's true, you don't have to use the &amp;quot;vector&amp;quot; PICT path (e.g., QDPictRef), you can use the bitmap approach via ImageIO (or QuickTime since I'm not generally in a position to require users to have the most current revision of the OS).  However for bitmaps, unless you need alpha there's no particular benefit to TIFF or PNG vs. PICT. If I had hundreds of PICT images that are working fine in a resource file, I'm not convinced I'd be &amp;quot;better served&amp;quot; by investing the time to move them to a more &amp;quot;modern&amp;quot; format.  For many things, PICT images in resources files work as well or better than TIFF images in nibs. It's actually kind of ironic, given the move to more resolution independence in the UI, that we're talking about the deprecation of a lightweight vector format in favor of bitmaps. Nick</body>
  </mail>
  <mail>
    <header>Re: Get PICT resource directly from NIB file by Quartz API ?</header>
    <body>True, but generally you are better served using more modern image formats regardless. And if you really must support PICT, then ImageIO will better insulate you from the exact format of your image, as well as giving you a CGImageRef for your image, which you can use in a lot more places than a QDPictRef. The only disadvantage is ImageIO requiring 10.4 or later. -- David Duncan Apple DTS Quartz and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Re: Get PICT resource directly from NIB file by Quartz API ?</header>
    <body>On Dec 7, 2006, at 7:54 AM, Scott Thompson wrote: Technically this is incorrect.  QuickDraw is deprecated but PICT lives on via the QDPictRef API family.  For example, you can continue to maintain a collection of PICT resources and image them without using any deprecated functions. Nick</body>
  </mail>
  <mail>
    <header>Re: Get PICT resource directly from NIB file by Quartz API ?</header>
    <body>On Dec 7, 2006, at 3:55 AM, Cotton Chen wrote: The short answer is No. PICT Images and QuickDraw are deprecated technologies.  You would be well served by converting your PICT images into another format (say PNG) and using those instead. See: NIB files, in general, do not store images.  Actually, let me change that... Cocoa NIB files can store images, but it's not recommended. I don't believe that Carbon NIB files even have that ability. Instead, you should place the images in your resources folder and get at them using the mechanisms of CFBundle. CFBundleRef/Reference/reference.html#//apple_ref/c/func/</body>
  </mail>
  <mail>
    <header>Get PICT resource directly from NIB file by Quartz API ?</header>
    <body>-- Cotton Chen email@hidden</body>
  </mail>
  <mail>
    <header>Re: When do you have hardware for Quartz &amp;amp; Core Imaging?</header>
    <body>On Dec 5, 2006, at 5:16 PM, Steve Sheets wrote: ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: When do you have hardware for Quartz &amp;amp; Core Imaging?</header>
    <body>On Dec 5, 2006, at 4:16 PM, Steve Sheets wrote: I'm not sure this will really solve your problem, but you can use CGDisplayUsesOpenGLAcceleration(). Nick</body>
  </mail>
  <mail>
    <header>Re: When do you have hardware for Quartz &amp;amp; Core Imaging?</header>
    <body>Frank, Yes, the complexity of the Core Image operation vs speed of GPU/ CPU is always a balancing act. However, isn't there a way to just identify if there is a GPU? Or more specifically, if there is hard acceleration on the graphic card? For awhile, Quartz Extreme required this hardware, so they must have correctly identified the computers configuration.</body>
  </mail>
  <mail>
    <header>Re: When do you have hardware for Quartz &amp;amp; Core Imaging?</header>
    <body>This cannot easily be answered in an API call as the complexity of the your Core Image operation as well as the size of the images plays a large role in how &amp;quot;smooth&amp;quot; an operation is. So depending on what you are doing the answer will be different on different graphic cards and CPU's</body>
  </mail>
  <mail>
    <header>When do you have hardware for Quartz &amp;amp; Core Imaging?</header>
    <body>Here is a simple question. How does a program identify if that computer the program is running on has sufficient GPU power to run a Core Image application smoothly? Is there a configuration call to identify if the Mac has a powerful graphics card? One sufficient for most Core Image transitions? All the Quartz &amp;amp; Core Image API run on all Mac (assuming you have the correct Mac OS). If the computer has the correct graphic card, the processing of the image is handle by the card. However, if the code is running on an older machine, with slower processor and no graphic acceleration hardware, then the CPU tries to do the best it can. This is adequate for most Quartz calls, but painfully slow when doing Core Image transition.  For example, when I run my code on my older G3 (yes, there are lots of those machine out there still), the transition effect is completely useless. I have seen programs, mostly games, that actually has the user decide if the hardware can handle the better animation effect. The Options menu of the game has the checkbox &amp;quot;Slower Graphics&amp;quot; or something similar, to indicate the code will not use Core Image effects. I find this approach good enough for a fall back option, but I would prefer to have the program identify if the computer has graphic acceleration hard.   For example, Quartz Composer only runs on certain hardware configurations. Steve Sheets email@hidden</body>
  </mail>
  <mail>
    <header>Re: Extending Quartz</header>
    <body>Greetings Will, As far as the soft masks are concerned, I typically let Core Image handle specific aspect.    I realize that it is a weird work around, but it does have some speed advantages. The portion about Quartz parsing the PDF data, I tend to agree with you.   However, I have not yet had Quartz write back to the file. Only places I have allowed Quartz to write to is the screen or the clipboard. Dan Beatty, M.S. CS (B.S. EECS) Ph.D. Student Texas Tech University email@hidden On Dec 4, 2006, at 2:48 PM, Haroon Sheikh wrote:</body>
  </mail>
  <mail>
    <header>Re: Extending Quartz</header>
    <body>There really is no supported way to augment Quartz. Best suggestion would be to file a feature request using bugreporter for the specific extensions you need and document why you need them.</body>
  </mail>
  <mail>
    <header>Re: transforming/releasing/appending CGPDF...</header>
    <body>If he has 300-500 images, he'll likely see a noticeable performance impact in the Resource Manager too. I personally would never consider putting 300-500 images in a single PDF as my applications primary resource format. However, if you can reduce this to a number of working sets of images that are usually loaded together, then break them out into separate PDFs, then you should see a performance improvement. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: transforming/releasing/appending CGPDF...</header>
    <body>Off topic, but I'd go ahead and use a resource file to keep the PNG images in.  The resource manager has not been deprecated... just the PICT format. Attachment:</body>
  </mail>
  <mail>
    <header>Re: Re: transforming/releasing/appending CGPDF...</header>
    <body>Good point. I am experimenting right now on how best to handle this kind of things. We haven't finalized on the method we'll be using, but here are my findings... 1) using PDF as a general-purpose pseudo-rsrc file simplifies many things. If I need to access a particular image in there, just go to a particular CGPDFPageRef. Great if I have an enum'ed list of images, and I can even add resolution-independent vector art / text as well, not just bitmaps. (Right now, I'm mainly using it as a container for 300-500+ PNGs). The downside is the noticeable performance hit... The advantage, of course, is we only need to put one (or a small number of) PDF in a bundle that has everything we need, and we know how to create that now. There's also a possibility(?) of us being able to tinker with it, to protect it against end-user 'mods', since these elements are not GUI elements only but used for other purposes as well... 1b) I'm also in need of a Windows version of doing the same thing... libpdfx / verypdf seems to do what we need to do easily using very similar methods... though $$$$$... ?? 2) OTOH, using completely separate TIFFs seem to have less of a performance hit. The downside being there are that many files in the resources (or wherever) folder that we have to manage... and possibly check for end-user tampering as well. 2b) Looking at apps like Final Cut Pro, their resources are of this type... Tiffs + PSDs. Convenience-wise, I'm leaning towards the PDF method, but I'm open to suggestions...</body>
  </mail>
  <mail>
    <header>Re: transforming/releasing/appending CGPDF...</header>
    <body>Possibly a wrong use for PDFs. You generally should use PDFs to contain vector art not bitmap based art. If you are dealing with bitmap based art you likely should tiffs. In other words make sure you are using PDF resource for sensible reasons. -Shawn</body>
  </mail>
  <mail>
    <header>Re: transforming/releasing/appending CGPDF...</header>
    <body>Just to clarify, you should only release a CGPDFPageRef *if you've previously retained it*. Note that you get a CGPDFPageRef via CGPDFDocumentGetPage, a Get call, which means that you don't own the reference. Read Memory Management Programming Guide for Core Foundation here: CFMemoryMgmt/index.html Specifically the Ownership Policy of Core Foundation, as it is generally applicable across all of the OS. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: transforming/releasing/appending CGPDF...</header>
    <body>On Nov 30, 2006, at 5:27 PM, shin kurokawa wrote: The header says the rotation value must be a multiple of 90.  You can also rotate a CG context's CTM before drawing into it, but you'll want to pay attention to where the center of rotation is. You should release the page after you're finished with it.  A crash probably means you're accessing something somewhere after releasing it. You have to create a new PDF, drawing the old pages at the appropriate places. Nick</body>
  </mail>
  <mail>
    <header>Question about CoreImage and CISampler</header>
    <body>Hi all, I have some question about CoreImage usage and I hope it is the best place to do that. I'd like to understand more how CoreImage works inside. More precisely, How the class CISampler works. The documentation, I think, gives all the necessary to use correctly this class but I'd like to do something less usual:  find a way to retreive the value of the AffineMatrix associate with a CISampler object. To construct an instance of CISampler one can specify the affine matrix by using the key 'kCISamplerAffineMatrix'. Now given an instance of CISampler, how can we retreive the affine matrix ? I tried something like this, but  it doesn't work : NSArray *affine_matrix = [my_sampler Jaonary</body>
  </mail>
  <mail>
    <header>color spaces and CGImageDestinations</header>
    <body>So I am using the following to take a CIImage with a bunch of filters run over it and I want to save it with a particular color profile (&amp;quot;Adobe RGB (1998)&amp;quot;) which is installed on the system in question and when I loop through the profiles (using the code from ImageApp) it is there and available.  How does one specify the rendering context color space or embed the profile for the saved image that is something other then the Generic RGB? theBitMapToBeSaved = [[NSBitmapImageRep alloc] initWithBitmapDataPlanes:NULL pixelsWide:newSize.width pixelsHigh:newSize.height bitsPerSample:kBitsPerSample samplesPerPixel:kSamplesPerPixel hasAlpha:NO isPlanar:NO CGColorSpaceRef	colorSpace = CGColorSpaceCreateWithName CIContext		*exportContext = [CIContext contextWithCGContext: [[NSGraphicsContext graphicsContextWithBitmapImageRep:theBitMapToBeSaved] graphicsPort] options:[NSDictionary dictionaryWithObject:(id)colorSpace CGImageRef image = [exportContext createCGImage:postProcessImage if (image==nil) CGImageDestinationRef dest = CGImageDestinationCreateWithURL if (dest==nil) CGImageDestinationAddImage(dest, image, (CFDictionaryRef)[self</body>
  </mail>
  <mail>
    <header>Re: How do I get from a CGContextRef to a CGRect?</header>
    <body>On Dec 26, 2005, at 6:57 PM, Michael A. Crawford wrote: You assume incorrectly (sort of). Here's an analogy that you may find useful: the CGContext is like a mechanical pencil. You tell the pencil, &amp;quot;draw a circle&amp;quot;, and the pencil draws a circle. The pencil, however, does not know what is underneath it when it draws. You can put a 8&amp;quot;x11&amp;quot; piece of paper on the table, and that's where the pencil will draw, or you can put an 11&amp;quot;x17&amp;quot; piece of paper on the table, and that's where the pencil will draw. Now, it is true in practice that a given CGContext is actually connected to some sort of output medium, which may be a window buffer, or a bitmap context, or a print job. But there's no way to get the &amp;quot;size&amp;quot; of the output medium given the CGContext.</body>
  </mail>
  <mail>
    <header>Re: Programming with Quartz&amp;quot; book is out</header>
    <body>I ordered mine when it was first announced about six months ago and received my copy right before Christmas.  Haven't had a chance to look throught it yet in detail, but what I've seen so far looks excellent. On Dec 26, 2005, at 10:59 AM, Roland Torres wrote: Attachment:</body>
  </mail>
  <mail>
    <header>Re: Programming with Quartz&amp;quot; book is out</header>
    <body>On Dec 26, 2005, at 9:59 AM, Roland Torres wrote: My copy shipped on the 24th from Amazon and should arrive on the 27th. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Programming with Quartz&amp;quot; book is out</header>
    <body>On Dec 26, 2005, at 8:53 AM, Gordon Apple wrote: Well it *is* 700 pages! Not defending the pricing, but that seems to be the going rate for the current crop of niche technical books, a bump of $10 from about 2 or 3 years ago. Remember, fat paperbacks such as Anguish/Buck/Yacktman &amp;quot;Cocoa Programming&amp;quot; went for $60 in 2003. Heck even the 200p paperback Smalltalk patterns book went for $60 in 1997. I guess they figure those who need it are going to buy it regardless (as long as it's under $100 or so). They're probably right. BTW - Amazon states, &amp;quot;This item has not yet been released&amp;quot; but below that they add, &amp;quot;Only 2 left in stock--order soon (more on the way)&amp;quot;. Anyone know if it's available yet?</body>
  </mail>
  <mail>
    <header>Re:Programming with Quartz&amp;quot; book is out</header>
    <body>&amp;gt;  1. Re: &amp;quot;Programming with Quartz&amp;quot; book is out (Felix Schwarz) I want it.  But $70 US ???   Geesh! -- Gordon Apple Ed4U Redondo Beach, CA and Little Rock, AR email@hidden</body>
  </mail>
  <mail>
    <header>Re: &amp;quot;Programming with Quartz&amp;quot; book is out</header>
    <body>thank you. Now that's interesting indeed. Is that method functional again? A previous quick research yielded that it seemed to be deprecated/ unsupported back in 2001: -- Felix Schwarz email@hidden Postal address: IOSPIRIT GmbH, Berckhauser Stra√üe 11, 90409 N√ºrnberg, Germany Phone: +49 (0)911 / 3677 423 Fax: +49 (0)911 / 3677 424 Homepage: Trade register number and place of registration: HRB 21960, Amtsgericht N√ºrnberg</body>
  </mail>
  <mail>
    <header>Extracting PDF Contents</header>
    <body>When Panther was young I filed an enhancement request (rdar:// 3569618) to expose an API to extract images, color profiles, font IDs, unicode text, etc. from PDF documents.  In the early days of Tiger this bug report was closed and I got a message saying it was in Tiger.  I don't have the message anymore but my impression was that this could be done using the new PDF scanner features (or perhaps PDFKit) in Tiger.  I've been reviewing these API's recently and I can't find a way to do this.  Am I just missing something, or should I reopen this enhancement request? For example, consider the case of converting a bitmap image embedded in a PDF document into a CGImageRef with the correct color profile. Frequently this is fairly straightforward, but it can get very complicated.  Surely Quartz is already doing this when it renders the PDF document and I'd rather not reinvent that wheel if I can help it.  The same is true of getting a Mac font ID from a PDF font reference. Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: NSOpenGLView InitWithFrame</header>
    <body>Am 18.12.2005 um 04:49 Uhr schrieb David Nolen: Try to overwrite initWithCoder: or implement awakeFromNib. See: ApplicationKit/ObjC_classic/Protocols/NSNibAwaking.html &amp;quot;During the instantiation process, each object in the archive is unarchived and then initialized with the method befitting its type. Cocoa views (and custom views that can be customized using an associated Interface Builder palette) are initialized using their initWithCoder: method. Custom views are initialized using their initWithFrame: method. Custom classes that have been instantiated in Andreas</body>
  </mail>
  <mail>
    <header>NSOpenGLView InitWithFrame</header>
    <body>Is there any reason why my own InitWithFrame method would not get called? I have a nib file with a custom opengl view and in the source code file I'm overriding the initWithFrame NSView method, but for some reason my method is not getting called.  I've looked at other code that looks identical, but mine does seem to work. Very frustrating, any clues? David</body>
  </mail>
  <mail>
    <header>Documentation Bug</header>
    <body>The page below contains the following code fragment.  Unless I'm mistaken CGPDFPageRelease should not follow CGPDFDocumentGetPage. drawingwithquartz2d/dq_pdf_scan/chapter_15_section_3.html</body>
  </mail>
  <mail>
    <header>Re: Photoshop Stamp</header>
    <body>Thanks for help Jerry. Blurring &amp;gt; High threshold (128) &amp;gt; then low threshold (64). I did it in Photoshop. The edge of the image comes out to be very noisy at the edge where stamping does very smoothly. I wish I can send you two image to compare. If I get your permission I can send you directly, since the listing does not allow any image. In the mean time I am playing with different level of filtering that you mention.</body>
  </mail>
  <mail>
    <header>Photoshop stamp</header>
    <body>Hey guys I know that my question may not directly related to the quartz development but it is related to image manipulation. I need to develop something like Photoshop has it in its Stamp filter (Filter &amp;gt; Sketch &amp;gt; Stamp). I am sure it is a combination of few standard filters which is programmatically achievable very easily. In this case I tried putting Threshold and blurriness together but it is not quite the same. First option: If it is a combination of some standard filter please let me know. This way I can develop in my application. Second Option: If I can use Photoshop plug-Ins from my code you can advise me? Last option may be going to Photoshop and buy it. My question may not be clear to you as I am still working on paper not on code. Any hint will be helpful. Wahid.</body>
  </mail>
  <mail>
    <header>Re: Quartz &amp;amp; Performance Problems</header>
    <body>Thanks everybody for your suggestions. I guess I have to try and control the number of points sent to Quartz in at least the horizontal dimension, which I believe will be the easiest to calculate. Maybe I should have two sets of code, one for Jaguar and another one for Panther and beyond. This way I'll get better quality if the system permits and acceptable speed with the minimum requirements. How does this sound? Well, I am trying to avoid duplicating any plotting data, because the data set can grow to some serious amounts. The audio buffers are preserved until the user makes changes and plots again. This could mean hundreds of Megs occuyping memory, while the program is in use, depending on the length of the audio. Basicaly, I can only afford to do optimizations that could be applied to one frame at a time (perhaps with a small buffer if needed). I can't rebuild the data set as CGPoints because I can't throw away the original set.</body>
  </mail>
  <mail>
    <header>Re: Obtaining image from window back buffer</header>
    <body>A good idea, but I'll skip this one.  I'd like to have a single window at all times (see below). This looks very promissing.  I just recently found some good info on initWithFocusedViewRect too. I think I'm going to go the route of not only having a single window, but just one custom view in that window.  The contents of that window will simply be an offscreen NSImage. I'm not worried about keyboard focus as I won't have any traditional controls.  The window will route all keyboard events to a custom keyboard handler.  I then iterate through my collection of widgets and handle the key appropriately. In terms of performance, all of my widgets will cache their respective data types (e.g. CGImageRef, CGPDFPageRef).  This will allow me to rebuild the off-screen NSImage as quick as possible.  But my window re-draw will simply be a &amp;quot;blit&amp;quot; of that entire NSImage. I'll use the process you mentioned above to capture the bits of that image and convert to B&amp;amp;W.  When re-rendering my offscreen image, I'll first &amp;quot;blit&amp;quot; the B&amp;amp;W version, then any of the PDFs/bitmaps that make up the dialog UI. Thanks for your comments; this really helps out! Rick Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: Rendering to PDF</header>
    <body>Good suggestion. However, I tried several other drawing commands to no avail. Also, since the code works when I create the PDFContext using an URL, I think my problem is definitely in the data consumer callback. -Mick</body>
  </mail>
  <mail>
    <header>Re: Rendering to PDF</header>
    <body>I tried flushing after every page and it makes no difference. The online sample code indicates that you can (and should) release the data consumer after creating the context since the context retains it. -Mick</body>
  </mail>
  <mail>
    <header>Re: Obtaining image from window back buffer</header>
    <body>You have at least two options: 1) Implement your dialog as an NSPanel. If you want to change the look of your custom views depending on whether the containing window is active or not, then your views can subscribe to the NSWindowDidResignMainNotification and NSWindowDidBecomeMainNotification notifications. In the notification handlers, just set a flag and call -setNeedsDisplay on your view. The view drawRect: implementation can then customize its drawing based on the state of this flag. In theory the AppKit will make sure that only views which are not completely covered by your panel will actually get redrawn. At least this is the case with opaque views that completely cover up other views below them. 2) Implement your dialog as an NSView subclass. In this case you would take a snapshot of the current window contents with the -[NSWindow dataWithPDFInsideRect:] method before you display your dialog. Then, create an NSPDFImageRep from the NSData and put it in an NSImage. Finally, remove the window's content view (don't forget to retain it !) and replace it with an NSImageView just as large as the original content view and assign this one your NSImage. You would then add your dialog NSView as a subview to the NSImageView. When the user dismisses the dialog, just throw out the NSImageView and put back in your original content view. You could also create your own specialized NSView subclass which draws your image if you don't want to use an NSImageView for some reason. If you want to create a B/W version of your original window content, then, well its going to be a bit tougher. In this case you would have to use the -[NSBitmapImageRep initWithFocusedViewRect:] method. However, this method will only take a snapshot of the view which is currently focused, thus it may not work correctly with subviews (I've forgotten if this works as expected or not). You could then process the pixels as desired and put the resulting NSBitmapImageRep into sour NSImage from above. In short you would call -lockFocusIfCanDraw on your content view, then -initWithFocusedViewRect: and finally -unlockFocus. Note however, that its in general not a very good idea to lightly move views in and out of a window because its very easy to destroy / screw up the keyboard navigation loop. Further the window maintains some view state like tracking rectangles. In 10.3 you could take advantage of the new view show / hide APIs to achieve the same effect as above but without potentially screwing up the keyboard loop and tracking rects. Add both your NSImageView and the top-level view (original content view from above) of your regular view hierarchy to the window's content view. Now you keep always one of them hidden and the other one shown. I.e. by default the top-level view would be shown, the NSImageView hidden. When you are about to display your dialog, then you would hide your top-level view and show your NSImageView. Which one of those techniques is faster or ultimately better is hard to say. I personally, however, would start with the first one because its much easier to implement and you get shadows around your dialog for free :) Regards, Dietmar Planitzer</body>
  </mail>
  <mail>
    <header>Re: Quartz &amp;amp; Performance Problems</header>
    <body>I also suggested the following in response to Carlos' query on Carbon-Dev: Would that help?</body>
  </mail>
  <mail>
    <header>Re: Rendering to PDF</header>
    <body>I know it sounds silly and probably won't make a difference, but have you tried using more direct drawing commands? In particular, try something like CGContextFillPath(mContext) Is it the case that none of your drawing commands show up, or is it just something peculiar to CGContextFillRect and CGContextStrokeRect. As I said... it shouldn't make any difference... but you might try a few other commands. -- Macintosh Software Engineer consultant</body>
  </mail>
  <mail>
    <header>Re: Rendering to PDF</header>
    <body>I don't think what you're doing will work unless you call CGContextFlush(mContext) after each CGContextEndPage(mContext). If you don't, the contents of the new page seem to be undefined. Derek can probably tell us if that's correct. Also, I think you need to retain the data consumer until after calling CGContextRelease(mContext). Dave</body>
  </mail>
  <mail>
    <header>Re: Quartz &amp;amp; Performance Problems</header>
    <body>In your case, it looks like what you are drawing are hairlines (or map as hairlines). We wrote a new (with better performance) hairline scanconverter in Panther. Unfortunately Jaguar still used a general purpose scan-converter which was quite slow. Tiger is even faster for drawing lots of lines. I assume the worst case scenario is when you are zoomed all the way out and can see the entire data set. The only thing I can suggest is that sample your data set and not draw it all. For example if you have 2K data points that you are drawing in a view that is only 800 pixel wide, you may want to only draw a maximum of 800 data points. You have to evaluate the performance vs. quality tradeoff for youself. haroon</body>
  </mail>
  <mail>
    <header>Re: Rendering to PDF</header>
    <body>I verified that the problem is with my consumer callback. When I create using an URL, I get the correct output. Here's what my callback looks like: size_t pdfConsumerPutBytesCallback(void* info, const void* buffer, size_t count) if (count &amp;gt; 0) else printf(&amp;quot;pdfConsumerPutBytesCallback - bytesWritten = %ld, gByteCount = A couple of notes: 1) count always seems to be 1 2) gByteCount gets incremented up to 458 but the end file size is 866 3) It is definitely NOT called during my drawing commands -Mick</body>
  </mail>
  <mail>
    <header>Re: opening .eps and .pdf file</header>
    <body>I would create a CG bitmap context and draw the PDF into it.  Then I would use the same image buffer to create a CGImage object.  If I started with an EPS input file I would first convert it to a PDF (requires Panther and later). Specifically look at the functions: CGBitmapContextCreate CGImageCreate CGContextDrawPDFDocument CGPSConverterConvert Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>opening .eps and .pdf file</header>
    <body>Hi how do I open .eps and .pdf and convert to CGImage. I have one sample using Quicktime but for some reason I cannot use them in my Code warrior 9.2 mach-o project. It gives me linking error. I tried adding quicktime frameworks and other stuff. but I cannot figure it out. Thanks in advance. Wahidun Chowdhury Software Engineer SNX Software 90 Park Ave NY, NY 10016</body>
  </mail>
  <mail>
    <header>Obtaining image from window back buffer</header>
    <body>In my QuickDraw-based Carbon kiosk app, I maintain a single window.  An offscreen GWorld is used to render a collection of &amp;quot;widgets&amp;quot; that represent the current screen.  When drawing (in response to the draw carbon event), I simply blit the contents of the offscreen GWorld to the window.  As users interact with widgets, I redraw the necessary areas in the GWorld and reblit (either the whole screen or portions as needed). Now then, I simulate dialogs by overlaying a &amp;quot;dialog screen&amp;quot; over the current screen.  For argument sake, say the initial screen is made up of the following widgets (in order from backmost to frontmost): - color background taking up the entire window - a button widget Clicking the button widget will bring up a dialog screen.  This is exactly what I do: Take the current GWorld (which represents the current screen) and covert it into black &amp;amp; white (I use direct pixel access techniques for this).  The resultant image becomes the background for my dialog.  The dialog's background is then put on top.  So, at this point, the GWorld is now made up of the following layers: - black and white background - dialog background I do this for two reasons: (1) I wanted to visually de-emphasize widgets below my virtual dialog; (2) Performance since the uderlying screen could be made up of several layers...this process allows me to effectively collapse all those layers into one. I'm trying to figure out how I can do this in my Cocoa application that uses Quartz.  I don't maintain an offscreen buffer anymore and my window will actually now contain many custom NSView objects (one for each widget). What I'd ultimately like to do here is to overlay another custom NSView over the whole window area, have it be opaque (so items underneath it do not re-render) and have it's contents be the black &amp;amp; white form of whatever was showing in the window beforehand. Thanks for any pointers, Rick Sharp Instant Interactive(tm) P.S.  The absolute minimum baseline of this app will be 10.3 if that helps in directing me to any particular APIs.</body>
  </mail>
  <mail>
    <header>Re: Rendering to PDF</header>
    <body>I believe I have setup the putBytes callback correctly. I am not using the releaseConsumer callback and have set it to NULL. It does create a valid PDF document. My callback seems to get called for all the commands except the actual drawing commands. The app that I'm writing requires that I write the PDF to stdout, which is why I'm creating my own consumer. I'll try creating with an URL to see if it makes a difference. -Mick</body>
  </mail>
  <mail>
    <header>Re: Quartz &amp;amp; Performance Problems</header>
    <body>I tried that and it made it even slower. The reason I am synching the entire path at once was because I read somewhere (technote?) about Quartz Debug that too much flushing impacts performance. I don't know if that is the reason, but when I try showing one line at a time it gets slower. The thing is want to get the plot view as accurate as possible. I am showing audio sample data and the data comes from the same buffers I feed my AudioOutpuUnit (CoreAudio). These samples are in canonical float format (-1.0 to 1.0). I am not trying to be lazy, but I figured that if Quartz draws so nicely with float coordinates, I wouldn't have to figure out when to paint a pixel and when not too, specially since, given the antializing quality of the drawing, the test would be much more involved than just checking if the values reached the next pixel. What I don't understand is why the code I have that operates on the entire audio buffer, mixing hundreds of channels (in a not very optimized way) can be much faster than just drawing a small portion of the same data. I know my machine is pretty old, so this, probably, already is much faster on a G4 (which I don't have access to at the moment). What I want to know is if the way I wrote the code has something inherently bad about the way I am using CG.</body>
  </mail>
  <mail>
    <header>Re: Rendering to PDF</header>
    <body>Tried that but it didn't help. -Mick</body>
  </mail>
  <mail>
    <header>Re: Rendering to PDF</header>
    <body>are your callbacks for the data consumer correct?  is it actually creating a valid pdf document? if you are writing to a file, try simply using CGPDFContextCreateWithURL() or creating your data consumer with a url. this works for me: int main(int argc, char **argv) NSString *path = [NSHomeDirectory() stringByAppendingPathComponent: CGContextRef mContext = CGPDFContextCreateWithURL((CFURLRef)url, // cc -o pdftest pdftest.m -framework ApplicationServices -framework Foundation</body>
  </mail>
  <mail>
    <header>Re: Rendering to PDF</header>
    <body>I haven't looked to hard at your code but you might need to flush the context before releasing it. Dave</body>
  </mail>
  <mail>
    <header>Quartz &amp;amp; Performance Problems</header>
    <body>Hi, I posted this message on the Carbon-Dev list and everybody suggeste I ask it here... ======================================================== My carbon app is giving me a hard time when doing some simple drawing. I have a large data buffer - audio data - that I am plotting in a window with CG. I only draw , at a time, the amount that fits the window, by reading a data point, rescaling the value to fit the drawing area and then adding a line to my path. So this means a loop of a few hundred to a few thousand turns, depending on the time zoom. But the drawing takes several minutes. Even the audio processing on the same data buffers, which mixes hundreds of input channels, sample by sample, rendering the entire buffer for both output channels, only takes one or two seconds. I found it great that I can use floats for specifying my points with CG and leave Quartz to figure out how to draw correctly. The drawing looks GREAT! But the thing is just too slow on my G3 (PB in Jaguar 10.2.8). I figured if I can make it work decently on my machine, it will probably look pretty good on a newer G4. My question (just speculating): Is it possible that, since a lot of points end up over the same pixels because of the scale, this makes it harder for Quartz to do its anti-aliasing magic? Anything I could do to make it faster? Thanks in advance for any help. Carlos. Here is the drawing code: ======================================================== if(!theErr) // define left channel drawing position // start left channel sub-path for(i = firstPoint; i &amp;lt; numOfPoints; i++) y = kPlotUpperScreenZero + (((leftValues[i] * // draw left channel // define right channel drawing position // start right channel sub-path for(i = firstPoint; i &amp;lt; numOfPoints; i++) y = kPlotLowerScreenZero + (((rightValues[i] * // draw right channel // Update screen</body>
  </mail>
  <mail>
    <header>Re: Rendering to PDF</header>
    <body>I mean there is nothing in the pages stream of drawing commands. If I take out: I get the same results than if they were in. -Mick</body>
  </mail>
  <mail>
    <header>Re: Rendering to PDF</header>
    <body>Do you mean that the output file is completely empty, or that there's content but nothing in the page's stream of drawing commands?  For what it's worth, I don't see any obvious problems in the code above. Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Rendering to PDF</header>
    <body>As part of the project I'm working on, I have to be able to convert some legacy Quickdraw code to draw into a PDF document. Before I start the part of the code that will use QDPictDrawToCGContext to draw into the PDF context, I implemented a small amount of code to verify that I had created the PDF context correctly: 1) I setup a data consumer that simply writes the data to a file 2) I create the PDF context using my data consumer. I discovered that it doesn't seem to like NULL for the bounding box (creates PDFs that are 0.0 x 0.0). 3) I attempted the simplest possible drawing code - drawing a rectangle 4) De-initialize Here is most of the code: callbacks.putBytes = pdfConsumerPutBytesCallback;	// calls fwrite and eventually I call: I've looked at some sample code on Apple's website and this seems like it should work but it creates a PDF document with no drawing commands. Am I missing something? Mitchell J Laurren-Ring Critical Path Software</body>
  </mail>
  <mail>
    <header>Re: Optimal image object</header>
    <body>Yes.  Each image object would hold on to the CGPDFPageRef, etc during its lifetime.  The constructor/destructor is where I'll do all the loading/unloading. Thanks, Rick</body>
  </mail>
  <mail>
    <header>Re: Optimal image object</header>
    <body>By &amp;quot;caching&amp;quot; do you mean &amp;quot;reusing the same CGPDFPageRef and CGImageRef for each redraw operation&amp;quot;?  If so, then, yes, this is the most optimal approach.  When you reuse an object, Quartz 2D can take advantage of any data cached for the object on the Quartz 2D side to make drawing fast.  When an object is deallocated, however, Quartz 2D discards all of the data cached for the object. Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Optimal image object</header>
    <body>I'm just about done running experiments in using PDF images for my application's custom UI.  The current application uses PICT resources that I composite together with CopyMask. Anyhow, I'm very pleased with the results.  Not only do I now have completely scalable artwork, but it's also saving quite a bit on storage space.  BTW, I'm using the Illustrator CS 30-day demo to create PDF files in the 1.4 spec. I'd like to now re-create my image objects.  They'll be a base object and several derived objects.  One derived object will wrap a CGPDFPageRef (for vector artwork), one will wrap a CGImageRef (for any 300dpi artwork) and others will wrap CG primitives (e.g. a filled rectangle, or gradient-filled circle). I'm planning on having a single NSView and my drawRect: would simply render the PDF pages, ImageRefs, CG drawing operations that represent the current state of the UI.  The current state could be represented by 100's of these image objects; many of which will overlap. By caching the CGPDFPageRefs and CGImageRefs, would this give me the most optimal drawing solution? Thanks, Rick Sharp Instant Interactive(tm) P.S.  I do plan on using QuartzDebug and Shark to ensure I'm writing optimal code once I know a bit more of the basic approaches.</body>
  </mail>
  <mail>
    <header>Re: named color</header>
    <body>Not at this time. Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: named color</header>
    <body>Hmmm... It does support &amp;quot;indexed&amp;quot; color spaces (see CGColorSpaceCreateIndexed) which is pretty similar. You can also create a color space based on any ICC color profile (see CGColorSpaceCreateICCBased).  The ICC spec provides for the creation of &amp;quot;Named Color Profiles&amp;quot;  (section 6.3.4.4).  I've not encountered one of those before (which is not saying much because much of the ICC spec is new to me) but they might be used for handling spot colors. It would be interesting to see what the PDF created by printing against one of those profiles might look like. Scott [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Watermarks</header>
    <body>Hi, at present We're writing a carbon application that creates PDFs that our customers use in their documents. We want to add a watermark to the PDF when one is produced without a valid licence. We tried adding text on top of the image, using an alpha value so that the text appears semi transparent over the image. We tried this two ways. First we wrote the text directly to the PDF context. Second we wrote the text to a bitmap context, created an image from this and drew that image into the PDF context. We found that due to some art applications not handling the PDF alpha channel correctly, it was fairly easy to remove our watermark, I think in Illustrator it just showed up as a plain black text. Does anyone know of any techniques that would prevent the watermark from being easily removed from on top of an image. I thought perhaps there may be an API call to lock the PDF, but I can't find this in the apple reference library. Many thanks for any help, Max.</body>
  </mail>
  <mail>
    <header>Re: PDFs and Shadows</header>
    <body>Hi Michael, Yeah, I found this too.  It was a rather frustrating discovery.  It's not just Acrobat that fails to draw the shadows - Illustrator doesn't show them too.  However Microsoft Word and AppleWorks both seemed to display the shadows OK when the PDF file is added to a document. My experimental results with EPS file output was even worse. My solution was to not use Quartz shadows, but rather to draw my own. To do this I just duplicate all the graphics objects I'm going to draw, render them in the shadow colour at an appropriate offset (using a transformation matrix), and then render my graphics over the top.  To render stuff with transparent colours it's slightly more complex - you need to multiply the alphas. Obviously this solution isn't entirely satisfactory - it's a lot more work for a start, and you also loose the blurred shadow edges. Steve</body>
  </mail>
  <mail>
    <header>named color</header>
    <body>Does CoreGraphics support named color or spot color ColorSpaces? thanks, Alex</body>
  </mail>
  <mail>
    <header>Re: PDFs and Shadows</header>
    <body>Hmm...I make apps that process PDF files, and it is surprising just how bad Adobe's products are at (correctly) implementing their spec, never mind the documented implementation limits.  That may not be the case here, but it is something to keep in mind. Cheers, Marcel</body>
  </mail>
  <mail>
    <header>PDFs and Shadows</header>
    <body>Today I ran into some kind of inconsistancy with shadows: Being curious about transparency layer, shadow effect and the resulting PDF when &amp;quot;printing&amp;quot; I added printing to TLayer example of Quarz example projects and opened the resulting PDF with acrobat reader 6.0. With Acrobat no shadows are displayed. Opening the same PDF file with apple's preview program the shadows are displayed! How can I get quarz shadows being displayed in Acrobat correctly? MiMo -- ======================================================= And refashioning the fashioned lest it stiffen into iron is work of endless vital activity. [Goethe] All about CALAMUS DTP suite: =======================================================</body>
  </mail>
  <mail>
    <header>Re: Preventing Window Selection</header>
    <body>Ahh...that'll teach me to move off of Carbon and start with Cocoa :) Seriously though, I believe I've found a decent workaround for now: Blanking window levels set at document level + 1; content window level set at document level + 2.  Both of these levels fall below the floating level. Expose now has the following behavior (default function keys used): F9 - all my windows are hidden.  Clicking on another app's window of course switches to that app/window.  Clicking on dead space between windows returns control to my application; my windows are shown again. F10 - no action taken; system provides audible feedback.  I'm guessing this is due to the fact that my app has no windows at the base document level. F11 - all windows (including my app's) moved to the edge.  Clicking on either the content or blanking windows makes my app the frontmost and all windows show up in proper z-order.  Clicking on other app's windows does the expected behavior.  And of course, clicking on the desktop keeps the windows hidden (i.e. you can work with items on the desktop). So while my app is not a first-class Expose citizen, this is as close as I can get. Rick Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: Preventing Window Selection</header>
    <body>Yes, I've implemented something like that for Carbon for Tiger, but I don't believe Cocoa has provided any control over Expose-ness of windows in Panther or Tiger. -eric</body>
  </mail>
  <mail>
    <header>Re: Preventing Window Selection</header>
    <body>LOL. I got Expose to behave, but my blanking windows are placed below windows in all other applications :P I think someone filed an enhancement asking for there to be some property you could set that Expose would look at to basically ask &amp;quot;should I Expose this?&amp;quot;. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: Preventing Window Selection</header>
    <body>This is good info to know, thanks. Cool. I set the &amp;quot;hide on deactivate&amp;quot; flags on my windows, so this won't be an issue. However, what is an issue is that Expose hides my content window and displays the blanking windows! Ugh :P  But, clicking on a blanking window in Expose returns control to my app and reshows the content window where it belongs. I'll do some experimentation where my content window stays in the document (normal) level, and place the blanking windows at that level minus one. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: Preventing Window Selection</header>
    <body>This approach probably won't work in a Cocoa app, because Cocoa configures its windows such that the Window Server automatically selects (i.e., advances to the top of its window level) any clicked window. You can't prevent it. That's probably the right choice. The CGWindowLevel value is actually an SInt32, so you have 4G of different levels in which you can place windows. This is fully supported in the window server, and won't be going away. The only tricky part may be that the frameworks themselves use window levels too; for example, modal windows in Cocoa (and in Carbon in Tiger) are at a non-zero window level as well. If you chose a window level for your window that was above the modal level (the floating level is not, but if you did), then you could wind up with normal alert windows behind your content window. Something else to be aware of is that when your app deactivates, you'll probably want to set the window level of your content window back to zero. Otherwise, your content window will float above all document windows of other apps. -eric</body>
  </mail>
  <mail>
    <header>Preventing Window Selection</header>
    <body>I posted a question this evening to Cocoa-dev to ask the preferred mechanism to prevent window selection. Basically, I have a Cocoa app that has blanking windows all on displays and a content window centered on the user's selected display.  All windows are in the document layer. The original app came from Carbon, so what I did there was to handle thus preventing window selection and moving them forward.  For numerous reasons, putting the content window in the floating layer was problematic, so I stuck with the event handling solution. Anyhow, someone sent me a private message in response to the Cocoa post and asked if I could use the CG window level values. I ran a quick test: Stepping through the code, aLevel was first 0, then 3. I then modified my Cocoa app to set the window level of my content window to CGWindowLevelForKey (kCGNormalWindowLevelKey) + 1. It does work great; it stays &amp;quot;floating&amp;quot; above my blanking windows. Question though...can I bank upon this behavior? i.e. was it intentional that normal windows could be effectively put into three distinct layers (i.e. level 0, level 1 and level 2)? ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Speed of drawing lines together?</header>
    <body>I'm going to refactorise my fractal code so that it draws lines of the same width together, by creating a &amp;quot;cache&amp;quot; of lines per width. So, I'm wondering on the size for my cache that gives the fastest draw times. IE, does drawing 50 lines together like this pseudo-code, become fastest when count is a certain value? CGContextSetLineWidth ... CGContextStrokePath ... My guess is, that there is a midpoint which is faster. Too small means I'm going to be doing excessive calls to CGContextStrokePath. And too many (my fractal can regularly draw 100,000 lines of the same width!) means I'll be bloating the RAM that the MacOS is using a lot. -- Theodore H. Smith - Software Developer.</body>
  </mail>
  <mail>
    <header>Re: Iterating active screens</header>
    <body>Now that I think about (and having read more APIs), seems like this is overkill.  I can simply use CGDisplayBounds to obtain the bounds for my blanking windows. So... for (all displays as returned by CGGetActiveDisplayList) obtain bounds of display via CGDisplayBounds and build blanking window Rick Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Iterating active screens</header>
    <body>My goal is to iterate over all active screens so that I can place a full-screen &amp;quot;blanking&amp;quot; window on it. I've been seeing some sample code do stuff like this: for (i = 0; i &amp;lt; dspyCnt; i++) ... But this seems problematic to me.  From what I read, [NSScreen screens] returns an array of all screens available on the system.  The number of screens seems like it could be larger than what CGGetActiveDisplayList would return (because not all displays may be active).  And there's no documentation on the order of screens as placed in the returned array (e.g. would active screens be listed first?). Thus, would it be proper to iterate over _all_ items in the NSScreen array, and for each screen, see if the NSScreenNumber property matches an item in the list returned by CGGetActiveDisplayList? The psuedocode would be something like this: for (all items in screens array) if (screen at index is active; i.e. matches an item in the active display list) then obtain screen bounds and build blanking window Rick Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: Simple question</header>
    <body>Am 13.07.2004 um 15:21 Uhr schrieb Theodore H. Smith: I think this topic would fit better in cocoa-dev. You could use: which does: which in turn does probably result in: Note that these messages do not visually activate the control. If you want to do that use: Andreas</body>
  </mail>
  <mail>
    <header>Simple question</header>
    <body>Within ObjC, how do I call an action of a UI element, from code? This is for my fractal project. For example: Say I have a subclass of a UI element, and within this subclass's code, I need to call it's action. It's action is already connected in Interface Builder, to my fractal View class. Thanks! -- Theodore H. Smith - Software Developer.</body>
  </mail>
  <mail>
    <header>Re: How to get double buffering and speed up my fractal?</header>
    <body>Hi Shawn Thanks for the answers, I'm able to use all of them, except for the ones on the WWDC sessions. Did you mean the page from Apple, at ? If so, then I can't find any place to download useful information. Do I need to create an acount first? -- Theodore H. Smith - Software Developer.</body>
  </mail>
  <mail>
    <header>Re: CGContextFillRect vs CGContextStrokePath</header>
    <body>You can let CoreGraphics handle the rotation for you by modifying the Current Transformation Matrix (CTM). Everything goes through some form of transformation, so the cost should not be heavy. haroon [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>CGContextFillRect vs CGContextStrokePath</header>
    <body>I'm assuming that CGContextFillRect isn't suited to my fractal, because it draws horz or vertical lines only. Unless I use some kind of transformation, but I think that would be slower than CGContextStrokePath. I've got a 2x speed up anyhow now, because I'm locking and unlocking my context myself, so that'll do for the moment. I'll look into grouping my use of CGContextStrokePath, so that I'm drawing long paths instead of only singular lines. -- Theodore H. Smith - Software Developer.</body>
  </mail>
  <mail>
    <header>Re: Alerts above a shielding window</header>
    <body>[very useful info snipped] Mike, Thank you very much for this info.  I was thinking about managing my own blanking windows (which is what my Carbon app already does), but thought I'd get more milage in capturing the display. I had no idea that placing windows above the sheilded windows could prove to be problematic. Rick Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: New Quartz 2D related documentation</header>
    <body>That sounds very useful. How would that be announced (to WWDC participants and/or ADC members)? Or would that be one of those things you need to accidentally find out? thanks, david.</body>
  </mail>
  <mail>
    <header>Re: Alerts above a shielding window</header>
    <body>You should be aware that the CGCaptureDisplay APIs are intended as a way to effectively remove a display from the window system.  The display still has to maintain a place in the global coordinate system so that drawing mechanisms can find it, so it's possible to 'trick' windows into appearing on the captured display. Captured displays are typically used for porting full screen games and some similar software, which contain their own graphics engine and do not or can not interact with the window system.  Some common cases include full screen OpenGL context drawing systems as found in games, where the context can achieve better performance on some hardware by placing the frame buffer into unusual modes of operation. In particular, a full-screen GL context may place the hardware in states that cannot be supported by the window system.  That's why the documentation on using full-screen GL contexts recommends capturing the display, removing the display from the window system. There are some clever hacks out there that will get a window on a captured display in some circumstances, however, this will definitely not work on all hardware, particularly not with an OpenGL full-screen context.  The window may simply not appear, or little bits and pieces may appear where a tiled framebuffer happens to map some of the tiles holding the window content on-screen. If an application has captured the displays and wants to present a window or other UI elements, then the application must release the display first.   If an application needs to present windows and other UI elements during it's operation, then instead of capturing the display, it should use a full screen plain-style window and use HideMenubar() to remove the Dock and menu bar. In development, sometimes using a two-display system and capturing the non-main display works well.  Put the alerts, debugging tools, and whatnot on the main display, while you experiment with the captured display.</body>
  </mail>
  <mail>
    <header>Alerts above a shielding window</header>
    <body>I've started migrating my Carbon app to Cocoa and am using the CG direct display APIs to capture/release all displays. Here's what I do in my controller object when capturing the displays: - (void) applicationDidFinishLaunching: (NSNotification*) inNotification if (CGCaptureAllDisplays() == kCGErrorSuccess) mMainWindow = [[NSWindow alloc] initWithContentRect:contentRect styleMask:NSBorderlessWindowMask backing:NSBackingStoreBuffered After this runs, I get the expected behavior in that all displays have a sheilding (blanking) window, and I see my content window (mMainWindow).  However, if use NSRunAlertPanel (or sibling APIs), the alert does show, but beneath the shielding window.  The reason I know it is showing is that I need to press Return (to dismiss the alert) in order for application flow to continue. I was planning on using simple alerts to display &amp;quot;I am here&amp;quot; and other types of useful info while I develop the app. On Cocoa-dev central, someone mentioned that critical system alerts (e.g. low battery) also do not display above the shielding window. I don't see any mechanism in &amp;quot;telling&amp;quot; the NSRunAlertPanel APIs to use a higher window level (I'll ask that on the Cocoa-dev list), but in general, are all alerts going to display below the shielding window?  Is this limitation by design? I know that one workaround is to create my own alert panel and put it in the proper window level. Rick Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: Stroke or fill in clear?</header>
    <body>How about: 1) Fill the path. 2) Save the graphics state. 3) Clip to the path. 4) Stroke the path in opaque white. 5) Restore the graphics state. 6) Stroke the path with the intended stroke color. This will &amp;quot;erase&amp;quot; the area of overlap between the fill and the stroke to white before drawing the stroke. Nick</body>
  </mail>
  <mail>
    <header>Test For Quartz Extreme</header>
    <body>How does one test for the presence of Quartz Extreme? I'm looking at some performance tuning where the hardware acceleration in QE means I should do things differently in that case. Thanks, Nick Nallick</body>
  </mail>
  <mail>
    <header>Path Computation</header>
    <body>There's been a bit of talk recently about computing new paths based on old paths (like the path that encloses a stroked path, and like that path that encloses all non-transparent pixels in an image, for example).  Does anyone have any references on ways of accomplishing these calculations? I'm looking for something analogous to the Porter-Duff paper on compositing models, but for path computation instead of color channel computation. Thanks! Jason Harris [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: Problem: drawing a window with Quickdraw after a Window  resize.</header>
    <body>Thanks Eric, that did the trick!!</body>
  </mail>
  <mail>
    <header>Re: Stroke or fill in clear?</header>
    <body>Yes, the ability to control the composite operation of a CG graphics context would be a very welcome feature addition. I.e. a CGContextSetCompositeOperation(CGContextRef, CGCompositeMode) function where the CGCompositeMode enumeration would list all the Porter-Duff modes and maybe even the Adobe PDF 1.4 blend / transfer modes. If you agree, then please file an enhancement request asking for this feature so that we see it as soon as possible :) One solution to your problem would be to write a routine which computes a new path from your original path which represents the inner edge of the stroke. You would then set this path as the clip path, fill your original path, and then remove the clip and finally stroke the original path. Things would be easier if CG would have something like the PostScript &amp;quot;strokepath&amp;quot; operator which computes the stroked version of the current path - say a CGPathCreateStrokedPath() function which would operate on CGPathRefs. Then you could use this function to get the geometry of your original path when stroked. This new path would then contain two subpaths, assuming that the original path was closed, consisted of just one subpath and that no dashing was applied. The shorter of the two subpaths would then be the one representing the inner edge. You should be able to get the CGContextRef for the image cache of an NSImage by doing this: ... ... Regards, Dietmar Planitzer</body>
  </mail>
  <mail>
    <header>Re: Stroke or fill in clear?</header>
    <body>OK, my problem is solved (I hope), thanks to Kurt Revis @ Apple who replied to me off-list filling in the final piece of the puzzle. What I wanted to be able to do was stroke and/or fill arbitrary bezier paths in a completely clear colour.  Haroon pointed out I could do this kind of thing with NSImage, but not with CG.  I wanted to be able to draw things making use of both CG and Cocoa functionality, and I could not see how to use CG with NSImage.  As I had hoped, the answer was fairly simple, as NSImage has two calls, lockFocus and unlockFocus which set and unset the graphics context to a suitable image representation. So the full solution to my little problem, summarised, seems to be as follows: Create an NSImage into which everything will eventually be drawn Add an appropriate NSImageRep lockFocus onto this image Draw into that whatever is desired Create a second NSImage and NSImageRep for our &amp;quot;clear&amp;quot; graphics lockFocus onto this image Draw the &amp;quot;clear&amp;quot; graphics in a totally opaque colour unlockFocus on this (second) image Composite the second image onto the first with compositeToPoint using NSCompositeDestinationOut as the operation parameter Do whatever other drawing I might want to in the first image unlockFocus on the image I should then have an NSImage containing exactly the graphics I want complete with areas that have been cleared. Phew! Thanks again guys! Steve</body>
  </mail>
  <mail>
    <header>Re: Stroke or fill in clear?</header>
    <body>Hi Guys, Thanks for the URL for the Porter &amp;amp; Duff paper - I must admit that I have not read it in depth, but my brief read through has already added a fair bit to my understanding of what is going on with CG and the AppKit graphics systems. It seems that what I would really like is not transparency layers but compositing layers.  Of course a transparency is a type of compositing layer, but it gets combined with the layer below using only a single operation (the layer gets pasted over the layer below, obeying transparency).  Providing an ability to specify which compositing operation is used when combining layers would be very nice indeed and would simplify my problems immensely. Actually I'm not sure this would work.  Remember I want a general solution to stroke and fill a bezier path with transparent colours - the colours may not be identical, and may have differing opacity levels.  I wish to be able to stroke the entire line, not just the part outside the bezier path, without the line rendered inside the path being mixed with the fill.  The way to accomplish this is render the fill and then mask out the line, so that the line can be drawn properly.  It seems to me that your solution would give the same result as a plain stroke and fill, i.e. a mixed colour overlap between the stroke and fill. This is frustrating stuff.  It seems to me that the features are there to allow me to accomplish everything I wish to do, except that some bits are CG, and other bits are Cocoa, and whilst I can mix the two to an extent I cannot mix them completely.  In fact I can accomplish most of what I want in plain CG, with the exception of compositing (specifically masking), which I can only do in NSImage. If, as I asked in my last email, somebody could provide me with a way of getting a CGContextRef that corresponded to an NSImage then I'd be a very happy chap.  As I stated before I cannot see how to accomplish this right now so some help would be appreciated. If this is impossible I would like to know so that I can plan accordingly. Regards, Steve</body>
  </mail>
  <mail>
    <header>Re: Auxiliary info in CGPDFContextCreate</header>
    <body>The only keys supported are those specified in the header file (and in our CG documentation). Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Auxiliary info in CGPDFContextCreate</header>
    <body>Hi all, What kinds of info can I put into the auxiliary info parameter of CGPDFContextCreate? I see documented Name, Author, Title and OutputIntents, but I can put in any of the usual PDF stuff like PageLabels etc.? Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com</body>
  </mail>
  <mail>
    <header>Re: Problem: drawing a window with Quickdraw after a Window  resize.</header>
    <body>It sounds like you're drawing in response to an update event. During update event processing in Carbon, drawing is clipped to the newly exposed area of the window, because the visrgn of the window is set to only include the area to be updated. This is OK if the area of the window that was already visible doesn't need to be redrawn, but in your case, it sounds like it does. The solution to this is to use the InvalWindowRect or InvalWindowRgn APIs when your window is resized to invalidate the entire window content area, so that all of the window contents will be redrawn. -eric</body>
  </mail>
  <mail>
    <header>Problem: drawing a window with Quickdraw after a Window resize.</header>
    <body>Hi, I am using DrawThemeButton to refresh a window after the window is resized by the user. The size of the button image expands or contracts to maintain the binding of the image to the window edges. As the window size is changed by the user I am seeing visual artifacts where the old image (pre-resize) is not over-drawn by the new image (post-resize). In effect the new image appears to be drawn under the old image. The problem is not confined to DrawThemeButton as other QD calls such as LineTo,  EraseRect and PaintRect also exhibit the same behaviour. I think it may be a problem with Quickdraw drawing into a Quartz window. On the face of it the backing store image before window resizing is placed outside some kind of clipping region and so any new drawing done after the resize  ends up appearing as if it is under the old drawing instead of being drawn over by the new image. Doing an erase does not work either. Using SetClip to set the Window clipping region to be a region of maximum size,  just before drawing the button, does not help. Also putting a PowerPlant  LPicture view inside the LTableView has exactly the same problem so on the face of it is more than just some misunderstanding on my part. Is there some peculiarity of doing Quickdraw drawing under both Jaguar and Panther that I need to be aware of??? Regards from Richard</body>
  </mail>
  <mail>
    <header>Re: Stroke or fill in clear?</header>
    <body>Porter &amp;amp; Duff were the authors of a 1984 paper on &amp;quot;Compositing Digital Images&amp;quot; in Computer Graphics: That paper defines the pixel algebra we use to composite images. I'm not sure what Haroon meant either though. Maybe that you can't wire the output of a CGContext into the input of a CGImageRef? Right. You can just create a temporary CGContext that refers to an image buffer (akin to a GWorld), render the stroked/filled bezier path into that temporary CGContext, flush the context, then create an imageRef that refers to the image buffer, and render that into your output CGContext, with transparency. That'll do it. Unfortunately, it won't be blindingly fast because it requires rendering to that intermediate buffer. Dave Howell Apple Pro Apps</body>
  </mail>
  <mail>
    <header>Re: Stroke or fill in clear?</header>
    <body>Hi Haroon, Damn. Oh well, thanks for letting me know. Whilst getting the full stroke as a new CGPath would be really cool and quite nice it's actually a little more than I need.  Indeed I'm not really sure this alone would work for my current purposes. What I would really like to be able to do is draw some graphics with no colour mixing (or alpha blending) within a transparency layer.  Maybe this could be one of the things that the CGBeginTransparencyLayer auxiliaryInfo parameter could provide for in the future?  Or maybe this could simply be a feature of colour selection? The main problem that I'm trying to solve is drawing thick bezier paths with transparent outlines and fills, allowing for different opacity levels of the outline and fill but with no overlap between the two. Setting the overall opacity level for a transparency layer with CGContextSetAlpha would seem to provide me with a partial solution to this problem, but means that the fill must be as opaque or less opaque than the outline, since the outline must be drawn after the fill at full opacity for that layer.  (I haven't actually tried that yet, so I'm not sure how CGContextSetAlpha interacts with colour setting calls with alpha parameters as this is unclear in the docs.)  The only alternative I have worked out (and tested) is to stroke the bezier first, then set the clipping path to the bezier, clear it, and then fill, which results in only half the stroked line being left behind, i.e. the bit outside the path. That might be what I'm asking for above since I'm not sure what you mean by &amp;quot;porter-duff composite operations&amp;quot;.  It seems like a simpler mode of drawing than the current forced alpha blending for all draw operations. I actually found this out shortly after writing my email when I stumbled across the CompositeLab sample code.  This would indeed seem to provide a solution, and I am writing my application in Cocoa, but this leads me to another problem, which is as follows... In my application I would like to be able to make use of some CG functionality that is not made directly accessible in the AppKit, specifically shading.  CG calls however generally require a CGContextRef, yet I cannot see how to obtain one for an NSImage.  Any hints? Thanks again for the advice, Steve</body>
  </mail>
  <mail>
    <header>Re: alpha brush</header>
    <body>No when you draw using the mask function it will still draw in SourceOver mode (which is the default mode in CoreGraphics). You still need the composite operations / blend modes to solve the problem. haroon [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: alpha brush</header>
    <body>Haroon, Thanks.  With regard to using the Mask function, can that help me at all with the first problem?  I mean, if the destination image has an alpha buffer, and I draw my new image using an alpha buffer as a mask, is it still the same problem of there not being supported operations? Bob</body>
  </mail>
  <mail>
    <header>Re: Stroke or fill in clear?</header>
    <body>We currently only provide CGContextClearRect as the only way to perform the clear composite operation. There is no way to set the stroke of a path as the clip path. This would be nice. One way we could provide this would be if we provided the stroke of a CGPath as another CGPath. A few people have asked for this already. Maybe for a future Mac OS X release. CoreGraphics does not provide APIs for the porter-duff composite operations. Maybe for a future Mac OS X release it could. Then you could fill and stroke with the clear composite operation to get what you wanted. AppKit does provide NSCompositeClear as a NSCompositingOperation for NSImages. You maybe able to workaround the lack of composite ops for BezierPaths by drawing into an image and then compositing that with clear. Haven't tried this myself though. haroon [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: alpha brush</header>
    <body>What you are asking for are PDF 1.4 like blend modes. In your case, you may want a max or a &amp;quot;Lighten&amp;quot; mode.  This is currently not available in the CoreGraphics API. W.r.t. to the 'current color', use CGImageMaskCreate instead of CGImageCreate. Then when you draw using CGContextDrawImage it will use the current fill color in the gstate. haroon [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Stroke or fill in clear?</header>
    <body>Hi gang, Is there a way to stroke and/or fill paths in a truely clear colour? Let's see if I can explain this...  What I would like to be able to do is draw some stuff on screen, then start a transparency layer so that I can draw some things on top of this stuff.  Within the transparency layer I'll draw more things. Within this transparency layer I would then like to be able to draw some things in a clear transparent colour as a second, separate step, effectively erasing parts of the graphics that I may have already drawn within the transparency layer (resetting those parts back to fully transparent). Once I'm done I end my transparency layer and it gets drawn. I had hoped that using [[NSColor clearColor] set] would accomplish this, but this just seems to result in nothing being drawn.  Opaque white is not an option, since that covers up the image underneath thus defeating the purpose of using a transparency layer. I realise that I can set a clipping path and use CGContextClearRect to accomplish what I want for fills, but I also need to be able to do this for stroked lines of arbitrary widths. Any ideas? Steve</body>
  </mail>
  <mail>
    <header>alpha brush</header>
    <body>Hello, I would like to implement an airbrush using quartz and coregraphics. I have been able to make a brush that tapers off in alpha, using CGImageCreate.  I just make an offscreen GWorld and then make a CGImage from that, using CGDataProviderCreateWithData.  The Gworld is 32 bits and is basically all black but the alpha has a circle whose alpha drops off towards the edge.  The image uses the kCGImageAlphaPremultipliedFirst flag. I draw it onto my window using CGContextDrawImage(context, rectangle, image).  This works as I expect. My two questions:  when I repeatedly draw this over and over, the airbrush quickly overwrites itself and turns to black.  Is there a way to have the airbrush copy itself into the destination so that it doesn't overwrite where the alpha in the destination is already greater or equal to what is there already?  So that drawing over and over in exactly the same place would basically only draw it once, and sliding it around would only affect pixels that are untouched or of less alpha than the brush's pixel? Second question:  Can I have a 'current color', so that the black in my brush is replaced by whatever other color I want? Thanks! Bob</body>
  </mail>
  <mail>
    <header>Re: What changes to CoreGraphics in 10.3?</header>
    <body>Haroon - So far the console has not yielded any usual messages and I do have code in place to check for a null context.  I also verified, using the link you provided, that my parameters are correct.  Now get this... I decided to wipe my drive, install 10.1, then 10.2, then 10.3 plus all the updates in between.  And all of a sudden, my visualizer works perfectly like it used to.  All I can say is that I am happily stumped. So I am left with this impression:  a freshly installed 10.3 is somehow, in regards to CoreGraphics, different that an upgraded 10.3.  Also, this difference is only apparent on my TiBook 667 with 16 MB VRAM. Someday I'll come back to this, but for now I'm just happy it works again. Thanks to all for hints and suggestions :) Cheers, Chrsitoph</body>
  </mail>
  <mail>
    <header>Re: What changes to CoreGraphics in 10.3?</header>
    <body>Check your console log for errors. Also make sure your CGContextRef is non null. i.e. you are not using an unsupported parameters to CGBitmapContextCreate. Check out for the supported list. haroon [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: What changes to CoreGraphics in 10.3?</header>
    <body>Both good suggestions, but I'm afraid neither apply.  I must stress that everything WORKED correctly until a fresh install (read not an upgrade over an existing install) was done.  There was ZERO code changes between installs. So today I took a fresh drive, wiped it, installed 10.3 on it.  Put my project on it (which I should note is an iTunes visualizer) and tried to run it.  I got the same thing as before, a black cube rendered on a black background. By using a fixed texture instead of a quartz bitmap context as a texture I've verified that my OpenGL calls are doing the right thing.  What is wrong is that my CG calls are not modifying the bitmap context since it is perpetually black.  I've also put in debug output that confirms my drawing code is drawing at the correct locations. The only possibilities left that I can think of are: 1)  The 10.3 install disks I have are bunk somehow, maybe some weird pre-release version that has slightly different/incomplete CG calls? 2)  Quartz underwent a change, like coordinate inversion or an initialization call, that I am not accounting for. However, as two people have already stated, 2) did not happen so I guess that leaves me with 1).   Guess I need to borrow someone's install disks and see what happens... Thanks for your time, Christoph PS.  On a side note, I went back and read the CGContextFlush() documentation and it says calling this on bitmap contexts does nothing and isn't necessary.</body>
  </mail>
  <mail>
    <header>Re: What changes to CoreGraphics in 10.3?</header>
    <body>Also make sure you are correctly zeroing and/or intializing the origin in your rects, etc. I have seen folks think that they are drawing starting at  0,0 but forget to zero it out in the struct. -Shawn</body>
  </mail>
  <mail>
    <header>Re: What changes to CoreGraphics in 10.3?</header>
    <body>Nothing this fundamental changed from 10.2 to 10.3.  Are you calling CGContextFlush() at the end before releasing the context? Sure you don't have a clip setup that is clipping out the content. You may want to post some sample code. haroon [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>What changes to CoreGraphics in 10.3?</header>
    <body>I have a project that uses both CoreGraphics and OpenGL, essentially I draw to a bitmap Quartz context and use it as a texture for an OpenGL cube.  It was originally developed using 10.2 and then the OS was upgraded to 10.3.3 with a Panther install disk and subsequent Software Updates.  Then I changed drives in my laptop, which required installing a fresh copy of the 10.3. Now it seems that the quartz context is never drawn i.e. rendered to the bitmap.  The OpenGL code works fine (as long as I use a different texture). Even going in and checking the bits of the bitmap I noticed that there are not changing, which means all my CGDrawXXX calls are not really doing anything. So my question is, did something change in 10.3 from 10.2 regarding CoreGraphics and the flushing of contexts?  Anyone have a clue? Thanks for your time, Christoph</body>
  </mail>
  <mail>
    <header>Re: CGContextSelectFont has no return value</header>
    <body>Your text won't show up, for one thing :)  In general, you should stick with higher-level parts of the system, such as ATSUI &amp;amp; Cocoa, for text drawing. Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: CGColorRef from NSColor</header>
    <body>Sadly, CG objects are not toll-free bridged with any NS object.  Even an NSRect isn't identical to a CGRect (at least, from the compiler's point of view). Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: Drawing and filling beziers with transparent colours</header>
    <body>Sorry, I must have hit the wrong button when replying so it didn't go to the entire list. You didn't say you wanted different opacity levels for the fill and stroke.  In your example you're using 0.5 for both the fill and stroke alpha so you could just draw the path with the transparency layer. Try this: 1) Set the context alpha to 50%. 2) Begin a transparency layer. 3) Fill the path with color containing an alpha value of 60%. 4) Stroke the path with a different color containing an alpha value of 100%. 5) End the transparency layer. Since the context alpha is 50% you'll now have a fill of 30% and a stroke of 50% but you won't be able to see the fill through the stroke. The limitation is that the stroke must be less transparent than the fill or you will see the fill through the stroke.  In this case you could draw the stroke twice in the transparency layer, once with the opaque background color (e.g., white) to erase behind, and once with the desired color. Nick</body>
  </mail>
  <mail>
    <header>CGColorRef from NSColor</header>
    <body>Is there any way to go from an NSColor to a CGColorRef? Are they 'toll free bridged'? Does anyone know of a place in the Apple documentation that discusses which CG objects are toll free bridged (if any)?</body>
  </mail>
  <mail>
    <header>Re: Drawing and filling beziers with transparent colours</header>
    <body>Hi All, As you may have noticed I posted a question here the other day about drawing and filling bezier paths with transparent colours - I could not work out how to do this without getting an overlap of the fill and stroke. Unfortunately there were no suggestions posted to the list as to how to solve this little conundrum.  However I did receive one message off-list from Nick Nallick who suggested the following: Now this is a very interesting idea, but I couldn't see how to adapt this to render a bezier outline and fill of different colours with different opacity levels. I had already thought about this a little more myself and had come up with a partial solution, which was to effectively clear the fill area by filling with opaque white first.  However this is flawed - any shadow rendered will be dark (since the white is fully opaque) and you also can't see through to anything that might have already been rendered underneath the fill of the transparent bezier path. The principle though is sound - clear the fill area first and then perform the fill in the colour we wish to use.  I thought a while longer and looked at the docs a bit more, and came up with this.  The following code renders a background green empty rectangle, and above it renders a transparent blue rectangle filled with transparent red, and also has a shadow.  It renders correctly - you can see through the transparent rectangle, and there is no overlap between fill and outline. This code, as before, needs to be placed in the drawRect routine of an NSView subclass: //  Get a white background //  Code that actually works - the CG way CGContextRef context = [[NSGraphicsContext currentContext] //  Draw and fill a beziers //  Draw a background image, a green rectangle with no shadow //  Turn on our shadow - this will be drawn over background image //  Turn on a transparency layer, since we want our transparent rectangle drawn on top //  Transparent red for outline //  Clear the fill area //  fill our rectangle with transparent blue Now this solution works for rectangles, but obviously cannot work for an arbitrary bezier path since CGContextClearRect can only clear a rectangle.  Having read the docs a bit further it seems to me that the true solution is to use a solid pattern fill which is set as a mask. However I must admit that I found the documentation on pattern fills slightly confusing, and couldn't see any sample code for doing this. So the question now is does anyone know anything about using CG pattern fills?  Any pointers to any examples? Cheers, Steve</body>
  </mail>
  <mail>
    <header>CGContextSelectFont has no return value</header>
    <body>Dabbling around in Quartz I wonder, if I pass some parameter into the function CGContextSelectFont, that Quartz doesn't like - maybe a font that is not installed - how am I supposed to notice that ? Ciao Nat! ------------------------------------------------------ But didn't I always tell you, honey, if I just stayed in place and I never spoke up, good things are bound to happen ?  -- Jonathan Gems</body>
  </mail>
  <mail>
    <header>Drawing and filling beziers with transparent colours</header>
    <body>Hi gang, I have a curious problem...  I have a bezier which I would like to draw with a wide line width and transparent line and fill colours.  When I do this I get a curious fill overlap effect - the line gets drawn centred around the bezier path description, and the fill is conducted also from the bezier path description, thus there's an overlap of half the line width. Here's a code snippet that, when placed into a suitable Cocoa application drawRect routine, should demonstrate what I mean. CGContextRef context = [[NSGraphicsContext currentContext] I get the same behaviour when I use Cocoa NSBezier calls too. Ideally I would like a way to avoid this colour mixing at the overlap between the line and the fill.  Any ideas as to how I could accomplish this? Steve</body>
  </mail>
  <mail>
    <header>Re: Boldface mystery w/ linked image</header>
    <body>Probably makes more sense when adding that it was with Monaco 12 pt. :)</body>
  </mail>
  <mail>
    <header>Boldface mystery w/ linked image</header>
    <body>Hi folks... Environment:  Mach-O under Carbon. I use ATSUI commands to setup a font so that I can do rendering in an offscreen port (that I later go on to use for custom bit-blitting elsewhere).  However, when I draw text on white over black, the text seems to be overly thin and whispy.  When I compare it to it's equivalent in TextEdit, there's clearly a difference.  The only cause I can think of is that I render white over black, but there's a good reason I list below that suggest this isn't the problem.  I posted a simple clipping comparing the two at:   Note how there's clearly anti-aliasing on my text (so the possibility that ATSUI isn't anti-aliasing isn't the reason why it's overly thin).  Note the how the &amp;quot;fl&amp;quot; seem match TextEdit's rendering of &amp;quot;fl&amp;quot; -- I have no explanation of this.  Finally note that spacing between characters is definitely less with my text, suggesting there are actual rendering differences going on.  Comments/suggestions? Thanks in advance. Andy O'Meara</body>
  </mail>
  <mail>
    <header>Re: Why does CG &amp;quot;smarten&amp;quot; quotes in PDF?</header>
    <body>Do you get the same behavior in Acrobat?  If so, then there may be something in the PDF file you generate which is causing this mapping.  If not, then this sounds like a bug in CG.  It would help if you could file a bug about this, along with a PDF file which illustrates the problem. Thanks, Derek</body>
  </mail>
  <mail>
    <header>Why does CG &amp;quot;smarten&amp;quot; quotes in PDF?</header>
    <body>BT 1 0 0 -1 101 204 Tm /F1 153 Tf (') Tj ET Preview (as well as Adobe Acrobat) display this single quote as a smart/culy single quote.  If I select and copy this text from Preview I get unicode character 0x2019 (RIGHT SINGLE QUOTATION MARK), when the character that was intented to be displayed was unicode 0x0027 (APOSTROPHE). Why does Preview (i.e. CoreGraphics PDF renderer) change the character that's being displayed from a regular single quote to a smart single quote? Rob Raguet-Schofield (rob ra gA skO fEld)</body>
  </mail>
  <mail>
    <header>Re: pattern vs. dash</header>
    <body>Thanks. Well yeah, it usually ugly, but using the correct pattern will animate in a satisfactory way. However, animating the dash is the clear winner as far as esthetics goes. I started giggling when I first got it working after living with pattern-based marching ants all these years. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: inputScale limitation in CIPixellate</header>
    <body>On Jun 28, 2005, at 5:20 PM, Ralph Brunner wrote: Splendid! Works beautifully! Thanks! Roland</body>
  </mail>
  <mail>
    <header>Re: pattern vs. dash</header>
    <body>I'm now doing that as well, but this selection is for our magic wand tool, so it's possible for a transparent colored fill to not been seen as well in some areas if the colors are similar to the overlay color. In those cases, it's really nice to see an animated frame. A non-animated dashed frame can also be chosen by the user, but that can be very hard to see in other cases, like with very complex bitmaps. For all these reasons, I still want to support marching ants as an option. All I want to know is if someone on the Quartz team that worked on dash stroking and patterned stroking could say offhand which is faster. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: pattern vs. dash</header>
    <body>Is it important to stick with marching ants? Consider semi transparent overlay window instead (what Finder does). Mike</body>
  </mail>
  <mail>
    <header>inputScale limitation in CIPixellate</header>
    <body>I'm using the CIPixellate filter to simulate machine vision through differing sensor array grids. This means I need to be able to convert an input image to 1x1, 2x2, 4x4, 8x8, 16x16, etc., up to full resolution. Instead of providing some sort of &amp;quot;resultBlocks&amp;quot; argument, the CIPixellate filter uses an &amp;quot;inputScale&amp;quot; argument, which goes from 1 to 100. So, starting with an input image of 800x800, the lowest pixellation value I can achieve is 8x8. How can I achieve 1x1, 2x2, and 4x4 pixellations? I also have cases that use larger images. Is it possible to go beyond the 100 maximum for inputScale?</body>
  </mail>
  <mail>
    <header>Re: Path gradient/shading</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Python-Quartz Shadings</header>
    <body>I have used the python quartz bindings to enerate a range if images in pdf &amp;amp; png formats. I'm stuck on how to apply shadings to objects using the python bindings. Is it possible to use shading with the python API's, and if so, does anyone have sample code showing how? TIA Ian B</body>
  </mail>
  <mail>
    <header>Path gradient/shading</header>
    <body>Hello, I am trying to implement a complicated gradient/shading filling using Quartz. I need something like path gradient in GDI+ on Windows. The examples of path gradient are: I would like to divide the shape into triangles with one corner in the center point of the shape and other 2 corners in the shape path. In the first example it would be 3 triangles inside the big triangle, in the 2nd example it would be 4 triangles inside the rectangle. Each triangle would use the regular linear shading created with CGShadingCreateAxial(). But the problem is when you draw 2 shaded triangles side by side, they do not &amp;quot;stick&amp;quot; together nicely. There is an antialiased edge between them. So the end effect is that you see the lines where those triangles are connected. I tried to turn off antialiasing by calling CGContextSetShouldAntialias(context, false) and the effect is that the edges of neighbor triangles overlap. It looks OK with opaque colors but unfortunately it shows edge when you use transparency (the same happens if I try to expand the triangle size by 1 pixel so  the edges overlap). Any advice how to glue shaded shapes together in a way that the edges don't show? Thanks, Tom</body>
  </mail>
  <mail>
    <header>Re: CoreImage Interfilter Pixel Values</header>
    <body>All components are treated the same when passing values from one filter to another.</body>
  </mail>
  <mail>
    <header>PNG images drawing blank</header>
    <body>I have some PNG images in a document that are coming out completely white when drawn with Quartz.  The image is 32bit.  Other PNG's are fine.  The problem image also draws fine when decompressed with Quicktime.  I suspect the alpha channel in the PNG is what is blanking it out.  The pixels acquired via Quicktime have zeros for the alpha. Is there a way to get Quartz to disregard the alpha in the compressed image?</body>
  </mail>
  <mail>
    <header>Re: CGImageCreate versus CGImageCreateCopyWithColorSpace</header>
    <body>Hi Haroon, Thanks a lot for your irrefragable answer and valuable information. But I still have one question on the advisability of using CGImageCreateCopyWithColorSpace. You said that this function is more effective in the case ``CGImageRef is passed from some other piece of code and there is no ability to access to the original data'', would you please explain me once again this sentence? Because if I've realized correctly I've always got a possibility to get a data provider from a reference CGImageRef: // ... CGImageRef newImage = CGImageCreate(..., ..., ..., ..., ..., ..., ..., CGImageGetDataProvider(oldImage), Thanks for your time. Best regards, Vladimir Magaziy. ----- Original Message ----- Sent: Sunday, March 27, 2005 4:12 AM Subject: Re: CGImageCreate versus CGImageCreateCopyWithColorSpace No difference is performance. CGImageCreateCopyWithColorSpace will call CGImageCreate. If you have access to the original data, then calling CGImageCreate is one option. There are times when a CGImageRef is passed to you from some other piece of code and you don't have access to the original data. in that case CGImageCreateCopyWithColorSpace more appropriate. Either way will work in your case. __ haroon sheikh Ÿáÿßÿ±ŸàŸÜ ÿ¥ŸäÿÆ</body>
  </mail>
  <mail>
    <header>Re: CGImageCreate versus CGImageCreateCopyWithColorSpace</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>CGImageCreate versus CGImageCreateCopyWithColorSpace</header>
    <body>Hi, I need to create an image (CGImageRef) from other image (CGImageRef) with different color space, as you know, to do it I can use two CoreGraphics functions: a) CGImageCreate with the same image characteristics and the same data 2) CGImageCreateCopyWithColorSpace starting from Panther. Would you recommend me the best way in the view of performance? What variant will be faster -- to switch according to a system version and use CGImageCreate and CGImageCreateCopyWithColorSpace or to use CGImageCreate in both cases? If it's not a confidential information, does CGImageCreateCopyWithColorSpace use internally CGImageCreate? Thanks in advance for your help and time. Regards, Vladimir. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGPSConverterCreate and thread safety</header>
    <body>On Mar 25, 2005, at 9:21 AM, Bryan Pietrzak wrote: Why not do the conversion in a secondary thread always since you are doing the wok to create such a thread? This would keep you UI responsive regardless how long the conversion process takes (the one time delay could change to an every time delay since it isn't documented behavior). Also this would avoid the work unless the user actually wanted to do such a conversion.</body>
  </mail>
  <mail>
    <header>Re: CGPSConverterCreate and thread safety</header>
    <body>On Mar 25, 2005, at 10:21 AM, Bryan Pietrzak wrote: I don't know the answer to your specific question, but you could always just assign responsibility for communicating with the converter to the thread that created it.  That way it would never try to talk to the converter before it was fully initialized.</body>
  </mail>
  <mail>
    <header>CGPSConverterCreate and thread safety</header>
    <body>We use CGPSConverterCreate to take EPS files and convert them to PDF for internal use. The only problem we have is that the first instance of this is really slow, spinning cursor of death and everything. Preview's the same, so we know it's not just our app. So, we were wondering, could we spawn a thread when our app starts up to prime the converter so-to-speak? So that if the user then imports an EPS the converter is already ready and waiting? But, it's not clear if this is thread safe: what happens if we need to convert EPS to PDF while the other thread is still initializing it.</body>
  </mail>
  <mail>
    <header>CGContextDrawPDFDocument and Art Box drawing</header>
    <body>I have an app that needs to run on 10.2 and uses CGContextDrawPDFDocument to draw a PDF document into a CGContext. However, the PDF document is US Letter size but the contained art is much smaller so I would like it to draw into my rectangle using the art box as a basis and not the media box (otherwise the art gets way too small). With CGContextDrawPDFDocument that does not seem to be possible, but I figure that using some clever transforms I could achieve the right result. Now the transforms are not my greatest strength so if anyone here has solved this issue and is willing to share the appropriate transforms that would be appreciated :-) david.</body>
  </mail>
  <mail>
    <header>Re: Poor scaling of JPEG images</header>
    <body>While I was building the sample app I discovered what had been going on. Turns out it was my error. I was calling CGImageCreateWithJPEGDataProvider with shouldInterpolate false because originally my code was working with the images at full size and interpolation was not necessary. I forgot to pass true for that parameter when I started scaling my images. So that was why I was getting all kinds of artefacts. Passing true fixed it.</body>
  </mail>
  <mail>
    <header>Re: Poor scaling of JPEG images</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Poor scaling of JPEG images</header>
    <body>I just noticed that when I draw a JPEG image (created with CGImageCreateWithJPEGDataProvider) into an offscreen CGBitmapContext that is some 8 times smaller than the original image I get an image with many artefacts. If instead I draw the JPEG into a full size CGBitmapContext and then after that draw the result as a CGImage into the 8 times smaller CGBitmapContext I get a much better looking picture. The difference between these two approaches is huge, with the first looking really bad and the second one looking great. Is this difference to be expected for some reason, should I file a bug? I tried chaing the interpolation quality but it had hardly any effect on this issue. david.</body>
  </mail>
  <mail>
    <header>Re: CGContextDrawPDFDocument versus CGContextDrawPDFPage</header>
    <body>Ok, thanks. So I will just get rid of it once I set my minimum system requirement to 10.3 and keep using it for now. From your reply I gather there are no major speed penalties when using use CGContextDrawPDFDocument instead of CGContextDrawPDFPage. david.</body>
  </mail>
  <mail>
    <header>Re: CGContextDrawPDFDocument versus CGContextDrawPDFPage</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>CGContextDrawPDFDocument versus CGContextDrawPDFPage</header>
    <body>I need to support 10.2 and higher, are there reasons why I should not just use CGContextDrawPDFDocument on 10.3 and later as well if it meets my needs? From the docs I understand CGContextDrawPDFPage is recommended for 10.3 and later, but they do not explain why it would be a problem to still use CGContextDrawPDFDocument if older OS versions need to be supported too. Of course I can create to different code paths for use on 10.2 versus 10.3, but before doing that I just want to make sure there is a real benefit to it. david.</body>
  </mail>
  <mail>
    <header>Re: How to get a pointer to PNG data</header>
    <body>You can always create a new a new buffer, then  CGBitmapContextCreate to point to the buffer and draw your PNG image into it using CGContextDrawImage. BTW, you do know that PNG images do support Alpha channels, so you could make sure the source image was created using an alpha channel and then it's possible (depending on your application), you may not need to create a mask?</body>
  </mail>
  <mail>
    <header>How to get a pointer to PNG data</header>
    <body>Hi, I have an PNG image and trying to get an image mask of that, I tried the &amp;quot;CGImageMaskCreate&amp;quot;,it returnes the image mask reference but after drawing with CGContextDrawImage it draws a black rectangle. Since I tried all of the options and had no success, I want to get the pointer to source image data to be able to scan the image and generate my own mask from that image file, I searched in header &amp;quot;CGDataProvider.h&amp;quot; but didn't find any API. Does anyone have idea how i can do it. Thanks for any help, Nima</body>
  </mail>
  <mail>
    <header>Slightly OT: Resource for Quartz Devs</header>
    <body>Sorry to suddenly jump in but I wanted to tell the developers here that iDevGames (www.idevgames.com), the Mac Game Developers' Community, has launched a new website devoted to Macintosh application developers. Located at , this new„ÄÄbulletin board features message boards related to topics such as application development (API, framework, APIs, etc.),  porting applications to/from Mac OS X, development tools, product promotion/announcements, and developer business issues. I'm of course hoping that iDevApps can be a great place for Quartz developer discussions. :) The community forum at iDevApps is the first step in building an active community devoted to those developers who create commercial, shareware, and open source Mac OS X software applications. In the future, we would like to provide Mac developer news, interviews, articles and much more. If you are interesting in writing articles or tutorials, please let me know off-list. Carlos A. Camacho Editor-in-Chief iDevGames.com iDevApps.com email@hidden</body>
  </mail>
  <mail>
    <header>Re: Getting any pixel on screen</header>
    <body>There's a sample called GLGrab on the developer.apple.com site somewhere. According to what I remember from WWDC, it demonstrates the preferred way to read back the screen buffer.</body>
  </mail>
  <mail>
    <header>Re: extra bytes from gWorld pixmap</header>
    <body>Nice try; It's what I'd like to believe except that 400 pixels per row * 4 bytes per pixel == 1600 bytes per row, divided by 16 == 100; A nice 16-byte boundary; 404 pixels per row &amp;amp; 4 bytes per pixel == 1616 bytes per row, divided by 16 == 101 which seems a little &amp;quot;odd&amp;quot; to me. ;-) -- Enjoy, George Warner, Schizophrenic Optimization Scientist Apple Developer Technical Support (DTS)</body>
  </mail>
  <mail>
    <header>re: extra bytes from gWorld pixmap</header>
    <body>Nope; the last 4 bytes are &amp;quot;extras&amp;quot; (sometimes used for hardware cursor info, etc.). If wasting those 4 bytes per row bothers you then you might consider using NewGWorldFromPointer. -- Enjoy, George Warner, Schizophrenic Optimization Scientist Apple Developer Technical Support (DTS)</body>
  </mail>
  <mail>
    <header>Re: Getting any pixel on screen</header>
    <body>It looks like the function it might use is _CGBlt_copyBytes and _CGBlt_fillBytes, and _CGSCurrentInputPointerPosition - the last one I assume to get the position (all from AppKit, although they may be undocumented.)  I haven't poked further with than nm though.</body>
  </mail>
  <mail>
    <header>Re: Getting any pixel on screen</header>
    <body>Poke with otool, nm, and gdb..</body>
  </mail>
  <mail>
    <header>Re: extra bytes from gWorld pixmap</header>
    <body>Quickdraw will align the scanlines for 16-byte boundaries, but it is already at a 16-byte boundary at 400. In my experience QD almost never uses the rowbytes that you expect given the dimensions of your GWorld. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: extra bytes from gWorld pixmap</header>
    <body>Actually, I believe QT is aligning the scanlines on 16-byte boundaries for Altivec.</body>
  </mail>
  <mail>
    <header>Re: Custom view with Quartz-composited layers?</header>
    <body>On Mar 18, 2005, at 6:28 PM, Hamish Allan wrote: The simple answer is that if you want to have to separate images composited in a single view you're responsible for that composite function.  You could create a transparent overlay window and let the Quartz compositor take care of the compositing but then (as you hint at) you're responsible for things like alignment to your scrolled position.  In the end I suspect you're probably better off just managing the composite function yourself and drawing into the view as if it was a blank canvas.  I believe there are some compositing functions in Cocoa to help you with this but that's going to depend on exactly what you want to do. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Re: extra bytes from gWorld pixmap</header>
    <body>The first 4 bytes are your first pixel. The extra 4 bytes are at the end of the row, they are just there as a cache optimization (so that you use more of the cache when you access the entire image). -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Custom view with Quartz-composited layers?</header>
    <body>I'm new to drawing in Quartz, so please forgive me if this is a daft question. I'm writing an application which can be used to edit sound files. I have a custom NSView subclass drawing the amplitude envelope of the audio waveform, along with various vertical lines for the current editing cursor, the current playback position, etc. This view is inside an NSScrollView with a horizontal scroller, and a slider underneath the NSScrollView controls the zoom level (ranging from the whole audio clip in view when slider is to the left, to sample-level resolution when the slider is to the right). When one of these lines should move (e.g. when audio is playing, the current playback position cursor moves constantly), I invalidate the view (e.g., from the audio player callback) so that my view's drawRect: is called the next time round the run loop. drawRect checks whether the rect and bounds have changed since its last invocation, to see whether the NSBezierPath describing the amplitude envelope needs to be recalculated. Then it draws the bezier path and the vertical lines on top. At some point I thought I might render the bezier path into an image I can then blit straight onto the view, which should improve performance. But ideally what I want to have is two views overlaid onto one another, one for the amplitude envelope and one on top of it for the vertical lines, and only to invalidate the top view. Now, I realise that I would have to redraw the bottom view anyway to get rid of the old cursor position, so what I really want is for my views to be composited in the way that windows are by Quartz, i.e., buffered and only redrawn when their contents change rather than when they change position. So my first question is this: do I need to use a separate window, and how would I keep it aligned in the scrollview? And secondly, has anyone ever tried anything like this before, and have experiences to share? Many thanks, Hamish</body>
  </mail>
  <mail>
    <header>extra bytes from gWorld pixmap</header>
    <body>I know gworld's are on the way out, but for some applications, i.e. Quicktime it seems like you have to resort to using them. Thus my question.  I've created a offscreen gworld to decompress a QT video into that is 400x300 pixels in size.  After generating the gworld, I get its pixmap.  However when I call GetPixRowBytes() on the pixmap (and divide by 4 for rgba), I get 404.  Is the first 4 bytes a header? Thanks, David</body>
  </mail>
  <mail>
    <header>CGImageMaskCreate</header>
    <body>Hi, I was playing with  &amp;quot;CGImageMaskCreate&amp;quot; and tried to draw the  image mask on my window, but looks for all given PNG source image, it returns  mask files that are completely black ( if I dont use &amp;quot;decode&amp;quot;). Does anybody have any idea what I am doing wrong or missing? the same code can draw the source image file on screen perfectly. Thanks for any help, Nima // somewhere in the main .... maskRef = CGImageMaskCreate (CGImageGetWidth(source), CGImageGetHeight(source),CGImageGetBitsPerComponent(source), CGImageGetBitsPerPixel(source), CGImageGetBytesPerRow(source), err = CreateNewWindow(kDocumentWindowClass box	= CGRectMake( 0, 0, CGImageGetWidth(source) , CGContextDrawImage (cgContext,box,maskRef);  // ********** DRAWING ***********// QDEndCGContext( GetWindowPort(/*getParentWindow()*/ // OSStatus createCGImageFromResource(CFStringRef inPictureFileName,CGImageRef*  outCGImageRef ,CGDataProviderRef* ioProvider) CFURLRef url = CFBundleCopyResourceURL(CFBundleGetMainBundle(), *outCGImageRef = CGImageCreateWithPNGDataProvider(provider, NULL,</body>
  </mail>
  <mail>
    <header>Re: Getting any pixel on screen</header>
    <body>I had thought it was called &amp;quot;Pixie,&amp;quot; but it turned out that's the name of the developer tool which does the same thing. My searching didn't turn up anything useful, so that's why I asked here. Thanks! I've downloaded it and am looking through it now. My searches didn't turn up the above for some reason. (Probably because I was looking for information about getting the color of a pixel rather than searching for information about magnifying stuff.) Well, I have a Quartz version working. I get the (first) display for the current mouse coordinates like so: Then (assuming that succeeded), I get the address of the point for that display like so: and convert the pixel at pixAddr to the format I need. As I say, this all seems to work fine, but I'm just wondering if that's the best way to do it. For example, I don't currently handle the 8-bit per pixel case because I don't see any way to get the current palette. But it's exceedingly unlikely that my users will have their monitors set to that setting when working in After Effects, so I don't care too much about that case. I just get a little concerned that I'm not seeing all possible cases in my testing, or there might be other gotchas that aren't immediately obvious. Thanks, Darrin -- Darrin Cardani - email@hidden President, Buena Software, Inc. Video, Image and Audio Processing Development</body>
  </mail>
  <mail>
    <header>Re: Getting any pixel on screen</header>
    <body>This is the sample code to which I think Bryan is referring:</body>
  </mail>
  <mail>
    <header>Re: Getting any pixel on screen</header>
    <body>On Mar 17, 2005, at 2:51 PM, Darrin Cardani wrote: Seems to me there is sample code showing how to do this, I don't remember the name offhand...</body>
  </mail>
  <mail>
    <header>Getting any pixel on screen</header>
    <body>I have a plugin that provides color swatches to the user as they work in the host application (which in this case is Adobe After Effects). I have an eyedropper tool which allows the user to choose a color from anywhere on screen. In the past I had been making it work using QuickDraw calls, like so: But I've found that it seems not to work randomly. As the user scrubs across the screen, it seems almost like every other value it gets is bogus, or is pulled from another part of the screen. So I'm looking for the proper way to get this information. I've tried using CoreGraphics Direct Display APIs and it's working correctly. But I'm curious if that's the best way to do this, or if there's another recommended way. Any pointers would be greatly appreciated. Thanks, Darrin -- Darrin Cardani - email@hidden President, Buena Software, Inc. Video, Image and Audio Processing Development</body>
  </mail>
  <mail>
    <header>Re: Fuzzy PDF Display</header>
    <body>As Haroon says, this is dangerous and unsupportable, but FWIW this is the code that fixes my problem in 10.3/Xcode: // use PRIVATE API to enable reduction box filter CG_EXTERN void CGContextSetInterpolationQualityRange(CGContextRef c, CGInterpolationQuality quality, CGInterpolationQuality quality2) if ((CGContextSetInterpolationQualityRange) != (void*)kUnresolvedCFragSymbolAddress)) CGContextSetInterpolationQualityRange(gc, kCGInterpolationHigh, else I'm not sure how you do &amp;quot;__attribute__((weak_import))&amp;quot; in CW.</body>
  </mail>
  <mail>
    <header>Re: Fuzzy PDF Display</header>
    <body>I've verified that this is the interpolation quality problem we've been discussing and Alex alluded to earlier. Unfortunately as I mentioned earlier, this bug is fixed in Tiger, but I cannot provide an official workaround in Panther. Others maybe able to help in how to get the private, unsupported API CGContextSetInterpolationQualityRange to work for you.</body>
  </mail>
  <mail>
    <header>Re: Approaches for 20fps full-screen arbitrary images</header>
    <body>Sorry for the delay in this post, but just to close the thread for now: a quick thank you for the advice. From an initial test (thanks Alex) it looks like performance will be faster than 30fps for me to to load, decompress, and display an unlimited series of 1024x768 images on the fly using a circular buffer, without needing to resort to generating Quicktime movies. That will allow for the most flexibility in my database project. Will post back here, or in the OpenGL forum, once I'm deeper into it. ... Why not make objects which load on first view the needed image in RAM and than draw it from RAM? ... And this way you can let the system decide which pages could be send to disc (Virtual memory). ... Assuming GL_UNPACK_CLIENT_STORAGE_APPLE causes the memory texture to be read directly from app memory with good performance and little constraints on when that data can change ... ... But the number of possible movies must be somewhat limited. Couldn't all of them be generated in advance? ...</body>
  </mail>
  <mail>
    <header>Re: Fuzzy PDF Display</header>
    <body>I have filed a bug report for this. The Problem ID is 4055480.  As you mentioned in previous post in this thread. A workaround for this problem, which has been used by Preview is to use the private API CGContextSetInterpolationQualityRange(...)? I have tried to use it as a weakly-linked function, but unsuccessful. My OS is Mac OS 10.3.8 and my IDE is CodeWarrior 5.5.2. Could you give me some specific information on how to use the private API function. I really need to solve this anti-aliasing problem on 10.3. If the above workaround is not recommended as someone has mentioned in the posts, is there any other workaround? Many thanks for your help regards, Yu</body>
  </mail>
  <mail>
    <header>Re: Fuzzy PDF Display</header>
    <body>bugreporter/index.html Attach a sample application, and the PDF file in question. Also include screenshots of what you seen and what you expect. Send me the bug id number.</body>
  </mail>
  <mail>
    <header>Re: Fuzzy PDF Display</header>
    <body>It does this by itself, periodically flushing the window from a separate thread. CoreGraphics does not provide any progressive rendering support.</body>
  </mail>
  <mail>
    <header>Re: Fuzzy PDF Display</header>
    <body>On Mar 15, 2005, at 2:09 AM, Haroon Sheikh wrote: how does preview accomplish that progressive rendering using coregraphics directly?</body>
  </mail>
  <mail>
    <header>Re: copy between CGContextRef's</header>
    <body>All I have two CGContextRef's, created with CGBitmapContextCreate() How best to 'blit' one's data to the other?</body>
  </mail>
  <mail>
    <header>copy between CGContextRef's</header>
    <body>All I have two CGContextRef's, created with CGBitmapContextCreate() How best to 'blit' one's data to the other? Thanks Nick _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Fuzzy PDF Display</header>
    <body>I have tried to draw directly into a window, but still got the same fuzzy display. I have searched through the Quartz archive. Found the following two threads might be related to my problem: 2. &amp;quot;Using CG private APIs (Was: Anti-Aliasing Quality when Drawing PDF)&amp;quot; () It may be an anti-aliasing problem.  I have tried functions like: CGContextSetInterpolationQuality(..), CGContextSetShouldAntialias(...), CGContextSetShouldSmoothFonts(...), CGContextSetRenderingIntent(...) but they have no effect on the output. In the second thread, you have discussed about using the private API CGContextSetInterpolationQualityRange(...) to fix the anti-aliasing problem. My system is 10.3.8 and I am using CodeWarrior. May I know how can I use this private function in my application. I have used &amp;quot;nm&amp;quot; to analyze Preview, and I found Preview used the above function. So I think Preview must have used some private API so that its output is better that what I can get by using the published Quartz API. How can I improve my PDF drawing? Is there any other ways? regards, Yu</body>
  </mail>
  <mail>
    <header>Re: Fuzzy PDF Display</header>
    <body>Thank you for this confirmation. I don't know if it helps the original poster, but it is good to know that there is a solution in place in Tiger for the problems I was having. :)</body>
  </mail>
  <mail>
    <header>Re: Using CG private APIs (Was: Anti-Aliasing Quality	when	Drawing	PDF)</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Fuzzy PDF Display</header>
    <body>This was a bug in Panther and should be fixed in Tiger so that the interpolation quality using CGContextSetInterpolationQuality() should be respected when a PDF page is drawn, which wasn't the case in Panther. While Preview did use the private API to workaround the issue, that API was not ready for prime-time and that is why we chose not to expose it publicly. Not ready for prime-time implies that we are not confident the API is something we can support or is the right model. Given that we have had enough internal debates on that particular private API, it was prudent to not release it. Once it is made public we cannot take it back. So, the Tiger solution is the recommended solution. Unfortunately this does mean that there is no public API solution for Panther. Using the private SPI on Panther maybe an unsupported solution - Do so at your own risk. So as you say, if the PDF document has embedded bitmaps for the chinese glyphs, it is possible this could be the cause.</body>
  </mail>
  <mail>
    <header>Re: Using CG private APIs (Was: Anti-Aliasing Quality when	Drawing	PDF)</header>
    <body>This is different from Preview; &amp;quot;Anti-alias text and line art&amp;quot; will use a nice box filter on reduced PDF bitmaps. There is no &amp;quot;safe way&amp;quot; to check that the API exists and that it uses the same arguments you think it uses.  That's the entire point behind Private APIs :-)</body>
  </mail>
  <mail>
    <header>mode switch confirmation box...</header>
    <body>I am wondering if there's an API to bring that nice Mac OS X confirm box after I have done a mode switch. there's an API called DMConfirmConfiguration() but it's not implemented on X (I have never used it on 9, even). In CGDirectDisplay.h, it mentioned there's a flag called &amp;quot;kCGDisplayModeIsSafeForHardware&amp;quot;.  So, if  I copied the CFDictionary from CGDisplayBestModeForParameters() but removed that key-value, will the system do the confirmation box automatically? Thanks in advance for any insight. pete</body>
  </mail>
  <mail>
    <header>Re: Fuzzy PDF Display</header>
    <body>This is false. Preview uses a private API to improve bitmap antialiasing quality. See thread: If the original poster's Chinese glyphs are embedded bitmaps, this could be the cause. If they are actually fonts, then this is unrelated.</body>
  </mail>
  <mail>
    <header>Re: CGDataProviderCreateDirectAccess</header>
    <body>Interesting, this implies I can leave the &amp;quot;get byte pointer&amp;quot; null and Quartz will not call it? There are three types? I thought only direct-access and sequential-access. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com</body>
  </mail>
  <mail>
    <header>Re: CGDataProviderCreateDirectAccess</header>
    <body>If you supply the &amp;quot;get byte pointer&amp;quot; callback, then yes, all of the data must be available and you must return a pointer to all of the data. The docs aren't quite accurate.  Depending on the type of the URL, any of the three flavors of data provider may be created.  There's no way to guarantee which flavor you'll get for a given URL, however. If you need finer control over the memory usage, writing your own data provider is a good idea. There's no public API to fetch the bytes. Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: PICT with alpha from a CGBitmapContext?</header>
    <body>James, I've run into this same issue.  I never found a satisfactory solution and wound up using TIFF temp files and apple events to move my image to/from Photoshop.  If you find a better solution, I'd love to hear about it. Jason Harris</body>
  </mail>
  <mail>
    <header>PICT with alpha from a CGBitmapContext?</header>
    <body>I've got a CGBitmapContext with an alpha channel that I'd like to put on the clipboard as a variety of different flavours (this is in a Carbon app). Using QuickTime's GraphicsExportSetInputCGBitmapContext routine, I can make some TIFF data with alpha from my context in a couple of lines of code. No problem. Cocoa apps can handle this fine. Photoshop doesn't see TIFF data on the clipboard though and the PICT graphic exporter component doesn't seem to want to make a PICT with an alpha channel. I think that when Photoshop makes a PICT with alpha, it creates a PICT with an embedded QuickTime compressed image, but I've been unable to figure out a good way to create one from my context such that the alpha channel is preserved. Do I need to manually convert my context to a PixMapHandle and use something like FCompressImage? Thanks for any help, James</body>
  </mail>
  <mail>
    <header>CGDataProviderCreateDirectAccess</header>
    <body>All: 1. When I create direct-access Quartz data provider, as with CGDataProviderCreateDirectAccess, must all the bytes be available at the same time? That seems to be the implication behind the callback CGDataProviderGetBytePointerCallback. 2. If that is the case, and CGDataProviderCreateWithURL actually creates a direct-access data provider, as stated in the docs, does that mean the entire download of the resource happens before the using function (e.g. CGPDFDocumentCreateWithProvider) runs? Thus if I wanted a data provider that progressively loads a resource as the using function runs (e.g. to minimize memory overheads), I would have to write my own data provider? 3. Is there any way to intercept or filter the bytes of a data provider? I don't see any functions which when given a data provider, actually allow you to fetch bytes out of it. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com</body>
  </mail>
  <mail>
    <header>Re: Capabilities of Quartz: rotation? zooming?</header>
    <body>First and foremost, sorry for the cross-post, but there's been parallel threads on this topic on both lists, and I'm too lazy to write two emails. :P Anyway, the big news is that it's working properly now.  All in-window events are translated correctly, so far as I know, so you can do everything as normal - select text, drag &amp;amp; drop, etc.  The new version is up at: So no more complaining about file formats. :) There are still issues with certain things that the window manager controls, for instance the dragging by title-bar; this can only be done from the top N pixels (whatever the title bar width is) of the window, which must coincidentally contain at least some part of the title bar. So you can't move the window when it's upside-down.  I imagine metal windows can be moved as normal (or any move-by-content windows), but I haven't tested it.  I'm not sure if there's anything I can do to the window server to fix this problem... conceivably I could check the translated event co-ordinates for each incoming mouse-down, and hand them off to the appropriate window method if they're in the title bar. But I'll have to test that. On the up side, resize on an angle/scaling does work properly, which is cool. Additionally, the code is a fair bit cleaner now. I've been looking at the Carbon side of things... I'm hoping that Carbon just calls the Cocoa methods, but I've got a feeling if anything it would be the other way around.  If someone with a bit of Cocoa/Carbon knowledge could test it out and see what happens, that'd be great. So, thanks everyone for your patience.  I hope this is of use to some people. Wade Tregaskis (aim: wadetregaskis) -- Sed quis custodiet ipsos custodes? P.S.  I've forgotten again to put licensing stuff in the files, but it's all BSD licensed.  Do what you will with it.</body>
  </mail>
  <mail>
    <header>Re: Capabilities of Quartz: rotation? zooming?</header>
    <body>Yes, before anyone runs up screaming about it to me, I've just become aware that the effects of scaling aren't taken into account with the event transforms... I have no idea why.  I guess it's not as simple as I first thought.  As with most things, I'll fix it later... it's 2am here again, and we all know what happened last time I coded at this hour. :) Wade Tregaskis (aim: wadetregaskis) -- Sed quis custodiet ipsos custodes?</body>
  </mail>
  <mail>
    <header>Re: what's vector information (quartz)?</header>
    <body>This is hard to answer ... are you ask at just at the Quartz layer or Cocoa/Carbon/Java layers as well? This may be the best way to start to get a handle on things... -Shawn</body>
  </mail>
  <mail>
    <header>Re: what's vector information (quartz)?</header>
    <body>A bitmap is a two dimensional array of pixels.  A vector is the information required to draw a graphic object.  For example a vector description would be to &amp;quot;draw a line from point A to point B with color C and width W&amp;quot;.  A bitmap has a fixed resolution, but a vector is resolution independent. To the best of my knowledge the Quartz Compositor and Quartz 2D are two completely different unrelated things that unfortunately cause confusion by sharing a name.  The Quartz Compositor combines bitmaps that it shares with multiple applications into the single bitmap displayed on the screen.  Quartz 2D is a library used by applications to draw graphic objects into bitmaps or other representations. You should probably be more specific about what information you want regarding window drawing.  The basic idea is that an application draws into a bitmap and that bitmap is imaged on the screen.  The Quartz Compositor is used to combine the various application bitmaps (using alpha blending) into the screen buffer. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Re: what's vector information (quartz)?</header>
    <body>The vector information is any Quartz primitives you've drawn such as lines, curves, circles, etc. If you draw a line from (0,0) to (100,100), the window server knows nothing about that line. It only knows about the bits that were changed when Quartz drew it. That I don't know. Sorry. Darrin -- Darrin Cardani - email@hidden President, Buena Software, Inc. Video, Image and Audio Processing Development</body>
  </mail>
  <mail>
    <header>Re: Capabilities of Quartz: rotation? zooming?</header>
    <body>Just a note, the newest version of the tech demo is up at Windows.sitx&amp;gt;.... yes, I'm sorry about the sitx again, but I'm in a hurry at the moment (got an assignment due in 9 hours, and somewhere in there I have to fit 8 hours of sleep :) ).  And sorry about all the crap everywhere, with all the needless junk... I'll clean it up for the next release. Essentially, the transform is done properly now - let the ridiculous number of [commented out] failed attempts in the source be testimony to the perils of coding at 2am in the morning. :) Unfortunately, as I think I mentioned, a lot of drag [or similar] events don't go through NSApplication's sendEvent:, so they're not transformed... I may be able to catch them too using another method [as mentioned previously, I think] but I haven't had time yet. It's at least at the point now where you can still use certain controls, like buttons and the like.  Pop-up menu's humerously pop up where they used to be in their &amp;quot;normal&amp;quot; orientation... I haven't a clue how to go about fixing that. Anyway, I hope everyone enjoys it.  There's a few other minor changes here and there - for instance, the window shadow is turned off during oscillating scaling for performance reasons.  I think I made a few other tweaks too.  Enjoy. Wade Tregaskis (aim: wadetregaskis) -- Sed quis custodiet ipsos custodes?</body>
  </mail>
  <mail>
    <header>what's vector information (quartz)?</header>
    <body>Hi, I posted this mistakenly to carbon-dev list. And got some interesting answers. Perhaps it may become more clearer here I'd really appreciate if anyone could elaborate on this. From &amp;quot;system overview&amp;quot;: One of the primary duties of the window server is window compositing. It composites and recomposites each pixel of an application's window as the window is drawn, redrawn, covered, and uncovered. Each window is represented as a bitmap that includes translucency (alpha channel) information. The bitmap serves as a buffer, allowing the window server to &amp;quot;remember&amp;quot; an application's window contents and to recomposite it without the application's involvement. However, the window server (Quartz Compositor) does not retain vector information that a graphics library (such as its own Quartz 2D) might have used to create a window or any other image. Instead, the bitmap is shared between the application and window server; the application draws directly into the shared bitmap and the window server composites the bitmap appropriately. What is this &amp;quot;vector information&amp;quot; that they are talking about? And what's the entire chain of events that leads to window drawing on the screen? Thanks in advance. Navneet</body>
  </mail>
  <mail>
    <header>Re: Capabilities of Quartz: rotation? zooming?</header>
    <body>Quartz does not expose the ability to rotate windows at the moment. Quartz does not provide public APIs for window manipulations. All public APIs for window manipulation have to come from AppKit or Carbon's High Level Toolbox. But unfortunately your requested functionality is currently not available today. I'll add it to our list of things to do. Better yet, please file an enhancement request through That being said, the standard disclaimer of &amp;quot;Don't use private or undocumented APIs as your application can break from release to release&amp;quot; still applies. haroon</body>
  </mail>
  <mail>
    <header>Re: Capabilities of Quartz: rotation? zooming?</header>
    <body>Window shadows are translucent so when scaling the shadow itself has to be modified to follow the scaling and then has to be composited over those things behind it. This particular flow may not be well accelerated by QE at this time, who knows... you can likely use Apple's Shark tool to get an idea (part of CHUD). You could try scaling a window that is translucent or has translucent parts to see how that affects performance. If Quartz Extreme has to deal with more surfaces (windows) then can fit in VRAM then I believe it has to do a lot more data pushing (actually pulling) over the AGP bus and/or has to fall back on CPU based compositing. I believe Apple has implements a virtual memory scheme to held with this, a scheme to manage VRAM memory. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Capabilities of Quartz: rotation? zooming?</header>
    <body>Just as a follow-up, I've narrowed down the performance issue with scaling to be because of the window shadow; removing that offers excellent scaling performance, very similar to what Expose achieves. I'll have to test with 20 windows at a time to really verify that assumption, but the outlook is good. My problem now is how to both get the window shadow back for rotated windows, and improve the performance so it's not such an issue anymore. I can't image generating the shadow could possibly be that computationally expensive, so I don't understand the performance hit I'm seeing.  I remember some discussion of this a long time ago on one of the lists, although I don't think anything fruitful came from it.. has anyone else investigated window shadow performance, or better yet how to improve it? Secondly, I've noticed if you drag the window around while it's scaling around, you end up creating a real skew between it's graphical location and it's apparent location (as seen by itself for it's frame, and so forth).  i.e. it ends up spinning around some other point, and the virtual controls are shifted too.  The window dragging is a unique case, though, because it only works if you click the top N pixels of the rotated window, on (I think) the edge that has the title bar.. so the window server presumably plays some part in detecting the drag initially, but then when it's passed to NSWindow the lack of knowledge of the transform is playing havok with the dragging... Ideally this'll resolve itself &amp;quot;automagically&amp;quot; when I get event co-ordinate translation working. Anyway, I'll leave it at that - I just thought people might want to be kept abreast of developments, given the strong interest I've seen in it.  I haven't posted a new version yet - I'll wait till something really good is done, like the event transforms or a big performance boost or somesuch. :) Wade Tregaskis (aim: wadetregaskis) -- Sed quis custodiet ipsos custodes?</body>
  </mail>
  <mail>
    <header>Re: Capabilities of Quartz: rotation? zooming?</header>
    <body>Well, I'll look at putting up a few different formats in future. Personally I hate disk images, so I'm loathe to use them.. zip is okay, but I've had trouble with Panther's implementation (i.e. it crashes a bit).  If I use something like bzip2, everyone screams blue murder.  On a developer list.  Where they must have it installed in order to develop.  D'oh. :) Wade Tregaskis (aim: wadetregaskis) -- Sed quis custodiet ipsos custodes?</body>
  </mail>
  <mail>
    <header>Re: Capabilities of Quartz: rotation? zooming?</header>
    <body>As a result of using sitx... I had the pleasure (worse then a bad sun burn) of visiting the Aladdin Systems website and hunting for how to download the new free version of stuffit expander (since the one I had refused to open the file even after forcing binary download. I suggest you use compressed disk images or Panther archive (zip) instead. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Capabilities of Quartz: rotation? zooming?</header>
    <body>Nope, the test was done on a Titanium PowerBook G4 with a ATI Radeon LW Mobility 7500 M7. The WindowServer did not show more than 25% CPU usage with zooming. Even for those who are not interested in the code, IT IS WORTH TO TAKE A LOOK AT THE APPLICATION!! It shows a pulsing and rotating window, which is fully functional (except right mapping of mouse events). Where else have you seen that? Cool work, Wade! 8-) For those who want to see a little movie of how it looks, I have put some on a web page, which also includes a little summary about the There are some questions left, which Wade partly has already mentioned: (1) How can the mapping of mouse events be done best? (2) Why does the shadow disappear at the rotating window? (3) How can a rotating and zooming window work even in Exposi? (4) How can rotation and zooming be applied to windows of any application? (5) How can the performance be optimized? Thanks for any improvement suggestion! Have fun with this fancy window. :-) Greetings, Claus</body>
  </mail>
  <mail>
    <header>Inconsistent colours in PDF output?</header>
    <body>Hi gang, I have a curios thing here.  When I output graphics from my app (in PDF format using NSView's dataWithPDFInsideRect) that use transparent colours the colours that I see rendered on-screen by Adobe Reader are darker/muddier than Preview or my application display. Can anyone think why this might be? Additionally if I load the same file into Adobe Illustrator (10.0.3) the transparency settings are all ignored, and everything is drawn at 100% opaque.  My brief thinking on that is since I'm using different colours of varying transparency levels, at least one of which being 100% opaque, maybe I need to use transparency layers when rendering in each colour?  Maybe it's just a problem with the version of Illustrator I tried though. Ideas? Steve</body>
  </mail>
  <mail>
    <header>Re: Capabilities of Quartz: rotation? zooming?</header>
    <body>I've been working on this problem in collaboration with the original poster, and have come to a point where a lack of time prevents me making as much progress as I'd like.  So, I'm hoping someone can shed light on a few 'problems' I've encountered, and perhaps point me in the right direction for the remaining things to do. First, I've managed (thanks to Finlay) to connect to the window server [using undocumented methods] and manipulate the transform matrix for a given window.  This provides the scaling/rotation/translation desired. However, it only performs this transform on the window graphic - event's going into the window are not translated.  Consequently the window is not really all that useful.  First and foremost I want to rectify this by catching incoming position-dependant events (e.g. mouse events) and transforming them, then passing them on in the normal manner.  I'm not sure how to do this, though.  I know this isn't, on the face of it, a Quartz question, but I believe that mouse events (if not keyboard and others) come from the window server, so I'm thinking there might be some way to manipulate the window server's event transform matrix, or perhaps establish my own custom handler for events.  Does anyone know of such a thing? Failing that, any other methods of achieving the desired effect are of course most welcome also. :) On another line, the performance of the window transformations is presently sub-optimal.  While rotation seems to be pretty fast - a smooth rotation can be achieved using [usually] less than 15% of the CPU on my 800mhz G3 iBook - scaling seems much more expensive; I have been unable to achieve smooth results of any description, hitting 100% CPU usage at approximately 10 fps.  This is clearly unacceptable, and much slower than I perceive Expose and other Apple techniques performing.  The undocumented _scale: method of the NSWindow class appears to do something special with the window's caches, but unfortunately that results in a crash when the window is next drawn... presumably something must be done to prevent this, and presumably the cache manipulation is performed to increase performance.  Does anyone know what this is?  I've noticed the scaling performance seems no better than that done in iPhoto (which seems to be an optimised version of NSImage's scaling).  This runs completely counter to the idea that Quartz uses OpenGL, and consequently the scaling (and rotation) should be effectively costless in terms of CPU time. Are my existing ideas about Quartz and OpenGL acceleration incorrect? It certainly seems like scaling (if not rotation too) is not accelerated, and is instead being done in software.  Strangely, however, 	Claus Atzenbeck has reported that scaling is quite fast on his Quicksilver-era G4 (with, I presume, a GeForce 2 MX). I do know at least that Quartz is using hardware acceleration in general, due to past experience (the 10.3 install this iBook shipped with was in some way corrupt, and Quartz Extreme was not in fact in use for the first 6 months of it's life).  As I mentioned, Expose works smoothly and quickly, without huge CPU use.  I can't image Expose goes much further &amp;quot;to the bone&amp;quot; than I am presently doing with the window server. Anyway, thanks in advance to anyone who can offer tips or suggestions. Also, the source of a little demo app showcasing the scale/rotation is Windows.sitx&amp;gt;.  It is BSD licensed, for what it's worth, although I've been lazy and forgotten to put the licensing info in the files. :) I'd actually appreciate it too if anyone with a spare 5 minutes could download, build &amp;amp; run the project, and send their observations and performance measurements [off list] to me.  Perhaps then I can determine if the performance issue is related to any one particular computer/GPU. Wade Tregaskis (aim: wadetregaskis) -- Sed quis custodiet ipsos custodes?</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>I've used motion blurring in games and it's a very effective technique. In my cases, I have pre-existing sprites with the motion blur already in the artwork. I can then animate with fewer steps and a slower frequency and yet to the eye it looks like something is really zipping smoothly across the screen. The motion blur really works well at fooling the brain. Bryan</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>That is what I would imagine, at least for very fast movements. Now the question would be is there a way to determine the current refresh rates of attached LCD and CRT monitors. The CRTs probably report this through some API (which one, CGDisplayCurrentMode?), buit I guess TFT monitors do not report they cannot do more then 35 ms or 45 ms or so. Actually on my flat panel I already get motion blur :-) no, seriously, i do not want to play with motion blur because that would increase the drawing overhead and I need to keep that in balance with the visual experience. Or would you say that the motion blur would be better then more frequent updates? Thanks, david.</body>
  </mail>
  <mail>
    <header>Multiple Master Font instances and Quartz</header>
    <body>Hi, Mac OS X and Quartz support 'LWFN' Multiple Master base fonts nicely. But I can't find documentation about the support for Multiple Master font instances. I think I read somewhere that existing MM instances are supported, but new instances can only be created under Mac OS 9. Such MM instances are accessible in QuickDraw-based Carbon applications, but seem not to be available in Quartz-based applications like TextEdit. This may be due to the fact that MM instances look like bitmap fonts at the FOND level. Is it possible to use MM instances with Quartz? If yes, how do I tell Quartz to use an MM instance? Thanks and best regards, Kai -- RagTime GmbH                          Tel: [49] (2103) 9657-0 Itterpark 5                           Fax: [49] (2103) 9657-96 D-40724 Hilden Deutschland</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>True always exceptions to the norm and what looks good is subjective. I generally attempt to do the minimal amount of refreshing needed to make it look good enough and avoid going as fast as one can go with the hardware (since hardware keeps improving, why should my drawing grow to consume what likely isn't needed). -Shawn</body>
  </mail>
  <mail>
    <header>Re: Hit testing, a different approach</header>
    <body>Ken, As the guy who posted the original approach perhaps I'm a little bit too defensive but... So what you want to do is: 1) convert an 8 bit image into a 1 bit image 2) convert that bitmap into a compressed format as a region 3) test a bit in the compressed image Why not just skip the region and test a byte in the 8 bit image? Nick</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>Well, sometimes you (or at least I) do, even though I agree with your points in general. There's a fast pan near the beginning of &amp;quot;Who Framed Roger Rabbit&amp;quot; showing the streetcars that sticks in my mind because the pan is too fast and looks &amp;quot;stuttery&amp;quot;, at least on a movie screen. Also, movies not only play at 24 FPS, they projector light flickers at twice that rate AFAIK, so you don't notice the flicker as much.  (Then again, I can't stand having to have my refresh rate at 60 FPS.) -- top-posting: It's just a bad idea.</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>David, NTSC interlace and film frame rates are a red herring here. While for some kinds of animation a low frame rate is fine (Bugs Bunny was 12-fps), it sounds like you want to refresh your line as often as the hardware will allow. You'll see the difference. If you want to play with motion blur, that's cool. You know how far your line has moved since the last update, so you could just create a wider rectangle and draw it with lower opacity if you want to simulate motion blur, or you could actually blur a line, or you could draw several semi-transparent lines. Dave Howell Professional Applications Apple</body>
  </mail>
  <mail>
    <header>Hit testing, a different approach</header>
    <body>Several weeks ago,  one fine member of this list proposed a hit-testing algorithm for paths. The algorithm creates a 1-pixel by 1-pixel greyscale context, moves the context to the point of interest, redraws the path, and tests the context (content) to see if anything was drawn through the pixel. For a variety of reasons, the algorithm is not workable ---- I am trying to add hit testing for an SVG enabled Mozilla --- the creation of the GC's for mouse movement will not gracefully fit into that design. I am working on a different approach that I would suggest in pseudo code. I think we can, upon the initial creation of the constructs that specify the path, ultimately build a bitmap that would be converted to a quickdraw region. Mouse movement would then use the PtInRegion function. So, upon the initialization of the path ---- - Create some low resource GC like an 8 bit greyscale the size of the path bounding box - Draw the greyscale curve. - Somehow (?) scan the greyscale pixels and build a bitmap - Use the BitmapToRegion function to create the region for hit testing. My questions.... - Has anyone tried a similar approach? - Is there any way to quickly 'slice' one of the bits from the greyscale pixmap into a bitmap (or do I bruteforce check each pixel?) Thanks for any feedback. Ken Feldt</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>Well, more expert than I am in any case :-) I don't have a television :-) but I understand the basic principles you are talking about. I think I will probably go for 60 FPS for the high speed movements and less for slow movements. That way  I will already reduce the processor load compared to what I was doing before. thanks, david.</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>I am not an animation expert but I play one on this list ;-) (seriously I am not an expert by any stretch of the imagination, so take things with a grain of salt) Your television refreshes at 60 Hz yet it only refreshes every other line in a given cycle (at least historically) and toggles between the odd/even lines on adjacent cycles (top to bottom scan of gun). This is half field or interlaced style drawing. Computer monitors/adapters refresh the whole screen from top to bottom in a given cycle. This is called full field. At least I believe that is the generally correct names. I was simply trying to making a note of what type of refresh I was talking about when I said 30 FPS. Anyway yes the speed at which objects move across the field of view can affect at which rate one needs to draw if you want help trick the mind into continuos movement on an object, it needs enough key frames to help it fill in the gaps so to speak. Note that movies play at (IIRC) 24 frames a second yet you don't have an issue watching fast moving objects fly around the field of view... in this case motion blur of the moving object itself is helping to trick your eyes and brain. The motion blur either comes from the capture method used when filming (object moves while a given frame is imaged/exposed) or it is generated using computer graphic affects. So depending on what you are doing you could attempt to use motion blur in the direction of travel to help trick folks brains but that is likely overkill for what you are doing. -Shawn</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>I am not an animation expert, so what do you mean with &amp;quot;full field&amp;quot;? Also, if the speed of movement is really high, say a line moving across the screen in 0.1 second, I would imagine that it looks better if the lines is refreshed 10 times along the way (100 FPS) then if it is only refreshed only 3 times along the way (30 FPS). this may be an extreme case, but in some cases my line has to move really fast. Isn't the 30 FPS as a good cap only valid if the motion is not too fast. I mean a man raising an arm can be well captured in 30FPS, but if a rockect is flying across the screen it might be gone by the next frame. Or am I missing something here? Thanks fro your feedback in helping me understand the issues involved. david.</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>Ok, thanks for all your feedback. I will change my code accordingly. Thanks, david.</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>Likely little difference but if concerned profile the application once going. -Shawn</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>30 FPS a second full field is generally sufficient for humans for most types of animation, in fact for many types 15 FPS is just fine as well. If it looks good at 30 FPS don't bother refreshing faster then that, save those CPU cycles for something else. For things I have done I often cap at 30 FPS (drag scrolls, etc.) and haven't seen visual issues. -Shawn</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>Yes, there is.  You will reduce the amount of CPU time your app and the system spend processing the calls, and since you are posting events, you will also reduce the time spent routing and processing each event in the system and foreground application.  The event processing overhead can be significant.</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>Your suggestion of 30 hz is based on what a human can still distinguish or based on what typical TFT monitors can still show? I am asking as I should not optimize just for my own monitor as other people may be using different monitors, and the monitor and graphics cards may improve over time, but if your suggestion is based on a limitation of the human visual system that is something that will definitely not imporve over time so would warrant hard coding a frequency lower then what current or future hardware can handle. Thanks, david.</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>Ok, so if I understand you correctly, the hardware &amp;amp; driver design minimize the costs of moving the cursor too frequently with CGPostMouseEvent. So is there still a benefit to me reducing the number of calls to CGPostMouseEvent during fast cursor movement to say at most 0.01 times per second, or will that make hardly any difference because of your clever design? Thanks, david.</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>Moving the hardware cursor incurs a fixed cost per call to move it, but courtesy of some clever hardware design and device driver behavior, there's no additional blocking while waiting for a display refresh. Effectively, the entire cursor drawing and display operation is asynchronous with respect to your calls.  When it's time to present the cursor again, it will appear at the latest appropriate position. The cursor image is updated at each display refresh by hardware.</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>The hardware cursor is independent of the flushing mechanism. But at the moment the cursor updates are also tied to the refresh rate of the display. They don't necessarily need to be, but that is the current behavior. So updating the cursor faster than the display refresh rate is unnecessary also. haroon</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>Haroon, Thanks for all your very useful feedback. One last question at this point ;-) Does what you said about not drawing/flushing more frequently than the screen refresh rate also apply for cursor movements? In another part of my application I need to move the (hardware) cursor around the screen (using the CGRemote API). Sometimes it needs to move really fast. Does it, also for updating the cursor position, make no sense to do it more frequently than the screen refresh rate or does the cursor have its own (higher) refresh rate? Thanks, david.</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>Ah... yeah that makes sense and kicked me to recall the session better, thanks. You can request a flush of a finished frame and it will not block on the flush but will return allowing you to proceed in preparing for the next frame as needed, without having to utilize threads to avoid blocking while flushing. So you do your drawing preparation/calculation as much as possible before you call any actual drawing operation to help streamline things by avoid blocking (of course you only need to do this if you notice a problem in refresh rates). -Shawn</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>When you call CGContextFlush, the flush operation is started, but it takes time to complete. The CGContextFlush call does not block until the flush operation is complete but instead returns immediately. Any subsequent drawing operation (EraseClock in your case) will block until the flush completes. No see comments above. Correct. Any drawing / flush operation you try and do faster than the refresh rate of the display is wasted effort - one because the user cannot see the intermediate result and, two, you're chewing up the CPU. Correct. You don't need to draw and flush faster than the display refresh rate. 30 Hz should be good enough also. Try experimenting with the refresh rate for visually smooth animations. [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>Correct. Any drawing operation (CGContextClearRect in this case) will block until the previous flush is complete. What we said at WWDC was that you can prepare work (non-drawing operation) for your next frame while the previous flush is still in flight. As soon as you have to touch the backing store because of a drawing operation, Quartz cannot do so until the previous flush is complete. [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>I believe he is saying that CGContextFlush in a way schedules a flushing of the current context to the screen and the real flush doesn't happen until the next screen refresh cycle, at the earliest, and flushing is not instantaneous in general. So any operations that cannot proceed on a context while it is being flushed will block until the context inishes flushing to the screen. So flushing isn't taking place as part of a CGContextClearRect it is just that CGContextClearRect cannot proceed until the pending / in process flush is completed on the given context. The thing I am confused about is I recall that it has been touted that you can be drawing the next &amp;quot;frame&amp;quot; (so to speak) while the prior one was still being flushed to the screen, I thought that was automatic and managed by Quartz... I could be misremembering things from a prior WWDC. -Shawn</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>This is the basic sequence in the Carbon timer that handles the drawing: sRadians = sStartRadians + SecondsPassed(sMoveStartTime) / if (sRadians &amp;gt; (2 * pi)) So if I understand you correctly you are saying two things: (1) the &amp;quot;real&amp;quot; flush does not happening when I call CGContextFlush, but when the timer fires the next time and the call to CGContextClearRect is made as part of the EraseClock function; (2) or, if the time between timer fires is long it happens during the next display refresh. Is this correct? The monitor is an Apple Studio Display (LCD) so no refresh rate is specified in the displays Preference Panel. The computer is a G4 800 DP with a GeForce2 TwinView card, which as far as I know is Quartz Extreme capable. I realize that this needs to happen, I just thought this was happing at the time I am calling CGContextFlush, but now I gather that the real flush probably takes place as part of the CGContextClearRect and/or at the next screen refresh. So if I understand correctly the clear operation is carrying the load of the flush. Does that mean the clear first flushes (leading the screen to show the previous state) and then clears? The ideal update frequency ranges from every 0.06 to 0.002 seconds depending on how fast the line must go. This is basically the timer frequency so this is how often i am currently flushing 9if the timer actually gets called in time (with the highest frequence the timer is typically skipping). But, what you are basically saying is that it makes no sense letting the timer run faster than every 0.01 seconds because the screen won't be able to update anyway. As timers are some times skipped I may need to allow for up to 0.0025 or so to be sure the timer does not skip a whole screen refresh cycle but then only draw and flush if more then 0.01 seconds have passed since the last flush (or less if I want to let the line move more slowly). does this make sense? It would probably not make my drawing any faster, but it would reduce the processor load, which I am apparently wasting by flushing more frequently then the screen gets updated. Yes, I think that would be overkill. Thanks, david.</body>
  </mail>
  <mail>
    <header>Re: How to speed up erase?</header>
    <body>FYI, the flush operation is asynchronous and so the ClearRect is the first operation after the flush that will block until the previous flush has completed. So the ClearRect is not in itself slow, you are just waiting for the flush to complete.  Also the flush operation is sync'ed to the refresh rate of the display so you can go faster than that anyways. Out of curiosity, do you have a Quartz Extreme capable graphics card? If so AGP uploads should be used to perform the flush. Given that you are drawing into a transparent overlay window, Quartz Compositor is involved as it has to composite the result of your window's dirty region (effectively the bounding box of your drawing) with any windows underneath the overlay. You won't be able to flush faster than the refresh rate of the monitor, so trying to update the screen faster than 60 or 100 Hz is probably not wise - the user is not necessarily going to see every single rotated line anyways. Also what is the update frequency you are trying to achieve here? You may want to only flush at the frequency of your desired update. Not sure if I can recommend any faster approach. The only other option I can think of would be to jump to OpenGL which may be overkill in your case? haroon</body>
  </mail>
  <mail>
    <header>How to speed up erase?</header>
    <body>I am new to the quartz list, but after posting a question on the carbon list it was suggested I try it here. I am posting the issue here with the modifications I already made based on some initial feedback from the carbon list and some additional research and timing I did myself. I need to have a one pixel thick line move clockwise across the screen in an overlay window. Drawing the line is relatively fast, but erasing it at the previous position is disturbingly slow. To erase the line I am using: The CGContextClearRect call (so not including any of the other calls listed above) takes an average 14 ms according to the MW Profiler (on my G4 800 DP with 17&amp;quot; Apple LCD monitor). To circle across the entire screen and touch every pixel several thousands of steps are needed to come full circle (exact number depends on the size of the screen). Even for just 1000 steps already 14 seconds are needed just to delete the line. To draw the line I am using: This takes an average 2.9 ms (anti-aliased). Not too fast either, but not as bad as the CGContextClearRect operation. Originally, I was drawing the line by constructing a path with the right slope rather than rotating the context. To my surprise the context rotation approach for drawing was not only 5% faster, but actually resulted in a 10% faster erase (which takes place before the draw). To increase the speed of the erase I have tried to just erase the smallest rectangle surrounding the begin and end points of the line, but this is about 40% slower on average than my current solution (it is fast for the practically horizontal or vertical case, but really slow for the diagonal case). Any suggestions on how I could improve the erase speed? Or perhaps an explanation on why erase is much slower then drawing. (Note that I am not flushing the context until after the erase and draw operations have completed, so I would image the speed of the erase is unrelate to what is visually below my overlay window, right?) Thanks, david.</body>
  </mail>
  <mail>
    <header>Capabilities of Quartz: rotation? zooming?</header>
    <body>Hi all: For a research project, I need some strange things for user testing. I am new to Quartz, but as far as I have read about it, it may be my friend for what I am trying to do. Please tell me your opinion if the following things can be done with Quartz and how difficult it sounds to you: (1) Rotation of complete windows Windows usually are straight. However, I want to allow to rotate a window as it is. Functionality -- e.g.  modifying the content, clicking on buttons, or using Aqua's close/minimize/maximize buttons -- should be preserved even in when rotation is applied. (2) Zooming all windows and moving them around I need a zooming function, which is a mix of Exposi and the zoom function in System Preferences -&amp;gt; Universal Access -&amp;gt; Seeing. The following features should be implemented: (a) The user can stop at any level of zooming, like in Universal Access zooming. However, the zoom goes the other way, like in Exposi: Windows are down-scaled compared to the original size. (b) Also here, functionality should stay. For example, the content can be modified as well as buttons can be used independently of the current scale. This also includes moving a window freely as it can be done with windows of original size. The system also should be able to apply rotation and zooming at the same time. This means that I can rotate some windows and zoom out. - Is Quartz my friend? - Are there built in methods for rotating and zooming which can be applied easily to Aqua windows or do I have to write my own graphic routines? - Actually, is there a way to add this behavior to Aqua's window manager easily, e.g. through plug-ins? If yes, I would be able to use any MacOS X application for testing without writing my own window manager. Thanks for your opinions! Greetings, Claus</body>
  </mail>
  <mail>
    <header>Re: CGImage Questions</header>
    <body>Sorry, I think I misunderstood you.  CGImageRefs do support grayscale + alpha interleaved (not planar) data with 16 bits/pixels, 8 bits/component.  They don't support planar data at present.  Sorry for the confusion. Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>Re: CGImage Questions</header>
    <body>CGImageRefs don't support planar data in this form at the present time. Sorry, Derek [demime 0.98b removed an attachment of type application/pkcs7-signature which had a name of smime.p7s]</body>
  </mail>
  <mail>
    <header>colorSpace for bitmapContext in Tiger</header>
    <body>Hi, Which color space should I use while creating a bitmap context in Tiger? After creating it I fill a rect with RGB color (0, 128, 128). On Mac OS X 10.3.9 the resulting argb pixels will be 0xff008080 as expected, but on Mac OS X 10.4.5 I got 0xff0f817f. (I used kCGColorSpaceUserRGB on 10.3.9). Setting the renderingIntent to kCGRenderingIntentAbsoluteColorimetric results in pixels 0xff11817f but still not 0xff008080. At least I do not know why the red component is not zero. Please find my sample code below: CGColorSpaceRef cs = CGColorSpaceCreateWithName CGContextRef bmcontext = CGBitmapContextCreate (myBM, width, height, CGContextSetRGBFillColor(bmcontext, 0.0, 0.5, 0.5, 1.0);  // float RGBA // examine memory at myBM here Thanks for any help Geza Fabry Graphisoft</body>
  </mail>
  <mail>
    <header>Drawing a small portion of an image, scaled</header>
    <body>I've been battling a problem of speed in one aspect of drawing pictures. The drawing behavior that I want to have is: draw (50,50,100,100) of Image A to the context at (50, 50, 400, 400). In QuickDraw, CopyBits does this behavior for us quite easily. In CoreGraphics, I originally updated the code to: - Clip to the destination rect - Draw the image scaled and translated so that the clipping only lets through the 100x100 square (but scaled to 400x400) - Restore the clipping rect However, this is actually different behavior. The interpolation that takes effect when scaling the picture shows through on the edges. IE, if i have a 3x1 image, with the pixels (Red, White, Red), and draw the image rect (1, 0, 1, 1) scaling it to 50x50, what I will see is pink on both sides. QuickDraw, however, appeared to only interpolate the image region drawn. So, I decided I must make a temporary sub-image, unfortunately. The problem is that, while every other drawing operation has been sped up by using only CoreGraphics, this manipulation is quite a bit slower, as we have to create a new image each time through this operation, with little ability to cache images for our users. I'm wondering if anyone else has attempted to solve this difference in behavior, and if there are any other ideas of approaches to take that might yield in better overall performance (hopefully removing the need to allocate a new image). Thanks, Jon -- Jonathan Johnson REAL Software, Inc. REAL World 2006, The REALbasic User's Conference</body>
  </mail>
  <mail>
    <header>Re: Incompatible CIFilters</header>
    <body>Attachment:</body>
  </mail>
  <mail>
    <header>Re: Incompatible CIFilters</header>
    <body>This is the intended behavior, not a bug. If you consider what the CICircularWrap filter is attempting to do (wrap an image around the circumference of a circle) there's no way it could do anything sensible with an infinitely wide source image,</body>
  </mail>
  <mail>
    <header>Re: Incompatible CIFilters</header>
    <body>Yes, that's the problem. I have verified that if I insert a CICrop between the CIAffineTile and CICircularWrap, then it behaves as expected. So I guess the problem is with the &amp;quot;infinite domain of definition&amp;quot;: either (1) it shouldn't be generated uncropped or (2) all filters should be able to handle it. And all this time I thought it was my app...</body>
  </mail>
  <mail>
    <header>colorSpace for bitmapContext in Tiger</header>
    <body>Hi, Which color space should I use while creating a bitmap context in Tiger? After creating it I fill a rect with RGB color (0, 128, 128). On Mac OS X 10.3.9 the resulting argb pixels will be 0xff008080 as expected, but on Mac OS X 10.4.5 I got 0xff0f817f. (I used kCGColorSpaceUserRGB on 10.3.9). Setting the renderingIntent to kCGRenderingIntentAbsoluteColorimetric results in pixels 0xff11817f but still not 0xff008080. At least I do not know why the red component is not zero. Please find my sample code below: CGColorSpaceRef cs = CGColorSpaceCreateWithName CGContextRef bmcontext = CGBitmapContextCreate (myBM, width, height, CGContextSetRGBFillColor(bmcontext, 0.0, 0.5, 0.5, 1.0);  // float RGBA // examine memory at myBM here Thanks for any help Geza Fabry Graphisoft</body>
  </mail>
  <mail>
    <header>Re: Incompatible CIFilters</header>
    <body>CIAffineTile is outputting an infinite domain of definition. And CICircularWrap is politely declining to render it. CICircularWrap doesn't &amp;quot;know&amp;quot; that the input is CIAffineTile, but rather just can't handle an infinite input. You can verify this, if you want, by applying CIAffineTile, then CIHueAdjust, and then CICircularWrap, which presumably will fail in the same way. Is that a bug? I don't know, sounds like one to me. You should get a non-NLUL CIImage , and it should be cropped to the region of interest when you render it. You should file a bug report and attach a code snippet. On Feb 21, 2006, at 10:27 PM, Roland Torres wrote: Attachment:</body>
  </mail>
  <mail>
    <header>Incompatible CIFilters</header>
    <body>CIAffineTile and CICircularWrap don't seem to work properly with each other. As a sanity check, I modified the CIExposureSample project to chain just these two filters. Feeding the output of CIAffineTile into CICircularWrap results in strange &amp;quot;outputImage&amp;quot; CIImages: CICircularWrap outputImage: (null) And of course, the CIAffineTile filter works fine when its outputImage is fed into a different CIFilter's inputImage. It's just the CIAffineTile and CICircularWrap combo that doesn't seem to work. How could CIAffineTile know that CICircularWrap is utilizing its outputImage?</body>
  </mail>
  <mail>
    <header>colorSpace for bitmapContext in Tiger</header>
    <body>Hi, Which color space should I use while creating a bitmap context in Tiger? After creating it I fill a rect with RGB color (0, 128, 128). On Mac OS X 10.3.9 using kCGColorSpaceUserRGB the resulting argb pixels will be 0xff008080 as expected, but on Mac OS X 10.4.5 using kCGColorSpaceGenericRGB I got 0xff0f817f. Setting the renderingIntent to kCGRenderingIntentAbsoluteColorimetric results in pixels 0xff11817f but still not 0xff008080. At least I do not know why the red component is not zero. Please find the sample code below: CGColorSpaceRef cs = CGColorSpaceCreateWithName CGContextRef bmcontext = CGBitmapContextCreate (myBM, width, height, CGContextSetRGBFillColor(bmcontext, 0.0, 0.5, 0.5, 1.0);  // float RGBA // examine memory at myBM here Thanks for any help Geza Fabry Graphisoft</body>
  </mail>
  <mail>
    <header>Re: How do you determine PDF version?</header>
    <body>Other people have answered this, but I wanted to point out  an additional bit.  The version of a PDF file isn't solely determined by the first few bytes of the PDF; in PDF 1.4 and later, the version specified at the start of the PDF file can be overridden by a / Version entry in the document's catalog.  (See section 3.4.1 of the PDF 1.5 specification for more info.) This is why Quartz PDF files always start with %PDF-1.3 but may, in fact, be PDF 1.4 or later. Derek</body>
  </mail>
  <mail>
    <header>Re: How do you determine PDF version?</header>
    <body>On Feb 21, 2006, at 12:48 AM, James W. Walker wrote: That is my understanding as well.  If you use a feature of Quartz that requires PDF 1.4, it will &amp;quot;upgrade&amp;quot; your PDF to 1.4.  There is no way to ask it to create a specific PDF version. One thing to realize is that Quartz is not a general PDF toolkit. Just like PICT is the metafile format for QuickDraw, the metafile format for Quartz 2D, by carefully-planned-yet-serendipitous design, happens to be PDF.  To put it another way, all Quartz metafiles are stored in PDF files, but not all PDF files are Quartz 2D metafiles. Scott</body>
  </mail>
  <mail>
    <header>Re: Subsampled images with multiples cameras on different FW ports</header>
    <body>It might be that the cameras are negotiating Firewire bandwidth - I think the &amp;quot;built-in&amp;quot; ports on the G5 are on a shared bus.  This has been a problem for us in the past with faster cameras (often the first camera will reserve too much bandwidth and the second one just will not work). What happens if you put one of the cameras on a PCI firewire card? This is the solution we have used in the past. Stephen Baxter Software Development Manager Improvision email@hidden +44-2476-692229</body>
  </mail>
  <mail>
    <header>Re: How do you determine PDF version?</header>
    <body>On Feb 20, 2006, at 9:19 PM, Frederick C. Lee wrote: It's my understanding that Quartz will use the minimum version that it can, depending on which features you use.  Why do you want to create PDF 1.6?</body>
  </mail>
  <mail>
    <header>How do you determine PDF version?</header>
    <body>Ric.</body>
  </mail>
  <mail>
    <header>Re: Subsampled images with multiples cameras on different FW ports</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Drawing a NSAttriubtedString into a PDF context.</header>
    <body>Thanks for info! I did get positive results working with the code you wrote. I got RTF text to write to the PDF page. However RTFD blew up.  I can't transfer an embedded image. Still, I'll play with what I got and do further research from your info.</body>
  </mail>
  <mail>
    <header>Flipping NSView</header>
    <body>I am trying to accurately draw bezier paths, images, and eventually text into a flipped NSView and have run into what appear to be some discrepancies between flipped and non-flipped views.  If I draw a 50x50 image with NSImage:drawInRect: and fill and stroke a 50x50 path that was created with NSBezierPath:bezierPathWithRect:, I get the following behavior: NSView Not-flipped - Drawing the path at 0,0 and the image at 100,0 will place both graphics at the bottom of the view. NSView Flipped - Drawing the path at 0,0 and the image at 100,0 will place the image at the top of the view, but the path will be 1 pixel too high with its top border clipped.  It appears as though the view is not being &amp;quot;flipped in place&amp;quot; which is affecting bezier paths but that NSImage makes the adjustment and draws the image at the correct location. Why does this discrepancy between image drawing and path drawing exist or am I doing something wrong?  Why doesn't the view &amp;quot;flip in place&amp;quot;?  What is a good way around this problem?  My current solution involves adjusting the points in my bezier path creation routines rather than messing with image drawing, since image drawing seems to be behaving correctly. Thanks, - Paul</body>
  </mail>
  <mail>
    <header>Re: CV=&amp;gt;jpeg</header>
    <body>On Feb 19, 2006, at 8:58 PM, Mike Schrag wrote: It doesn't seem that there would be.  CIImage is a set of steps to create an image, that image doesn't exist as pixels until it is realized into a context.  You use a Core Video context where I might choose to use a Core Graphics context but I don't really see that it makes any difference.  Once you've got the image rendered then CGImageDestination seems like a fine way to go. I don't know. One way you could get such a context is to use CGBitmapContextCreate... it should be able to create an 8 bit grayscale context.  Once you have done so, you could get your CGImage from using CGImageCreate on the same memory space that the CGBitmapContext uses.  Just be sure that once you've created a CGImage that you don't change it (by drawing in the CGBitmapContext) until you are done with the image.  The context of CGImages is supposed to be immutable.</body>
  </mail>
  <mail>
    <header>Re: Drawing a NSAttriubtedString into a PDF context.</header>
    <body>On Feb 20, 2006, at 1:08 AM, Frederick C. Lee wrote: There are a couple of ways you could do this.  The easiest, perhaps is to use Cocoa's drawing tools.  The simplest of these is  the &amp;quot;drawInRect:&amp;quot; method of NSAttribuedString. (the other way would be to use the attributed string with a self-constructed layout manager calling drawGlyphsForGlyphRange:atPoint:. You can read about that technique at: ) The tricky part is fact that Cocoa always draws in the current context.  That means that you need to set the current context to your PDF context before you draw.  You can do that using the setCurrentContext method of NSGraphicsContext.  However there is a bit of a trick to it.  The code would look something like this (typed into mail): [NSGraphicsContext setCurrentContext: [NSGraphicsContext [NSGraphicsContext setCurrentContext: savedContext] The first thing to note is that this will only work on Tiger since graphicsContextWithGraphicsPort:flipped: only exists on that platform or later.  The trick that this code plays involves the autorelease pool.  The  graphicsContextWithGraphicsPort:flipped: method creates an NSGraphicsContext that is also added to the autorelease pool.  For the PDF to be generated properly, however, the PDF context must be destroyed.  If a copy of that context is still hanging around in the autorelease pool then the PDF context won't go away when it's supposed to and the print job will fail.  By putting your own autorelease pool in place and releasing it before you leave your printing routine, you ensure that your reference to the context is released before your application's pool is released.</body>
  </mail>
  <mail>
    <header>CV=&amp;gt;jpeg</header>
    <body>When it comes to CV/CI/ImageIO/etc there are about 256 ways to skin a cat ... At the moment, I'm producing JPEG snapshots from a CV capture stream and I'm curious what the fastest (performance, not typing) way to get from CV to a JPEG NSData is?  My current method is to [CIImage imageWithCVImageBuffer:] the pixel buffer, create a CIContext with a CGLContext, [CIContext createCGImage] the CIImage, then CGImageDestinationCreateWithData a JPEG.  I'm curious if there is a more direct route?  I see in the WhackedTV example that you can set the codec for the capture to be &amp;quot;JPEG&amp;quot;. What does that ACTUALLY mean (i.e. does that mean the Ptr into my DataProc is literally an encoded jpeg?)  and is that potentially a faster way to do this? Secondly, I'm curious how to get a CIContext that is 8-bit gray ... I can set kCGLPFAColorSize to 8 and AlphaSize to 0, but I don't see a way to tell it that I want it to be grayscale vs RGB.  Would it just be up to the CIFilters to turn it into gray and render it into a CGImage create from an 8-bit CIContext? Thanks a lot, Mike Schrag email@hidden</body>
  </mail>
  <mail>
    <header>Re: How to convert/stuff a PNG (or TIFF) image into a PDF context?</header>
    <body>On Feb 18, 2006, at 5:04 PM, Frederick C. Lee wrote: Lookup the function CGContextDrawImage. CGContext/Reference/reference.html#//apple_ref/doc/uid/TP30000950- - Shawn</body>
  </mail>
  <mail>
    <header>Re: fastest path for copy texture to multiple windows?</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>creating transparent image</header>
    <body>Hi, I have a function which creates a transparent image from an other one: the pixels of a specific color should be transparent. My function worked fine on Mac OS X v10.3 but it is broken on 10.4. It draws the image to a bitmap context to obtain the pixel data in ARGB form, then changes the alpha bytes and creates a new image from the data. But after drawing the image into a bitmap context, the color of the pixels will be different than in the original image. They will be 0x0f8180 instead of 0x008080 (RGB). Could you please help me what is wrong? Thanks Geza CGImageRef CreateTransparentImage (CGImageRef image, MyColorType transpColor) CGColorSpaceRef colorSpaceUserRGB = CGColorSpaceCreateWithName CGContextRef bmcontext = CGBitmapContextCreate (bitmap, width, height, if ((*actPixel &amp;amp; 0x00FFFFFF) == transPixel) CGDataProviderRef prov = CGDataProviderCreateWithData (NULL, (void*) CGImageRef transparentImage = CGImageCreate (width, height, 8, 32, bytesPerRow, colorSpaceUserRGB, kCGImageAlphaFirst, prov, NULL, TRUE,</body>
  </mail>
  <mail>
    <header>Re: CGContextSetLineDash lengths</header>
    <body>Gelphman &amp;amp; Laden &amp;quot;Programming with Quartz&amp;quot;, ISBN: 0-12-369473-6, say on page 125: &amp;quot;The current line cap parameter of the context also determines the way Quartz paints the ends of each dash-on segment. At each end of They then show in Fig. 6.17 that both a Square line cap and a Rounded line cap make the dash-on segment longer. Only the Butt line cap leaves the dash-on the same length. Dave (Sending again from my mac.com address)</body>
  </mail>
  <mail>
    <header>Re: Where to start with 2.5D drawing application?</header>
    <body>On Feb 15, 2006, at 4:10 PM, Andrei Tchijov wrote: You may want to be aware that there is a bug (rdar://problem/4328259) in &amp;quot;Height Field From Mask&amp;quot; that causes it to work incorrectly on G4 systems without sufficiently powerful graphics card (e.g., the Mac Mini, some iMacs, TiBooks).  Therefore this solution won't work universally on all machines running Tiger. Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Re: Where to start with 2.5D drawing application?</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Where to start with 2.5D drawing application?</header>
    <body>Depending on what you really want to achieve you might have to use OpenGL but PS like effects can easily done with CoreImage. Try playing with Core Image Fun House in /Developer/Application/Graphic Tools/. I have done an inner bevel with it.</body>
  </mail>
  <mail>
    <header>Where to start with 2.5D drawing application?</header>
    <body>Hi, I am not sure that 2.5D is correct term, what I am trying to do is to be able to draw flat shapes/lines which has &amp;quot;thick-nes&amp;quot; and &amp;quot;cross- shape&amp;quot; attribute then point source of light on them and build realistic image.  Sort of similar to &amp;quot;Inner Bevel&amp;quot; effect in Photoshop but with much more control.  Which tool of many provided by Apple should I use?  Should I go straight to OpenGL or maybe I can &amp;quot;stretch&amp;quot; Quartz 2D capabilities to achieve similar results? Any comments will be highly appreciated,</body>
  </mail>
  <mail>
    <header>Re: CGContextSetLineDash lengths</header>
    <body>On Feb 15, 2006, at 7:46 AM, Nick Nallick wrote: Having gone through that, it is probably a very bad idea to assume that just because you've found a rendering scheme that looks right on your screen that you will get the same effect on all devices.  Try printing your [2,4] array to a printer and you will probably find that you are getting exactly what you are asking for. PS.  For an interesting effect using the end caps, try setting the line dash to something like [0.1, 2] and put the end caps on round end caps... a dotted line :-)</body>
  </mail>
  <mail>
    <header>Re: CGContextSetLineDash lengths</header>
    <body>On Feb 14, 2006, at 10:39 PM, James W. Walker wrote: Yes, the end caps affect dash segments, but unless you've changed them that's probably not your problem.  More likely, antialiasing at the end of each line segment is expanding it slightly.  You could try turning off antialiasing before drawing (CGContextSetShouldAntiAlias) or just use trial and error to get an array you're happy with (as you apparently did). Regards, Nick Nallick</body>
  </mail>
  <mail>
    <header>Re: CGContextSetLineDash lengths</header>
    <body>On Feb 14, 2006, at 8:08 PM, Ken Worley wrote: You're saying the endcaps affect the dashes??  But the end of a dash is not the end of a line.</body>
  </mail>
  <mail>
    <header>Re: CGContextSetLineDash lengths</header>
    <body>On Feb 14, 2006, at 8:25 PM, James W. Walker wrote: Just a thought...check the type of endcaps being used. They do affect dashed lines. -- Ken Worley, Software Engineer, Tiberius, Inc., email@hidden</body>
  </mail>
  <mail>
    <header>CGContextSetLineDash lengths</header>
    <body>The docs on CGContextSetLineDash say &amp;quot;For example, passing an array with the values [2,3] sets a dash pattern that alternates between a 2- user-space-unit-long painted segment and a 3-user-space-unit-long unpainted segment.&amp;quot;  Fine, but that doesn't seem to be the way it really works.  I passed the array [3,3], and got 4-pixel painted segments alternating with 2-pixel unpainted segments.  To get a 3,3 pattern, I had to use a [2,4] array.  What's up with that?</body>
  </mail>
  <mail>
    <header>Re: An interesting problem with	CGContextRef-based	NSGraphicsContexts</header>
    <body>A hacky way would be At this point, &amp;quot;gc&amp;quot; has a single retain count and isn't in any autorelease pool.</body>
  </mail>
  <mail>
    <header>Re: An interesting problem with CGContextRef-based	NSGraphicsContexts</header>
    <body>On Feb 14, 2006, at 10:29 AM, Scott Thompson wrote: FWIW, the same thing happens with CoreImage.  CIContext is added to the autorelease pool and because it retains the CGContext my PDF context doesn't get released until the containing autorelease pool is drained.  This was biting me when I tried to fulfill a promise for a PDF to a drag.  The default autorelease pool is drained in the main event loop which didn't happen until after I'd already tried to supply the drag data from the PDF context.  My solution was also to add more autorelease pools when using CI. I asked for a new function CGPDFContextFinalize to allow me to force a PDF context to complete it's work before it was deleted (rdar:// problem/4066059). Nick</body>
  </mail>
  <mail>
    <header>Re: An interesting problem with CGContextRef-based	NSGraphicsContexts</header>
    <body>On Feb 14, 2006, at 11:37 AM, Corey O'Connor wrote: Yes.  How do you remove an object from the autorelease pool?</body>
  </mail>
  <mail>
    <header>Re: An interesting problem with CGContextRef-based	NSGraphicsContexts</header>
    <body>Would removing the NSGraphicsContext from the autorelease pool and calling release on it manually solve your problem? -Corey -- -Corey O'Connor</body>
  </mail>
  <mail>
    <header>Re: CGLayer Pixel Depth</header>
    <body>The specific info Nick is referring to is most likely that on p373 in &amp;quot;Programming with Quartz&amp;quot;: &amp;quot;Quartz optimizes how drawing to the layer is captured based on the type of context that you use to create the layer. A CGLayer object created from a bitmap or window context is matched to the source context in terms of destination color space, pixel format, and so on. ... &amp;quot;You should always create a CGLayer object from a Quartz context (or a context of the same type) that you want to later draw the layer &amp;quot;A CGLayer object should always be created from a context that is of the same type as the context that you are planning to draw the contents to. Doing otherwise may produce pre-rasterized drawing to a context that is not bit-based or lower performance when drawing to a Hope this helps, David</body>
  </mail>
  <mail>
    <header>Re: Processing TIFF/PNG Layers</header>
    <body>Scott Thompson: TIFF files actually support multiple pages, as well as multiple layers. If you open a multi-page TIFF in Preview, it shows the pages separately (in the same manner as for icns files). It may be worth seeing if a) this happens for layered TIFFs and b) if NSImage provides multiple image reps for these cases. -- Jens Ayton</body>
  </mail>
  <mail>
    <header>Re: Processing TIFF/PNG Layers</header>
    <body>On Feb 13, 2006, at 1:19 PM, Frederick C. Lee wrote: CGLayers are not related to layers in TIFF images.  It's an offscreen drawing environment. To my knowledge there is no system service that can separate layers out of a TIFF image.  If it's there, however, I would expect it to be part of Image I/O or the QuickTime Image Importers.  I know that Image I/O supports image file formats that can contain multiple images.  If you have a multi-layer TIFF you might see if Image I/O treats those layers as separate images in the TIFF file.</body>
  </mail>
  <mail>
    <header>Re: Processing TIFF/PNG Layers</header>
    <body>Frederick C. Lee: [...] PNG does not support layers. -- Jens Ayton</body>
  </mail>
  <mail>
    <header>Re: CGLayer Pixel Depth</header>
    <body>Nevermind, I've identified my own problem.  After digging through Gelphman/Laden I realized my Layer was being created with the alpha- only bitmap context and was thus optimized for that situation.  I've revised my code to not cache that case.</body>
  </mail>
  <mail>
    <header>Re: How to get CIImage from data stream?</header>
    <body>On Feb 13, 2006, at 6:21 AM, Raj wrote: Converting RGB to ARGB shouldn't be a problem.  There are functions in Accelerate.framework to do this.  Try: source.data = *your void* src* source.width = *width* source.height = *height* source.rowBytes = *probably width* dataWithBytesNoCopy:target.data length:(target.rowBytes * target.height) freeWhenDone:YES] bytesPerRow:target.rowBytes size:CGSizeMake(target.width, target.height) format:kCIFormatARGB8 Brendan Younger</body>
  </mail>
  <mail>
    <header>Re: flashing overlay window</header>
    <body>It might be better to ask this on the Carbon-dev mailing list, but I do believe some HLTB folks monitor this list also.</body>
  </mail>
  <mail>
    <header>flashing overlay window</header>
    <body>We've got a large system that uses many different technologies.  As one small part of this system, I built some Carbon/Quartz code for making an overlay window the size of the screen and drawing stuff into it. Recently, someone within our company updated their Mac, and when they use this code, the screen flashes.  I see just a bit of a flash--really just a very subtle flicker, but they get a real flash. I'm not an expert on either Carbon or Quartz.  I'd appreciate some input on what we need to do here. The overlay window is created with CreateNewWindow.  I use kOverlayWindowClass, and the attributes are: kWindowNoShadowAttribute | kWindowIgnoreClicksAttribute | kWindowNoActivatesAttribute | kWindowOpaqueForEventsAttribute We then do a ShowWindow. A bit later, when we start to draw, we erase what is there and draw rectangles into the overlay window.  It appears that when this flash is visible (I'm not seeing it on my system), the overlay window is coming up solid until it gets cleared later. However, when we try to clear just before or just after the ShowWindow call, it doesn't seem to make a difference. Is there a way to ensure that the overlay window will come up completely transparent the first time? Am I using any attributes I shouldn't?  Does the OpaqueForEvents one keep me from getting drawn?  Any attributes I should use that I'm not? Thanks for help, andy</body>
  </mail>
  <mail>
    <header>Re: How to draw a bitmap with Alpha values preserved in CGContext?</header>
    <body>On 29 Jun 2006, at 09:45, Jerry wrote: Indeed it returns NULL. also this technote says you are not allowed to use this: marc</body>
  </mail>
  <mail>
    <header>Re: How to draw a bitmap with Alpha values preserved in CGContext?</header>
    <body>On 29 Jun 2006, at 14:26, Scott Thompson wrote: It's been some time since I tried this, but I believe that CGBitmapContextCreate just returns NULL if you try to create one with, say, kCGImageAlphaFirst.</body>
  </mail>
  <mail>
    <header>Re: How to draw a bitmap with Alpha values preserved in CGContext?</header>
    <body>I don't believe this is the case.  I should write some code to check it, though. Part of the information you supply to a bitmap context when you create it is the alpha channel behavior you desire.  At that point you can specify whether or not you want the context to contain premultiplied or &amp;quot;straight alpha&amp;quot; pixels.  Do you really get premultiplied even if you ask the context for straight alpha?</body>
  </mail>
  <mail>
    <header>Re: How to draw a bitmap with Alpha values preserved in CGContext?</header>
    <body>GraphicsImportCreateCGImage always ignores the Graphics Mode set with GraphicsImportSetGraphicsMode. So I think apple doesn't support straight alpha drawing for the moment in their NEW  api 'you have to use this api from now on'????? I found this so weird that Apple will bring out a new API and doesn't support straight Alpha???? I also tried to play with &amp;lt;CGContextSetBlendMode&amp;gt; didn't worked neither... So the only way to read images with is going back to the QuickTime Importer code and using GWorlds? and then transforming the GWorld's to CGBitmapContext. marc On 28 Jun 2006, at 20:20, Marc Van Olmen wrote:</body>
  </mail>
  <mail>
    <header>Re: What's wrong with HDR ? (and Preview.app)</header>
    <body>I found out that it works perfectly on a PowerBook. I assume the Intel version of the OpenEXR library is completely buggy. I'll file a bug to apple about this. ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Re: How to draw a bitmap with Alpha values preserved in CGContext?</header>
    <body>On 29 Jun 2006, at 01:18, Marc Van Olmen wrote: Last time I looked there was no way to get Quartz to draw with straight alpha , only premultiplied. You can use vImage to unpremultiply the pixels.</body>
  </mail>
  <mail>
    <header>Re: How to draw a bitmap with Alpha values preserved in CGContext?</header>
    <body>On 28 Jun 2006, at 20:18, Marc Van Olmen wrote:</body>
  </mail>
  <mail>
    <header>Re: How to draw a bitmap with Alpha values preserved in CGContext?</header>
    <body>when I draw the CGImageRef (obtained by QT GraphicsImporter or by CGImageSource...) with CGContextDrawImage  and I look with the debugger (and also onscreen) the values of each pixel are the following R(cg)= R*A, G(cg)= G*A, G(cg)= G*A A(cg)=A R(cg)= R, G(cg)= G, G(cg)= G A(cg)=A</body>
  </mail>
  <mail>
    <header>Re: How to draw a bitmap with Alpha values preserved in CGContext?</header>
    <body>I think you need to better explain the issue and exactly what you are trying to achieve before folks can help much. First what type of CGContext are you attempting to draw into? What is the source for the image you are drawing? etc. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Image File -&amp;gt; CIImage RGBAf buffer</header>
    <body>On 28 Jun 2006, at 15:04, email@hidden wrote: I'm wondering how you can preserve the full 32-bit pixels... in that case. when i call: CGContextDrawImage the RGB values are the values obtained from doing R(cg)= R*A, G(cg)= G*A, G(cg)= G*A</body>
  </mail>
  <mail>
    <header>How to draw a bitmap with Alpha values preserved in CGContext?</header>
    <body>Now that Apple says we need to use CG and no more GWorld... I'm trying to figure this one out: An alpha channeled image. When it is drawing with CG in CGContext, it applies the Alpha value to the 3 RGB colors and so doesn't preserve the full 32-bit ARGB pixel colors. thx, looked into doc for 2 hours and thought that's too long so must be something tricky..</body>
  </mail>
  <mail>
    <header>Re: using the Quartz Python bindings to convert a TIF to a JPEG</header>
    <body>On Jun 28, 2006, at 1:10 PM, Michael B. Johnson wrote: My first thought is that instead of jumping into Python, could might be able to do the same thing using the sips command line tool? (&amp;quot;sips -h&amp;quot; or &amp;quot;sips --helpProperties&amp;quot; from the command line).  I believe that sips can convert files between formats without having to write Python code. If sips doesn't help you then trying to do it through python should be as simple as creating a data provider from the file URL to the source image and reading a file from it dataProvider = CGDataProviderCreateWithFilename(&amp;lt;insert path to source tiff file here&amp;gt;) cgImage = CGImageImport(dataProvider) cgImage.writeToFile(&amp;lt;output file name here&amp;gt;, kCGImageFormatJPEG, null)</body>
  </mail>
  <mail>
    <header>Re: What's wrong with HDR ? (and Preview.app)</header>
    <body>Just a quick additional note on this issue. I tried Apple's sample code named 'ImageApp'. It behaves the same as Preview.app (20 kb file output when loading/saving OpenEXR file).  The funny thing is that in ImageApp all images have the bug (the one that bugged with Preview and the offficial ones from OpenEXR.com). Le 28 juin 06 √† 20:04, Santiago (Jacques) Lema a √©crit : ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>What's wrong with HDR ? (and Preview.app)</header>
    <body>despite the fact that CoreImage supports HDR it seems that very few apps care about it when saving the document. CoreImage Fun house itself converts all images back to good old 8 bits/component, even if you load a 32bit/component TIFF. It turns out many of the CoreImage apps out there do the same. My problems started when I wanted to saved 32 bits floating point images. It works fine now with TIFF and other formats but when opening and saving an OpenEXR file the result looks liked crap. Somehow it destroyed the content of the file. First I thought I didn't know how to code (which is rather true) ... But then I tried Apple's Preview.app which does the same thing (10.4.7) and it's worse than my app itself. Why do loading and saving some HDR images result in destroying them? In Preview.app, I  open a 3 mb OpenEXR file.. and resave it. I get no error but the ouput is only 20kb and unreadable. When I save it with my app it is slightly better. The image seems to have the correct size but the content is full of noise. You can still distinguish the image but all the colors are mixed up (like random noise and colors). Here is the image I use to test (sent by a tester, created with Shake, this is the one that causes Preview.app to save a 20 kb file): Somehow the example OpenEXR files from the official site seem to be loaded and saved correctly by Preview.app: However for both type of images my app have the same garbling problem when saving to OpenEXR. Is there anything special that has to be done for OpenEXR to be saved correctly? I mean something different than you have to do for TIFF, PNG or PSD. This is the short version of the code I use to save my image (assuming here we have a 32 bit floating point image as input). //------------------ // SAMPE CODE BELOW //------------------ //My method returning a CGImageRef, based on output from some CIImageAccumulator CGImageRef image = [outletPaintView convertToCGImage:[outletPaintView //This does return correctly 8, 16, or 32 CFDictionarySetValue(mSaveMetaAndOpts, kCGImagePropertyIsFloat, CFDictionarySetValue(mSaveMetaAndOpts, kCGImagePropertyDepth, This works correcttly for TIFF files (8,16,32 bits) or PNG files (8, 16 bits), but outputs a garbled image if I select OpenEXR. Look for the lin named 'all_probes.tar.gz' or just download any of them (for some reason they look like executable files to the OS, but Preview.app can open them). So I just open one of these files, which open fine in Preview ... but when I try to save them (as TIFF 8 bits or 32 bits), the whole document turns black! Bye, bye content.  I guess alpha is set to 0 or very close to that... For some reason this only happens with Preview, not with my app. ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Re: Image File -&amp;gt; CIImage RGBAf buffer</header>
    <body>On Jun 28, 2006, at 11:18 AM, Thomas Engelmeier wrote: My understanding is that Core Image will take the source image, whatever it's format, and convert to an appropriate device- independent, RGB, floating-point working space using ColorSync. The advantage of allowing Core Image to handle this, as opposed to doing the conversion to floating point pixels yourself, is that Core Image has a better idea of the context in which the filters are being run.  For example, given a particular filter chain, Core Image may only have to color convert a portion of your original image to floating point pixels through ColorSync because only those pixels have any influence on the result.  If you do the integer-&amp;gt;floating conversion up front, you may be converting a lot more pixels than are strictly necessary. If you want more control over the conversion from the integer to the floating point space than is provided by the default ColorSync conversion then by all means, you can handle the conversion yourself. Scott</body>
  </mail>
  <mail>
    <header>Re: Image File -&amp;gt; CIImage RGBAf buffer</header>
    <body>Excuse my ignorance, it might be I did not pick up everything from the whole bunch of web pages I read the last couple of days... The instantiated CISampler data output format is not dependant of the source CIImage?  I simply do not want to encounter clipping at e.g. an initial brightness adjusts just because the source CIImage presented itself as an integer based format... additional custom import formats while with the more modern methods I lost the bookmarked references due to the latest Apple documentation dance beyond Googelability... Regards, Tom_E</body>
  </mail>
  <mail>
    <header>No floating point when saving JPeg2000?</header>
    <body>Correct me if I am wrong but I think the Jpeg2000 spec allows for 16 and 32 bits floating point pixel formats. But for some reason Apple's implementation only seems to allow saving as 8 bit. Since Preview.app does not do it either I assume it's not a problem with my app (works with other formats). ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Knowing which image formats support 16 or 32 bits pixel formats</header>
    <body>Is there any simple way to find out if a file type supports 16 or 32 bits (per sample) pixel formats? Currently I have hardcoded the tests based on the file type to match what Preview.app does: - PDF, TIFF and OpenEXR support 32 bits float mode - PSD, PNG, TIFF, OpenEXR and PDF support 16 bits half float mode ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Re: Image File -&amp;gt; CIImage RGBAf buffer</header>
    <body>On Jun 28, 2006, at 4:54 AM, Thomas Engelmeier wrote: The aspect of CIImage that it seems that you are missing is that fact that CIImage, in of itself, does not necessarily contain an image buffer. A CIImage is a series of instructions for drawing an image, an &amp;quot;image program&amp;quot; if you will. Consider, for example, that CIImages can be infinite in extent... a concept that is very hard to represent in an image buffer. If you look in the header file for CIImage there are several ways to get image data into a CIImage without having to draw the image out first. In particular, you can create a CIImage directly from a URL (imageWithContentsOfURL:) or you can create one from a CGImage (imageWithCGImage;) that you obtain from Image I/O or some other source. It's not clear to me why you want to convert the image to a floating point format before creating the CIImage. CIImage will be more than happy to convert any image you send from it's current form to an appropriate working space. If that means converting an integer image to floating point as part of it's processing, then so be it. However, if you feel you must do the conversion yourself, I personally wouldn't use either of the techniques you describe above. The first is based on QuickDraw which has been deprecated and should not be used for future development (although you could use new QuickTime calls and take everything through the CGImageRef route which would bypass QuickDraw). Still my personal preference would be to use Image I/O to read the image from disk since it presents a superset of the functionality of QuickDraw. What I would do is: Read the image from disk using Image I/O Create an offscreen CGBitmapContext with a floating point pixel format Draw the image into that context Create a CGImage from the context's buffer Create the CIImage from that CGImage. Scott</body>
  </mail>
  <mail>
    <header>Image File -&amp;gt; CIImage RGBAf buffer</header>
    <body>is there any recommendation how to get an float CIImage buffer from an arbitrary image file on disk completely offscreen? Currently it seems I need either to: o 	Open an GraphicsImporter draw into an a offscreen GWorld, copy by the means of Accelerate.Framework to an float buffer Create an CIImage buffer o	Open an NSImage get the best NSBitmapimageRep copy by the means of Accelerate.Framework to an float buffer Create an CIImage buffer Am I missing the point where I can specify I need an float buffer anywhere? TIA, Tom_E</body>
  </mail>
  <mail>
    <header>Re: Saving a layered image</header>
    <body>On Jun 27, 2006, at 7:58 AM, Pete Bates wrote: Another way to store multiple images that I don't think anyone has mentioned is as pages of a PDF.</body>
  </mail>
  <mail>
    <header>Re: Saving a layered image</header>
    <body>On Jun 26, 2006, at 12:18 AM, Santiago (Jacques) Lema wrote: A good point.  I have the multiple image tiff method working, but can see an advantage to using bundles for storage... especially since there is other data I need to archive.  I could stick the multi-image tiff inside the bundle with the other data too. I am having difficulty understanding how to create the bundles, however.  I read the Bundle Programming Guide and have searched the archives, but don't seem to get the picture entirely.  Please tell me if I'm on the right track.  To create a bundle, I simply need to create a directory, include a subdirectory called &amp;quot;Content&amp;quot; and then put an Info.plist file and whatever other files I may wish to include in the Content sub-directory?  Then I need to edit the applications Info.plist to include something like: This is probably getting to the point where it might better be sent to cocoa-dev, but since a couple of you have solved the problem, I was hoping you'd be willing to indulge me. Thanks, Pete</body>
  </mail>
  <mail>
    <header>Re: Can I change the gamma of CILinearGradient with a colorspace?	[SOLVED]</header>
    <body>CGColorSpaceRef colorSpace = CGColorSpaceCreateWithName NSDictionary *contextOptions = [NSDictionary dictionaryWithObjectsAndKeys:(id)colorSpace, kCIContextWorkingColorSpace, (id)colorSpace, myCIContext = [CIContext contextWithCGContext:[[NSGraphicsContext I had initially tried contextWithCGContext last week, but I was calling it wrong. My only regret is the hours (days) I wasted on this. :/ But I am happy to have finally found the correct solution. I've created a test app showing the difference between CILinearGradient with the default context and CTGradient: I am going to file a doc enhancement request for this, since I don't believe it behaves as expected. :)</body>
  </mail>
  <mail>
    <header>Re: Saving a layered image</header>
    <body>On Jun 23, 2006, at 10:12 AM, Peter Bates wrote: That sounds about right.  It seems a bit unusual to use Cocoa and NSImage on one end then Core Image on the other... but I guess that's the advantage of a &amp;quot;standard&amp;quot; image format like TIFF. Scott</body>
  </mail>
  <mail>
    <header>Re: Saving a layered image</header>
    <body>On 26 Jun 2006, at 08:18, Santiago (Jacques) Lema wrote: I do exactly the same thing, but instead of creating a folder, I create a ZIP archive. That way you end up with one file, but the format is still open - users can just unzip it to see the XML and layers.</body>
  </mail>
  <mail>
    <header>Re: Saving a layered image</header>
    <body>I am not sure using tiff is a very good idea since the risk is that people could lose content by just opening the tiff file and resaving it. For my app which also does image manipulation with core image I settled to using a bundle. The advantage of such a folder-based format is that even if people do not understand your format they can still load each layer individually by just using 'show package contents'. In my case I have flat tiff image with all the image combined and then a tiff image for each element (layer, layer mask or else). And inside I added a custom XML file which describes the layers in a rather self-explaining way. I described the file format here (a bit outdated buf you get the idea): Le 23 juin 06 √† 17:12, Peter Bates a √©crit : ......................................................... Santiago (Jacques) Lema - link-u .........................................................</body>
  </mail>
  <mail>
    <header>Re: Saving a layered image</header>
    <body>So I would do something along the lines of: NSArray *myImageArray=[NSArray arrayWithObjects: [myLayer1Accumulator NSBitmapImageRep *myImageRep= [NSBitmapImageRep Then archive this? CGImageSourceCreateWithURL CGImageSourceGetCount and CGImageSourceCreateImageAtIndex and then back to core image? Thanks for your help Scott... as you can surmise, I did use Core Image and am succumbing to the &amp;quot;creeping featurism'... drat and blast!</body>
  </mail>
  <mail>
    <header>Re: Saving a layered image</header>
    <body>On Jun 22, 2006, at 7:55 PM, Peter Bates wrote: There are image file formats that  can include more than one image in them.  A TIFF file, for example, can contain an arbitrary number of images.  You should be able to store and retrieve those images using Image I/O. Another alternative would be to make your application's file format a package.  Applications on MacOS X are collected into packages, and some applications (Keynote and Pages are good examples) also create documents that are packages.  You could save your program's output into a package that encapsulates separate images for each layer as well as the original image and any metadata you care to include.</body>
  </mail>
  <mail>
    <header>Saving a layered image</header>
    <body>I'm looking for advice and options.  I have an application in which the user creates various image annotations and markups in layers represented in image accumulators, which are composited to yield an output image.  I'd like to archive the layers so they can be reloaded from disk and re-edited by the user.  Should I archive the image of each accumulator as separate images?  Is there a better way? Thanks, Pete</body>
  </mail>
  <mail>
    <header>Re: QDPict routines</header>
    <body>Re: The QDPict routines for drawing QuickDraw pictures to a Quartz context can work with a data provider that provides the PICT data starting with the first byte of the data stream or starting with the 512th byte of the data stream.</body>
  </mail>
  <mail>
    <header>CoreVideo and multithreading performances</header>
    <body>I saw that in many cases the Quicktime player gets better performances than most of the CoreVideo samples out there. I tried to understand what was causing this difference by monitoring the Quicktime player with the ThreadViewer application. I saw that the QT player (nearly) always decodes movies from a background thread (which is great). However when I monitored our application or some CoreVideo samples (like the LiveMixer) I saw that most of the time the decoding was done in the main thread which slows down the rendering and the user interface. You can see the difference by checking these two screenshots (using an MPEG 4 movie): And for LiveVideoMixer (we have the same problem in our application: Modul8) : - is there a way to force QT to decode from a background thread all the time ? - When and how QT decides to decode from a background thread or from the main thread ? Best Regards, Yves Schmid * GarageCube.</body>
  </mail>
  <mail>
    <header>Re: drawing 24-bit pict to 32-bit CGBitmapContext</header>
    <body>Like I said in my followup, there is no bitmap data in the PICT. It stops after the color table. Looks like a Photoshop bug. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: drawing 24-bit pict to 32-bit CGBitmapContext</header>
    <body>On Jun 21, 2006, at 6:49 PM, Eric Schlegel wrote: That is correct.  More specifically, the header is used only for the traditional data-fork type of file.  'PICT' resources (and perhaps any other restype storing PICT data as thumbnails) also do not contain the header. If this particular image doesn't have the header, one can look at the various opcodes and troubleshoot what may have happened.  I've only done such a thing years ago, but I seem to remember an old Inside Mac volume had an appendix of all the opcodes. ___________________________________________________________ Ricky A. Sharp Instant Interactive(tm)</body>
  </mail>
  <mail>
    <header>Re: drawing 24-bit pict to 32-bit CGBitmapContext</header>
    <body>On Jun 21, 2006, at 4:25 PM, Nick Nallick wrote: I believe the 'PICT' in-memory data format never has a 512-byte header. It's the 'PICT' file format that has the header.</body>
  </mail>
  <mail>
    <header>Re: drawing 24-bit pict to 32-bit CGBitmapContext</header>
    <body>On Jun 21, 2006, at 4:24 PM, Steve Mills wrote: Sometimes PICT has a 512 byte header and sometimes it doesn't (a remnant of the MacPaint file format).  I don't remember which way CG prefers it (but I think it's without).  You may want to try starting your data provider 512 bytes into your PICT and see if that makes a difference. Nick</body>
  </mail>
  <mail>
    <header>Re: drawing 24-bit pict to 32-bit CGBitmapContext</header>
    <body>Wait, sorry. I lied. It's actually getting an 8-bit PICT resource, not the 24-bit QT-compressed PICT resource that I thought it was using. And it turns out the 8-bit PICT is incomplete. There's a color table, but no bitmap data. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>drawing 24-bit pict to 32-bit CGBitmapContext</header>
    <body>I have a sample customer Photoshop EPS that has a 24-bit PICT preview. When I draw that PICT to a 32-bit CGBitmapContext, nothing happens - my bits remain 0's. Here's my code to draw the PICT to the context: void DrawPICTToCGContext(PicHandle pict, const CGRect&amp;amp; cgr, CGContextRef context) CGDataProviderRef       provider = CGDataProviderCreateWithData(nil, *pict, GetHandleSize((Handle)pict), QDPictRef               qdPict = QDPictCreateWithProvider OSStatus                err = QDPictDrawToCGContext(context, QDPictDrawToCGContext returns noErr. Anybody see anything wrong with this? It works for 32-bit PICTs. Since Quartz doesn't do 24-bit bitmaps, maybe there's a bug here (or a total lack of 24-bit implementation throughout Quartz)? _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Crash with CGPDFPageRelease/CGPDFDocumentRelease</header>
    <body>Now, I remember having read this in the documentation. Thanks a lot for your help.</body>
  </mail>
  <mail>
    <header>Re: Crash with CGPDFPageRelease/CGPDFDocumentRelease</header>
    <body>On Jun 21, 2006, at 8:46 AM, Eric Arlotti wrote: Since you got the page with a &amp;quot;Get&amp;quot; call, not a &amp;quot;Create&amp;quot; or &amp;quot;Copy&amp;quot;, you are not supposed to release it.</body>
  </mail>
  <mail>
    <header>Crash with CGPDFPageRelease/CGPDFDocumentRelease</header>
    <body>void QZ_DrawPDFInGWorld(CFURLRef urlRef , GWorldPtr gworld) pageRef = CGPDFDocumentGetPage(pdfDocRef, 1);		// get page 1 This code crashes each time, on any PDF document. I tried to remove the drawing code but the crash still occurs. If I remove one of the two calls (CGPDFPageRelease or CGPDFDocumentRelease), everything works. Here is a sample of the crash log. Seems to indicate that the crash occurs around CFRelease 0   &amp;lt;&amp;lt;00000000&amp;gt;&amp;gt; 	0xfffeff20 objc_msgSend_rtp + 32 1   com.apple.CoreGraphics         	0x903def70 pdfDocumentFinalize + 132 2   com.apple.CoreFoundation       	0x907c3f34 _CFRelease + 240 3   AjarisImage                    	0x0d09143c QZ_DrawPDFInGWorld__FPCcP13OpaqueGrafPtri + 332 Does it mean that CGPDFDocumentRelease cannot be called after CGPDFPageRelease ? Thanks for your help Eric Arlotti email@hidden</body>
  </mail>
  <mail>
    <header>Re: CIImage to char/float data via OpenGL?</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CoreImage drawImage messing with my clean lines</header>
    <body>On May 27, 2007, at 1:34 PM, Jonathan Johnson wrote: After some further investigation, I decided to move my question over to the OpenGL list. If anyone does have any suggestions, I'm still open to them. I've added more details on the OpenGL list posting. Thanks, Jon</body>
  </mail>
  <mail>
    <header>CoreImage drawImage messing with my clean lines</header>
    <body>Sorry for the strange title -- couldn't come up with a smaller summary that still made sense :) I have a NSOpenGLView which I aquire its CGLContextObj/CGLPixelFormatObj to create a CIContext. I then draw an image using drawImage:inRect:fromRect:. This all works great, and my code to ensure pixel alignment works for this call. However, I then attempt to draw some geometry in OpenGL, mainly noticed with GL_LINE_STRIP. If I don't call drawImage, these lines are crisp and pixel-aligned. However, after calling drawImage they are no longer crisp -- they've undergone subpixel rendering. I've tried to do a few things, but am out of ideas. I'm not an advanced OpenGL developer, so I'm sure I'm just missing some setting. I've tried glLoadIdentity() thinking it was an issue with the matrix, I've tried re-setting up the view to its old state as well. I'm also explicitly setting the line width to 1.0. So I'm curious what else CIContext drawImage could be messing with in my OpenGL context. Any ideas would be greatly appreciated. Thanks in advance, Jon</body>
  </mail>
  <mail>
    <header>Re: Speeding drawing of images as backgrounds</header>
    <body>Am 23.05.2007 um 22:08 Uhr schrieb Rick Mann: Don't know about Carbon, sorry. In Cocoa there's -addChildWindow:ordered: to attach a child to an NSWindow. And demonstrates how to create a transparent window. Yes. I'd like to think that the system is quite good in optimizing that kind of compositing. :)  But you'd probably want to take measurements anyway. Andreas</body>
  </mail>
  <mail>
    <header>Color Depth</header>
    <body>Hi, If, for example I have a display set to HighColor and I want to capture the screen as 32-bit color, without changing the hardware configurations.&amp;nbsp; Is there an existing function which allows you to read the bits directly in the desired color depth or should I implement one myself? Thanks, Ben from someone who knows. Yahoo! Answers - Check it out.</body>
  </mail>
  <mail>
    <header>Deadlock with QCRenderer?</header>
    <body>I am seeing periodic hang ups with QCRenderer.  I haven't created any threads but there are several threads in the app.  None of the other threads are stuck passing through glcGetIOAccelService.  Anyone have any ideas how to debug this? #0	0x90025027 in semaphore_wait_signal_trap #1	0x90001d02 in pthread_mutex_lock #2	0x931d1387 in glcGetIOAccelService #3	0x931d09d9 in CGLSetPBuffer #4	0x97af1989 in -[QCPBuffer attachToCGLContext:] #5	0x97afc232 in -[QCGLCIImage(Override) _uploadTexture:] #6	0x97afaa0e in -[QCGLImage textureName] #7	0x97afb978 in -[QCGLImage(Private) _setTextureOnContext:unit:useTransformationMatrix:] #8	0x97afb91e in -[QCGLPort_Image set:unit:useTransformationMatrix:] #9	0x97b44cb6 in -[QCBillboard execute:time:arguments:] #10	0x97af5c1c in -[QCPatch(Private) _execute:arguments:] #11	0x97af6499 in -[QCPatch(Execution) executeSubpatches:arguments:] #12	0x97af5c1c in -[QCPatch(Private) _execute:arguments:] #13	0x97af55f4 in -[QCPatch(Runtime) render:arguments:]</body>
  </mail>
  <mail>
    <header>Re: Speeding drawing of images as backgrounds</header>
    <body>I believe that CarbonSketch does this. You just setup an overlay window to draw your content into. In CarbonSketch I believe that technique is used for &amp;quot;new object&amp;quot; drawing. This is correct. It's something to test and see if it's a win at least =). -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Speeding drawing of images as backgrounds</header>
    <body>On May 23, 2007, at 8:58 AM, Andreas Mayer wrote: Yep! Can you elaborate a little on how this would be done? Perhaps there's an example? I seem to recall a Carbon drawing program example that used a transparent window on top of the main canvas while the mouse was down... I guess the drawing is pushed down into the window compositing layer? -- Rick</body>
  </mail>
  <mail>
    <header>Re: Speeding drawing of images as backgrounds</header>
    <body>Am 23.05.2007 um 02:25 Uhr schrieb Rick Mann: So it's just a background? And it stays always the same? Then why not use a separate window and draw everything else onto a child window with transparent background on top of the background window? Andreas</body>
  </mail>
  <mail>
    <header>Re: Forcing CIImageAccumulator calculation</header>
    <body>For those that are interested there is an undocumented CIImageAccumulator method commitUpdates: which takes a CIContext. So my problem is solved for the moment by inserting this after setting a's image.</body>
  </mail>
  <mail>
    <header>Re: Speeding drawing of images as backgrounds</header>
    <body>Yup, you can defeat that cache, but if you always (or at least commonly) draw it at the same size, the CG cache will usually beat any manual caching. But to really understand your drawing performance you should definitely run Shark on it to see timing info and Quartz Debug to see what issues (such as over flushing or redrawing the same content). You'll often find that it has a lot of info for you as well. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Speeding drawing of images as backgrounds</header>
    <body>On May 22, 2007, at 7:46 PM, David Duncan wrote: I ran the caching demo. It seems that bitmaps can offer a 15% - 20% gain, but the ease of using CGLayers makes them more compelling for now. I'll start by caching the image as a CGLayer, at the scaled size, and move on from there. Hmm. I thought it was taking between 120 and 250 ms to draw my image (scaling included), but now I'm noticing that if the image is drawn at the same size as it was previously, it's much faster (0.3 ms). It appears that CG is already caching the image at the last-drawn size, so there's not much to be gained by doing it myself. -- Rick</body>
  </mail>
  <mail>
    <header>Re: Speeding drawing of images as backgrounds</header>
    <body>Offscreen bitmaps (via CGBitmapContextCreate) offer you complete control over the format of the image. This can be very valuable if you need to modify the image afterwards. The disadvantage is how much you need to know about the format for performance sake. Layers offer the advantage of not needing to know anything about the destination to get an optimal cache. You get no access to the image data however, and if you want to maintain performance it can be complicated to determine when to recreate the layer (this is not tracked automatically). Both images (which you'll need to get from a bitmap context) and layers are cached by Quartz, so if you use the content repeatedly, you should get additional internal performance benefits. You can thwart the cache however, and that is generally the case for which doing your own caching is a performance win (otherwise it is usually just a memory hog). And speaking of memory - all of these methods consume memory which may end up making your caching a loss overall, especially on small-memory machines. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Speeding drawing of images as backgrounds</header>
    <body>On May 22, 2007, at 5:44 PM, Wim Lewis wrote: I see they do a couple of different versions of caching a scaled image. One uses offscreen bitmaps, the other CGLayers. It's not clear to me what the advantage of one is over the other, other than apparently one need not know as much to use a CGLayer. -- Rick</body>
  </mail>
  <mail>
    <header>Re: Speeding drawing of images as backgrounds</header>
    <body>It causes Image I/O to decode the image for the CGImageRef that it hands back to you. Without the flag you get an image that is decoded everytime you draw it. Decoding most image formats causes the image to consume considerably more memory so you should be careful when you use that flag (that is, only use it after you have measured a performance improvement). -- David Duncan Apple DTS Quartz and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Re: Speeding drawing of images as backgrounds</header>
    <body>How much are you drawing would be of considerable help. A lot of it would be a combination of knowing what your drawing needs are and why you are doing what you are doing. Off hand however, it may be valuable for you to cache a pre-shrunk version of your image for use in compositing, especially if you typically expect your view to be considerably smaller than the image itself. Additionally if you find that you are often drawing a small subsection of an image, then you can use CGImageCreateImageInRect() to create an image for that portion, which will avoid having to do clipping work inside Quartz. scrolling and after these operations end re-render at the higher resolution. Need more information to tell you honestly. -- David Duncan Apple DTS Quartz and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Re: Speeding drawing of images as backgrounds</header>
    <body>I haven't done any tests, but the simplest thing you could try is to see if setting the kCGImageSourceShouldCache property to True when you read the PNG speeds anything up. It's not clear exactly what that flag does, except for the description in the header or the reference documentation: /* Specifies whether the image should be cached in a decoded form. The * value of this key must be a CFBooleanRef; the default value is * kCFBooleanFalse. */ QuartzCache/ which does timing tests with a handful of different caching techniques.</body>
  </mail>
  <mail>
    <header>Speeding drawing of images as backgrounds</header>
    <body>Hi. I'm drawing satellite tracks onto an image of the Earth. In order to support a large window size, my image is a large PNG (1600 x 1024, I think). As a PNG it dramatically bloats my app, so I may change some aspect of that, but that's where I stand for now. Drawing is very slow (I'm looking into gathering hard data for the various drawing operations; never used CHUD before). However, just drawing the image takes a lot of time. I'm wondering how to improve the drawing speed. Right now I use CGContextDrawImage() or HIViewDrawCGImage(), depending on whether or not the context is inverted (this is handled by the Nano framework I'm using). Obviously one thing I can do is render just the portion of the image, at the appropriate scale, that is being shown at any given time. However, that helps less when resizing or scrolling the view. Is there another technique that might be faster (than CGContextDrawImage())? Should I set up layers, or draw the image once into some offscreen buffer and use that? Perhaps one of these techniques loads the image into graphics hardware memory, or some similar advantage? -- Rick</body>
  </mail>
  <mail>
    <header>Re: Reg : Problem in PDF Document Printing using Carbon.</header>
    <body>It looks like the data provider is trying to get the data from the URL and is somehow failing to return a proper CF object, while also not flagging an error. The root cause appears to be in the HTTP handling - it may be that if there is no data returned that it returns a NULL object when the code above is always expecting a non- NULL object (due to the same API call saying that the result was success). &amp;gt;, providing the URL that your are trying to get data from. -- David Duncan Apple DTS Quartz and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Re: Reg : Problem in PDF Document Printing using Carbon.</header>
    <body>It looks like you're probably crashing when CGDataProviderCreateWithURL is calling CFRelease with a null pointer.  You should check what you're passing into these functions. It's also possible you're getting a double release.  For instance, this can happen if you release something you obtain from the system using a Get... call (instead of a Copy... or Create... call). On May 22, 2007, at 10:02 AM, Janakiram Palepu wrote:</body>
  </mail>
  <mail>
    <header>Reg : Problem in PDF Document Printing using Carbon.</header>
    <body>Hi ALL, I'm developing a Carbon based Mozilla's XPCOM Component using Carbon. I'm using Carbon Printing Manager  &amp;amp; Quartz to print a PDF Document , which i get it from HTTP Site. I'm using CGPDFDocumentCreateWithURL for constructing the In memory PDF Object with the response from the HTTP Site . I'm facing a problem when i receive a PDF Document with Zero Size , This Method is Crashing in the middle. I  tried googling this but didn't get any clue. One more Unfortunate thing is Exception Handling is not there in C/Carbon Programming. Please help me to solve this. or else any pointers which helps me to solve ?? Thanks, JanakiRam. Note : I'm attaching the Crash Log for your reference. Exception: EXC_BAD_ACCESS (0x0001) Codes: KERN_PROTECTION_FAILURE (0x0002) at 0x00000000 Thread 0 Crashed: 0 com.apple.CoreFoundation 0x908046c9 CFRelease + 56 1 com.apple.CoreGraphics 0x90376c8a CGDataProviderCreateWithURL + 231 2 com.apple.CoreGraphics 0x9067aed8 CGPDFDocumentCreateWithURL + 19 3 libPDFPrinter.dylib 0x0efbee73 PDFPrinter::printPDF(__CFString const*, __CFURL const*, char const*, char const*, OpaquePMPrintSession*, OpaquePMPrintSettings*, OpaquePMPageFormat*, __CFString const*) + 31 4 libPDFPrinter.dylib 0x0efbf741 PDFPrinter::PrintUrlWithPrinterName (char const*, char const*, int*) + 409 5 XUL 0x0105c977 XPTC_InvokeByIndex + 81 6 XUL 0x010e7ec7 XPCWrappedNative::CallMethod(XPCCallContext&amp;amp;, XPCWrappedNative::CallMode) + 3363 7 XUL 0x010eb29c XPC_WN_CallMethod(JSContext*, JSObject*, unsigned, long*, long*) + 244 8 libmozjs.dylib 0x002333da js_Invoke + 2089 9 libmozjs.dylib 0x0023c2e1 js_Interpret + 32774 10 libmozjs.dylib 0x0023343e js_Invoke + 2189 11 libmozjs.dylib 0x00233641 js_InternalInvoke + 188 12 libmozjs.dylib 0x0020c993 JS_CallFunctionValue + 62 13 XUL 0x014eeaa7 nsJSContext::CallEventHandler(JSObject*, JSObject*, unsigned, long*, long*) + 537 14 XUL 0x015210ec nsJSEventListener::HandleEvent(nsIDOMEvent*) + 1172 15 XUL 0x01434af1 nsEventListenerManager::HandleEventSubType (nsListenerStruct*, nsIDOMEvent*, nsIDOMEventTarget*, unsigned, unsigned) + 779 16 XUL 0x01435590 nsEventListenerManager::HandleEvent(nsPresContext*, nsEvent*, nsIDOMEvent**, nsIDOMEventTarget*, unsigned, nsEventStatus*) + 888 17 XUL 0x0140c585 nsGenericElement::HandleDOMEvent(nsPresContext*, nsEvent*, nsIDOMEvent**, unsigned, nsEventStatus*) + 1473 18 XUL 0x0145c4ab nsHTMLButtonElement::HandleDOMEvent(nsPresContext*, nsEvent*, nsIDOMEvent**, unsigned, nsEventStatus*) + 283 19 XUL 0x01284d83 PresShell::HandleEventInternal(nsEvent*, nsIView*, unsigned, nsEventStatus*) + 635 20 XUL 0x012850b1 PresShell::HandleEventWithTarget(nsEvent*, nsIFrame*, nsIContent*, unsigned, nsEventStatus*) + 69 21 XUL 0x0143779b nsEventStateManager::CheckForAndDispatchClick (nsPresContext*, nsMouseEvent*, nsEventStatus*) + 487 22 XUL 0x0143f000 nsEventStateManager::PostHandleEvent (nsPresContext*, nsEvent*, nsIFrame*, nsEventStatus*, nsIView*) + 1058 23 XUL 0x0128501e PresShell::HandleEventInternal(nsEvent*, nsIView*, unsigned, nsEventStatus*) + 1302 24 XUL 0x012885a7 PresShell::HandleEvent(nsIView*, nsGUIEvent*, nsEventStatus*, int, int&amp;amp;) + 1353 25 XUL 0x014e6fb7 nsViewManager::HandleEvent(nsView*, nsGUIEvent*, int) + 733 26 XUL 0x014e881e nsViewManager::DispatchEvent(nsGUIEvent*, nsEventStatus*) + 2540 27 XUL 0x014df13c nsIView::GetViewFor(nsIWidget*) + 78 28 XUL 0x017b4c53 nsWindow::DispatchEvent(nsGUIEvent*, nsEventStatus&amp;amp;) + 103 29 XUL 0x017b4cb6 nsWindow::DispatchWindowEvent(nsGUIEvent&amp;amp;) + 34 30 XUL 0x017b4f6c nsWindow::DispatchMouseEvent(nsMouseEvent&amp;amp;) + 68 31 XUL 0x017a255e nsMacEventHandler::HandleMouseUpEvent(EventRecord&amp;amp;) + 228 32 XUL 0x0179f437 nsMacEventHandler::HandleOSEvent(EventRecord&amp;amp;) + 195 33 XUL 0x017a5ebc nsMacWindow::DispatchEvent(void*, int*) + 58 34 XUL 0x017a3239 nsMacMessagePump::DispatchOSEventToRaptor (EventRecord&amp;amp;, OpaqueWindowPtr*) + 73 35 XUL 0x017a33c1 nsMacMessagePump::DoMouseUp(EventRecord&amp;amp;) + 73 36 XUL 0x017a3e99 nsMacMessagePump::DispatchEvent(int, EventRecord*) + 205 37 XUL 0x017a4073 nsMacMessagePump::DoMessagePump() + 53 38 XUL 0x01797cd9 nsAppShell::Run() + 45 39 XUL 0x017154b3 nsAppStartup::Run() + 31 40 XUL 0x01091f29 XRE_main + 10663 41 xulrunner-bin 0x0000282c main + 2082 42 xulrunner-bin 0x00001a6e start + 270 43 xulrunner-bin 0x00001989 start + 41</body>
  </mail>
  <mail>
    <header>Re: CGContextDrawShading &amp;amp; CGContextFillRect</header>
    <body>I would expect it to be slower, but the exact speed difference would depend on a number of factors.  Not the least of which is the complexity of the shading routine, the number of gradient steps that actually intersect the clipping rectangle, etc..</body>
  </mail>
  <mail>
    <header>CGContextDrawShading &amp;amp; CGContextFillRect</header>
    <body>Hi folks, I want to draw a gradient in a rectangle that's 500 x 50 pixels wide. In my code, I have a drawing function that gets called many times per second. Because of this efficiency is a big concern. I setup my shading elsewhere in my code so that in my drawing function all I have to call is: As far as efficiency goes, I am wondering how this function compares to calling: I am not in an environment where benchmarking is the easiest thing to do, and I would like to understand if CGContextDrawShading is significantly slower than CGContextFillRect. Thank you. - Luigi ------------------------------------------------------------ THIS E-MAIL MESSAGE IS FOR THE SOLE USE OF THE INTENDED RECIPIENT AND MAY CONTAIN CONFIDENTIAL AND/OR PRIVILEGED INFORMATION.  ANY UNAUTHORIZED REVIEW, USE, DISCLOSURE OR DISTRIBUTION IS PROHIBITED.  IF YOU ARE NOT THE INTENDED RECIPIENT, CONTACT THE SENDER BY E-MAIL AT email@hidden AND DESTROY ALL COPIES OF THE ORIGINAL MESSAGE. WITHOUT PREJUDICE UCC1-207. ------------------------------------------------------------ ____________________________________________________________________________________Be a better Heartthrob. Get better relationship answers from someone who knows. Yahoo! Answers - Check it out.</body>
  </mail>
  <mail>
    <header>Re: CGContextRestoreGState failing</header>
    <body>Thanks for replying Scott but I found the problem. I was manually wrapping a CGContext in an NSGraphicsContext and always setting the flipped in the context to NO Like this After a bit of rethinking/rewriting, was able to eliminate this step and the problem went away. -Ken -------------- Original message ----------------------</body>
  </mail>
  <mail>
    <header>Re: CGContextRestoreGState failing</header>
    <body>On May 19, 2007, at 2:36 PM, Ken Tozier wrote: How are you rendering the text?</body>
  </mail>
  <mail>
    <header>Re: NSColorSpace vs CGColorSpaceRef</header>
    <body>A and F are equivalent, as are C and G. However, a CGColorSpace does not keep track of the CMProfile used to create it (when it is created from one) so there is no equivalent to B. An equivalent to neither D nor E is available. Nope and nope. =/ 1 could indicate either Gray or Indexed (or a few other situations). You can distinguish the two by trying to create an indexed colorspace based on the given colorspace. If it succeeds, it would indicate a non-indexed colorspace with 1 component (which is likely to be, but may not always be, a Gray colorspace). Aside from your assumptions above, this is not currently possible. I highly recommend filing feature enhancement requests at &amp;lt;http:// bugreport.apple.com&amp;gt; to register your desire for these capabilities. -- David Duncan Apple DTS Quartz and Printing email@hidden</body>
  </mail>
  <mail>
    <header>one pixel discrepancy &amp;amp; antialiasing</header>
    <body>Hi there, I want to draw a grid. To this goal I have a CGRect of a certain size and I need to stroke lines that span the width and the height of the rectangle. I would also like to fill the rectangle with a certain color to create the grid background. Unexpectedly, I noticed that the span of the lines will always be one pixel longer than the fill of the rectangle. But the results change depending whether antialiasing is on or off. In my case I cannot stroke the lines with antialiasing on, since it is possible that my lines are spaced one pixel apart from each other. Antialiasing would just blend the lines into an uniform color. Is there any way to make the behavior more consistent so that I could have the length of the lines and the fill of the rectangle span the same number of pixels, regardless of antialiasing being on or off? Thank you. - Luigi ------------------------------------------------------------ THIS E-MAIL MESSAGE IS FOR THE SOLE USE OF THE INTENDED RECIPIENT AND MAY CONTAIN CONFIDENTIAL AND/OR PRIVILEGED INFORMATION.  ANY UNAUTHORIZED REVIEW, USE, DISCLOSURE OR DISTRIBUTION IS PROHIBITED.  IF YOU ARE NOT THE INTENDED RECIPIENT, CONTACT THE SENDER BY E-MAIL AT email@hidden AND DESTROY ALL COPIES OF THE ORIGINAL MESSAGE. WITHOUT PREJUDICE UCC1-207. ------------------------------------------------------------ ____________________________________________________________________________________ Looking for earth-friendly autos? Browse Top Cars by &amp;quot;Green Rating&amp;quot; at Yahoo! Autos' Green Center.</body>
  </mail>
  <mail>
    <header>CGContextRestoreGState failing</header>
    <body>Hi I'm drawing a CGLayer into a CGContextRef initialized as follows CGContextRef	context = (CGContextRef)[[NSGraphicsContext if ([superView isFlipped]) else These commands are encapsulated by an NSView subclass method and are invoked from within a loop and I'm finding that for every instance except the first, the text renders upside down. I know the problem is somewhere in this method as if I comment out the method invocation, the text renders correctly. In desperation, I also tried to undo the scale transform, like the following, immediately before calling CGContextRestoreGState but it didn't work if ([superView isFlipped]) else Ken</body>
  </mail>
  <mail>
    <header>Re: 2D Gradient Shading</header>
    <body>I don't quite follow your description of what it's supposed to look like, but yes, you can simply draw another gradient on top of the previous one, using high alpha values will give you a wash of one gradient over the other. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>2D Gradient Shading</header>
    <body>Hi folks, After much reading and trying, I have successfully setup an shading function, managed to create a CGShading and finally got Quartz to draw it. The shading is an axial one. It is clipped to a simple rectangle 500 pixels high and 50 pixels wide. It linearly goes from green at the bottom to yellow at the top. Now the next step would be to give it a more 3D rounded look, almost if the shading was applied to a round pipe. To do that I need to make the color closer to the left and right edges of the rectangle less saturated (darker) and make them become more and more saturated (lighter) as they come close to the center of the rectangle. Basically I need to create an horizontal shading on top of the previous vertical one. Can someone shed some light on how to do that? Should I set up another shading function? How do I combine the two functions then? Any help highly appreciated. Best. - Luigi ------------------------------------------------------------ THIS E-MAIL MESSAGE IS FOR THE SOLE USE OF THE INTENDED RECIPIENT AND MAY CONTAIN CONFIDENTIAL AND/OR PRIVILEGED INFORMATION.  ANY UNAUTHORIZED REVIEW, USE, DISCLOSURE OR DISTRIBUTION IS PROHIBITED.  IF YOU ARE NOT THE INTENDED RECIPIENT, CONTACT THE SENDER BY E-MAIL AT email@hidden AND DESTROY ALL COPIES OF THE ORIGINAL MESSAGE. WITHOUT PREJUDICE UCC1-207. ------------------------------------------------------------ ____________________________________________________________________________________You snooze, you lose. Get messages ASAP with AutoCheck in the all-new Yahoo! Mail Beta.</body>
  </mail>
  <mail>
    <header>Black Point Compensation in Quartz?</header>
    <body>Starting in 10.4 CMICCProfile.h added the symbols cmBlackPointCompensationMask and cmBlackPointCompensation. I can use Black Point Compensation successfully using the call NCWConcatColorWorld(). Can I get images drawn with or without Black Point Compensation using CG calls? Does Quartz support black point compensation yet? If so, how?</body>
  </mail>
  <mail>
    <header>NSColorSpace vs CGColorSpaceRef</header>
    <body>I need to use CG calls for a project and I need certain functionality that NSColorSpace provides. Are there equivalents to the following using CG calls? B:      - (CMProfileRef)colorSyncProfile C:       - (int)numberOfColorComponents D:       - (NSColorSpaceModel)colorSpaceModel E:       - (NSString *)localizedName F:      CGColorSpaceRef CGColorSpaceCreateWithPlatformColorSpace(CMProfileRef profile) G:      size_t CGColorSpaceGetNumberOfComponents(CGColorSpaceRef colorSpace) Questions: ======== - Is there a simple way to get the color space model (D) or color space name (E) from a CGColorSpaceRef? - Can we get the information by way of the underlying CMProfileRef? (apparently not) We can create a NSColorSpaceRef directly from a CMProfileRef (A), and conversely we can get the CMProfileRef back from the NSColorSpace (B). Although we can create a CGColorSpaceRef directly from a CMProfileRef (F), we cannot directly get the CMProfileRef back from the CGColorSpaceRef. So this possible workaround does not work. We could call CGColorSpaceGetNumberOfComponents() and assume that 3 components = RGB and 4 components = CMYK. But does 1 indicate Gray, or could it be an indexed color space? I doubt that there is a one-to-one mapping between component count and color model in general. Why isn't there a CGColorSpaceGetColorModel() function when there is a colorSpaceModel method? One could parse the CFStringRef returned by CFCopyDescription(CGColorSpaceRef) to determine some of this information, but this is ugly. - What is the correct way to get the color model (RGB, CMYK, Gray, LAB, XYZ, DeviceN) and color space name from a CGColorSpaceRef?</body>
  </mail>
  <mail>
    <header>Re: CIImage to char/float data via OpenGL?</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Forcing CIImageAccumulator calculation</header>
    <body>When a CIImageAccumulator is set to an image this is lazy, the image is not calculated until the CIImageAccumulator image is drawn. Which is correct and good. But when the accumulator's image is set to either another accumulator's image or a set of filters that use another accumulator's image laziness trips up. If the source accumulator's image is changed before the destination accumulator's image is drawn then the drawn image is wrong. This pseudo example will draw the new image using someNewFilter, not the original image. Now the question is:  is there any way to force the calculation without drawing the image? Forcing the calculation right after [a setImage:[b image]] would ensure that when it is actually drawn it is the correct image. thanks -- Will</body>
  </mail>
  <mail>
    <header>Re: generating Affine Transforms from points</header>
    <body>Thanks, Marco.  Based on what you said I did a search on &amp;quot;generate affine transform from three points&amp;quot; and found Maybe from the equations there I can get the transform coefficients. Rick</body>
  </mail>
  <mail>
    <header>Re: generating Affine Transforms from points</header>
    <body>On Wed, 16 May 2007 15:53:00 -0600, I think you need three points per image.  You want an affine transform T such that T(A) = A', T(B) = B' and T(C) = C'.  This is equivalent to solving a system of six linear equations in six unknowns (the coefficients of the affine matrix).  Each point mapping gives you two equations. In general, yes. -- marco -- It's not the data universe only, it's human conversation. They want to turn it into a one-way flow that they have entirely monetized. I look at the collective human mind as a kind of ecosystem. They want to clear cut it. They want to go into the rainforest of human thought and mow the thing down.</body>
  </mail>
  <mail>
    <header>generating Affine Transforms from points</header>
    <body>I'm writing an application that will align two images (one to the other) using NSAffineTransform or CGAffineTransform. The user will select two points A and B in one image and A' and B' in the second image.  (Perhaps three points per image are necessary).  From those selections I'll create the transform that aligns the first image to the second. Is there a good way to directly construct the affine transform that transforms A and B to A' and B'?  Up to this point, I've been finding the magnitude and angle of A-&amp;gt;B and A'-&amp;gt;B', making a NSAffineTransform from them, applying that transform to A and subtracting that result from A' to get the translation, then appending that translation to the transform. That gives me the transform to apply to the first image as a whole to align it to the second image.  But that seems clumsy and perhaps wrong. And if I did this process with three points per image instead of two, would that get me separate x and y scaling? Thanks, Rick</body>
  </mail>
  <mail>
    <header>Re: Clearing a CGPath</header>
    <body>I would either say continue to do what you're doing (i.e. re-creating the path in the update function) or simply change the draw routine to draw the path directly into the context then profile the result to see if you have to worry about the cost of creating the path to begin with.</body>
  </mail>
  <mail>
    <header>Re: Clearing a CGPath</header>
    <body>Here is my scenario: I am writing a GUI object which has the option to be resized. The object is pretty complex, but its main structure is made of four functions: - a Create function (called when the user instantiates the object) - a Delete function (called when the user deletes the object) - an Update function (called when the user resizes the object) - a Draw function (called when the object needs to redraw itself to display something different) So ideally I would like to have: - in the Create function: - in the Delete function: - in the Update function: - in the Draw function: In case that the Draw function (which is supposed to be more processor intensive than the other functions) is called faster than the computer can actually draw, a queue will be created and the Draw function will be placed in the queue and scheduled to be called some milliseconds in the future. This structure works very well for my needs, however it doesn't guarantee that the Draw function is being called every time after an Update function. So if the Update function gets called twice in a row before a Draw function, two or more sets of segments will be added to my path, which is obviously not what I want. That's the reason of my question. Right now I am dealing with this issue by first releasing the path and then creating it again in the Update function, however I thought it would be more elegant and efficient if I could just clear it. If you any advice in regard to this, please feel free to let me know. Thank you. - Luigi ------------------------------------------------------------ THIS E-MAIL MESSAGE IS FOR THE SOLE USE OF THE INTENDED RECIPIENT AND MAY CONTAIN CONFIDENTIAL AND/OR PRIVILEGED INFORMATION.  ANY UNAUTHORIZED REVIEW, USE, DISCLOSURE OR DISTRIBUTION IS PROHIBITED.  IF YOU ARE NOT THE INTENDED RECIPIENT, CONTACT THE SENDER BY E-MAIL AT email@hidden AND DESTROY ALL COPIES OF THE ORIGINAL MESSAGE. WITHOUT PREJUDICE UCC1-207. ------------------------------------------------------------ ____________________________________________________________________________________Ready for the edge of your seat? Check out tonight's top picks on Yahoo! TV.</body>
  </mail>
  <mail>
    <header>Re: Clearing a CGPath</header>
    <body>On May 15, 2007, at 5:13 PM, Luigi Castelli wrote: No.  Creating the path is going to be a minor part of anything you might do with a path that complex anyway. Why do you think you want to reuse the same path?</body>
  </mail>
  <mail>
    <header>Re: Save as PDF in print dialog</header>
    <body>Please write up bugs at bugreport.apple.com about those issues.</body>
  </mail>
  <mail>
    <header>Clearing a CGPath</header>
    <body>Hi everyone, I have created a mutable path for later use in my application as such: then I am adding a bunch of line segments to it like this: Is there a way to clear this cgPath (remove all line segments) without having to release it and then create it again ? Thanks. - Luigi ------------------------------------------------------------ THIS E-MAIL MESSAGE IS FOR THE SOLE USE OF THE INTENDED RECIPIENT AND MAY CONTAIN CONFIDENTIAL AND/OR PRIVILEGED INFORMATION.  ANY UNAUTHORIZED REVIEW, USE, DISCLOSURE OR DISTRIBUTION IS PROHIBITED.  IF YOU ARE NOT THE INTENDED RECIPIENT, CONTACT THE SENDER BY E-MAIL AT email@hidden AND DESTROY ALL COPIES OF THE ORIGINAL MESSAGE. WITHOUT PREJUDICE UCC1-207. ------------------------------------------------------------ ____________________________________________________________________________________Choose the right car based on your needs.  Check out Yahoo! Autos new Car Finder tool.</body>
  </mail>
  <mail>
    <header>Create a CFDictionary object in Python</header>
    <body>How do I create a CFDictionary object in Python for the auxiliary info in the CGPDFContextCreateWithFilename() function? I tried a native Python dictionary but the metadata is not visible in the PDF. aux_info['CGPDFContextAuthor'] = 'foo' output_pdf = CGPDFContextCreateWithFilename(output_filename, page_rect, aux_info) Kind regards, Sen Haerens</body>
  </mail>
  <mail>
    <header>Re: Save as PDF in print dialog</header>
    <body>PMSessionGetDestinationType() should do what you want. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Save as PDF in print dialog</header>
    <body>The reason I want to do this is actually really simple. Im surprised theres no way to know. When a user selects &amp;quot;save as pdf...&amp;quot;, I get a PDF context to draw into. When this happens, I know that I don't want to transform my color values before I draw them because the system will not do any conversion. So I can use my custom CGColorspace to create colors and just draw them into the context and my output PDF will reflect exactly what I want. If the user clicks preview, im just guessing here, but the system will use the Default proof profile for conversion. I don't want the system to do that. I want to do the transform beforehand and use the proof colorspace as my input into the context. This way the system will see a NULL transform and not do any conversion. Does this make sense or am I completely off? AC</body>
  </mail>
  <mail>
    <header>Re: Save as PDF in print dialog</header>
    <body>The accessibility API can pick up some of this by reading the screen, as I recall. But, in general, the PDF menu in the system print dialog appears to be a tacked-on hack that complies with virtually none of the standard Mac UI requirements. It really should be fixed. -- Bill Cheeseman</body>
  </mail>
  <mail>
    <header>Re: Core animation delegates and animationDidStop</header>
    <body>&amp;nbsp; &amp;nbsp; topHalfCharLayerFront &amp;nbsp; &amp;nbsp; bottomHalfCharLayerFront I have two animations in a custom UIView, bottomFlip and topFlip. Both have their delegate set to self and I have an animationDidStop method defined in the class. Is there a way to distinguish between the two animations in that method? Here's what I've tried: UPDATE I declare two animations as iVars: I build each animation and set delgate to self e.g. &amp;nbsp;&amp;nbsp;&amp;nbsp;- (CABasicAnimation *)bottomCharFlap: (CALayer *)charLayer // method which builds bottomFlip Animation charLayer.transform = CATransform3DMakeRotation(DegreesToRadians(0), 1, 0, 0); //set to end pos before animation I then try and find bottomFlip in animationdidStop: //insert the next one??? "Animation &amp;lt;memory address&amp;gt; stopped" is logged but nothing else i.e. it doesn't seem to recognise the bottomFlip iVar. How can I provide a switch in animationDidStop which recognises which animation stopped? thanks,</body>
  </mail>
  <mail>
    <header>Core animation delegates and animationDidStop</header>
    <body>I have two animations in a custom UIView, bottomFlip and topFlip. Both have their delegate set to self and I have an animationDidStop method defined in the class. Is there a way to distinguish between the two animations in that method? Here's what I've tried: UPDATE I declare two animations as iVars: I build each animation and set delgate to self e.g. - (CABasicAnimation *)bottomCharFlap: (CALayer *)charLayer // method which builds bottomFlip Animation charLayer.transform = CATransform3DMakeRotation(DegreesToRadians(0), 1, 0, 0); //set to end pos before animation I then try and find bottomFlip in animationdidStop: //insert the next one??? &amp;quot;Animation &amp;lt;memory address&amp;gt; stopped&amp;quot; is logged but nothing else i.e. it doesn't seem to recognise the bottomFlip iVar. How can I provide a switch in animationDidStop which recognises which animation stopped? thanks, Luke. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Autoresize in CALayer</header>
    <body>Hi All,</body>
  </mail>
  <mail>
    <header>Re: Converting arbitrary color spaces to RGB</header>
    <body>You may make a CGBitmapContext with whatever color space you want, then draw the image to that context. After that you have two solutions : - you may pass a buffer to the bitmap context which will hold the data. In that case you may directly access the pixels without using any non portable code. - you may get a CGImage from the bitmap context, to use it later in a CG composition. Raphael</body>
  </mail>
  <mail>
    <header>Converting arbitrary color spaces to RGB</header>
    <body>Hi, is there a convenient way to make sure that my CGImage is always in RGB colorspace? I'm loading arbitrary images using the CGImageSource API and I want to access the raw pixels of every image using CGImageGetDataProvider(). As I don't want to write code for all the different color spaces out there, I'd like to stick with RGB. So is there any way to make sure that I'll always get RGB raw pixels? I tried to call CGImageCreateCopyWithColorSpace() to convert all non-RGB formats to RGB but the function returns NULL for CMYK and grayscale images... Any ideas how do to the trick? I'm using Carbon, btw. Tks, Andreas --</body>
  </mail>
  <mail>
    <header>Re: Core Animation layer not visible until animation is complete</header>
    <body>I'm actually rather confused, but I'm not looking at the whole program. But looking at this one line: If I assume Core Animation has the same coordinate space/system as math and OpenGL, using the right-hand-rule for fields/torque, you are rotating along the x-axis, so take your right-hand, point the thumb to the right, and your fingers curve towards you. To me, this implies that the top part of a layer would start falling towards you. None of this explicitly sets the zPosition value of your layer. The transform would happen independently of setting zPosition. -Eric -- Beginning iPhone Games Development</body>
  </mail>
  <mail>
    <header>Re: Core Animation layer not visible until animation is complete</header>
    <body>Turns out the main reason for this is that I had the animation set like this: If I change that to -90 (signed negative) then the zPosition issue goes away. I guess this is just because I had just assumed that a rotation towards the viewer was the equivalent of clockwise? A rotation like this appears to be the same whether it is positive or negative but I think my rotation was actually moving away from the viewer into negative z, hence the strange behaviour. Eric/Jim - does that sound right to you? thanks, Luke.</body>
  </mail>
  <mail>
    <header>Re: Manipulating alpha channel</header>
    <body>The typical way to do something like this is to use clipping or masking to specify the part of the image that you want to draw transparently. For example you might add a path that specifies the area you want to draw through, then draw the image. This will clip out the parts that you don't want, effectively making them transparent. A mask lets you do something similar, but specify translucency on a pixel by pixel basis. But if you wanted a uniform opacity change, you would use CGContextSetAlpha() before you draw your clipped image. -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: CGPDF memory catastrophe</header>
    <body>Images use a separate cache, but since the generated images should be associated with the PDF pages they should also go away with the PDF ‚Äì that said, this is also likely a large source of memory usage you didn't need. There are other ways you can optimize PDFs that may help you further ‚Äì I'm not specifically familiar with them but many others in this space have experimented. You can use various calls to obtain the same information that VM Tracker does, see 'man vmmap' for some info, and in particular the vm_region mach call. I'm not familiar with the details of its usage however, and in general the problem with monitoring your own memory usage is that you don't know when you run out of physical memory, but there is no call that tells you &amp;quot;you are out of physical memory&amp;quot;. Absolutely put that feedback into a bug. I'm not immediately familiar with the current state of the issue, but it can't hurt to have more feedback from developers that this is causing them problems. -- David Duncan</body>
  </mail>
  <mail>
    <header>Manipulating alpha channel</header>
    <body>What are my options for manipulating the alpha channel of an image? For example, let's say I have an image that is fully opaque.  I'd like to draw a diagonal line on the image such that the image is transparent along that line. Is there a special drawing mode that effectively sets the value of the alpha channel (instead of using it to manipulate the RGB components)?  If not, how might I go about tackling this problem? Thanks for any help/pointers/insight that you'd be willing to share. David</body>
  </mail>
  <mail>
    <header>Re: CGPDF memory catastrophe</header>
    <body>Hello, You are absolutely right. I meant smaller in terms of pixel dimension, not file size and I actually used Acrobat Pro's zip compression for images to save me a few discrete cosinus transforms... Also I exported the pdfs with pdf version 1.3 which does not support some newer features like transparency which will speed things up a bit more, hopefully. I also noticed that opening and resaving a pdf with Preview often produces smaller files, but I didn't see any noticeable runtime improvements yet. Thanks and regards, Sebastian Mecklenburg</body>
  </mail>
  <mail>
    <header>Re: CGPDF memory catastrophe</header>
    <body>Also something I did notice when working on similar issue is that graphic designer tends to not &amp;quot;flatten&amp;quot; images and sometimes some apparently very simple images is in fact made of 8 elements which have to be &amp;quot;composited&amp;quot; on the fly and take a lot of memory and CPU. laurent</body>
  </mail>
  <mail>
    <header>Re: CGPDF memory catastrophe</header>
    <body>Hello, Thanks for you patient reply :-) The pdfs in question had very large images embedded in them which would fill up the memory really quickly. I used optimized pdfs with smaller images and so I could stretch the apps life expectancy over the normal user attention span so I'm fine for now. Thanks for the hint with the VM Tracker. Is there any reliably way I can monitor memory usage from within my application? I will try to find the problem and file a bug report if I think it's actually related to the CoreGraphics framework. However, I still think I should be able to open a CGPDFDocument and render as many pages from it as I want without having to purge the whole thing from memory now and then to avoid low memory situations. It should be the job of the pdf renderer to keep the memory usage of the cache down to a reasonable level. Thanks and Regards, Sebastian Mecklenburg</body>
  </mail>
  <mail>
    <header>Re: CGPDF memory catastrophe</header>
    <body>Host statistics are useless for a fairly large number of reasons, the most important of which in this case is that it isn't telling you what you actually want to know (can I use X amount more memory). It is 100% plausible for there to be 5MB free and to ask for, receive and successfully write to 6MB of memory, even without getting a memory warning. Please track this with VM Tracker rather than host statistics. There can be any number of reasons why free memory is reduced by this much when you open a PDF document, the most likely of which is that the document is about that large in size (in which case the memory is probably being lost to the unified buffer cache, and that memory is lost, it is simply speculative and will be reused for other purposes). There are memory allocations that Allocations doesn't see. Run with VM Tracker and watch the Dirty Size statistic. Well, there hasn't been evidence produced to the contrary :). We are happy to be wrong, but we need evidence and a bug report in order to do anything about the issue. Unfortunately if your deadline is today, it is highly likely that you will not get a solution in time, if only because I imagine it is close to or past closing time by the time you get this e-mail... If the memory is being freed when the app quits, that is only evidence that he memory was somehow associated with the application. Assuming you could unload the Core Graphics framework, it is highly likely that doing so would not do what you want. It would also possibly crash your application before you could reload it (since you cannot control all of the users of that framework). -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: CGPDF memory catastrophe</header>
    <body>Sorry, i forgot to mention, this is on an iPad with iOS 3.2.2. Since the memory is freed when I quit the app I wonder wether there is a way to manually unload and reload the CoreGraphics framework? Would that help? Yes, I'm desperate.</body>
  </mail>
  <mail>
    <header>CGPDF memory catastrophe</header>
    <body>Hello, I monitor my memory with the method below. Every time I open, render and and close a pdf document I lose between 2 and 10 mb. At the beginning I start with &amp;gt; 110 mb free mem and after I looked at a few pdf the system kills me because there's too little memory left. The Allocations and Leaks instruments show me that I use way below 20 MB all the time and that I produce no leaks. What really annoys me is that Apple folks refuse to accept this problem and claim it to be a feature because there's a cache that speeds up drawing and that has to be released now and then by calling CGPDFDocumentRelease(). Even if it would work that way it would be a bug because a cache that builds up until the app crashes is not working right and in fact it doesn't work at all, because CGPDFDocumentRelease does not free all the memory. I'm so frustrated, it's hard to describe. I have a very annoyed customer sitting in my neck, the deadline is today and I have no idea what to do. Yuck! Sebastian Mecklenburg Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Core Animation layer not visible until animation is complete</header>
    <body>Thanks Eric! I was not disabling the transform animation as that was the whole point of the exercise. It seems that you have to set the zPosition if you are using 3d transforms and I have it working now. Thanks to all who replied. Luke.</body>
  </mail>
  <mail>
    <header>Re: Core Animation layer not visible until animation is complete</header>
    <body>&amp;gt; I had assumed that adding a layer above another with addSublayer or insertSublayer would implicitly be setting the z position higher. Is that not the case? It's been quite a while since I ran the experiment (and that experiment should be easy to recreate), but my recollection was that all layers get a &amp;quot;0&amp;quot; z position regardless of their position in the sublayer list unless you change it explicitly.  To put it another way, the operating system itself doesn't really use the &amp;quot;z&amp;quot; position, but your application can do so. Scott</body>
  </mail>
  <mail>
    <header>Re: Core Animation layer not visible until animation is complete</header>
    <body>Thanks Eric, I think 'z fighting' might be involved, though commenting out the perspective line doesn't actually make a difference. If I set the zPostion on my top layer high to 100 then the animation occurs on top but with the perspective on, it is noticably smaller than the layer below. I had assumed that adding a layer above another with addSublayer or insertSublayer would implicitly be setting the z position higher. Is that not the case? If I set the zPostion of my top layer zPosition to 10, the animation 'pokes through' the 'R' layer. I think I will end up using a UIKit element to work around the problem but it would be good to understand what is going on here. I have put the code at Thanks again, Luke</body>
  </mail>
  <mail>
    <header>Re: Core Animation layer not visible until animation is complete</header>
    <body>This is somewhat speculation, but since you &amp;quot;enabled&amp;quot; 3D perspective, you might be experiencing z-fighting. A quick way to fix this is to set the zPosition of the layer you always want drawn on top to be slightly closer than the layer you want drawn behind. -Eric -- Beginning iPhone Games Development</body>
  </mail>
  <mail>
    <header>Re: Preventing CALayer Sublayers From Getting Scaled</header>
    <body>Thanks for the additional suggestion. Alright, so I implemented some approaches here, starting with this last one 4). It seemed to work fairly well at first, except for one problem. Since these controls are in a separate layer from the workspace, I need to add this controls layer after I add the workspace layer so that it has a larger z position which allows me to access the controls within it (alternatively I could just set the controls layer's z position larger than the workspace's z position). The controls layer itself I set to width and height zero and place off-screen (actually at 0,0). And I make sure that I set the property masksToBounds = NO. Now, the real problem is that when I move my layers over one another, and I hover over part of a layer covered by another layer, the controls for the covered layer show up on top of (through!) the layer above the covered layer! I could remedy this by forcing hovered layers to become frontmost. However, for our application, we don't want to force hovered layers to become frontmost since that would be too distracting for the user, as we have a lot of layers, and moving the mouse cursor through the workspace would result in quite a few layers flashing to frontmost position from non-frontmost position, which seems to be not a good user experience. Instead we require the user to select (click) on a layer to make it frontmost. It looks like these two approaches 2) and 4) are fraught with problems related to z positioning -- but only if the application can't have layers becoming frontmost on hover. It's too bad, because it was soooo close. However, this process helped me to much better understand how the -layoutSublayers and -setNeedsLayout work as well as what might be required for a custom constraint manager implementation (limited by the same z position issue). So unless someone has some bright ideas on how to deal with the z position issue, I'm thinking I'll have to implement the strategy that applies the accumulated inverse scale transform of all superlayers of a control layer to each control layer in an item layers' -layoutSublayers method instead. db</body>
  </mail>
  <mail>
    <header>Re: Getting resolution data out of a PDFPage</header>
    <body>Review and consider the following for your editable document format and then add an export to PDF capability. -Shawn</body>
  </mail>
  <mail>
    <header>Re: UIGestureRecognizer question</header>
    <body>As a CALayer doesn't get events, I would imagine not, but this would probably be a better question for Cocoa-Dev or the Cocoa Touch section of the developer forums (devforums.appple.com). -- David Duncan</body>
  </mail>
  <mail>
    <header>UIGestureRecognizer question</header>
    <body>Are UIGestureRecognizers allowed to be attached to individual CALayers within a UIView layer hierarchy?</body>
  </mail>
  <mail>
    <header>Re: Preventing CALayer Sublayers From Getting Scaled</header>
    <body>I thought of another: 4) As per 2, but simply store references to your control layers in your content layer, then set the position of those layers in your content layer's -layoutSublayers. No constraint manager required! (There's no reason that I know of why -layoutSublayers, or indeed layoutSublayersOfLayer:, can't have side effects such as changing properties of layers that are not sublayers, though it might cause an extra invalidation.) H</body>
  </mail>
  <mail>
    <header>Implementing Custom CAConstraintLayoutManager</header>
    <body>I'd like to get some input on implementing a custom layout manager for CALayers that breaks the CAConstraintLayoutManager requirement of two layers having to be in a strict sibling or parent-child relationship, and instead allow constraints to be applied between any two layers in the layer hierarchy (siblings, parent-child, ancestor-descendant, or common ancestor). Some of the methods like layoutSublayersOfLayer: would appear not particularly useful in this scenario. Maybe to be replaced with methods like layoutConstrainedLayersOfLayer: or similar. Any thoughts, tips, hints, musings on such an implementation would be most welcome. Incidentally, the underlying motivation for this is to be able to layout custom control layers (CA buttons, sliders, handles, etc.) associated with a layer but that are not sublayers of the layer -- due to wishing to avoid transforms being applied to control layers when parent layers are transformed in some way. There are other ways of realizing this, but I'd like to focus on doing it with a custom layout manager if possible. Best, Dalmazio</body>
  </mail>
  <mail>
    <header>Re: Preventing CALayer Sublayers From Getting Scaled</header>
    <body>Thanks Hamish for the thoughtful ideas. I'm going to have a closer look at 1) first as this seems to be the path of least resistance as you say. I'm quite curious about 2) though, so I'll post a separate query about implementing custom layout managers to get some feedback on a possible custom layout manager implementation. Unfortunately, 3) is not an option for our application since we require size changes to be implemented as scale transforms rather than bounds manipulation due to the prevalence of CATiledLayers. Best, Dalmazio</body>
  </mail>
  <mail>
    <header>Re: Preventing CALayer Sublayers From Getting Scaled</header>
    <body>Sorry, yes, I hadn't understood your original description properly. In that case, I think you have three main options: 1) Using your original layer tree, and preventing your controls from being scaled by setting an inverse transform. You don't have to override -drawLayer:inContext: for this, but you do need to override -setLayout for your control layer to examine all of its ancestor layers to determine its effective transform, and set its own transform to have the inverse scale. 2) Using the &amp;quot;common ancestor&amp;quot; layer tree (separate zoomable content and controls), and having each zoomable layer communicate changes in its position or size with the controls layer. The cleanest way to do this might be to write a new CALayoutManager which handles relationships between layers that are neither parents nor siblings. 3) Changing the size of the workspace when you zoom, rather than transforming it. Each individually resizable element would have a content layer tree and a controls layer tree; the controls would resize the parent layer, but the content layer could be transformed rather than resized if that fits better with the existing content tree. I'd say that 1 is probably the path of least resistance at this stage, 2 is probably the most efficient, and 3 is the cleanest. Best wishes, Hamish</body>
  </mail>
  <mail>
    <header>Re: Preventing CALayer Sublayers From Getting Scaled</header>
    <body>Correction: please replace all occurrences of CALayoutManager with CAConstraintLayoutManager in my last post.</body>
  </mail>
  <mail>
    <header>Re: Preventing CALayer Sublayers From Getting Scaled</header>
    <body>Currently, we are laying out controls in the layer hierarchy using constraints that are relative to their superlayer -- which are CALayer subclasses each holding specialized content. We create controls in the init method of these custom CALayer subclasses and then add these control layers via addSublayer:. So the constraints + layout manager fully specifies which layer the constraints for these controls are relative to. The problem with implementing as you suggest, is we can no longer use the standard CALayoutManager. Unless I'm misreading, CALayoutManager requires the CAConstraint's between two layers to be such that either 1) the layers are siblings, or 2) they are in a parent-child relationship. From the CALayoutManager documentation: And the suggested implementation would put the two layers outside of a strict sibling, parent-child, and even ancestor-descendant relationship. They would have a common ancestor but not more. Ideas? (Apart from re-implementing CALayoutManager)</body>
  </mail>
  <mail>
    <header>Re: Preventing CALayer Sublayers From Getting Scaled</header>
    <body>If you simply insert a new root layer as the parent of your existing layer tree, a sibling of the &amp;quot;controls&amp;quot; layer tree), there should be no reason to special-case anything relating to your responder chain. No: simply layout the controls in the new root layer just as you were previously doing in the root of your &amp;quot;primary&amp;quot; (now &amp;quot;zoomable content&amp;quot;) layer heirarchy. H</body>
  </mail>
  <mail>
    <header>Re: Preventing CALayer Sublayers From Getting Scaled</header>
    <body>Ahh, it's a bit problematic for us to do it this way because we've got a bunch of spiffy core animation controls that we've implemented using a light-weight core animation responder chain (mirroring the NSView responder chain) that requires them to be in the same layer hierarchy to correctly generate mouse entered/exited events in order to respond well to a variety of user interactions. Hmm. Actually, we use a next responder property (defined in a custom subclass of CALayer) to traverse the hierarchy to respond to events. So I suppose we could move the controls out of the primary layer hierarchy as you suggest, and into a separate sibling 'controls' layer hierarchy, but keep the next responder property pointing to the logical parent layer inside the primary layer hierarchy and event handling should not be affected. I think this could work. One question though: are you suggesting then to implement our own constraint manager that doesn't involve having layers as sublayers in order to manage the position of these control layers in a separate hierarchy? Best, Dalmazio</body>
  </mail>
  <mail>
    <header>Re: Preventing CALayer Sublayers From Getting Scaled</header>
    <body>Why not add your layer-hosted view to another view, and add your controls to that view (as siblings of the layer-hosted view)? That way they don't need to be zoomed with the layer-hosted view, and can also handle events as individual responders in the chain. Best wishes, Hamish</body>
  </mail>
  <mail>
    <header>Preventing CALayer Sublayers From Getting Scaled</header>
    <body>I have some CALayers in a layer hosted view. Each of these layers has additional layers to hold (1) a close button, and (2) a resize handle. These additional 'control' layers are added as sublayers to the parent layer. Currently, when I zoom the workspace in which these layers reside, everything scales -- including these control layers. However, I would like to prevent these control layers from scaling. Is there a way to override the behaviour of having scale transforms being applied to all sublayers, without having to override the drawInContext: method for each control layer (presumably to invert any existing scale transform -- assuming this is possible) and without having to manually send each of these control layers a setNeedsDisplay: with every zoom? I found this thread which discusses doing something similar but the discussion goes in the direction of CATiledLayers which is not what I'm looking for here. Best, Dalmazio</body>
  </mail>
  <mail>
    <header>Re: Resizing CATiledLayer's Using Scale Transforms vs. Bounds	Manipulation</header>
    <body>Hamish, thanks for the confirmation.</body>
  </mail>
  <mail>
    <header>Re: Resizing CATiledLayer's Using Scale Transforms vs. Bounds	Manipulation</header>
    <body>Please excuse the brevity -- sent from my phone In my experience, the latter. (I have also tried the former.) Hamish</body>
  </mail>
  <mail>
    <header>Resizing CATiledLayer's Using Scale Transforms vs. Bounds	Manipulation</header>
    <body>I've got my layer hosted workspace working so that using CATiledLayers for hundreds of images works nicely when the workspace is zoomed out substantially. All the images use lower resolution representations, and my application is much more responsive when panning and zooming large numbers of images. However, within my application I also provide the user the ability to resize layers with a resize handle. Before I converted image layers to use CATiledLayers I was doing layer resizes by manipulating the bounds of the image layer according to the resize delta (mouse drag), and it worked well. But now with CATiledLayers in place, this is causing CATiledLayers to get confused when I mix resizing of layers through bounds manipulation and zooming/unzooming the workspace through scale transforms. Specifically, if I resize a CATiledLayer to half the width/height size (1/4 the area), the image inside it will suddenly scale to a further 1/2 the resized frame leaving 3/4 of the frame empty. This seems to be exactly when the inner CATiledLayer logic gets invoked to provide a lower resolution image representation. It works fine if I don't touch the resize handler and just zoom/unzoom the workspace. Is there a way to make zooming/resizing play nice together with CATiledLayers, or am I going to have to convert my layer resize logic to use scale transforms instead of bounds manipulations? Best, Dalmazio</body>
  </mail>
  <mail>
    <header>Re: Creating a CIContext from the current NSGraphicsContext's graphicsPort causes console spew.</header>
    <body>It's whatever is available in my view's drawRect: method. &amp;nbsp;Here's the whole method: &amp;nbsp;&amp;nbsp;&amp;nbsp;NSMutableDictionary *contextOptions = [NSMutableDictionary dictionaryWithObjectsAndKeys: &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;(id)cs, kCIContextWorkingColorSpace, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[NSNumber numberWithBool:YES], kCIContextUseSoftwareRenderer, If I comment out the kCIContextUseSoftwareRenderer line, then the problem goes away- but I need to use the software renderer in this instance.</body>
  </mail>
  <mail>
    <header>CAShapeLayers and CATextLayers Performance</header>
    <body>My basic question is what is the theoretical layer limit of the iPad? I ask because I&amp;#8217;m having performance issues with my app and it appears to be from too many layers. Now what I don&amp;#8217;t know is if I have truly reached the limit of how many layers I can have or if I&amp;#8217;m just doing things wrong. Here&amp;#8217;s a screenshot of what I have: &lt;img width=384 height=483 id="Picture_x0020_0" src="/attachments/pngmRQcebDoCJ.png" alt="dashboard_screenshot.png"&gt; Each pie graph is a UIView subclass. The items on the graph are made up of various layers attached to the pie graph&amp;#8217;s UIView.layer. Here is the hierarchy: UIView.layer &amp;#8211;shouldRasterize is set to yes as this improves scrolling performance CATextLayer (the title of the graph) CALayer CALayer &amp;#8211; I use this layer to scale its sublayers using sublayersTransform CAShapeLayer &amp;#8211; there is one of these for each slice with a fill color for that particular slice CALayer &amp;#8211; this layer has a custom drawing method (using drawLayer:inContext:) to draw the radial gradient around the edge of the pie CALayer &amp;#8211; all the label text layers and dots go on this layer CAShapeLayer &amp;#8211; this is the dot next to each label (it&amp;#8217;s kind of hard to see in the screenshot) CATextLayer &amp;#8211; this is the actual label for each slice. It has a shadow but I am using a rectangular shadowPath. CAShapeLayer &amp;#8211; this layer provides the translucent circle in the center of the pie CATextLayer &amp;#8211; the total label that is in the center of the pie All in all this can result in a lot of layers per pie. Things become even more difficult because we can have (at max) 24 of these pies. The user can scroll the dashboard left and right kind of like the home screen, with 6 pies per page. The biggest problem comes either when the whole thing is rotated, or when we transition to a different view. The animations for those are choppy and are definitely not performing well. I&amp;#8217;ve run this with instruments and have verified that everything is pixel-aligned (so not antialiasing) and very few things are being blended (the text labels and the radial gradient layer) are the only things. I feel like the iPad should be able to handle this number of layers, and that I&amp;#8217;m just doing something wrong to cause these performance issues. &lt;span style='font-family:"Palatino Linotype","serif";color:#5A5A5A'&gt;Ryan Bunker&lt;span style='font-size:10.0pt;font-family:"Palatino Linotype","serif";color:#5A5A5A'&gt; Software Developer color:#E36C0A'&gt;Come see us this summer at NBTA in Houston August 8 - 11! Both color:#5A5A5A'&gt; 700 Central Expressway, South, Suite 230 color:#5A5A5A'&gt; color:#5A5A5A'&gt;Phone: 972-612-7121 Fax: 972-612-7021 &lt;span style='font-size:7.5pt;color:#5A5A5A'&gt;Confidentiality Notice:&lt;span style='font-size:7.5pt;color:#5A5A5A'&gt; This e-mail contains information that is confidential. It is intended for the exclusive use of the individual or entity to whom it is addressed. If you are not the named recipient, disclosure or distribution of the information transmitted herewith is strictly prohibited and may be subject to legal restriction or sanction. Please notify the sender, by return e-mail or telephone, of any unintended recipients and delete the original message without making any copies.</body>
  </mail>
  <mail>
    <header>Re: Creating a CIContext from the current NSGraphicsContext's	graphicsPort causes console spew.</header>
    <body>It's whatever is available in my view's drawRect: method.  Here's the whole method: NSMutableDictionary *contextOptions = [NSMutableDictionary dictionaryWithObjectsAndKeys: (id)cs, kCIContextWorkingColorSpace, [NSNumber numberWithBool:YES], kCIContextUseSoftwareRenderer, If I comment out the kCIContextUseSoftwareRenderer line, then the problem goes away- but I need to use the software renderer in this instance. -gus -- August 'Gus' Mueller Flying Meat Inc.</body>
  </mail>
  <mail>
    <header>Re: Getting resolution data out of a PDFPage</header>
    <body>I don't know, it depends on your app and your user base. But I imagine that if your users are the kind that regularly deal with PDF files, a large portion of them might too be taken aback by using it as an intermediate, rather than a final, on-disk representation. In this day and age, perhaps a simple directory bundle containing multiple images would be the best idea. Maybe include an index of contents of some sort. If what they want is to be able to produce a read-only PDF from your app, then you'll save a lot of your sanity by implementing PDF support that way. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: Getting resolution data out of a PDFPage</header>
    <body>Yes (and it used QuickDraw, and much older technology to do it...) Also, most of the programs that we interface with want a PDF file at the end... What data file format would you expect to use for this? a multi-page TIFF file?  A separate TIFF or JPEG file for each page? some other format I haven't thought of? We DO work with both of those other formats, but PDF seems to be the one that people want. Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: Getting resolution data out of a PDFPage</header>
    <body>That seems quite bizarre. PDF was never intended to be an editable picture-containing file format. Is this established user behavior for your app (that is, is there a previous version of your app that uses PDF as its on-disk representation)? I could see offering a workflow in which Export to PDF was the final step. That's a fairly common scenario, and it's well-supported in the OS X print architecture. But attempting to round-trip through PDF seems like a bad idea. Even Adobe Illustrator doesn't try to do that anymore. Yeah, down that way madness lies. :) --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: Getting resolution data out of a PDFPage</header>
    <body>I'm not using it as an image container. I'm using it as a saved document type. Internally, I have an NSMutableArray of objects that have a CGImageRef and a resolution. But when I save the document to a file, most people expect that file to be a PDF... The PDF only exists on disk. I did go through an intermediate stage where I WAS using a PDFDocument as the internal image container, but as you said, I realized that that was a bad idea, and I stopped doing it... Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: Getting resolution data out of a PDFPage</header>
    <body>Hmm. Could you walk the PDF document using CGPDFDictionary as described in this thread: That said, what you're doing‚Äîusing PDF as an image container‚Äîis highly unorthodox and very much conflicts with the original PDF design goals. PDF is a page description language; it evolved from PostScript as a way of specifying instructions to follow in order to create a page. It's not really an arbitrary read-write container format. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: Getting resolution data out of a PDFPage</header>
    <body>Correct. I want the pixel size of the image. Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: constrain cursor inside a circle</header>
    <body>The point (x,y) is in a circle of radius r centered at the origin iff x^2 + y^2 &amp;lt; r^2.</body>
  </mail>
  <mail>
    <header>constrain cursor inside a circle</header>
    <body>Hi there, I am stuck on a simple problem: I have a rectangle as follows: I build a mutable path of the circle inscribed in the above rectangle: So the center of the circle will be at 0,0. The movement of the cursor have already been scaled to fit the -1 to 1 range. So if I were to constrain the cursor to stay inside the rectangle I would write something like: Now, what I want to do is to constrain the cursor movements to stay inside the circle. I am approaching this problem similarly to the rectangle case but for the circle situation things are more complex. I would need to calculate sines and cosines with possibly a square root to get the X and Y coordinates of the circle to use as constrains. I have a feeling that there is a much simpler and elegant way to solve this problem. Does anybody have suggestions ? Thanks for your time. - Luigi ------------------------------------------------------------ THIS E-MAIL MESSAGE IS FOR THE SOLE USE OF THE INTENDED RECIPIENT AND MAY CONTAIN CONFIDENTIAL AND/OR PRIVILEGED INFORMATION.  ANY UNAUTHORIZED REVIEW, USE, DISCLOSURE OR DISTRIBUTION IS PROHIBITED.  IF YOU ARE NOT THE INTENDED RECIPIENT, CONTACT THE SENDER BY E-MAIL AT email@hidden AND DESTROY ALL COPIES OF THE ORIGINAL MESSAGE. WITHOUT PREJUDICE UCC1-207. ------------------------------------------------------------ ____________________________________________________________________________________ Expecting? Get great news right away with email Auto-Check. Try the Yahoo! Mail Beta.</body>
  </mail>
  <mail>
    <header>Re: Quartz vs. Illustrator PDF file sizes</header>
    <body>The &amp;quot;Adobe way&amp;quot; is to use Acrobat and some helper programs it comes with, to really fine tune your PDF output and workflows. My usage of that stuff has been more involved in the printed media arena, so I can't say with any certainty that it can do what you need. But it may be worth digging into, since they are the PDF kings. Have fun, -Rick</body>
  </mail>
  <mail>
    <header>Re: Coverflow?</header>
    <body>Be warned, the sample is very bare bones and is more like 40% of the solution (from my brief look at it). Once again, every developer is being forced to write their own version of a control that Apple is promoting heavily. I recommend logging a feature request with Apple's BugReporter to add a Coverflow-like control to the system controls so they realize developers want this type of control. -- Mark Munz unmarked software</body>
  </mail>
  <mail>
    <header>Re: how to create an axial shading along a circular line ?</header>
    <body>On Jun 19, 2007, at 9:22 PM, Luigi Castelli wrote: You could use a wide line if you like.  I was afraid that a simple line would leave small gaps in the shape.  A wedge fills a circle a lot better so I thought that would be the better choice.  If you can get with you like from a line, though... go ahead. :-)</body>
  </mail>
  <mail>
    <header>Coverflow?</header>
    <body>‚ÄúDid somebody mention Piscacadawadaquoddymoggin Parkway?‚Äù</body>
  </mail>
  <mail>
    <header>Re: how to create an axial shading along a circular line ?</header>
    <body>Thank you Scott. Could I use a simple line starting from the center of the circle going outward and spanning the radius? I am thinking about a line because it provides better color resolution compared to a wedge. Is there a particular reason why you suggested a wedge instead ? - Luigi ------------------------------------------------------------ THIS E-MAIL MESSAGE IS FOR THE SOLE USE OF THE INTENDED RECIPIENT AND MAY CONTAIN CONFIDENTIAL AND/OR PRIVILEGED INFORMATION.  ANY UNAUTHORIZED REVIEW, USE, DISCLOSURE OR DISTRIBUTION IS PROHIBITED.  IF YOU ARE NOT THE INTENDED RECIPIENT, CONTACT THE SENDER BY E-MAIL AT email@hidden AND DESTROY ALL COPIES OF THE ORIGINAL MESSAGE. WITHOUT PREJUDICE UCC1-207. ------------------------------------------------------------ ____________________________________________________________________________________ Yahoo! oneSearch: Finally, mobile search that gives answers, not web links.</body>
  </mail>
  <mail>
    <header>Missing dictionary key for [CIImage imageWithCVImageBuffer:options:]</header>
    <body>I want to use the [CIImage imageWithCVimageBuffer:options:] method to create a CIImage with float pixel format from a CVPixelBufferRef. According to the docs I'd have to supply a dictionary containing the kCIFormatRGBAf pixel format (as a NSNumber). Unfortunately the docs don't mention the key for this value. This problem has already been discussed on the cocoa-dev mailing list: This thread proposes to use CIFormat as the key but this can't work as CIFormat is a typedef to an int. I already filed a documentation request but I would like to know the proper key before the next documentation update. Does anybody know the correct key? Thanks, Christian</body>
  </mail>
  <mail>
    <header>Re: Known Issues (was: Can I save a CIImage as a TIF with more than 8 bits per component?)</header>
    <body>On Jun 18, 2007, at 3:18 AM, Paul Sargent wrote: Externally reported bugs are private between you the submitter and Apple. Internally reported bugs are Apple private. -Shawn</body>
  </mail>
  <mail>
    <header>Re: Known Issues (was: Can I save a CIImage as a TIF with more than	8 bits per component?)</header>
    <body>I'm just wondering, and this may be a naive notion, is there anywhere that information on known bugs is collated to stop people falling down the same holes? I ask because I could have seen myself falling down this one had I not read this. ...or is what I'm looking for called Radar and therefore only available to Apple employees?</body>
  </mail>
  <mail>
    <header>Re: not understanding CVTimeStamp values sent to display link	callback...</header>
    <body>Here is a guess (since i think the pros are tied up at WWDC and can correct me when the get back). The spec says that it is indexed at the given time. So this is not movie time. I think when the movie is read it sets up a map or dictionary of frames whose indexes are offset by the given time. It also states you can request ahead but not below. So its probably a stack. When you scrub the current index gets the frame scrubbed to. So that will be the top frame on the stack. Whose index is the current system time. All that NULL does is grab the current frame that is on the top of the stack. On a fast machine that may be OK. I wonder, however, if what happens if you keep it NULL and frames need to be skipped. At that point it may need to look further ahead.</body>
  </mail>
  <mail>
    <header>Re: not understanding CVTimeStamp values sent to display link	callback...</header>
    <body>in fact, i can even pass NULL in for the time stamp and it STILL plays back fine: ... so it begs the question... what's the point of sending this thing a time stamp at all? i need to understand this because ultimately i need to sync audio and video that are being streamed to me. note that the movie i'm testing with is just an mpeg file, not separate audio/video streams. i want to understand this first with simple movie files before i roll it into the main project that accepts the two separate a/v streams. - chase</body>
  </mail>
  <mail>
    <header>Re: not understanding CVTimeStamp values sent to display link	callback...</header>
    <body>the movie rewinds fine.  movie playback looks great and i can jump/scrub wherever i want in the timeline and it works great. the problem is that i don't understand how it's able to jump around like that since the timestanps that i'm sending to QTVisualContextCopyImageForTime() completely IGNORE all that jumping around and scrubbing back and forth.  i just don't understand how QTVisualContextCopyImageForTime() could possibly know what the correct next frame is when i call it with a timestamp that's so completely oblivious to actual playhead position. - chase</body>
  </mail>
  <mail>
    <header>Re: not understanding CVTimeStamp values sent to display link	callback...</header>
    <body>Is your video actually rewinding?? If not try calling MovieTask after you tell the movie to rewind. I haven't tried that yet. I am trying to solve the how to have pass the mouse events to a movie when using CoreVideo to draw.</body>
  </mail>
  <mail>
    <header>Re: not understanding CVTimeStamp values sent to display link	callback...</header>
    <body>WAIT A MINUTE!!! is this just hostTime converted to the movie's time scale? if so, then how could QTVisualContextCopyImageForTime() possibly be able to send back the right frame if we start scrubbing/jumping forward and backward in the movie's timeline? it seems more logical to me to send QTVisualContextCopyImageForTime() a time stamp that really represents where we are in the timeline. i mean... obviously this is not the case, because all of apple's demo apps work just fine and they all just send QTVisualContextCopyImageForTime() the time stamp that always advances regardless of jumping around in the timeline, so obviously i'm missing something here.... so what is it? thanks. - chase</body>
  </mail>
  <mail>
    <header>Re: not understanding CVTimeStamp values sent to display link	callback...</header>
    <body>i'm not sure that i asked this as clearly as i could have.... what i'm asking is... when i rewind to the beginning of the movie again and start playing from there, why doesn't inOutputTime-&amp;gt;videoTime go back to 0 again? why does videoTime just keep advancing regardless of the actual current position in the movie timeline? i guess i'm just not sure what this number actually represents if not where i am in the movie. - chase</body>
  </mail>
  <mail>
    <header>not understanding CVTimeStamp values sent to display link	callback...</header>
    <body>in my display link callback, if i print out the value of videoTime, the numbers continue to advance even though i have rewound to the begining again: can someone explain why this number doesn't represent where i am along the timeline of the movie that's being played? thanks. - chase</body>
  </mail>
  <mail>
    <header>Re: does movie fps affect display link callback frequency?</header>
    <body>As the name says it is linked to the refresh rate of the display nothing else.</body>
  </mail>
  <mail>
    <header>Re: relationship between quicktime and core video</header>
    <body>Well, not to be glib, but how you'd rather think of it is not necessarily how it was designed.  QuickTime is not &amp;quot;just a decoder&amp;quot; - the codec is the decoder (which seems obvious, but I think it's a key to your misunderstanding). Check out this paragraph in the reference at CoreVideoRef/index.html#//apple_ref/doc/uid/TP40001537&amp;gt;: &amp;quot;The main purpose of the CoreVideo display link to provide a separate high-priority thread to notify your application when a given display will need each frame. How often a frame is requested is based on the refresh rate of the display device currently associated with the display It's QuickTime's job to play time-based media accurately - to decode video and audio frames, move sprites, send MIDI notes, whatever is necessary for a particular point on the timeline.  Core Video, on the other hand, asks you for a new frame when the specified display needs that frame, based on that display's refresh rate. In other words, QuickTime _pushes_ data from a movie based on the current time, while Core Video _pulls_ frames (via callbacks) when it needs one.  Imagine a junky display that can only display ten frames per second, even though QuickTime can draw 60fps or maybe more.  If QuickTime just pushes all that data to the screen, you see tearing and other problems because you're changing data during refresh.  Core Video is the part that only calls your callback ten times per second, during refresh, when it actually _needs_ the frame to display during refresh so it won't tear. Now, Core Video lets you provide your own frames, so as I understand it, there's nothing that's stopping you from going down a level and asking a codec for only the frames you want - but since most of them use differential compression, they may need to back up to the last key frame and process a few frames to get where you want anyway.  But that doesn't get you time-based frames - if you go that way, you'll have to figure out on your own which frame goes for which timestamp.  With QuickTime running the movie, it does all that for you. That's how I understand it, at least. I think that the movie playback is actually decoding frames *ahead* of the current time so that when Core Video asks for them, they'll be ready.  That's what the skip-ahead constant specifies.  On most systems, in most cases, you're going to be able to display every frame that's available in a non-streaming movie, so you'd have to decode them all anyway.  If you somehow get bogged down and drop frames, Core Video will start passing timestamps that reflect it and dropping frames as necessary, so you're not losing anything. I think this would be more appropriate on the QuickTime-API list than the Quartz-Dev list, but that's where I see your conceptual problem. QuickTime pushes time-based media to buffers; Core Video asks for data from those buffers to synchronize the presentation based on actual display refresh rates, user interaction, and other hard-to-predict factors. I'm willing to be wrong. --Matt -- &amp;quot;Nothing fires up a crowd like Burl Ives.&amp;quot;  -- Stephen Colbert, 2007.05.23</body>
  </mail>
  <mail>
    <header>relationship between quicktime and core video</header>
    <body>i'm trying to get my head around the relationship between quicktime and core video. i don't understand why a QTMovie instance has to be running to play it (as in... [qtMovie play]). why can't i just call QTVisualContextIsNewImageAvailable() with the appropriate time stamp given to me as a parameter in the display link callback? to me, it seems like the QTMovie shouldn not have to be running to do this.  seems like i'm doubleing the load on the cpu/gpu by playing the QTMovie **AND** running all the actual decode/render code in the display link callback. i'd rather just think of quicktime as the decoder.  i get called by the display link and then i tell quicktime: &amp;quot;give me the frame a time t&amp;quot;. someone please help me to understand the relationship between these two. thanks. - chase</body>
  </mail>
  <mail>
    <header>Re: does movie fps affect display link callback frequency?</header>
    <body>This makes sense given the design of display link. The idea, as I understand it, is that a display link callback will be invoked only during the vertical blank interrupt period. Which is dependent on refresh rate, not the content being displayed. -- -Corey O'Connor</body>
  </mail>
  <mail>
    <header>Re: does movie fps affect display link callback frequency?</header>
    <body>just did a little test (rudimentary, but good enough to answer this question i think).  fps doesn't appear to affect the frequency at which the display link callback is called. i ran once with a 24 fps movie and once with a 12 fps movie and printed a 'y' every time QTVisualContextIsNewImageAvailable() returned true and 'n' every time it returned false: @ 24 fps: .y.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y.n.y @ 12 fps: .y.y.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n.n.n.y.n.n the increase in false returns from QTVisualContextIsNewImageAvailable() at 1/2 the fps tells me that the callback frequency remained constant (regardless of fps). Could have actually timed the callback, but this method seemed easier to me and answered the question sufficiently (for me at least). - chase</body>
  </mail>
  <mail>
    <header>does movie fps affect display link callback frequency?</header>
    <body>does movie fps affect display link callback frequency? - chase</body>
  </mail>
  <mail>
    <header>Re: using core image transitions for VIDEO transitions</header>
    <body>sorry.  that was **not** the question i meant to ask.  i'm assuming that we don't need to fire a timer because we've got the display link callback firing periodically. but....  since we're talking about two movies (movie A and movie B).... do we have to setup two display link callbacks (one for each movie)? i'm trying to figure out how the display link could know the &amp;quot;perfect&amp;quot; time time to call the callback (for a shared callback) is -- for example -- movie A and movie B we running at totally different fps. could someone please clear all this up for me.  thanks. - chase</body>
  </mail>
  <mail>
    <header>using cimage transitions fro VIDEO transitions</header>
    <body>the core image documentation is very very clear about how to using core image transition filters when transitioning from one still image to another and i've got that working fine, but it doesn't explain exactly how to use those same transitions between two videos. does anyone know how this is done or know where i can find some sample code showing how to do this. my biggest question is.... to transition from still image to still image, there has to be a timer that fires every so often that tells the app to advance the transition to the next step... but is this necessary for video transitions since we already have the diplay link callback being called periodically (like a timer callback)? - chase</body>
  </mail>
  <mail>
    <header>Re: how to create an axial shading along a circular line ?</header>
    <body>On Jun 10, 2007, at 6:37 PM, Luigi Castelli wrote: What you are trying to do is not possible with a CGShading.  More likely than not, what you're going to have to do is create a triangular wedge, draw it at one color, rotate it slightly, draw it with a slightly darker color, etc.... around the circle.</body>
  </mail>
  <mail>
    <header>CI Transitions of resized images of different sizes</header>
    <body>so, i've recently started to convert my app from NSImage to CoreImage, and have been pretty pleased so far, even witout converting my NSView to an NSOpenGLView. [cimage drawInRect: scaledRect fromRect: imageOrigRect now, the problem arises when I try to transition between two images in my program. 1. when I set up the Dissolve transition, the two images are drawn at their full size, not their &amp;quot;resized&amp;quot; size. 2. the images are both drawn with the same lower-left corner, even if they're of quite different sizes. i'm suspecting that i need to use some sort of transforms or stuff here, but am not 100% sure which. a. how can i scale imags during a transition, or am i better off pre-scaling these for all displays? b. how can i maintain image positions in a transition, even if they're of different sizes, etc? thanks, marc.</body>
  </mail>
  <mail>
    <header>Using ImageIO to parse Photoshop Layers?</header>
    <body>Can ImageIO parse Phtoshop layers? Or are quicktime importers they only way to parse PS layers? Scott Andrew</body>
  </mail>
  <mail>
    <header>how to create an axial shading along a circular line ?</header>
    <body>Hi there, I am trying to figure how to create in Quartz an axial shading that produces a gradient along a circular line. So it is not the usual straight axial shading, but it's not a radial shading either. It is more like if an axial type of shading were to evolve along a cirular line. I attached a small picture to clarify the idea. Is the best strategy still to use one of the CGShadingCreateAxial or CGShadingCreateRadial functions ? If so, in what fashion ? Or should I be looking at a different technique altogether ? Any tips or advice is highly appreciated. Thank you. - Luigi ------------------------------------------------------------ THIS E-MAIL MESSAGE IS FOR THE SOLE USE OF THE INTENDED RECIPIENT AND MAY CONTAIN CONFIDENTIAL AND/OR PRIVILEGED INFORMATION.  ANY UNAUTHORIZED REVIEW, USE, DISCLOSURE OR DISTRIBUTION IS PROHIBITED.  IF YOU ARE NOT THE INTENDED RECIPIENT, CONTACT THE SENDER BY E-MAIL AT email@hidden AND DESTROY ALL COPIES OF THE ORIGINAL MESSAGE. WITHOUT PREJUDICE UCC1-207. ------------------------------------------------------------ ____________________________________________________________________________________ Food fight? Enjoy some healthy debate in the Yahoo! Answers Food &amp;amp; Drink Q&amp;amp;A.</body>
  </mail>
  <mail>
    <header>Re: How to tile part of an index color CGImage into a new	index	color CGImage</header>
    <body>This is the point where I really don't follow you :). If your using Quartz for masking, then the only thing that matters is the color information for that pixel, not the value representing the pixel. So indexed or not doesn't matter to Quartz. So I can only guess that your doing the masking yourself - but if your doing the masking yourself, then you don't have access to the data from the CGImage, so is that what you are trying to obtain? Unless you have a memory concern Quartz can expand the image trivially. But depending on how your doing your masking that might not be at issue... -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: How to tile part of an index color CGImage into a new	index	color CGImage</header>
    <body>How to hack it? We may have to do so. The destination image generated in the API will be drawn into a context. Under this situation, we should have no problem to have a non-index color CGImage. But sometimes, the destination image will be used as a mask image to mask another image. Under this situation, we have to guarantee to get a index-color CGImage. Or we will have a wrong masked image. For we use many index-color Windows bitmaps for the masking purpose, it may be impractical to change all the index color bitmaps to all non-index color bitmaps.</body>
  </mail>
  <mail>
    <header>Re: How to tile part of an index color CGImage into a new	index	color CGImage</header>
    <body>If your goal is to take an indexed image and replicate it multiply into an indexed target, that doesn't work. As you've already noted, Quartz doesn't have support for indexed bitmap contexts (you could hack it, but you'd have to be VERY careful with your drawing work -- it's not worth it). What is your ultimate goal? Without really understanding what your doing all I can really do is stab in the dark... -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: How to tile part of an index color CGImage into a new index	color CGImage</header>
    <body>......codes to load srcBitmap from index color Windows bitmap here ...... .... for (dsty = 0; dsty &amp;lt; dstHeight; dsty += TileHeight)    //TileHeight is an integer that passed in for (dstx = 0; dstx &amp;lt; dstWidth; dstx += TileWidth) //TileWidth is also an integer that passed in CopyBits((BitMap *) *(srcBitmap), (BitMap *) *dstBitmap, &amp;amp;SrcRect, So far, we has succeeded to load index color Windows bitmap into srcCGImage, also succeeded to get a index color sub image subSrcCGImage in SrcRect from srcCGImage, but we have no idea on how to do the tiling just like in the two &amp;quot;for&amp;quot; loops.  The difficulty here is when we draw the subSrcCGImage into any Quartz context, we can only get a non-index color image from the context after drawing.</body>
  </mail>
  <mail>
    <header>Re: How to tile part of an index color CGImage into a new index	color CGImage</header>
    <body>If I'm reading you right, you want to take a sub-rect of a CGImage and use it to create another CGImage? If you want to do that you can use CGImageCreateWithImageInRect() to get the subrect. It shares the data with the original image so you don't need to keep the original image around afterwards (and you won't consume additional memory). -- David Duncan Apple DTS Quartz and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Re: Core Image responsiveness</header>
    <body>In my own app (chocoflop) every render is done in a thread, but that doesn't fix the problem for me. If I open a big image and CoreImage is slow to process then the whole UI seems to slow down dramatically (sliders not responding and so). What I am talking about is the whole _operating_system slowing down. When my app is rendering a long calculation pretty much nothing responds smoothly until it's over. That's with little swapping and virtually no CPU usage (on a MBpro). Le 7 juin 07 √† 20:02, Jason Harris a √©crit : -------------------------------------------- Santiago (Jacques) Lema ID Mobile -------------------------------------------- Informations routi√®res sur mobile --------------------------------------------</body>
  </mail>
  <mail>
    <header>Re: How to tile part of an index color CGImage into a new index	color CGImage</header>
    <body>We also met the &amp;quot;magic pink&amp;quot; problem you mentioned. A few days ago, it is solved. Anyway, thank you so much for your kindly help. We have already had a index color CGImage in hand. Sometimes, it will be used as an mask image to mask a source image. Sometimes, we need to tile part of the CGImage into a new CGimage, which will also be used as an mask image. On Jun 8, 2007, at 11:55 PM, Jason Harris wrote:</body>
  </mail>
  <mail>
    <header>Re: How to tile part of an index color CGImage into a new index	color CGImage</header>
    <body>Are you trying to convert &amp;quot;magic pink&amp;quot; windows bitmaps to nicely masked images?  If so, I can contribute working code. On Jun 8, 2007, at 8:48 AM, superbady wrote:</body>
  </mail>
  <mail>
    <header>How to tile part of an index color CGImage into a new index color	CGImage</header>
    <body>We use many index color Windows bitmaps for masking. We create index color pixmaps from those Windows bitmap. And then manipulate those pixmaps for masking purpose. In our QuickDraw codes, we sometimes use copybits to tile part of the a index color pixmap into a new index color pixmap for masking. So far, we have succeeded to create index color CGImages from the index color windows bitmap. We found that whenever the index color CGImage is drawn into any context, the resulting CGImage will never be index color image anymore. Maybe it is because all the contexts (like BitmapContext, ....) do not support index color space. So how can we tile part of an index color CGImage into a new index color CGImage?  With QuickTime? With CIImage? Thank you in advance!</body>
  </mail>
  <mail>
    <header>[ANN] PreFab Event Taps Testbench 1.0</header>
    <body>I've always been fascinated by Apple technologies that let me control other applications on my Mac, like AppleScript. Having mastered the Accessibility API with PreFab UI Browser and PreFab UI Actions, I've now moved along to Quartz event taps, another Apple technology rooted in accessibility but having much broader potential. Event Taps Testbench 1.0, posted last night for download, is a FREE utility for software developers. Use it to explore what your applications can do with Quartz event taps. You'll find it at: Event taps are Core Graphics objects that you install in the system to monitor and respond to user input to any application from a keyboard, mouse, scroll wheel or tablet pointer. Using Event Taps Testbench, configure and install as many event taps as you like, and monitor the user input events that trigger them in real time. In addition, configure how the event taps respond to user input, both by performing the example actions provided and by modifying user input before the events are posted to their targets. This utility is based on an Objective-C framework I've written that wraps (and expands upon) the Quartz event taps C API. It enables a Cocoa application to respond to user input events from the keyboard, mouse, scroll wheel and tablet pointer using Cocoa delegate methods, such as Will Post Event, Should Post Event, May Post Modified Event For Event, and Did Post Event. In a month or two, we will make the framework available for licensing. Ditto our Objective-C Accessibility framework for writing Cocoa-based assistive applications. In a few months, the event taps framework will be incorporated into PreFab UI Actions to let you do some really amazing things with AppleScript. Say &amp;quot;Hi, Bill&amp;quot; if you see me at WWDC. If I look confused, remind me who you are. I might be wearing a Quechee t-shirt. -- Bill Cheeseman - email@hidden Quechee Software, Quechee, Vermont, USA www.quecheesoftware.com PreFab Software - www.prefabsoftware.com</body>
  </mail>
  <mail>
    <header>Re: Core image treating alpha=0 special?</header>
    <body>I think this must be right... I must have some premultiplied values slipping in someplace.</body>
  </mail>
  <mail>
    <header>Re: Core image treating alpha=0 special?</header>
    <body>Probably what's happening here is that the original RGBA pixel is getting its alpha premultiplied, which makes the RGB components (0 0 0).  You could try using float components, or using kCFNull as the colorspace. On Jun 7, 2007, at 2:01 PM, Mac Developer wrote: Attachment:</body>
  </mail>
  <mail>
    <header>Re: Need help with CGColorSpaceCreateCalibratedRGB</header>
    <body>On Jun 7, 2007, at 3:01 PM, Mark wrote: I believe your matrices should be transposed so that you multiply the color vector on the right.  Also, remember that they need to be in row-major order. Brendan Younger</body>
  </mail>
  <mail>
    <header>Core image treating alpha=0 special?</header>
    <body>I apply a CIColorMatrix to invert alpha using the following parameters for the vectors: inputRVector = [1 0 0 0] inputGVector = [0 1 0 0] inputBVector = [0 0 1 0] inputAVector = [0 0 0 -1] inputBiasVector = [0 0 0 1] However, all rgba values of (1 1 1 0) are converted to (0 0 0 1)! They should be (1 1 1 1). If I change the rgba values (1 1 1 0) to (1 1 1 epsilon) where epsilon is 0.0001, then I get expected behavior. Is this special treatment of alpha=0 intentional within CoreImage or is this a bug?</body>
  </mail>
  <mail>
    <header>Need help with CGColorSpaceCreateCalibratedRGB</header>
    <body>I seem to be too dumb to figure out how to correctly call CGColorSpaceCreateCalibratedRGB. I would really appreciate if someone could show me how to do it. For the demonstration, lets take the sRGB color space. Here's all data I have about it: 1) Primaries / Phosphors (ITU-R BT 709-5) R		G		B X	0,64		0,30		0,15 Y	0,33		0,60		0,06 Z	0,03		0,10		0,79 2) White Illuminant (D65) X	0,3127 Y	0,3290 Z	0,3583 3) XYZ to RGB Matrix X	3,2405		-1,5371		-0,4985 Y	-0,9693		1,8760		0,0416 Z	0,0556		-0,2040		1,0572 4) RGB to XYZ Matrix X	0,4125		0,3576		0,1804 Y	0,2127		0,7152		0,0722 Z	0,0193		0,1192		0,9503 5) Gamma 2,2 That would help me immensely. Cheers Mark</body>
  </mail>
  <mail>
    <header>Re: Blurring layer in iPhoneOS</header>
    <body>Bear with me as I try to beat this dead horse (for at least one or two more emails). 1.	Is there any way for a CALayer (or a OpenGL-based CALayer) to &amp;quot;know&amp;quot; what got rendered underneath it? The underneath could be 100's of CAShapeLayers etc. I really don't want to duplicate all 100 CAShapeLayers just to get a decent blur happening. 2.	I've heard in OpenGL there are things like fog etc. that could work? Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Re: Rendering a snapshot of a CATiledLayer</header>
    <body>Hi David, That works to make a copy of the layer; but when this newLayer is shown, it causes (the expensive) drawInContext: to be called again on my CATiledLayer subclass. Is this a bug? I just want to use the existing cached rendering. (The context passed to -drawInContext: is the same context, or at least, it's an NSCFType with the same address.) Thanks, Hamish</body>
  </mail>
  <mail>
    <header>Re: Rendering a snapshot of a CATiledLayer</header>
    <body>It isn't. If you just want to create another layer with the same contents, you can assign the contents of one layer to another layer and work with the new layer, just as simple as -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: Rendering a snapshot of a CATiledLayer</header>
    <body>Alternatively, is there any way I can use something like -[CALayer initWithLayer:] to make a copy of the layer I can manipulate without having to re-render it? The docs imply that we shouldn't ever call it ourselves, but it does sound like it could be exactly the right thing... Thanks, Hamish</body>
  </mail>
  <mail>
    <header>Re: Blurring layer in iPhoneOS</header>
    <body>Fundamentally, it&amp;#39;s as if you are compiling for a different OS (and it, in fact, is and has it&amp;#39;s roots in a system separate from OS X), with the developer app being a bit of a bridge.... so there are tons of unsupported methods as of now. (bummer) -George Toledo Hi All How do I make a CALayer (or equivalent technique) that blurs the layers/images it is on top of? In Mac OS X, I would set the CIFilters appropriately but we can&amp;#39;t do this in iPhoneOS. Is there some OpenGL magic incantation that will do this?</body>
  </mail>
  <mail>
    <header>Re: Blurring layer in iPhoneOS</header>
    <body>On Jul 30, 2009, at 10:08 PM, Glen Low wrote: Generally speaking, no.  You would have to write your own magic to do something like this. The graphics card on a desktop computer is much more powerful than that on an iPhone.  As a result services like Core Image are not found on the phone (yet).</body>
  </mail>
  <mail>
    <header>Re: Unpixellated CALayer under scaling</header>
    <body>Did you ever get an answer for this, or discover how to achieve it in your own subclass? Thanks, Hamish</body>
  </mail>
  <mail>
    <header>Blurring layer in iPhoneOS</header>
    <body>How do I make a CALayer (or equivalent technique) that blurs the layers/images it is on top of? In Mac OS X, I would set the CIFilters appropriately but we can't do this in iPhoneOS. Is there some OpenGL magic incantation that will do this? Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Rendering a snapshot of a CATiledLayer</header>
    <body>Hi, I have a UIScrollView with a content view whose CALayer contains several CATiledLayer sublayers that are expensive to render. I want to take a snapshot of the visible part of the content view so that I can animate it, but without re-rendering the tiled layers. However, the &amp;quot;contents&amp;quot; properties of those layers do not contain their renderings (as you might expect because those layers can get very big when zoomed in). Is there any way I can get at the already-rendered contents of those tiled layers? I only need the bit that's already showing! Thanks, Hamish</body>
  </mail>
  <mail>
    <header>Anyone had problems with multiple CATiledLayers crashing with	threads</header>
    <body>I've got a application that is making extensive use of CATiledLayer- derived layer-backed-views and have run into a problem recently when updating the contents of these. The scenario is that I have a main display window with a CATiledLayer layer-backed-view in it which has an observer set to a color well in an auxiliary window.  In the auxiliary window, I have another CATiledLayer backed-view (it's a subclass of the view used in the main window) that also observes the same color well. Under 10.5.7, clicking in the color panel (brought up by clicking on the well) will cause an immediate and effective change of both views that are observing the color.   I've put logging in to see how the views are drawing, and they're each being drawn to their own context (which is the same from invocation to invocation) and each in its own thread. Under 10.5.7, clicking and vigorously dragging in the color panel above updates both the views quickly during the drag, but will eventually (sometimes after a second, sometimes after 10-20 seconds of dragging) crash one of the two threads (usually the main window, as it takes longer to finish drawing) in a CGContext graphics call.   The stack traces always include ripc_ calls under the CGContextDrawPath (for example) call. It's pretty clear from the backtraces and running the code with CFZombies turned on that by the time the CGContext graphics call is made, some of the elements of the CGContext have been freed. However, I've spent about two and a half days trying to track this down so far, to no avail.   Often the problem involved getting the stroke color, so I made all references to the colors create and explicit copy (leaking like a sieve), but the same errors occurred and if I execute the internal CGContext routine CGContextGetStrokeColor, it was invalid during the crash, even though it had just been set to a copy that was still valid (confirmed both through 'po' and by an additional CFRetain in the code). I'm at a bit of a loss, and the problem doesn't happen under 10.6 (although updating is much, much slower under 10.6, which is also curious).   One thought we've had here is that it might be some additional locking under 10.6 that is saving us, but we can't even figure out how to emulate that without having to add a specific coordination layer between the two similar views. Any thoughts or problems of this type seen before on quick-changing CATiledLayer backed views? Other things I've looked at: - the observers are all being invoked on the main thread - Color doesn't seem to be the only important property (sometimes it'll crash on linemode or some similar method) - We are setting color using the CGContextSetXXColor( cs, rgba[] ) calls, although we've also tried using the SetXXXColorFromColor calls as well (complete with making copies every time and verifying the CGColor validity before the calls) and it doesn't make a difference - Hiding the auxiliary view causes the problem to disappear - We've confirmed that we're using threadsafe color setters and getters (using @synchronize(self) in the block around reading and writing) Thanks, -Gaige</body>
  </mail>
  <mail>
    <header>Re: Any experience with CIRowAverage and CIColumnAverage?</header>
    <body>cool I&amp;#39;ll have a look thanks. I tried the CIRowAverage kernel filter with Quartz Composer and it works OK. You get a horizontal image left to right showing the average for the rowscan. 2009/7/29 Steve Israelson Color correction + gamma? Pass in a nil color profile for no color matching when converting to and from a bitmap. Hi Folks, strangely I&amp;#39;m getting some weird data back from this. I&amp;#39;ve done a test. My filter sets the vec4 return type to all 1&amp;#39;s... so I&amp;#39;d expect the row average for my image to be the same. But it isn&amp;#39;t. Here&amp;#39;s what is says the average is for row 0: ¬†row alpha value is 213.000000 ¬†row red value is 192.000000 row green value is 192.000000 row blue value is 192.000000 instead of what I&amp;#39;d expect which is 255, 255, 255, 255. Any ideas? Can&amp;#39;t seem to find much documentation on this stuff unfortunately other than the Core Image and Image Unit manuals. Cheers, Max Cool. It does look like there is more data coming back. I&amp;#39;ll have a play round with it more tomorrow. Thanks, Max Makes total sense. 1 pixel in one dimension, same width or height as the input. Hi Steve, I have it going in as a CIVector... [rowAverageFilter setValue: [CIVector vectorWithX: CGRectGetMinX(inputExtent) ¬†Y: CGRectGetMinY(inputExtent) So perhaps I get back an image that is 1 pixel high, but several wide, each containing a row average. Does that make sense? Kind regds, Max 2009/7/28 ¬†&amp;lt;&amp;gt;:</body>
  </mail>
  <mail>
    <header>Re: Any experience with CIRowAverage and CIColumnAverage?</header>
    <body>Pass in a nil color profile for no color matching when converting to and from a bitmap.</body>
  </mail>
  <mail>
    <header>Re: Any experience with CIRowAverage and CIColumnAverage?</header>
    <body>Hi Folks, strangely I&amp;#39;m getting some weird data back from this. I&amp;#39;ve done a test. My filter sets the vec4 return type to all 1&amp;#39;s... so I&amp;#39;d expect the row average for my image to be the same. But it isn&amp;#39;t. Here&amp;#39;s what is says the average is for row 0: ¬†row alpha value is 213.000000 ¬†row red value is 192.000000 row green value is 192.000000 row blue value is 192.000000 instead of what I&amp;#39;d expect which is 255, 255, 255, 255. Any ideas? Can&amp;#39;t seem to find much documentation on this stuff unfortunately other than the Core Image and Image Unit manuals. Cheers, Max Cool. It does look like there is more data coming back. I&amp;#39;ll have a play round with it more tomorrow. Thanks, Max Hi Steve, I have it going in as a CIVector... [rowAverageFilter setValue: [CIVector vectorWithX: CGRectGetMinX(inputExtent) ¬†Y: CGRectGetMinY(inputExtent) So perhaps I get back an image that is 1 pixel high, but several wide, each containing a row average. Does that make sense? Kind regds, Max 2009/7/28 ¬†&amp;lt;&amp;gt;:</body>
  </mail>
  <mail>
    <header>Obtaining an ICC profile from a PDF</header>
    <body>The Quartz 2D Programming Guide's Inspecting PDF Document Structure section indicates the name of the embedded profile of each image on a page can be obtained.  My question is can that profile(s) also be obtained in a manner similar to: CGColorSpaceRef aColorSpace = CGImageGetColorSpace (theCGImage);? Stephen Herron email@hidden</body>
  </mail>
  <mail>
    <header>Re: Is a seed fill possible with a kernel routine?</header>
    <body>Yes, I was aware of that. That's essentially what I was talking about.  I think you could make it expand faster than that too, at the cost of sampling more pixels per pixel per pass. The fundamental problem is of course that for an WxH image the GPU is going to have to examine O(WH) pixels per pass whereas the conventional CPU-based algorithm is only looking at O(W) per inner loop even in the worst case (filling an empty image), and in a real- world scenario the CPU has much less work to do in most cases whereas the GPU can't really get out of examining all of the pixels on each pass. Agreed, I would expect the CPU running a conventional seed fill algorithm to be faster for this task also (which is why I mentioned that approach at the end of my post).  That said, I'd be interested to see how fast you *could* make a CI kernel-based seed fill run and whether there's any way of using the GPU to accelerate a CPU-based seed fill. --</body>
  </mail>
  <mail>
    <header>Any experience with CIRowAverage and CIColumnAverage?</header>
    <body>Hi All, would anyone have had any experience with either of these? I always get a bit nervous when I plug something into Google and nothing comes up! Well nothing of value in any case. The first issue I have is passing in the inputExtent. I assume that for the row scan, I pass in a CGRect representing the 1 pixel high row that I want to scan, e.g. Anything else seems to crash the kernel with a nasty error. The funny thing though is that I don't get back the average. I get back figures that suggest a colour image. That's strange because the previous filter applied made the image black and white, with mostly black rows. The other thing is to scan a whole image by re-invoking for each row is very slow. For example if I have an image 200 pixels high I need to loop through the Y coordinates, passing in an updated inputExtent each time. Does anyone have any experience with either of these filters? I'm a bit lost here. Many thanks, Max</body>
  </mail>
  <mail>
    <header>Re: Is a seed fill possible with a kernel routine?</header>
    <body>I reckon you could probably implement something using multiple kernel invocations.  I'd probably use an auxiliary image to store connectivity information; you'd start with it black with a single white pixel at the seed fill origin, then your kernel would set the pixel value for every pixel in the original image that matches the seed fill colour to the maximum (most white) surrounding pixel value (similar in a way to a cellular automaton).  The resulting image would consist of just those pixels that you needed to fill in colour in the original image.  You could then use a final kernel invocation to actually colour those pixels. Quite how fast it would go in practice I don't know.  It might be faster to code up a conventional seed fill routine on the CPU. --</body>
  </mail>
  <mail>
    <header>Re: Is a seed fill possible with a kernel routine?</header>
    <body>ps. I should clarify, I'm trying to label components. So if I have 5 shapes in the image they would each have a different label. 2009/7/28  &amp;lt;email@hidden&amp;gt;:</body>
  </mail>
  <mail>
    <header>Is a seed fill possible with a kernel routine?</header>
    <body>Hi Folks, the Image Unit Tutorial says it isn't.... &amp;quot;A kernel routine computes an output pixel by using an inverse mapping back to the corresponding pixels of the input images. Although you can express most pixel computations this way‚Äîsome more naturally than others‚Äîthere are some image processing operations for which this is difficult, if not impossible. For example, computing a histogram is difficult to describe as an inverse mapping to the source image. You also cannot perform seed fills or any image analysis operations that Here's what a seed fill is... No doubt it can be done in Quartz 2D, but I'm racking my brains to figure out if it's possible with a kernel routine, maybe by repeatedly invoking it. Any ideas? Any advice would be great. Many thanks, Max</body>
  </mail>
  <mail>
    <header>Re: Can a core image kernel return custom parameters?</header>
    <body>Yes. A filter returns an image, which is a big array of data. So return data in there.</body>
  </mail>
  <mail>
    <header>Can a core image kernel return custom parameters?</header>
    <body>Hi Folks, just wondering if I can do this. It'd be a great time saver to know if it's possible or not before I start messing around. I want to analyse an video image using a core image filter. Seems to be a fast way to do it. The kernel routine needs to create an array with the analysis on a pixel by pixel basis. The invoking code needs to get at this array. Does anyone know if this is possible? I see you can create custom input parameters. Could one of these be an array that the kernel routine populates, or would this fail? that an explicit custom return parameter is out of the question. So, the only way I can think to do this, which is a bit of a hack, is to somehow encode the analysis in the vec4's rgba. Any advice would be much appreciated. Many thanks, Max</body>
  </mail>
  <mail>
    <header>Re: Picture preloader or / and xml preloader</header>
    <body>i am trying to do a composition where i get some values from xml file. The problem is that when this xml file loads then fps goes down to 3-4 frames. Is there a way to preload graphics or/and xml file?</body>
  </mail>
  <mail>
    <header>Picture preloader or / and xml preloader</header>
    <body>i am trying to do a composition where i get some values from xml file. The problem is that when this xml file loads then fps goes down to 3-4 frames. Is there a way to preload graphics or/and xml file? Peter</body>
  </mail>
  <mail>
    <header>Few questions about CoreImage</header>
    <body>II suppose the number at the end is the order in which the diverse stages of the rendering pipeline occurs? I see that there is some color correction happening after my filter (myKernel). Is is possible to disable this stage, and display directly the RGB output from my kernel?</body>
  </mail>
  <mail>
    <header>Re: Nearest-neighbour downsampling</header>
    <body>Thanks Paul. But where do I specify that I want the iamge to be resized? Alex Hi all, Does anybody know if there is some code available for the nearest-neighbour downsampling CI filter available somewhere, perhaps in the Apple developer center? THanks in advance, Alex</body>
  </mail>
  <mail>
    <header>Re: Zoom in of CATiledView looks good, zoom out doesn't</header>
    <body>I'm not sure either! Unfortunately not. I'm still hoping it'll get fixed for 3.1... Hamish</body>
  </mail>
  <mail>
    <header>Re: Zoom in of CATiledView looks good, zoom out doesn't</header>
    <body>Done, bug #7077835. I've also added it to Open Radar. Don't know if I should mark it as a duplicate of your bug there, before Apple has done so? Have you found a workaround for this issue?</body>
  </mail>
  <mail>
    <header>Re: Zoom in of CATiledView looks good, zoom out doesn't</header>
    <body>See  (rdar://problem/6939869) But please do also file a bug on it yourself, and hopefully it'll get fixed quicker! Hamish</body>
  </mail>
  <mail>
    <header>Re: Nearest-neighbour downsampling</header>
    <body>I think it's basically &amp;quot;out = in&amp;quot; with the key point being how the CISampler is set up. You need to set the filter mode to nearest on the CISampler instance used. On 21 Jul 2009, at 11:55, Alexis Gatt wrote:</body>
  </mail>
  <mail>
    <header>Nearest-neighbour downsampling</header>
    <body>Hi all,</body>
  </mail>
  <mail>
    <header>Re: Debugging with CoreImage</header>
    <body>Hello again, Hi guys,</body>
  </mail>
  <mail>
    <header>Zoom in of CATiledView looks good, zoom out doesn't</header>
    <body>Hi, I'm using UIScrollView + CATiledView on the iPhone. When I zoom in: - the low-res tiles are enlarged - in a fraction of a second, the higher-res tiles are loaded and fade-in over the low-res tiles This looks super-cool. When I zoom out, I would expect the opposite: - the high-res tiles to be shrunk (borders would now be all jagged) - the lower-res tiles to fade in over them. High-res tiles shrink just fine until a lower-res level of detail is available (or if there is none). But once there is, they are *wiped clean* (painted white) instead of staying there, like they do when zooming in. This causes a momentary blanking of the screen between the wipe and when the new lower-res tiles are loaded, which is confusing to the user. Is this the expected behavior or am I just doing something wrong? This doesn't seem to happen in Maps.app (which I suspect also uses CATiledLayer), where both zooming in and out preserve the old tiles until the new ones are loaded. Regards, Jaka</body>
  </mail>
  <mail>
    <header>Making CGImageSource decoding synchronous?</header>
    <body>I&amp;#39;m working on a project that is using CG to decode images into bitmaps for use. Our code looks (very roughly) like this: You&amp;#39;ll notice that we&amp;#39;re missing a very important¬†call: CGImageSourceGetStatusAtIndex() to ensure that the image at index 0 is actually available before we want to use it. On Leopard that omission didn&amp;#39;t matter; image decoding was always synchronous, so we worked (albeit accidentally).</body>
  </mail>
  <mail>
    <header>Introspecting and editing a CGPath's elements' points</header>
    <body>I think I must be missing something obvious here. My app requires that a user can edit specific points of the shape (kind of like a standard vector graphics app with B√©zier curves), but I'm finding it difficult to introspect CGPathRef &amp;amp; CGMutablePathRef objects. I've looked in the online docs, the header files, and the Gelphman &amp;amp; Laden &amp;quot;Programming with Quartz&amp;quot; book (2006). [1] Can you quiz a CGPathRef (or CGMutablePathRef) object for the various path elements, akin to NSBezierPath's -elementCount, - elementAtIndex and -elementAtIndex:associatedPoints: methods [2] Can you quiz the element type (the CGPathElementType) to see if element 5 in the path is a curve / line / moveTo / close etc.? [3] Can you ask an element for its related coordinate points (in a read-only manner)? [4] Can you directly edit the coordinate points of specific path elements (eg. the 1st control point of the 4th element, which I happen to know is a cubic curve element)? Or, almost as good, replace a path's element with an updated version of itself? I'm guessing you can only do this on a CGMutablePathRef, but can it even be done on those objects? I can't see why a mutable path is called 'mutable', other than you can keep adding additional elements to the end of it. There seems to be no public API for changing anything that's already in it. Or is there? Maybe I'm looking in all the wrong places! I'm currently thinking I should just store my own coordinates (of control points) and construct CGPaths / path elements on the fly. However in the absence of being able to do [3], it means having to re- invent ways of doing CGPathAddArc() etc., as I couldn't add an arc using Quartz's CGPathAddArc() function, and then find out where it put the control points etc. Apologies if this sounds like a rant, it's really not, I'm just trying to determine the best way of doing editable path objects / how much I need to reinvent. Thanks in advance for any info you can give, and if I'm being a fool and have looked in all the wrong places, please don't hesitate to point that out! Cheers, Ken - - - - - - - - - - Dr. Ken Tabb Mac &amp;amp; UNIX Developer - Health &amp;amp; Human Sciences Machine Vision &amp;amp; Neural Network researcher - School of Computer Science University of Hertfordshire, UK</body>
  </mail>
  <mail>
    <header>Re: QDIsPortBuffered</header>
    <body>This is primarily because there is no 1:1 mapping between Quartz and Quickdraw. In particular, QDIsPortBuffered() is generally not useful for modern drawing, so the likely answer for this one is to just drop it entirely. If you are moving your drawing code from Quickdraw to Quartz, then I recommend reading the documentation or perhaps grabbing a good book on Quartz (the Gelphman-Laden book is highly recommended). Get a good feel for the API, then consider how to approach it with the problems you are trying to solve. If you try to do a port by mapping Quickdraw calls into Quartz, you are likely to hit a great number of roadblocks along the way. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>QDIsPortBuffered</header>
    <body>i am a newbie to the list.. so let me say a warm hello to all of you ..;) my question might look simple for you, but i am searching now for a quite long time without an result. so maybe some could please help me. in my code (audio plugins) i have several deprecated functions, one of these is QDIsPortBuffered(..) .. if i am looking into the docs i find also this &amp;quot;deprecated in 10.4&amp;quot; hint, but i can not find any hint by actually what it got replaced. only that is in Quartz 2D !? so here might be the next issue. i have not completely understood what this function is really doing. so maybe, please someone could help ? bastian</body>
  </mail>
  <mail>
    <header>Re: Debugging with CoreImage</header>
    <body>On point 2, I have no problem generating colors in a filter. What I had problems with is any constant (e.g., 1.0) that's in my filter code. For reasons I don't understand, constants sometimes just don't work, so I find ways to write filters without any constants. Sounds crazy, I know, but works for me. On Jul 16, 2009, at 7:29 PM, Alexis Gatt wrote:</body>
  </mail>
  <mail>
    <header>Re: passing arguments to a coreimage filter</header>
    <body>Hi all,</body>
  </mail>
  <mail>
    <header>Debugging with CoreImage</header>
    <body>Hi guys,</body>
  </mail>
  <mail>
    <header>passing arguments to a coreimage filter</header>
    <body>Hi all,</body>
  </mail>
  <mail>
    <header>Capturing active state</header>
    <body>I am working on application where there is feature to capture screen snap same as apples grab. In this user can select a region and can capture selected area as image. We have used a approach where selection is drawn on a transparent window and we capture the image by reading the data from frame buffer. But the problem here is, as our application is active, when we read image from the frame buffer  states of all other apps look disabled ie. scroller and buttons have greyish shade instead of blue. Similar problem is observed in SonOfGrab Apple sample application. ( Is there any workaround to capture the active state of all the windows in desktop? Thanks in advance, Srinivas</body>
  </mail>
  <mail>
    <header>Re: Lines at tiled image repeat?</header>
    <body>Ouch. User error; we were reencoding the image and when converting it from a CGImage to an SkBitmap we were drawing it one pixel to the right. Fixing it ( if anyone&amp;#39;s curious) fixes all problem. Thanks for the help, everyone. Avi</body>
  </mail>
  <mail>
    <header>Re: Lines at tiled image repeat?</header>
    <body>Does the integral size of the NSImage match the native size of the bitmap?  Is the bitmap specified as a 72 dpi image (open it in preview and do &amp;quot;get info&amp;quot; on it to check the DPI).  NSImage is often helpful about trying to scale images of various DPI.  That kind of scaling can lead to images whose size are an integral number of points, but because of the DPI of the image NSImage has to scale things to make them fit.  A 150 dpi image doesn't fit exactly into a 72 dpi grid. You could easily check this by putting your view in front of another view that is, say, solid red.  If your thin lines become red then there could be transparency issues. Grab the CGContext for your view and then get the CTM out of the context and dump it to the terminal.  See if there are any values (particularly scaling values) in the transform.  If you're seeing pixel cracks then it would usually be because of a slight scaling issue.  If you see anything in the CTM, try reproducing that matrix exactly in your sample app and see if you get the same thing. Try changing the format of the image that you are loading.  If you were previously using a TIFF, try changing it to a PNG.  If you are concerned about potential alpha problems, try using a JPEG or some other format that has no alpha. I sometimes find that going through the high level interfaces provided by the AppKit introduces subtle issues and NSImage is a frequent offender.  You might try bypassing the Cocoa stuff and drop directly down to the CGLayer where you have more precise control over the process.  By going through all the steps yourself you may find some subtle thing that will point out why the more convenient path is not working the way you want it to.  If you can resolve those issues then you can go back to the higher level path.</body>
  </mail>
  <mail>
    <header>Re: Lines at tiled image repeat?</header>
    <body>Use Quartz Debug?</body>
  </mail>
  <mail>
    <header>Re: Lines at tiled image repeat?</header>
    <body>Use Quartz Debug? Are you sure you're not falling victim to the half-pixel offset that arises because Quartz units are in the middle of pixels? --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Lines at tiled image repeat?</header>
    <body>This isn&amp;#39;t strictly a Quartz issue (considering I&amp;#39;m doing this in Cocoa) but it probably goes through Quartz at some point and I&amp;#39;m a bit desperate. On my project we&amp;#39;re using pattern images, like:</body>
  </mail>
  <mail>
    <header>Re: Memory leak from PDFPage drawWithBox: ?</header>
    <body>On 9 Jul 2009, at 03:41, David Duncan wrote: Filed bug 7044204.</body>
  </mail>
  <mail>
    <header>Re: Memory leak from PDFPage drawWithBox: ?</header>
    <body>Thanks Scott, although between here and rubycocoa-talk I'm beginning to think that you're stalking me ;-) For the record I found that the leak persists only as long as the PDFDocument, so that if I create and dispose a PDFDocument for each page then I can export every page without leaking. It sounds horrible, but disk caching and I guess the PDF indexes combine to mean that continually opening the document isn't perceptibly slower than iterating through the pages of an open document. Thanks for the CG code - I'll pull it in as part of my migration from RubyCocoa to ObjC. Duncan</body>
  </mail>
  <mail>
    <header>Re: Memory leak from PDFPage drawWithBox: ?</header>
    <body>I haven't quite got my head around how to use Instruments with OCUnit! Thanks - I wasn't sure whether I was misusing PDFKit. It seems to me that if this leaks PDFView should too, but I don't seem any sign of that. Duncan</body>
  </mail>
  <mail>
    <header>Re: Get rid of CGContextDrawImage / ImageIO caching?</header>
    <body>Hi Martin, I have not had an answer and my only other finding is that the memory cache gets worse as physical memory size grows. Right now on an iMac 27&amp;quot; with 1GB VRAM, 12GB RAM and an external 24&amp;quot; display memory use for my app averages around 450MB Private mem and 750MB Real mem for the majority of its lifetime. That may dip down to 300/550 but it goes back up as soon as new images are displayed. &amp;quot;Working as intended&amp;quot; is a bad answer for 100% of use cases. My app does not benefit in any significant way from such a large image cache for the reasons I explained before. All users see is that my app is most likely the worst memory consumer on their system - even those with 16GB+ of RAM watch their mem usage like a hawk - and they are likely to just stop using it because of its &amp;quot;piggy&amp;quot; behavior. That's fine if I have control over the mem usage, but I don't.</body>
  </mail>
  <mail>
    <header>Re: Get rid of CGContextDrawImage / ImageIO caching?</header>
    <body>I suppose the question is what are the nature of the complaints? If it is simply &amp;quot;memory usage goes up&amp;quot; then the only answer I can see to this is &amp;quot;Working as Intended&amp;quot;. If there is a performance issue here, then we would want to see a bug report. To my understanding we already are marking the memory as purgeable. That doesn't mean it will be unmapped immediately by any stretch, but does fit with your observation that the memory will be given up when another application needs it. -- David Duncan</body>
  </mail>
  <mail>
    <header>Re: iOS: Save a readable {UI,CG}Image?</header>
    <body>UIViewContentModeScaleAspectFit. Argh. I wasn't searching in the right places. The UIKit functions always throw me. Just out of curiosity, why are they functions instead of instance methods, do you know? ‚Äî F</body>
  </mail>
  <mail>
    <header>Re: Transform for *FlipFromLeft/Right?</header>
    <body>Thanks.  I'll try that. David</body>
  </mail>
  <mail>
    <header>Re: Get rid of CGContextDrawImage / ImageIO caching?</header>
    <body>Am 09. Okt 2010 um 09:19 schrieb Brian Bergstrand &amp;lt;email@hidden&amp;gt;: 10.6.4, Nvidia 9600GT &amp;amp; 9400M, 8GB RAM XCode 3.2.4, LLVM 1.5, 10.6 SDK, 10.6 min sys, 64bit app I have an application that is drawing medium size JPEG images (15-20 Mpixels, 8-12MB compressed) to a full screen window. These images are in the form of CIImages with various filters applied. These images are basically transient, they will be displayed once and than likely not again for quite a while (days or even weeks if the app is run that long). Once a CIImage is drawn to the screen my app sees an immediate increase of 80MB of dirty memory that never seems to go away. If I'm on a dual display system, that memory is double (or more). Instruments shows no leaks, but the Allocations tool shows 5 16MB malloc's all attributed to ImageIO's copyImageBlockSetJPEG. With dual display (and thus 2 windows) there is another set of 5 mallocs (though of a slightly different size) attributed to imageIO. And this is just after drawing one image per display. Drawing more images increases the memory use. &gt;From what I've gathered this is some kind of caching that ImageIO does of the uncompressed image data. Since my images are transient, this caching not only eats a ton of memory, it's also useless. I've narrowed this down to calling CGContextDrawImage. If I nop those calls out, no memory increase. My setup is NSWindow-&amp;gt;NSView-&amp;gt;[CALayer drawInContext:]. All drawing is done in the layer with the 2 lines of code below. ... // drawiImage: eventually calls CGContextDrawImage Right now all I can tell users that complain about the large memory use is "it's Apple's fault" - that doesn't fly too well. Is there anyway that I can disable this caching and get my app memory use in check? Thanks for any help. p.s. I also have an offscreen context that I use to get an ARGB bitmap for color sampling the CIImage data. The drawing is limited to 640x480 (by adding a scale filter to the CIImage) and is done into a CGContext that I create myself CGContextRef ctx = CGBitmapContextCreate (bitmapData, pixelsWide, pixelsHigh, ... // sample data const unsigned char *data = ""&gt; ... And again I see the same 80 (5 16MB mallocs) from ImageIO (even if I disable the on screen drawing code above).</body>
  </mail>
  <mail>
    <header>Re: iOS: Save a readable {UI,CG}Image?</header>
    <body>What is the content mode of your image view? UIImage[PNG | JPEG]Representation(). -- David Duncan</body>
  </mail>
  <mail>
    <header>iOS: Save a readable {UI,CG}Image?</header>
    <body>I have an iOS application that builds a 1.0-scale UIImage, and on demand builds a 1.75-scale UIImage with the same content. I substitute the up-scaled image into the image property of a UIImageView when the user zooms in. The effect is supposed to be that when the upscaled image comes in, all the user sees is the image getting sharper. Instead, the sharpened content jumps up and to the left a bit. As this happens while the edges of the images aren't visible, I can't visually compare the two, still less make measurements. What I'd like to do is: 1) Run the app in the iPhone Simulator. 2) Reduce the two images to some sort of file format. 3) Save the files. 4) On my Mac, dig up the files so I can compare and measure them. I'm thwarted at step 2. I conceive that I could render the image into a CGBitmapContext, and save the bitmap data, but I don't know how to turn that into a format that Preview or Photoshop could import for inspection. Has anyone a suggestion? ‚Äî F</body>
  </mail>
  <mail>
    <header>Re: Transform for *FlipFromLeft/Right?</header>
    <body>Then you might prefer to use UIViewAnimationOptionShowHideTransitionViews. You want to fade between the views at the same time as flipping between them? Have you tried setting up a basic animation for the fades separately, rather than doing it in the transition block? BTW, I wouldn't use any app that made a habit of locking me out of its UI for 2.5 seconds at a time. Hamish</body>
  </mail>
  <mail>
    <header>Re: Transform for *FlipFromLeft/Right?</header>
    <body>Actually, I am not removing any views.  I am just fading a view in and out.  But even if I add and remove views, the change in alpha isn't animated during the first half of the animation. [UIView transitionWithView: self.view duration: 2.5 options: UIViewAnimationOptionTransitionFlipFromLeft | UIViewAnimationOptionAllowAnimatedContent David</body>
  </mail>
  <mail>
    <header>Re: Transform for *FlipFromLeft/Right?</header>
    <body>The view you pass to +transitionWithView:... is the container view from which you remove and add the views being transitioned between. I think you should set the alpha of those views, not the container. H</body>
  </mail>
  <mail>
    <header>Re: Transform for *FlipFromLeft/Right?</header>
    <body>Good suggestion.  I hadn't noticed that option.  But still no joy.  Alpha is still not animated during the first half of the animation. [UIView transitionWithView: self.imgView duration: 2.5 options: UIViewAnimationOptionTransitionFlipFromLeft | UIViewAnimationOptionAllowAnimatedContent David</body>
  </mail>
  <mail>
    <header>Re: Transform for *FlipFromLeft/Right?</header>
    <body>Have you tried this on 4.0 with UIViewAnimationOptionAllowAnimatedContent? H</body>
  </mail>
  <mail>
    <header>Re: Transform for *FlipFromLeft/Right?</header>
    <body>That was the first thing I tried.  Didn't work either. Unfortunately, I can't take that route.  In addition to flipping the image, I want to fade the alpha value of the image in and out.  The UIView animations don't animate alpha changes for the first half of the flip--which means that if I am going from alpha=0 to alpha=1, the first half of the animation is completely invisible. David</body>
  </mail>
  <mail>
    <header>Re: Transform for *FlipFromLeft/Right?</header>
    <body>You're welcome. I'm just about out of my depth too, so perhaps someone else more knowledgeable would like to chip in. If I were attempting this*, I would set the layer's anchorPoint to a point on the leading edge, and perform a rotation through Z and a translation along X. The rotation should then occur around the leading edge, so the leading edge should never leave the view plane, and there should be no need for scaling. You might need some perspective, but you might find that fading and unfading the layer (as happens in the UIView transition animation) is enough. *(Actually, if I were attempting this, I would just use the UIView transition animation, since your layer is the primary layer of a view.) Hope this helps, Hamish</body>
  </mail>
  <mail>
    <header>Re: Transform for *FlipFromLeft/Right?</header>
    <body>Thanks for replying, Hamish. First off, the code I posted in my original message was wrong.  The second block of code shouldn't be there.  (It was commented out, but then I uncommented it after I copy-and-pasted it into the email message.  Who knows why.)  So the code that I am working against is this: // Spin // Scale [More below] Yes, that is exactly right.  And my code above obeys those constraints.  (Please correct me if I am mistaken.) If you are referring to the second block of code, that started with &amp;quot;CABasicAnimation *scale =&amp;quot;, then yes, you are right.  That shouldn't be there.  It isn't needed.  It was commented out before and I thoughtlessly uncommented it when I pasted it into my email. 1 / -850 comes from Apple's docs[1].  (See Listing 2.)  It configures the CATransform3D as a perspective transform.  I don't know how to derive it. I got the .835 constant just by eyeballing the animation--I don't know how to derive it.  At .835, the height of the leading edge of the layer matches the height of the view plane at the end of the animation.  (Without the CATransform3DScale, the height of the leading edge would be taller than the height of the view plane, corresponding to layer coming &amp;quot;out&amp;quot; of the front of the view plane.) The problem is that at the beginning of the animation, the image scales (shrinks) too quickly.  The leading edge of the layer (whose height should always equal the height of the view plane) becomes shorter than the height of the view plane.  After, say, the first quarter of the animation, the rotation part of the animation begins to catch up and the animation ends with the leading edge of the layer having the correct height. Since the rotation and scaling aren't &amp;quot;synced&amp;quot; such that the leading edge of the layer stays in the view plane, I am wondering if this animation is even possible using just a transformation matrix.  Perhaps this animation has to be built some other way? Just for fun, here is some more code.  My first attempt at building this animation was to put the anchorPoint on the leading edge of the layer, pivot the image around that, while also sliding the leading edge of the layer from one side to the other.  Like so: This animation has, effectively, the exact same problem as the first: the two pieces of the animation (pivot and slide) aren't in sync.  The animation slides too much at the beginning. So, I'm stumped.  (Or, perhaps more accurately, out of my depth.)  Any help would be greatly appreciated. Thanks, David [1]</body>
  </mail>
  <mail>
    <header>Re: Transform for *FlipFromLeft/Right?</header>
    <body>No time to look in detail now but a quick observation: your two constraints should be that the leading edge stays in the view plane (as you mention) and that the vertical line bisecting the transformed layer stays in the vertical plane perpendicular to and bisecting the view plane. With this in mind it's not clear why you need to scale, or where you get your constants (0.835, 0.850) from? H Please excuse the brevity -- sent from my phone</body>
  </mail>
  <mail>
    <header>Transform for *FlipFromLeft/Right?</header>
    <body>Can the iOS FlipFromLeft/FlipFromRight animations be created from a single transform (either a CGAffineTransform or a CATransform3D)?  If so, what is that transform? I'd like to create my own flip animation, but I can't duplicate the animation created by UIViewAnimationTransitionFlipFromLeft/Right.  I can get close, but not 100%.  The following code, for example, does the first half of the flip animation, but the image shrinks too quickly at the beginning of the animation.  (Or, if I keep it from shrinking too quickly at the beginning, then it hasn't shrunken enough by the end.) Any hints or suggestions? (Note:  In case you haven't noticed, the iOS Flip animations are not simple rotations.  The leading edge of the animation does *not* come &amp;quot;out&amp;quot; of the front of the view plane--it stays in the view plane.  The trailing edge of the animation goes out the back of the view plane.  Sorry if that's not a good explanation.) Thanks, David</body>
  </mail>
  <mail>
    <header>Re: Compositing colors with alpha</header>
    <body>So in preparation to changing our code to use Quartz compositing rather than precomputing blended colors, I wrote a demo app to quickly see the effects of drawing colors in different color spaces. But what disturbs me is that in both linear and calibrated RGB color spaces, drawing 50% transparent black atop 100% opaque white (or the converse, 50% transparent white atop 100% opaque black) does *not* yield the same result as drawing 100% opaque 50% gray. I have uploaded the demo app here: If anyone could shed any light on this topic, it would be much appreciated. Thanks, --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: CGContextShowTextAtPoint</header>
    <body>On Oct 21, 2010, at 9:39 PM, David F. wrote:</body>
  </mail>
  <mail>
    <header>Re: CGContextShowTextAtPoint</header>
    <body>Just out of curiosity, have you verified that it works for underline and strikethrough?  I think the context has to be flipped in order for underline and strikethrough to be drawn correctly. David</body>
  </mail>
  <mail>
    <header>Re: Compositing colors with alpha</header>
    <body>alpha blending in a nonlinear color space is not a simple operation. At this point, I'm tempted to just call CGRectFill and let the smart people on the Quartz team do this work for me. We can, of course, talk about this in person tomorrow, but I'd like to also solicit opinions from the list. ;-) --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: Compositing colors with alpha</header>
    <body>It looks like when you're converting the computed linear-RGBA components back into a color, you're interpreting them in the CalibratedRGB color space, instead of the linear color space from which you got them. I think you want an inverse of OQGetColorComponents() that uses the same linearRGBColorSpace as OQGetColorComponents() did, for round-trip-y-ness. (Or make the colorspace a parameter to OQCompositeColors(); interpolating in HSV might be useful for some things, though not for imitating Quartz.)</body>
  </mail>
  <mail>
    <header>Compositing colors with alpha</header>
    <body>Hello all, We're using CALayers in a view in our application. Some of our layers also contain text, so we need to fill the layers' contents with an opaque color before drawing our text into them so that we get nice sub-pixel anti-aliasing. Since the thing that's drawing text can also be selected, we need to compute the opaque color as a blend of the (transparent) selection color and the view's (opaque) background color. Up until today, we were na√Øvely blending the colors by converting them to Generic RGB color space and multiplying their RGB components by OQCompositeColors is the function we're calling; the blending algorithm is contained in OQCompositeLinearRGBA; the code that converts the colors to Generic RGB is in OQColorGetComponents): We noticed that the color we computed was different from the color Core Animation would come up with by compositing a layer whose background color is our selection color atop a layer whose background color is our view's background color. We realized that our math is invalid in a non-linear RGB colorspace, which the Generic colorspace almost certainly is. But if I instead change the definition of OQColorGetComponents to go through a linear RGB color space (by way of CGColorSpaceCreateWithName(kCGColorSpaceGenericRGBLinear)), we get differently bad results (very dark). Updated version of OQColorGetComponents here: I believe we need an actual color, or else we'd just use Quartz drawing functions to fill the layers' contents. So I have a few questions: 1. Does CA use the window color space, device color space, or something else for computing its colors? 2. Does that differ from the color space Quartz uses if you use Quartz drawing calls to draw into a layer's contents? 3. Can we fix our math to work in arbitrary color spaces? In general, I'd appreciate any advice. Thanks, --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: Draw Text</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Skew/distort transform</header>
    <body>but from the comments there, nobody has managed to successfully implement it. I tried too, and it just made a really weird transformation nothing like a quadrilateral. I have been hunting for proper documentation on the meaning of the matrix, but have come up empty handed so far - the apple docs don't seem to have much more than a brief overview - not enough for me to understand it. Has anyone either: a) managed to implement a transform like the one linked above, or b) got some documentation that would have sufficient detail for me to implement my own transform to do the job? Thanks Gideon</body>
  </mail>
  <mail>
    <header>Re: Core animation frame Rate</header>
    <body>The more transactions you commit per second, the more work Core Animation has to do to update the render state. Since your layers render out of process, this is somewhat more expensive (take a look at the CPU usage of the Springboard process rather than that of your own process). -- David Duncan</body>
  </mail>
  <mail>
    <header>Core animation frame Rate</header>
    <body>Hello, Federico</body>
  </mail>
  <mail>
    <header>Display Link 60fps</header>
    <body>Hi, Federico</body>
  </mail>
  <mail>
    <header>Get rid of CGContextDrawImage / ImageIO caching?</header>
    <body>10.6.4, Nvidia 9600GT &amp;amp; 9400M, 8GB RAM XCode 3.2.4, LLVM 1.5, 10.6 SDK, 10.6 min sys, 64bit app I have an application that is drawing medium size JPEG images (15-20 Mpixels, 8-12MB compressed) to a full screen window. These images are in the form of CIImages with various filters applied. These images are basically transient, they will be displayed once and than likely not again for quite a while (days or even weeks if the app is run that long). Once a CIImage is drawn to the screen my app sees an immediate increase of 80MB of dirty memory that never seems to go away. If I'm on a dual display system, that memory is double (or more). Instruments shows no leaks, but the Allocations tool shows 5 16MB malloc's all attributed to ImageIO's copyImageBlockSetJPEG. With dual display (and thus 2 windows) there is another set of 5 mallocs (though of a slightly different size) attributed to imageIO. And this is just after drawing one image per display. Drawing more images increases the memory use. From what I've gathered this is some kind of caching that ImageIO does of the uncompressed image data. Since my images are transient, this caching not only eats a ton of memory, it's also useless. I've narrowed this down to calling CGContextDrawImage. If I nop those calls out, no memory increase. My setup is NSWindow-&amp;gt;NSView-&amp;gt;[CALayer drawInContext:]. All drawing is done in the layer with the 2 lines of code below. ... // drawiImage: eventually calls CGContextDrawImage Right now all I can tell users that complain about the large memory use is &amp;quot;it's Apple's fault&amp;quot; - that doesn't fly too well. Is there anyway that I can disable this caching and get my app memory use in check? Thanks for any help. p.s. I also have an offscreen context that I use to get an ARGB bitmap for color sampling the CIImage data. The drawing is limited to 640x480 (by adding a scale filter to the CIImage) and is done into a CGContext that I create myself CGContextRef ctx = CGBitmapContextCreate (bitmapData, pixelsWide, pixelsHigh, ... // sample data ... And again I see the same 80 (5 16MB mallocs) from ImageIO (even if I disable the on screen drawing code above).</body>
  </mail>
  <mail>
    <header>Re: another workflow question. OOP with QC :)</header>
    <body>Also.... still sussing out where you are going/what you need to do (sorry, it&amp;#39;s taking a moment to sink in). -GT -GT ¬†_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list ¬† ¬† ¬†() Help/Unsubscribe/Update your Subscription: This email sent to</body>
  </mail>
  <mail>
    <header>Re: another workflow question. OOP with QC :)</header>
    <body>P.S. I accidentally posted this to the quartz dev list first. I reposted on the QC list too as its more related to that. Probably best not to continue this thread on this list. Apologies for the double post. -GT &amp;nbsp;_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list &amp;nbsp; &amp;nbsp; &amp;nbsp;() Help/Unsubscribe/Update your Subscription: This email sent to</body>
  </mail>
  <mail>
    <header>Re: another workflow question. OOP with QC :)</header>
    <body>Quick note: still reading through your case scenarios; the feedback patch will work inside of the iterator if you don&amp;#39;t have consumer patches. The results can then be gathered in a queue and published out, where the values can then be iterated through again if necessary. Not saying this is necessarily always efficient, but it&amp;#39;s possible. -GT ¬†_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list ¬† ¬† ¬†() Help/Unsubscribe/Update your Subscription: This email sent to</body>
  </mail>
  <mail>
    <header>another workflow question. OOP with QC :)</header>
    <body>Attachment:</body>
  </mail>
  <mail>
    <header>Re: Snow Leopard draws different colors from Leopard and before</header>
    <body>wrote: I'd be more inclined to believe it is your bitmap that is drawing differently. Using CGContextSetRGBFillColor is tantamount to telling Quartz &amp;quot;I know what I'm doing. Shove these exact pixel values down to the hardware.&amp;quot; Whereas images have their own color profiles, which Quartz uses to perform color correction. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: Snow Leopard draws different colors from Leopard and before</header>
    <body>&amp;gt;Read the documentation for CGContextSetRGBFillColor.  This function But why CGContextSetFillColor() writes different color value when system default display gamma is set to 1.8 and 2.2 ?  If it is 1.8, CGContextSetFillColor() sets the same value as UInt32 colorData = ((UInt32)(rgb.red &amp;amp; 0xFF00 ) &amp;lt;&amp;lt; 8) + ((UInt32)(rgb. if &amp;quot;Quartz sets the current fill color space to DeviceRGB.&amp;quot; in CGContextSetRGBFillColor reference means default display device dependent, I cannot use quartz to bitmapcontext destine to picture file or printer. Yoshiaki Katayanagi</body>
  </mail>
  <mail>
    <header>Re: Snow Leopard draws different colors from Leopard and before</header>
    <body>Read the documentation for CGContextSetRGBFillColor.  This function discards the colorspace information associated with the graphics context. You will want to convert the blue color from the generic RGB space to Device RGB space before calling CGContextSetRGBFillColor. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: Snow Leopard draws different colors from Leopard and before</header>
    <body>Thanks Matt, The issue is not &amp;quot;darker&amp;quot; level, but draw different colors. See example picture here. I misunderstood the result. Color (B) in the picture is correct, not color (A). Color (A) is drawn as background color using CGContextFillRect(). I chenged to not use this function here, and result is OK. Anyway CGContextFillRect() draws invalid color onto CGBitmapContext if display gamma is set to 2.2 in both 10.5 and 10.6. I will file a bug report. Yoshiaki Katayanagi</body>
  </mail>
  <mail>
    <header>QCCompositionLayer timing in Snow Leopard</header>
    <body>I recently wrote a subclass of QCCompositionLayer to control its timing to be equivalent to QCView. The default QCCompositionLayer speed was too fast and I always needed the time to start at 0. This worked fine in Leopard, but now in Snow Leopard I cannot get it to work again. I have overridden every method I can think of (including private methods) and cannot force it to start at time 0. Instead it uses the built-in layer timing which I do not want. I have played around with beginTime, speed, timeOffset, convertTime:fromLayer:, convertTime:toLayer:, no luck. Even overriding renderAtTime:arguments: (a private method) does not work. The way I had it working for Leopard was overriding drawInCGLContext:pixelFormat:forLayerTime:displayTime: and calling super with my own layer time value. This allowed me to implement the missing (why?) pause functionality. Why isn't this working anymore? What is the workaround? I am hoping I am just missing something small. Otherwise I think I'll have to write my own using QCRenderer, which I've done before with NSView but would rather avoid. Thanks, Kevin</body>
  </mail>
  <mail>
    <header>Re: Crashing in CAViewEndDraw</header>
    <body>I'm stripping it down to a small test case now.  Thanks. -josh</body>
  </mail>
  <mail>
    <header>Re: Crashing in CAViewEndDraw</header>
    <body>For now it is probably most prudent to file a bug with a reproducible case attached, if only so we can work in parallel. -- David Duncan</body>
  </mail>
  <mail>
    <header>Fwd: Crashing in CAViewEndDraw</header>
    <body>Hi, Our app has been crashing a bunch inside CAViewEndDraw.  I see that there was a bug related to this fixed in 10.6.3 (  ), but we're seeing this crash in 10.6.4.  We first saw these crashes shortly after we added some CAShapeLayers to our app.  The shape layers are essentially grid lines and row stripes for an irregular grid inside a very large scrollable region. At first it happened quite often, which was the result of us not freeing the CGPath after assigning it to the CAShapeLayer's path.  I'm not sure why that leak would lead to crashing, but it did get much better.  Once we fixed that the crashes became pretty rare, but some users are still getting this daily.  We were able to reduce the frequency of the crash even more by using several CAShapeLayers (one for every 100 lines) instead of one CAShapeLayer with a all of the grid lines in one path, but we haven't been able to eliminate it entirely.  It feels like we're exceeding the limits of some hidden buffer somewhere. Any advice on what we might be doing to trigger this?  I'm not sure if this is my bug or Apple's. -josh Path:            /Applications/ZZZ.app/Contents/MacOS/ZZZ Version:         ??? (0.9.11) Code Type:       X86-64 (Native) Parent Process:  launchd [484] Date/Time:       2010-11-12 13:59:58.467 -0800 OS Version:      Mac OS X 10.6.4 (10F569) Report Version:  6 Exception Type:  EXC_CRASH (SIGILL) Exception Codes: 0x0000000000000000, 0x0000000000000000 Crashed Thread:  0  Dispatch queue: com.apple.main-thread Thread 0 Crashed:  Dispatch queue: com.apple.main-thread 0   libSystem.B.dylib             	0x00007fff8291a342 semaphore_wait_signal_trap + 10 1   libSystem.B.dylib             	0x00007fff8291f88d pthread_mutex_lock + 469 2   com.apple.QuartzCore          	0x00007fff83789a9e CAViewEndDraw + 98 3   com.apple.AppKit              	0x00007fff86236adb -[NSView _displayRectIgnoringOpacity:isVisibleRect:rectIsVisibleRectForView:] + 3516 4   com.apple.AppKit              	0x00007fff861afff6 -[NSView displayIfNeeded] + 969 5   com.apple.AppKit              	0x00007fff861aaea2 _handleWindowNeedsDisplay + 678 6   com.apple.CoreFoundation      	0x00007fff80f2f077 __CFRunLoopDoObservers + 519 7   com.apple.CoreFoundation      	0x00007fff80f0aef4 __CFRunLoopRun + 468 8   com.apple.CoreFoundation      	0x00007fff80f0a84f CFRunLoopRunSpecific + 575 9   com.apple.HIToolbox           	0x00007fff86b6191a RunCurrentEventLoopInMode + 333 10  com.apple.HIToolbox           	0x00007fff86b6167d ReceiveNextEventCommon + 148 11  com.apple.HIToolbox           	0x00007fff86b615d8 BlockUntilNextEventMatchingListInMode + 59 12  com.apple.AppKit              	0x00007fff8618029e _DPSNextEvent + 708 13  com.apple.AppKit              	0x00007fff8617fbed -[NSApplication nextEventMatchingMask:untilDate:inMode:dequeue:] + 155 14  com.apple.AppKit              	0x00007fff861458d3 -[NSApplication run] + 395 15  com.apple.AppKit              	0x00007fff8613e5f8 NSApplicationMain + 364 16  com.mycompany.ZZZ		     	0x00000001000019a1 main + 549 (main.m:61) 17  com.mycompany.ZZZ		     	0x0000000100001774 start + 52 Thread 1:  Dispatch queue: com.apple.libdispatch-manager 0   libSystem.B.dylib             	0x00007fff8293308a kevent + 10 1   libSystem.B.dylib             	0x00007fff82934f5d _dispatch_mgr_invoke + 154 2   libSystem.B.dylib             	0x00007fff82934c34 _dispatch_queue_invoke + 185 3   libSystem.B.dylib             	0x00007fff8293475e _dispatch_worker_thread2 + 252 4   libSystem.B.dylib             	0x00007fff82934088 _pthread_wqthread + 353 5   libSystem.B.dylib             	0x00007fff82933f25 start_wqthread + 13 Thread 2: 0   libSystem.B.dylib             	0x00007fff8291a2fa mach_msg_trap + 10 1   libSystem.B.dylib             	0x00007fff8291a96d mach_msg + 59 2   com.apple.CoreFoundation      	0x00007fff80f0b3c2 __CFRunLoopRun + 1698 3   com.apple.CoreFoundation      	0x00007fff80f0a84f CFRunLoopRunSpecific + 575 4   com.apple.Foundation          	0x00007fff81c6b4c3 +[NSURLConnection(NSURLConnectionReallyInternal) _resourceLoadLoop:] + 297 5   com.apple.Foundation          	0x00007fff81bebe8d __NSThread__main__ + 1429 6   libSystem.B.dylib             	0x00007fff82953456 _pthread_start + 331 7   libSystem.B.dylib             	0x00007fff82953309 thread_start + 13 Thread 3: 0   libSystem.B.dylib             	0x00007fff8295ddce select$DARWIN_EXTSN + 10 1   com.apple.CoreFoundation      	0x00007fff80f2ce92 __CFSocketManager + 818 2   libSystem.B.dylib             	0x00007fff82953456 _pthread_start + 331 3   libSystem.B.dylib             	0x00007fff82953309 thread_start + 13 Thread 4: 0   com.apple.QuartzCore          	0x00007fff83966014 CA::OGL::fill_path(CA::OGL::Context&amp;amp;, CA::OGL::Layer const*, CA::Render::Path*, CA::ScanConverter::FillRule, CA::OGL::Gstate const&amp;amp;, CA::Bounds const&amp;amp;) + 1908 1   com.apple.QuartzCore          	0x00007fff839669e3 CA::OGL::render_path(CA::OGL::Context&amp;amp;, CA::OGL::Layer const*, CA::Render::Path*, CA::ScanConverter::FillRule, CGRect const&amp;amp;, unsigned int, CA::Render::Pattern*) + 769 2   com.apple.QuartzCore          	0x00007fff83966bf0 CA::OGL::render_shape(CA::OGL::Context&amp;amp;, CA::OGL::Layer const*, CA::Render::ShapeLayer*) + 97 3   com.apple.QuartzCore          	0x00007fff837a365c CA::OGL::LayerNode::apply(CA::OGL::Surface**) + 128 4   com.apple.QuartzCore          	0x00007fff837a34a5 CA::OGL::ImagingNode::render(CA::OGL::ImagingNode::RenderClosure*) + 143 5   com.apple.QuartzCore          	0x00007fff837a2e3a CA::OGL::render_layers(CA::OGL::Context&amp;amp;, CA::OGL::Layer*, CA::OGL::Gstate const&amp;amp;) + 981 6   com.apple.QuartzCore          	0x00007fff837a36a9 CA::OGL::LayerNode::apply(CA::OGL::Surface**) + 205 7   com.apple.QuartzCore          	0x00007fff837a34a5 CA::OGL::ImagingNode::render(CA::OGL::ImagingNode::RenderClosure*) + 143 8   com.apple.QuartzCore          	0x00007fff837a2e3a CA::OGL::render_layers(CA::OGL::Context&amp;amp;, CA::OGL::Layer*, CA::OGL::Gstate const&amp;amp;) + 981 9   com.apple.QuartzCore          	0x00007fff837a36a9 CA::OGL::LayerNode::apply(CA::OGL::Surface**) + 205 10  com.apple.QuartzCore          	0x00007fff837a34a5 CA::OGL::ImagingNode::render(CA::OGL::ImagingNode::RenderClosure*) + 143 11  com.apple.QuartzCore          	0x00007fff837a2e3a CA::OGL::render_layers(CA::OGL::Context&amp;amp;, CA::OGL::Layer*, CA::OGL::Gstate const&amp;amp;) + 981 12  com.apple.QuartzCore          	0x00007fff837a36a9 CA::OGL::LayerNode::apply(CA::OGL::Surface**) + 205 13  com.apple.QuartzCore          	0x00007fff837a34a5 CA::OGL::ImagingNode::render(CA::OGL::ImagingNode::RenderClosure*) + 143 14  com.apple.QuartzCore          	0x00007fff837a2e3a CA::OGL::render_layers(CA::OGL::Context&amp;amp;, CA::OGL::Layer*, CA::OGL::Gstate const&amp;amp;) + 981 15  com.apple.QuartzCore          	0x00007fff837a36a9 CA::OGL::LayerNode::apply(CA::OGL::Surface**) + 205 16  com.apple.QuartzCore          	0x00007fff837a34a5 CA::OGL::ImagingNode::render(CA::OGL::ImagingNode::RenderClosure*) + 143 17  com.apple.QuartzCore          	0x00007fff837a2e3a CA::OGL::render_layers(CA::OGL::Context&amp;amp;, CA::OGL::Layer*, CA::OGL::Gstate const&amp;amp;) + 981 18  com.apple.QuartzCore          	0x00007fff837a36a9 CA::OGL::LayerNode::apply(CA::OGL::Surface**) + 205 19  com.apple.QuartzCore          	0x00007fff837a34a5 CA::OGL::ImagingNode::render(CA::OGL::ImagingNode::RenderClosure*) + 143 20  com.apple.QuartzCore          	0x00007fff837a2e3a CA::OGL::render_layers(CA::OGL::Context&amp;amp;, CA::OGL::Layer*, CA::OGL::Gstate const&amp;amp;) + 981 21  com.apple.QuartzCore          	0x00007fff8379fe1a CA::OGL::Context::render(CA::Render::Update*) + 580 22  com.apple.QuartzCore          	0x00007fff83799a45 view_draw(_CAView*, double, CVTimeStamp const*, bool) + 2702 23  com.apple.QuartzCore          	0x00007fff83798f60 view_display_link(double, CVTimeStamp const*, void*) + 64 24  com.apple.QuartzCore          	0x00007fff83798ea7 link_callback + 219 25  com.apple.CoreVideo           	0x00007fff81b8221b CVDisplayLink::performIO(CVTimeStamp*) + 431 26  com.apple.CoreVideo           	0x00007fff81b81480 CVDisplayLink::runIOThread() + 754 27  com.apple.CoreVideo           	0x00007fff81b81153 startIOThread(void*) + 139 28  libSystem.B.dylib             	0x00007fff82953456 _pthread_start + 331 29  libSystem.B.dylib             	0x00007fff82953309 thread_start + 13 Thread 5: 0   libSystem.B.dylib             	0x00007fff82933eaa __workq_kernreturn + 10 1   libSystem.B.dylib             	0x00007fff829342bc _pthread_wqthread + 917 2   libSystem.B.dylib             	0x00007fff82933f25 start_wqthread + 13 Thread 0 crashed with X86 Thread State (64-bit): rax: 0x000000000000000e  rbx: 0x00000001003c46b0  rcx: 0x00007fff5fbfe4a8  rdx: 0x000000010036e650 rdi: 0x0000000000005103  rsi: 0x0000000000005203  rbp: 0x00007fff5fbfe4e0  rsp: 0x00007fff5fbfe4a8 r8: 0x00007fff70704c00   r9: 0x0000000000000001  r10: 0x0000000000000001  r11: 0x0000000000000202 r12: 0x0000000000005203  r13: 0x0000000000005103  r14: 0x00007fff706f8630  r15: 0x0000000012141968 rip: 0x00007fff8291a342  rfl: 0x0000000000000202  cr2: 0x000000010bf1100c Binary Images: 0x100000000 -        0x100092ff7 +com.mycompany.ZZZ ??? (0.9.11) &amp;lt;E19D60B3-DDAE-FE01-8B91-442BC7292BA9&amp;gt; /Applications/PixApps/ZZZ.app/Contents/MacOS/ZZZ 0x100120000 -        0x100146ff7 +com.brandonwalkin.BWToolkitFramework ??? (1.2.5) &amp;lt;CFAF6D1C-3A3A-5795-CB60-4973465C8CF7&amp;gt; /Applications/PixApps/ZZZ.app/Contents/Frameworks/BWToolkitFramework.framework/Versions/A/BWToolkitFramework 0x10017a000 -        0x1001a4fef +com.mycompany.Utilities ??? (1.0) &amp;lt;C456E25A-8A40-2E37-BE56-CD4139DF92FB&amp;gt; /Applications/PixApps/ZZZ.app/Contents/Frameworks/ZZZUtilities.framework/Versions/A/ZZZUtilities 0x1001d8000 -        0x1001ddff7 +com.divisiblebyzero.XMLRPC 1.0 (1.0) &amp;lt;CEBE53CB-38EF-AEFE-EC25-55E1CF534F4D&amp;gt; /Applications/PixApps/ZZZ.app/Contents/Frameworks/ZZZUtilities.framework/Versions/A/Frameworks/XMLRPC.framework/Versions/A/XMLRPC 0x10ccab000 -        0x10ccd1fff  GLRendererFloat ??? (???) &amp;lt;10AFC7E0-A5CE-8F0E-7084-439BE59F7E95&amp;gt; /System/Library/Frameworks/OpenGL.framework/Resources/GLRendererFloat.bundle/GLRendererFloat 0x10d908000 -        0x10da95fe7  GLEngine ??? (???) &amp;lt;57D733C2-F7CB-2B8F-CD34-C85A193145DE&amp;gt; /System/Library/Frameworks/OpenGL.framework/Resources/GLEngine.bundle/GLEngine 0x10dac6000 -        0x10dd10fe7  com.apple.ATIRadeonX1000GLDriver 1.6.16 (6.1.6) &amp;lt;B152C8F0-F0A4-BB0D-F509-114407DC2AED&amp;gt; /System/Library/Extensions/ATIRadeonX1000GLDriver.bundle/Contents/MacOS/ATIRadeonX1000GLDriver 0x7fff5fc00000 -     0x7fff5fc3bdef  dyld 132.1 (???) &amp;lt;B536F2F1-9DF1-3B6C-1C2C-9075EA219A06&amp;gt; /usr/lib/dyld 0x7fff8006e000 -     0x7fff80148ff7  com.apple.vImage 4.0 (4.0) &amp;lt;354F34BF-B221-A3C9-2CA7-9BE5E14AD5AD&amp;gt; /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vImage.framework/Versions/A/vImage 0x7fff80149000 -     0x7fff80192fef  libGLU.dylib ??? (???) &amp;lt;34D118CD-F9EE-D023-FEBF-74581DEF31CD&amp;gt; /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGLU.dylib 0x7fff80288000 -     0x7fff80288ff7  com.apple.Carbon 150 (152) &amp;lt;19B37B7B-1594-AD0A-7F14-FA2F85AD7241&amp;gt; /System/Library/Frameworks/Carbon.framework/Versions/A/Carbon 0x7fff80289000 -     0x7fff8030efff  com.apple.print.framework.PrintCore 6.2 (312.5) &amp;lt;C20F87CE-ACC1-552B-8A73-2B3846A01D80&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/PrintCore.framework/Versions/A/PrintCore 0x7fff8030f000 -     0x7fff8030fff7  com.apple.CoreServices 44 (44) &amp;lt;210A4C56-BECB-E3E4-B6EE-7EC53E02265D&amp;gt; /System/Library/Frameworks/CoreServices.framework/Versions/A/CoreServices 0x7fff80310000 -     0x7fff8031dfe7  libCSync.A.dylib 543.50.0 (compatibility 64.0.0) &amp;lt;7B891D4C-1F19-4DB0-FD12-7A7D5E8F47AE&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/CoreGraphics.framework/Versions/A/Resources/libCSync.A.dylib 0x7fff804d7000 -     0x7fff8091afef  libLAPACK.dylib 219.0.0 (compatibility 1.0.0) &amp;lt;0CC61C98-FF51-67B3-F3D8-C5E430C201A9&amp;gt; /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib 0x7fff8095a000 -     0x7fff80970fef  libbsm.0.dylib ??? (???) &amp;lt;42D3023A-A1F7-4121-6417-FCC6B51B3E90&amp;gt; /usr/lib/libbsm.0.dylib 0x7fff809ef000 -     0x7fff809f2ff7  com.apple.securityhi 4.0 (36638) &amp;lt;38935851-09E4-DDAB-DB1D-30ADC39F7ED0&amp;gt; /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/SecurityHI.framework/Versions/A/SecurityHI 0x7fff809f3000 -     0x7fff80a11fff  libPng.dylib ??? (???) &amp;lt;F6932C8D-E6B1-0871-B698-15180AA948F7&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ImageIO.framework/Versions/A/Resources/libPng.dylib 0x7fff80a12000 -     0x7fff80a53fef  com.apple.QD 3.35 (???) &amp;lt;78C9A560-E6F7-DC4F-F85E-E63CF8A98F0B&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/QD.framework/Versions/A/QD 0x7fff80a54000 -     0x7fff80a9cff7  libvDSP.dylib 268.0.1 (compatibility 1.0.0) &amp;lt;98FC4457-F405-0262-00F7-56119CA107B6&amp;gt; /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvDSP.dylib 0x7fff80ab9000 -     0x7fff80b27fff  com.apple.AppleVAFramework 4.9.20 (4.9.20) &amp;lt;78727165-8D44-0354-6F6C-68FD798E04A1&amp;gt; /System/Library/PrivateFrameworks/AppleVA.framework/Versions/A/AppleVA 0x7fff80d77000 -     0x7fff80e9cfef  com.apple.audio.toolbox.AudioToolbox 1.6.3 (1.6.3) &amp;lt;4DCCD01F-7516-4240-09DC-EE553317D345&amp;gt; /System/Library/Frameworks/AudioToolbox.framework/Versions/A/AudioToolbox 0x7fff80e9d000 -     0x7fff80ebefff  libresolv.9.dylib 41.0.0 (compatibility 1.0.0) &amp;lt;6993F348-428F-C97E-7A84-7BD2EDC46A62&amp;gt; /usr/lib/libresolv.9.dylib 0x7fff80ebf000 -     0x7fff81034ff7  com.apple.CoreFoundation 6.6.3 (550.29) &amp;lt;48810602-63C3-994D-E563-DD02B16E76E1&amp;gt; /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation 0x7fff81035000 -     0x7fff810f5fff  libFontParser.dylib ??? (???) &amp;lt;A4F8189D-1D5B-2F8D-E78E-6D934A8E8407&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/Resources/libFontParser.dylib 0x7fff8110a000 -     0x7fff8111fff7  com.apple.LangAnalysis 1.6.6 (1.6.6) &amp;lt;DC999B32-BF41-94C8-0583-27D9AB463E8B&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/LangAnalysis.framework/Versions/A/LangAnalysis 0x7fff8119c000 -     0x7fff8119fff7  libCoreVMClient.dylib ??? (???) &amp;lt;DBB2C09F-4BF4-326C-B775-B7A128C501E3&amp;gt; /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCoreVMClient.dylib 0x7fff811c5000 -     0x7fff8127bfff  libobjc.A.dylib 227.0.0 (compatibility 1.0.0) &amp;lt;F206BE6D-8777-AE6C-B367-7BEA76C14241&amp;gt; /usr/lib/libobjc.A.dylib 0x7fff8127c000 -     0x7fff8128efe7  libsasl2.2.dylib 3.15.0 (compatibility 3.0.0) &amp;lt;76B83C8D-8EFE-4467-0F75-275648AFED97&amp;gt; /usr/lib/libsasl2.2.dylib 0x7fff8148b000 -     0x7fff8149aff7  com.apple.opengl 1.6.8 (1.6.8) &amp;lt;0CDC4F98-7981-A114-1778-AF171075138E&amp;gt; /System/Library/Frameworks/OpenGL.framework/Versions/A/OpenGL 0x7fff817dc000 -     0x7fff817ddff7  com.apple.TrustEvaluationAgent 1.1 (1) &amp;lt;74800EE8-C14C-18C9-C208-20BBDB982D40&amp;gt; /System/Library/PrivateFrameworks/TrustEvaluationAgent.framework/Versions/A/TrustEvaluationAgent 0x7fff818b7000 -     0x7fff818cbff7  com.apple.speech.synthesis.framework 3.10.35 (3.10.35) &amp;lt;621B7415-A0B9-07A7-F313-36BEEDD7B132&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/SpeechSynthesis.framework/Versions/A/SpeechSynthesis 0x7fff818cc000 -     0x7fff818f7ff7  libxslt.1.dylib 3.24.0 (compatibility 3.0.0) &amp;lt;87A0B228-B24A-C426-C3FB-B40D7258DD49&amp;gt; /usr/lib/libxslt.1.dylib 0x7fff818f8000 -     0x7fff818fafef  com.apple.ExceptionHandling 1.5 (10) &amp;lt;F2867B93-A56A-974F-9556-266BCE394057&amp;gt; /System/Library/Frameworks/ExceptionHandling.framework/Versions/A/ExceptionHandling 0x7fff818fb000 -     0x7fff819aafff  edu.mit.Kerberos 6.5.10 (6.5.10) &amp;lt;F3F76EDF-5660-78F0-FE6E-33B6174F55A4&amp;gt; /System/Library/Frameworks/Kerberos.framework/Versions/A/Kerberos 0x7fff81a3c000 -     0x7fff81a4dff7  libz.1.dylib 1.2.3 (compatibility 1.0.0) &amp;lt;FB5EE53A-0534-0FFA-B2ED-486609433717&amp;gt; /usr/lib/libz.1.dylib 0x7fff81a50000 -     0x7fff81a54ff7  libmathCommon.A.dylib 315.0.0 (compatibility 1.0.0) &amp;lt;95718673-FEEE-B6ED-B127-BCDBDB60D4E5&amp;gt; /usr/lib/system/libmathCommon.A.dylib 0x7fff81a5e000 -     0x7fff81aaffe7  com.apple.HIServices 1.8.0 (???) &amp;lt;1ABA7802-C1E4-06A0-9035-2792CC915BF6&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/HIServices.framework/Versions/A/HIServices 0x7fff81b7d000 -     0x7fff81b7eff7  com.apple.audio.units.AudioUnit 1.6.3 (1.6.3) &amp;lt;D4183AC4-8A65-8368-A9AF-E2A13D18519C&amp;gt; /System/Library/Frameworks/AudioUnit.framework/Versions/A/AudioUnit 0x7fff81b7f000 -     0x7fff81ba4ff7  com.apple.CoreVideo 1.6.1 (45.5) &amp;lt;29F39070-5CEF-B286-66E3-7CC903519403&amp;gt; /System/Library/Frameworks/CoreVideo.framework/Versions/A/CoreVideo 0x7fff81bdb000 -     0x7fff81e5cfef  com.apple.Foundation 6.6.3 (751.29) &amp;lt;DAEDB589-9F59-9556-CF8D-07556317937B&amp;gt; /System/Library/Frameworks/Foundation.framework/Versions/C/Foundation 0x7fff81ea9000 -     0x7fff81ebffff  com.apple.ImageCapture 6.0 (6.0) &amp;lt;5B5AF8FB-C12A-B51F-94FC-3EC4698E818E&amp;gt; /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/ImageCapture.framework/Versions/A/ImageCapture 0x7fff8212a000 -     0x7fff8215bfff  libGLImage.dylib ??? (???) &amp;lt;2F18DB77-CF77-1311-9E20-FE460090C166&amp;gt; /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGLImage.dylib 0x7fff8215c000 -     0x7fff823e2ff7  com.apple.security 6.1.1 (37594) &amp;lt;5EDDC08C-C95B-2D24-E1D2-D30D233AB065&amp;gt; /System/Library/Frameworks/Security.framework/Versions/A/Security 0x7fff82428000 -     0x7fff8242afff  com.apple.print.framework.Print 6.1 (237.1) &amp;lt;4513DB2F-737C-B43C-2D0E-23CD6E838014&amp;gt; /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Print.framework/Versions/A/Print 0x7fff8242b000 -     0x7fff8243afff  com.apple.NetFS 3.2.1 (3.2.1) &amp;lt;0357C371-2E2D-069C-08FB-1180512B8516&amp;gt; /System/Library/Frameworks/NetFS.framework/Versions/A/NetFS 0x7fff824b2000 -     0x7fff824b3fff  liblangid.dylib ??? (???) &amp;lt;EA4D1607-2BD5-2EE2-2A3B-632EEE5A444D&amp;gt; /usr/lib/liblangid.dylib 0x7fff824b4000 -     0x7fff82672fff  libicucore.A.dylib 40.0.0 (compatibility 1.0.0) &amp;lt;0E53A4A6-AC06-1B61-2285-248F534EE356&amp;gt; /usr/lib/libicucore.A.dylib 0x7fff8267f000 -     0x7fff8267fff7  com.apple.Accelerate 1.6 (Accelerate 1.6) &amp;lt;15DF8B4A-96B2-CB4E-368D-DEC7DF6B62BB&amp;gt; /System/Library/Frameworks/Accelerate.framework/Versions/A/Accelerate 0x7fff827f2000 -     0x7fff8280bfff  com.apple.CFOpenDirectory 10.6 (10.6) &amp;lt;0F46E102-8B8E-0995-BA85-3D9608F0A30C&amp;gt; /System/Library/Frameworks/OpenDirectory.framework/Versions/A/Frameworks/CFOpenDirectory.framework/Versions/A/CFOpenDirectory 0x7fff82846000 -     0x7fff82895ff7  com.apple.DirectoryService.PasswordServerFramework 6.0 (6.0) &amp;lt;E6A9626A-E42D-7DB2-FC37-CF95F75342F7&amp;gt; /System/Library/PrivateFrameworks/PasswordServer.framework/Versions/A/PasswordServer 0x7fff82919000 -     0x7fff82ad9fef  libSystem.B.dylib 125.2.0 (compatibility 1.0.0) &amp;lt;95E02DD0-ADEA-745B-E7FA-ABA064E4658C&amp;gt; /usr/lib/libSystem.B.dylib 0x7fff82ada000 -     0x7fff82bf3fef  libGLProgrammability.dylib ??? (???) &amp;lt;B057FC52-6A97-F450-48D8-325A70423A53&amp;gt; /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGLProgrammability.dylib 0x7fff82bf4000 -     0x7fff82bf9fff  libGFXShared.dylib ??? (???) &amp;lt;1265FAEF-1C97-B339-28A4-4510589B067B&amp;gt; /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGFXShared.dylib 0x7fff82c3e000 -     0x7fff82e28fe7  com.apple.JavaScriptCore 6533 (6533.13) &amp;lt;C2D33B72-06F1-B789-6FD3-FB09EEB6C907&amp;gt; /System/Library/Frameworks/JavaScriptCore.framework/Versions/A/JavaScriptCore 0x7fff82e7d000 -     0x7fff82ec4ff7  com.apple.coreui 2 (114) &amp;lt;D7645B59-0431-6283-7322-957D944DAB21&amp;gt; /System/Library/PrivateFrameworks/CoreUI.framework/Versions/A/CoreUI 0x7fff82ec5000 -     0x7fff82fabfe7  com.apple.DesktopServices 1.5.7 (1.5.7) &amp;lt;8A697128-B6CA-E4A8-C200-6520D5A35FBE&amp;gt; /System/Library/PrivateFrameworks/DesktopServicesPriv.framework/Versions/A/DesktopServicesPriv 0x7fff82fac000 -     0x7fff83038fef  SecurityFoundation ??? (???) &amp;lt;6860DE26-0D42-D1E8-CD7C-5B42D78C1E1D&amp;gt; /System/Library/Frameworks/SecurityFoundation.framework/Versions/A/SecurityFoundation 0x7fff8309a000 -     0x7fff83250fef  com.apple.ImageIO.framework 3.0.3 (3.0.3) &amp;lt;A32D0B5A-7149-7739-22D3-84D38B07E9E5&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ImageIO.framework/Versions/A/ImageIO 0x7fff83251000 -     0x7fff832cffff  com.apple.CoreText 3.1.0 (???) &amp;lt;B740DA1D-EFD0-CCBF-F893-E3004FE58A98&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/CoreText.framework/Versions/A/CoreText 0x7fff83318000 -     0x7fff8331eff7  com.apple.DiskArbitration 2.3 (2.3) &amp;lt;857F6E43-1EF4-7D53-351B-10DE0A8F992A&amp;gt; /System/Library/Frameworks/DiskArbitration.framework/Versions/A/DiskArbitration 0x7fff8331f000 -     0x7fff83324ff7  com.apple.CommonPanels 1.2.4 (91) &amp;lt;4D84803B-BD06-D80E-15AE-EFBE43F93605&amp;gt; /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/CommonPanels.framework/Versions/A/CommonPanels 0x7fff83392000 -     0x7fff833d3ff7  com.apple.CoreMedia 0.484.11 (484.11) &amp;lt;AEE7E9C9-9604-B0A7-053B-28954659CFE3&amp;gt; /System/Library/PrivateFrameworks/CoreMedia.framework/Versions/A/CoreMedia 0x7fff833d4000 -     0x7fff833f7fff  com.apple.opencl 12.1 (12.1) &amp;lt;403E8F37-4348-B9BC-08E6-7693A995B7EC&amp;gt; /System/Library/Frameworks/OpenCL.framework/Versions/A/OpenCL 0x7fff833f8000 -     0x7fff8340eff7  com.apple.MultitouchSupport.framework 204.13 (204.13) &amp;lt;BFFEC259-F103-B25A-BB52-1AA79116DDBA&amp;gt; /System/Library/PrivateFrameworks/MultitouchSupport.framework/Versions/A/MultitouchSupport 0x7fff83452000 -     0x7fff834a1fef  libTIFF.dylib ??? (???) &amp;lt;A66CBA9C-A38D-5EDB-BFB5-CB398F033D6F&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ImageIO.framework/Versions/A/Resources/libTIFF.dylib 0x7fff834a2000 -     0x7fff8356dfe7  ColorSyncDeprecated.dylib 4.6.0 (compatibility 1.0.0) &amp;lt;3C223A94-EF14-28C5-844B-C25DFC87FB42&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Frameworks/ColorSync.framework/Versions/A/Resources/ColorSyncDeprecated.dylib 0x7fff8356e000 -     0x7fff835fefff  com.apple.SearchKit 1.3.0 (1.3.0) &amp;lt;4175DC31-1506-228A-08FD-C704AC9DF642&amp;gt; /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/SearchKit.framework/Versions/A/SearchKit 0x7fff8360b000 -     0x7fff83633fff  com.apple.DictionaryServices 1.1.1 (1.1.1) &amp;lt;9FD709FC-23F0-F270-EAC1-C590CD516A36&amp;gt; /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/DictionaryServices.framework/Versions/A/DictionaryServices 0x7fff8373f000 -     0x7fff83ad8ff7  com.apple.QuartzCore 1.6.2 (227.22) &amp;lt;76EE0A32-B20B-F316-ADDD-4230329253D5&amp;gt; /System/Library/Frameworks/QuartzCore.framework/Versions/A/QuartzCore 0x7fff83ad9000 -     0x7fff83b1efff  com.apple.CoreMediaIOServices 130.0 (1035) &amp;lt;567D7949-3115-4E78-8F27-B28968CC25F7&amp;gt; /System/Library/PrivateFrameworks/CoreMediaIOServices.framework/Versions/A/CoreMediaIOServices 0x7fff83c89000 -     0x7fff83c8dff7  libCGXType.A.dylib 543.50.0 (compatibility 64.0.0) &amp;lt;E666EBC7-2D87-A3C6-9461-A596B4E53593&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/CoreGraphics.framework/Versions/A/Resources/libCGXType.A.dylib 0x7fff83e84000 -     0x7fff83ec1ff7  libFontRegistry.dylib ??? (???) &amp;lt;B63FCC3A-F49E-B42E-6D57-5F59E3A8D8B9&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/Resources/libFontRegistry.dylib 0x7fff83ec2000 -     0x7fff83ec8fff  libCGXCoreImage.A.dylib 543.50.0 (compatibility 64.0.0) &amp;lt;2D72D55A-C8FE-78DD-602E-E934057EDF95&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/CoreGraphics.framework/Versions/A/Resources/libCGXCoreImage.A.dylib 0x7fff83ecb000 -     0x7fff83fe2fef  libxml2.2.dylib 10.3.0 (compatibility 10.0.0) &amp;lt;EE067D7E-15B3-F043-6FBD-10BA31FE76C7&amp;gt; /usr/lib/libxml2.2.dylib 0x7fff83fe3000 -     0x7fff8401bfef  libcups.2.dylib 2.8.0 (compatibility 2.0.0) &amp;lt;31A78904-A500-0DA9-0609-F1EB81383326&amp;gt; /usr/lib/libcups.2.dylib 0x7fff8401c000 -     0x7fff8418bfe7  com.apple.QTKit 7.6.6 (1742) &amp;lt;7E254184-757D-E87C-5B2A-7612A2C85243&amp;gt; /System/Library/Frameworks/QTKit.framework/Versions/A/QTKit 0x7fff8418c000 -     0x7fff8429bfe7  libcrypto.0.9.8.dylib 0.9.8 (compatibility 0.9.8) &amp;lt;36DA89A6-3AF5-86F2-BDD5-B94C7C0844D4&amp;gt; /usr/lib/libcrypto.0.9.8.dylib 0x7fff842d3000 -     0x7fff842e7fff  libGL.dylib ??? (???) &amp;lt;5AD69545-D1A3-C017-C7AF-B4AFD6F08FA2&amp;gt; /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGL.dylib 0x7fff84364000 -     0x7fff84419fe7  com.apple.ink.framework 1.3.3 (107) &amp;lt;FFC46EE0-3544-A459-2AB9-94778A75E3D4&amp;gt; /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Ink.framework/Versions/A/Ink 0x7fff8467a000 -     0x7fff84d77067  com.apple.CoreGraphics 1.543.50 (???) &amp;lt;46A7D60C-0500-B96C-ECAD-1D658487D213&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/CoreGraphics.framework/Versions/A/CoreGraphics 0x7fff84d78000 -     0x7fff84d78ff7  com.apple.ApplicationServices 38 (38) &amp;lt;10A0B9E9-4988-03D4-FC56-DDE231A02C63&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/ApplicationServices 0x7fff84d88000 -     0x7fff84e28fff  com.apple.LaunchServices 362.1 (362.1) &amp;lt;2740103A-6C71-D99F-8C6F-FA264546AD8F&amp;gt; /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/LaunchServices.framework/Versions/A/LaunchServices 0x7fff84e59000 -     0x7fff84fd5ff7  com.apple.WebKit 6533 (6533.16) &amp;lt;B0C2B2F5-9BC1-5907-17A7-E2039C1EF57B&amp;gt; /System/Library/Frameworks/WebKit.framework/Versions/A/WebKit 0x7fff850d7000 -     0x7fff85112fff  com.apple.AE 496.4 (496.4) &amp;lt;CBEDB6A1-FD85-F842-4EB8-CC289FAE0F24&amp;gt; /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/AE.framework/Versions/A/AE 0x7fff85113000 -     0x7fff851ccfff  libsqlite3.dylib 9.6.0 (compatibility 9.0.0) &amp;lt;5A15E12A-AE8F-1A36-BBC7-564E7D7AD0FB&amp;gt; /usr/lib/libsqlite3.dylib 0x7fff851fd000 -     0x7fff85200fff  com.apple.help 1.3.1 (41) &amp;lt;54B79BA2-B71B-268E-8752-5C8EE00E49E4&amp;gt; /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Help.framework/Versions/A/Help 0x7fff85201000 -     0x7fff8529bfff  com.apple.ApplicationServices.ATS 4.3 (???) &amp;lt;A7CD9E1F-C563-E940-130D-AA7E08C5A29F&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/ATS 0x7fff8529c000 -     0x7fff8529cff7  com.apple.vecLib 3.6 (vecLib 3.6) &amp;lt;96FB6BAD-5568-C4E0-6FA7-02791A58B584&amp;gt; /System/Library/Frameworks/vecLib.framework/Versions/A/vecLib 0x7fff85353000 -     0x7fff85491fff  com.apple.CoreData 102.1 (251) &amp;lt;32233D4D-00B7-CE14-C881-6BF19FD05A03&amp;gt; /System/Library/Frameworks/CoreData.framework/Versions/A/CoreData 0x7fff85492000 -     0x7fff855baff7  com.apple.MediaToolbox 0.484.11 (484.11) &amp;lt;F50B5552-8527-C75D-873F-66A61D04E32A&amp;gt; /System/Library/PrivateFrameworks/MediaToolbox.framework/Versions/A/MediaToolbox 0x7fff855bb000 -     0x7fff855e1fe7  libJPEG.dylib ??? (???) &amp;lt;4060F3E2-BAD3-244F-D777-51BA16569DA4&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ImageIO.framework/Versions/A/Resources/libJPEG.dylib 0x7fff855ee000 -     0x7fff855f5fff  com.apple.OpenDirectory 10.6 (10.6) &amp;lt;72A65D76-7831-D31E-F1B3-9E48BF26A98B&amp;gt; /System/Library/Frameworks/OpenDirectory.framework/Versions/A/OpenDirectory 0x7fff85614000 -     0x7fff85660fff  libauto.dylib ??? (???) &amp;lt;072804DF-36AD-2DBE-7EF8-639CFB79077F&amp;gt; /usr/lib/libauto.dylib 0x7fff856b0000 -     0x7fff856d0ff7  com.apple.DirectoryService.Framework 3.6 (621.4) &amp;lt;969734C3-D21E-2F30-5CBB-D9F23D123643&amp;gt; /System/Library/Frameworks/DirectoryService.framework/Versions/A/DirectoryService 0x7fff85700000 -     0x7fff857d0ff7  com.apple.CFNetwork 454.9.7 (454.9.7) &amp;lt;AA6EB690-6CCF-603D-AAC2-35B9E05D1593&amp;gt; /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/CFNetwork.framework/Versions/A/CFNetwork 0x7fff857d1000 -     0x7fff857d1ff7  com.apple.Accelerate.vecLib 3.6 (vecLib 3.6) &amp;lt;4CCE5D69-F1B3-8FD3-1483-E0271DB2CCF3&amp;gt; /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/vecLib 0x7fff857d2000 -     0x7fff85cd6fe7  com.apple.VideoToolbox 0.484.11 (484.11) &amp;lt;4577FF14-E6A7-AAD8-E6E6-ECA9CFCC6989&amp;gt; /System/Library/PrivateFrameworks/VideoToolbox.framework/Versions/A/VideoToolbox 0x7fff85cd7000 -     0x7fff8604cfe7  com.apple.RawCamera.bundle 3.0.3 (529) &amp;lt;2E6B251A-C5A5-A3F9-832B-BB1958F938E9&amp;gt; /System/Library/CoreServices/RawCamera.bundle/Contents/MacOS/RawCamera 0x7fff8604d000 -     0x7fff860abff7  com.apple.framework.IOKit 2.0 (???) &amp;lt;010C3398-7363-8F4B-719C-263867F15F63&amp;gt; /System/Library/Frameworks/IOKit.framework/Versions/A/IOKit 0x7fff860bc000 -     0x7fff8613bfef  com.apple.audio.CoreAudio 3.2.2 (3.2.2) &amp;lt;243E456E-7A74-BE76-FF18-E589BDCAA785&amp;gt; /System/Library/Frameworks/CoreAudio.framework/Versions/A/CoreAudio 0x7fff8613c000 -     0x7fff86b32fff  com.apple.AppKit 6.6.6 (1038.29) &amp;lt;7BDD335D-5425-0354-5AD6-41C4F1B4A2F4&amp;gt; /System/Library/Frameworks/AppKit.framework/Versions/C/AppKit 0x7fff86b33000 -     0x7fff86e31fe7  com.apple.HIToolbox 1.6.3 (???) &amp;lt;CF0C8524-FA82-3908-ACD0-A9176C704AED&amp;gt; /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/HIToolbox.framework/Versions/A/HIToolbox 0x7fff87293000 -     0x7fff87293ff7  com.apple.Cocoa 6.6 (???) &amp;lt;68B0BE46-6E24-C96F-B341-054CF9E8F3B6&amp;gt; /System/Library/Frameworks/Cocoa.framework/Versions/A/Cocoa 0x7fff87294000 -     0x7fff872e9fef  com.apple.framework.familycontrols 2.0.1 (2010) &amp;lt;239940AC-2427-44C6-9E29-998D0ABECDF3&amp;gt; /System/Library/PrivateFrameworks/FamilyControls.framework/Versions/A/FamilyControls 0x7fff8732a000 -     0x7fff8732cfff  libRadiance.dylib ??? (???) &amp;lt;D67C08B6-4D4A-916D-E936-528E145A56E2&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ImageIO.framework/Versions/A/Resources/libRadiance.dylib 0x7fff8732d000 -     0x7fff87332fff  libGIF.dylib ??? (???) &amp;lt;21FC6B02-6AC3-C4DB-0B50-98144802274C&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ImageIO.framework/Versions/A/Resources/libGIF.dylib 0x7fff87333000 -     0x7fff87364fef  libTrueTypeScaler.dylib ??? (???) &amp;lt;0A30CA68-46AF-3E74-AE9E-693DB5A680CC&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/Resources/libTrueTypeScaler.dylib 0x7fff87365000 -     0x7fff873a8ff7  libRIP.A.dylib 543.50.0 (compatibility 64.0.0) &amp;lt;DF457CB3-CE61-0FD4-1403-BB68BC2CC998&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/CoreGraphics.framework/Versions/A/Resources/libRIP.A.dylib 0x7fff873a9000 -     0x7fff8745efe7  com.apple.ColorSync 4.6.3 (4.6.3) &amp;lt;AA93AD96-6974-9104-BF55-AF7A813C8A1B&amp;gt; /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ColorSync.framework/Versions/A/ColorSync 0x7fff8745f000 -     0x7fff87792fe7  com.apple.CoreServices.CarbonCore 861.13 (861.13) &amp;lt;BC2F9B4E-D305-D717-D97E-EC78C7DE9EE9&amp;gt; /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/CarbonCore.framework/Versions/A/CarbonCore 0x7fff877bd000 -     0x7fff87fc7fe7  libBLAS.dylib 219.0.0 (compatibility 1.0.0) &amp;lt;FC941ECB-71D0-FAE3-DCBF-C5A619E594B8&amp;gt; /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib 0x7fff87fc8000 -     0x7fff87fe3ff7  com.apple.openscripting 1.3.1 (???) &amp;lt;FD46A0FE-AC79-3EF7-AB4F-396D376DDE71&amp;gt; /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/OpenScripting.framework/Versions/A/OpenScripting 0x7fff87fe4000 -     0x7fff88025fff  com.apple.SystemConfiguration 1.10.2 (1.10.2) &amp;lt;BC27BDD4-9CC8-9AF0-B4C2-DD50FD751CBF&amp;gt; /System/Library/Frameworks/SystemConfiguration.framework/Versions/A/SystemConfiguration 0x7fff88026000 -     0x7fff880a3fef  libstdc++.6.dylib 7.9.0 (compatibility 7.0.0) &amp;lt;35ECA411-2C08-FD7D-11B1-1B7A04921A5C&amp;gt; /usr/lib/libstdc++.6.dylib 0x7fff880ee000 -     0x7fff88158fe7  libvMisc.dylib 268.0.1 (compatibility 1.0.0) &amp;lt;514D400C-50A5-C196-83AA-1035DDC8FBBE&amp;gt; /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib 0x7fff88189000 -     0x7fff88194ff7  com.apple.speech.recognition.framework 3.11.1 (3.11.1) &amp;lt;F0DDF27E-DB55-07CE-E548-C62095BE8167&amp;gt; /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/SpeechRecognition.framework/Versions/A/SpeechRecognition 0x7fff88195000 -     0x7fff881a3ff7  libkxld.dylib ??? (???) &amp;lt;EE840168-1F67-6219-8BA3-C46039BCC8B3&amp;gt; /usr/lib/system/libkxld.dylib 0x7fff881a4000 -     0x7fff88e13fe7  com.apple.WebCore 6533 (6533.16) &amp;lt;8F1CAEF4-34FC-49D0-3DF2-937EE69B05D8&amp;gt; /System/Library/Frameworks/WebKit.framework/Versions/A/Frameworks/WebCore.framework/Versions/A/WebCore 0x7fff88e14000 -     0x7fff88e5eff7  com.apple.Metadata 10.6.3 (507.10) &amp;lt;641395B7-FF2C-B94C-965A-CE6A0830645F&amp;gt; /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/Metadata.framework/Versions/A/Metadata 0x7fff88f25000 -     0x7fff88f2bff7  IOSurface ??? (???) &amp;lt;EB2019F6-7C5C-3D59-E11F-6119466C12A9&amp;gt; /System/Library/Frameworks/IOSurface.framework/Versions/A/IOSurface 0x7fff88f95000 -     0x7fff89052ff7  com.apple.CoreServices.OSServices 357 (357) &amp;lt;718F0719-DC9F-E392-7C64-9D7DFE3D02E2&amp;gt; /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/OSServices.framework/Versions/A/OSServices 0x7fffffe00000 -     0x7fffffe01fff  libSystem.B.dylib ??? (???) &amp;lt;95E02DD0-ADEA-745B-E7FA-ABA064E4658C&amp;gt; /usr/lib/libSystem.B.dylib</body>
  </mail>
  <mail>
    <header>Re: CALayer animations notification?</header>
    <body>Hmm, OK.  I haven't been using Blocks because I was still writing for iOS 3.1+, but now that the iOS 4.2 release is soon I am updating a lot of my old code.  It is interesting the timing of when the CATransaction is complete, which seems misleading. The issue is that my animations are using CATransactions instead of CAAnimation and so I will need to refactor my code in order to use CAAnimation. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CALayer animations notification?</header>
    <body>Please be aware of the problems with transaction completion blocks as pointed out in this thread: The point I tried to make in that thread is that the transaction is &amp;quot;complete&amp;quot; when -commit is called. It is the animations that live long past the transaction. If you want to know when an animation is complete, that is what the -animationDidStop:finished: delegate method on CAAnimation is for. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: CALayer animations notification?</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>CALayer animations notification?</header>
    <body>What is the best method to know when a CALayer has completed its animation?  Right now I am thinking of having a timer run through all movable layers and test if its Presentation Layer exists (not nil) and verifying that the presentation layer and model layer are at the same animation property.  For example if the animation is a move to a position, verify that both layers have the same position. Is there a better approach?</body>
  </mail>
  <mail>
    <header>Re: Looking for a good book on color blends</header>
    <body>You may not get great response because of some aspects of your question. -GT I have a need to do some image blending, and just re-read both of the Apple Quartz-2D sections that cover Quartz (essentially PDF) blending. These sections use contrived examples to show the effects. What I&amp;#39;m looking for is a book that covers real world uses of these blends - why you might employ one to accomplish some task. My usage would be more in the line of scientific applications - hiliting one portion of a larger image - not photographic manipulation. I assume recommendations would be PDF related, not Quartz. Suggestions most welcome! David</body>
  </mail>
  <mail>
    <header>Re: Looking for a good book on color blends</header>
    <body>2010/11/5 Michael Prenez-Isbell &amp;lt;email@hidden&amp;gt;: The book does't explain blend modes at all, AFAICS. Wouldn't be my recommendation.</body>
  </mail>
  <mail>
    <header>Re: Looking for a good book on color blends</header>
    <body>bunny laden book on quartz. Can't recommend it enough.</body>
  </mail>
  <mail>
    <header>Looking for a good book on color blends</header>
    <body>I have a need to do some image blending, and just re-read both of the Apple Quartz-2D sections that cover Quartz (essentially PDF) blending. These sections use contrived examples to show the effects. What I'm looking for is a book that covers real world uses of these blends - why you might employ one to accomplish some task. My usage would be more in the line of scientific applications - hiliting one portion of a larger image - not photographic manipulation. I assume recommendations would be PDF related, not Quartz. Suggestions most welcome!</body>
  </mail>
  <mail>
    <header>Re: Flattening a CGPath / Offsetting a path</header>
    <body>Offsetting the path - useful where I have to draw the original path and lay out other things a certain distance from the path. Flattening the path - useful for measuring the path length, finding a line perpendicular to the path at any point, working out intersections of shapes, converting to linear geometry for other processing etc. Flattening by choosing an arbitrary number of points would be massive overkill on the number of points you have to deal with in some cases, and not enough in other circumstances. Flattening algorithms use a flatness factor which defines the maximum error tolerance for the points, so with a gradual part of the curve it will only use a few line segments but for tight curves you get more segments. There is a paper about what is apparently a very good algorithm called &amp;quot;Fast, precise flattening of cubic B√©zier path and offset curves&amp;quot; which you can download, but I would have no clue about how to translate their formulae into code. Seeing as flattening is a pretty common requirement, and the NSBezierPath has flattening, I would have thought that there would be something available already to work with CGPath. Regards Gideon</body>
  </mail>
  <mail>
    <header>Re: Flattening a CGPath / Offsetting a path</header>
    <body>Thanks for the explanation. Obviously I was talking about an entirely different thing. Out of curiosity, what is the practical use of those two? Offsetting a path probably gives some aesthetic result, but flattening? On a related note, I have used in the past the parametric equations for the different kinds of curves in a CGPath, in order to produce an array of points. I used a naive, fixed 100 point per curve algorithm (it was enough at the time). The wikipedia page on Bezier curves [1] defines a few alternatives. Presumably given those points you could construct the lines yourself? To get all the path elements use CGPathApply. [1]:</body>
  </mail>
  <mail>
    <header>Re: Flattening a CGPath / Offsetting a path</header>
    <body>Flattening a path means turning all the curves into line segments. Offsetting a path is quite a different thing to transforming the path. When you offset a path, you draw another path parallel to it, so &amp;quot;concave&amp;quot; curves on the original will have tighter curves on the offset path, and &amp;quot;convex&amp;quot; curves have a larger radius. Things get &amp;quot;interesting&amp;quot; when you have tight curves where the offset curve creates a loop, or in some algorithms where you have tight convex curves.</body>
  </mail>
  <mail>
    <header>Re: Flattening a CGPath / Offsetting a path</header>
    <body>I'm not sure what you mean about flatten, but about offset, wouldn't an overall transform of the context when drawing the path do the job? Alternatively, when creating CGPaths you can supply a transform, so you could use CGPathApply to get all nodes, and create new ones with the transform passed in. But I think that transforming the drawing context will be much simpler... Orestis _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Flattening a CGPath / Offsetting a path</header>
    <body>Is there any way to flatten a CGPath? On OSX, I can convert to NSBezierPath and flatten that, and convert back to CGPath, but on iOS, there seems to be nothing equivalent. I presume that there would probably be an applier function out there somewhere, but I haven't been able to find it or any other solution. I also would like a way to offset a path by a certain amount - I know this isn't a trivial operation, and I have some code that does a moderate job of it, but doesn't handle loops etc nicely. If there is something that does a good job of it out there, I would be very grateful. TIA Gideon</body>
  </mail>
  <mail>
    <header>CICrop with non 0.0, 0.0 origin problems</header>
    <body>Hi Folks, I have a weird one that's caused me much gnashing of teeth, banging of head against wall, and general wailing of obscenities. That's all over and I'm now at the staring at glass of brandy and revolver on desk stage. You know the type. I have a CIImage that's going through a few filters including some home grown ones, and others such as column row and area average. Everything works hunky dory and I'm making to jump to light speed no problem at all. I can also crop the image going in and as long as the origin is 0.0, 0.0 or 0.5, 0.5, everything still works fine. Now, when I change that origin to be something else, e.g. 10.0, 20.0 on my input crop, it seems to throw everything off. I tried setting the extents on all my average filters, but still no dice. Would anyone have any idea why this might happen? My expectation was that if you crop an image and use that as the input to a subsequent filter then that subsequent filter is only dealing with that input image as it is. Does that make sense? My worry is that's not what's happening and there's some ROI funkiness out there that I don't know about. Thanks folks! Help always much appreciated. I know I can make that jump to light speed and trade the revolver for a light saber, with a bit of Obi Wan style assistance. Cheers, Max</body>
  </mail>
  <mail>
    <header>Relationship between UIView and CALayer</header>
    <body>I'm a bit confused about how UIViews and CALayers on the iPhone interact.  I'm hoping someone can help straighten me out. I understand that on the iPhone each UIView has a CALayer associated with it.  But... When you draw on a graphics context from the UIView drawRect, does the drawing actually end up on the CALayer? When you set the background color of a UIView does it automatically set the background color of the associated layer? Who calls the drawInContext method of CALayer? Does it somehow automatically get called before of after the UIView's drawRect? Has anyone found a good explanation of how these two objects relate to one another? I haven't found Apple's docs to be particularly enlightening about their relationship. -- Bill Tschumy Otherwise -- Austin, TX</body>
  </mail>
  <mail>
    <header>Re: Core Image Background Subtraction</header>
    <body>Your question is a little bit vague, so I will have to assume that you meant something like a ChromaKey filter.  When you look at the available sample code you'll find: If, by &amp;quot;background subtraction&amp;quot; you mean something else, check out the various CIFilter compositing filters.  If memory serves, there's a CIDifferenceBlend filter that may be what you are looking for.</body>
  </mail>
  <mail>
    <header>Core Image Background Subtraction</header>
    <body>Hi All, I was just wondering if there was a core image filter for background subtraction readily available from anywhere? Cheers, Adam</body>
  </mail>
  <mail>
    <header>Re: Tinting screen red on iPhone app</header>
    <body>I know about blend modes and how to use them to do the composite.  I can definitely swizzle (override) selected drawRects and composite the red after the drawing has occurred.  This works.  I'm now sure what is the best way to catch all component drawing. That is why a simpler solution would be to have a red view on top of everything and change the composite mode as it gets composited to lower layers.  I don't see away to do this however.  I think Mike might be right and that I need to use the Core Image compositing filters.  The documentation I've found on this is very sketchy so I don't know for sure if it affects compositing between layers as Quartz builds up the final image.  It may not matter since CI is not available on the iPhone anyway. On Sep 28, 2009, at 9:42 AM, Santiago Lema wrote: -- Bill Tschumy Otherwise -- Austin, TX</body>
  </mail>
  <mail>
    <header>Re: Tinting screen red on iPhone app</header>
    <body>It seems you're looking for something like Blend modes, specifically: kCGBlendModeMultiply</body>
  </mail>
  <mail>
    <header>Re: Tinting screen red on iPhone app</header>
    <body>I also can&amp;#39;t provide a direct answer but may be able to offer some knowledge from my currently explororation of quartz on iPhone. What you are trying to achieve sounds like it would be a function of Core Image, the one part of quartz that I understand still¬†isn&amp;#39;t available on any iPhone OS yet. Assuming this is still correct and those are the Apple APis you would¬†need, you may not be able to do this on iPhone without resorting to coding your own Core Image functionality, or probably the simpler solution, find a third party framework that can accomplish this type of filtering. Not aware of any myself as I haven&amp;#39;t needed any Core Image functionality myself.¬†Hope that helps. I&amp;#39;m working on an iPhone app and want to tint the screen red for use at night (to help preserve night vision). ¬†An earlier version simply placed a translucent red UIView on top of all the other views. ¬†This results in a muddy appearance that is somewhat difficult to read. ¬†Through experimentation in Photoshop, I&amp;#39;ve found that compositing a solid red color using Multiply seems to give an effect very close to what I want (perhaps some other mode might do even better). My question is how I can achieve this in the actual app. ¬†It doesn&amp;#39;t appear I can alter the way my red overlay UIView gets composited onto underlying views. ¬†If there were, that would probably be the simplest solution. ¬†Am I missing something? I&amp;#39;ve also thought of swizzling all the drawRect: calls in the various UIView subclasses. ¬†In the alternative implementation I would call the original method and then composite red afterwards. ¬†Apparently some UIView subclasses do not implement drawRect to do there drawing so I might need to intercept other methods as well. ¬†However, I&amp;#39;m not sure how to intercept all the drawRect from UIView subclasses (most swizzling implementations I&amp;#39;ve seen only do it for a simple class and use categories) I&amp;#39;m about to give up, thinking this is not feasible. ¬†I&amp;#39;m hoping some Quartz expert here has some slick idea on how to do this. ¬†Thanks for any help you can provide. -- Bill Tschumy Otherwise -- Austin, TX</body>
  </mail>
  <mail>
    <header>RE: Tinting screen red on iPhone app</header>
    <body>&amp;gt; I'm working on an iPhone app and want to tint the screen red CGContextSetBlendMode(kCGBlendModeColor)? Chapter 7 in &amp;quot;Programming with Quartz&amp;quot;. It also mentions NSCompositingOperation which might give you more pointers. I'm not sure either will work, but they're the directions I'd look in. -DaveP Dave Polaschek email@hidden Sr. Computer Scientist 651-766-4705 Adobe Systems Incorporated 3900 Northwoods Dr. Suite 300, Arden Hills, MN 55112</body>
  </mail>
  <mail>
    <header>Tinting screen red on iPhone app</header>
    <body>I'm working on an iPhone app and want to tint the screen red for use at night (to help preserve night vision).  An earlier version simply placed a translucent red UIView on top of all the other views.  This results in a muddy appearance that is somewhat difficult to read. Through experimentation in Photoshop, I've found that compositing a solid red color using Multiply seems to give an effect very close to what I want (perhaps some other mode might do even better). My question is how I can achieve this in the actual app.  It doesn't appear I can alter the way my red overlay UIView gets composited onto underlying views.  If there were, that would probably be the simplest solution.  Am I missing something? I've also thought of swizzling all the drawRect: calls in the various UIView subclasses.  In the alternative implementation I would call the original method and then composite red afterwards.  Apparently some UIView subclasses do not implement drawRect to do there drawing so I might need to intercept other methods as well.  However, I'm not sure how to intercept all the drawRect from UIView subclasses (most swizzling implementations I've seen only do it for a simple class and use categories) I'm about to give up, thinking this is not feasible.  I'm hoping some Quartz expert here has some slick idea on how to do this.  Thanks for any help you can provide. -- Bill Tschumy Otherwise -- Austin, TX</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>Thanks David - comprehensive answers as always. The textures are static in nature so I think it might just be a bandwidth issue.</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>The CAOpenGLLayer is an offscreen render target, so double buffering doesn't really apply. The CAOpenGLLayer basically operates as a buffer queue when in async mode. I doubt it, but then I'm not really certain what would happen if you did :). Are these textures new every frame? If so it might just be a bandwidth issue. Although even if they aren't, you are pushing around 38MB of textures around, so it certainly might be part of the issue (and likely why it is more stressful on integrated graphics than discrete graphics). -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>Yes, I'm beginning to think so. What I've subsequently found is that the big hold up is with my rendering of some rather large textures (3 lots of 1920x1080 and 1 of 1920x1920). These textures are giving my MacBook a work out and making my Mac Pro stumble a little. So I think this has turned out to be a problem with the volume of pixels being rendered. One difference now to my previous implementation (which did not rely on Core Animation and used an NSOpenGLView), is that I used to double- buffer. Is double buffering applicable to a Core Animation scenario and, if so, does super's drawInCGLContext method swap buffers instead of performing a glFlush? Similarly would placing the context in multithreading mode be useful in a Core Animation scenario? I'm obviously now looking to squeeze out all of the performance I can, but I suppose smaller textures might also help. :-) The false parameter tells reshape not to set the viewport. Instead if the frame rect has changed then I just note the viewport's size and set up my identity matrices.</body>
  </mail>
  <mail>
    <header>Re: Core Animation OpenGL Layer performance jitters</header>
    <body>Small update: I didn't get the same log output in 10.6 so perhaps Core Animation sped up somewhat.... I'm still getting the jitters on 10.6 but I think that might be due to something else now (like perhaps some rather large textures I have hanging around combined with the performance of my MacBook Air). I'll upgrade my Mac Pro to 10.6 and see if the problem hangs around.</body>
  </mail>
  <mail>
    <header>Strange Filter Behavior in QTCaptureLayer</header>
    <body>Maybe this happens with all CALayers -- haven't checked. I am applying a ChromaKey filter to a QTCaptureLayer, both with and without a background image (within the filter).  I switch the background image on/off with         [self setValue:ciImage forKeyPath:@&amp;quot;layer.filters.chromaKey.backgroundImage&amp;quot; ];  &amp;quot;self&amp;quot; in my manager (controller) object.  One thing that has become apparent is that anytime a filter parameter is changed this way, the layer makes an internal copy of the filter, i.e., copyWithZone gets called. I was trying to optimize the code by only recreating the background CISampler if the background changed or the input image extent changed.  I also set the sampler transform to match the background size to the input image. - (CIImage *)outputImage This has been totally thwarted by the fact that the layer calls the filter three times for each video image, with or without background: 1.  Correct CIImage input size. 2.  A rect outset by one pixel from the (expected) CIImage input size. 3   A rect that appears to be maximum layer size: origin(-16384, 16400), size (32768,32768). Needless to say, my optimization did not work. When using a background, I'm having issues where the background randomly either works correctly or I only get one corner of the (larger) background image.  (This was also the case before I tried to optimize it and was computing the background sampler for each input image.) This was happening in 10.5.8, but today I went to 10.6.1 with same result.  BTW, works correctly in a QTCaptureView. Why is the filter being called three times and why the random background problem?  Bug report time? Or is there a logical explanation and possible workaround?</body>
  </mail>
  <mail>
    <header>kCGImageDestinationBackgroundColor troubles</header>
    <body>I can't seem to get kCGImageDestinationBackgroundColor working (10.6.1). Here's a chunk of code I'm playing with: CGImageDestinationRef imgDest = CGImageDestinationCreateWithData CFMutableDictionaryRef properties = CFDictionaryCreateMutable(NULL, 0, CFDictionaryAddValue(properties, kCGImageDestinationBackgroundColor, I am testing with a transparent png and converting it to a jpg. No matter what I do, the background color is always white. I've played with a variety of RGB colors and just can't get it working. The docs say &amp;quot;If present, the value of this key is a CGColorRef without any alpha component of its own.&amp;quot; So I'm assuming that means the alpha value should not be set, or set to 1. If the former, I can't figure out how to make a CGColor without setting the alpha value, and if the latter none of my tests are working. FWIW, I am also using kCGImageDestinationLossyCompressionQuality (not shown above) and that is working, so I know the properties dict is going through. Kevin</body>
  </mail>
  <mail>
    <header>CARenderer and Creating Quicktime Movie of Animation</header>
    <body>Hi all,</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate() and 16-bit formats</header>
    <body>It is not. Bitmap contexts only support AlphaNoneSkip and AlphaPremultiplied formats and pixel sizes of 32-bpp, 64-bpp (on 10.5+) and 128-bpp (Float, 10.4+). The full set of modes is currently documented (as noted by QA1037) in -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate() and 16-bit formats</header>
    <body>You can certainly do 16-bit RGBA, just pass kCGImageAlphaPremultipliedLast as the flag On Sep 22, 2009, at 5:46 PM, Aaron Alpher wrote:</body>
  </mail>
  <mail>
    <header>CGBitmapContextCreate() and 16-bit formats</header>
    <body>Is there a way to call CGBitmapContextCreate() passing 16-bit per component RGB (no alpha) data? This would be 16*3=48 bits per pixel. Alternately, is there a way to call CGBitmapContextCreate() with RGBA or ARGB where each component is 16-bits? This would mean 16*4=64 bits per pixel. On page 348 of the Gelphman book is a table showing supported bitmap formats as of Tiger. Has this changed under Leopard or Snow Leopard? I looked online and see Technical Q&amp;amp;A QA1037 which dates from 2007. Is there newer information? I see there is a floating point format which allows 32-bits per component. This is double the memory of a 16-bits per component data source and would limit the maximum size I could create. Aaron Alpher ColorBurst Systems</body>
  </mail>
  <mail>
    <header>How do you disable OpenCL</header>
    <body>I have a simple program that uses a CILanczosScaleTransform CIFilter to scale images. After upgrading to Snow Leopard, on large images I get the error: Which I believe happens when the image is too large to fit into memory in the GPU and results in the bottom part of the image being black. Does any one know if there is a way to tell Core Image / Quartz not to use the GPU? I have tried using kCIContextUseSoftwareRenderer as below: NSDictionary * contextOptions = [NSDictionary dictionaryWithObjectsAndKeys: [NSNumber numberWithBool: YES], CIContext* cicontext = [CIContext contextWithCGContext: context Rob.</body>
  </mail>
  <mail>
    <header>Want to set CABasicAnimation.toValue to really nil</header>
    <body>In the docs it says if I set the toValue of a CABasicAnimation to nil, the animation will interpolate to the current presentation layer value. However I actually want to interpolate to a real nil value (that's not the current presentation layer value), e.g. CAShapeLayer.lineDashPattern = nil. How do I achieve that? I tried using [NSNull null] but it has the same effect as using nil. I also tried to use whatever is the visible representation of the nil value, but it isn't always possible to figure this out. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Re: How to debug Core Image crashes in fe_tree_node_simplify	(10.6.1)?</header>
    <body>I believe this is a known bug, but thank you for filing the report so the CoreImage team can take a look at it.</body>
  </mail>
  <mail>
    <header>How to debug Core Image crashes in fe_tree_node_simplify (10.6.1)?</header>
    <body>I'm working on fixing a crasher in my app, and I'm wondering if anyone has any tips on how to debug this?  The crashes are occurring in fe_tree_node_simplify when trying to make a CGImage.  I'm guessing fe_tree_node_simplify is part of the new &amp;quot;built on top of OpenCL&amp;quot; Core Image stack that was mentioned at WWDC? I've filed a bug report on this (Bug ID# 7240584), but I'm not ruling out the possibility that it's entirely my fault :)  I can get it to reproduce when using the CIConstantColorGenerator filter, and I've got the stack trace below.  I'm hoping that since everything is done in OpenCL now, there might be some new debugging flags I can turn on that will help me out. Thread 0 Crashed:  Dispatch queue: com.apple.main-thread 0   com.apple.QuartzCore          	0x00007fff812ec994 fe_tree_node_simplify + 938 1   com.apple.QuartzCore          	0x00007fff812eb9f4 fe_tree_refactor_transforms_ + 96 2   com.apple.QuartzCore          	0x00007fff812ec57e fe_tree_refactor_transforms_ + 3050 3   com.apple.QuartzCore          	0x00007fff812ec5a3 fe_tree_refactor_transforms_ + 3087 4   com.apple.QuartzCore          	0x00007fff812ec5a3 fe_tree_refactor_transforms_ + 3087 5   com.apple.QuartzCore          	0x00007fff812ec57e fe_tree_refactor_transforms_ + 3050 6   com.apple.QuartzCore          	0x00007fff812ec5a3 fe_tree_refactor_transforms_ + 3087 7   com.apple.QuartzCore          	0x00007fff812eb986 fe_tree_refactor_transforms + 40 8   com.apple.QuartzCore          	0x00007fff812eb0d9 fe_tree_prepare_tree_ + 424 9   com.apple.QuartzCore          	0x00007fff812e92ca fe_tree_render_image + 763 10  com.apple.QuartzCore          	0x00007fff812e8cab fe_image_render_ + 271 11  com.apple.QuartzCore          	0x00007fff812e879a fe_image_get_bitmap + 534 12  com.apple.QuartzCore          	0x00007fff812e65ae fe_image_get_CGImage + 520 13  com.apple.QuartzCore          	0x00007fff81386b9e -[CIContextImpl createCGImage:fromRect:format:colorSpace:] + 269 August 'Gus' Mueller Flying Meat Inc.</body>
  </mail>
  <mail>
    <header>Fwd: HIImageView visibility problem (OS X 10.4 Carbon)</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>HIImageView visibility problem (OS X 10.4 Carbon)</header>
    <body>I have used Interface Builder to create an HIImageView (embedded within a scroll view) in a plain document window. But, I cannot make the HIImageView visible. In the nib file, the window has compositing and the standard handler enabled. The scroll and image views are enabled and visible.</body>
  </mail>
  <mail>
    <header>Re: CIFilter Questions</header>
    <body>Thanks for the responses, Max and Steve.  (I'm not going to copy the long replies here.)  Some of this I figured out on my own since posting, but some is still a bit muddled. To match different size (and position) background images to the input image of the CIChromaKey Filter, I did successfully apply the transform in the background sampler.  This works well for a QTCaptureView.  For some reason, it does not reliably work correctly in a QTCaptureLayer.  It tends to not resize the background image properly and to center the (filter's) background image at the output image's origin.  Instead of trying to use conditionals in the kernel, I simply wrote two separate kernels and I select the proper kernel in the filter's .m file, depending on whether or not I have supplied a background image. By experimentation and observation, I found out some interesting facts about filters in CALayers.  (I'm using a subclass of QTCaptureLayer.) In spite of the copy attribute in the layer's filters property, accessing the list returns the same filters.  (I guess it's a standard NSArray shallow copy.)  However, the filters in the list are not the filter objects executed by the layer.  The layer makes an internal copy of the filter and does not copy its ivars unless you include such in a copyWithZone (which does get called) in the .m filter code.  I have successfully been able to set parameters on the fly, but how the keyPaths get routed to the internal copy is something I don't understand, unless it just makes a new internal copy each time you change a parameter. I do occasionally get crashes.  I'm running 10.5.8 with GC on a 17&amp;quot; 2.8 GHz MacBookPro.  I do have SL on an external disk, but haven't installed it on my main drive yet.  I need to test under SL.  I rue the day I went to GC. It appears that I'm outrunning the collector.</body>
  </mail>
  <mail>
    <header>Re: Dumb question about best practices for pointers, objects,	structs, and CG types, e.g. CGRect</header>
    <body>Probably not, no.  I mean, you *could*, but you'll still have to unwrap it so I'm not sure what you'd save.  If you want to make the code read a little clearer, you could add a category on NSValue, but you won't gain much and there will be a small performance penalty. If you were using a lot of them, as I say, you might contemplate a custom array class, in which case it could store and return them unwrapped.  That's what I've done in the past when I've had large arrays of CG or NSRect values to store. --</body>
  </mail>
  <mail>
    <header>Re: Dumb question about best practices for pointers, objects,	structs, and CG types, e.g. CGRect</header>
    <body>Hi Max, One thing that's giving me bother though is structs, and the types in Core</body>
  </mail>
  <mail>
    <header>Re: Dumb question about best practices for pointers, objects, 	structs, and CG types, e.g. CGRect</header>
    <body>Hi Max, You do have to use an object wrapper, but NSValue is designed for this: and later: Best wishes, Hamish</body>
  </mail>
  <mail>
    <header>Re: Dumb question about best practices for pointers, objects,	structs, and CG types, e.g. CGRect</header>
    <body>For simple structures like CGRect, NSRect et al, the usual way is to use NSValue to wrap them. If you had a very large number of CGRects, or needed very high performance for some reason, you might write a specialised array class to deal with them (which may or may not derive from NSArray; if it does, it probably wants to wrap things in NSValues when implementing the -objectAtIndex: method, and have another -rectAtIndex: method to grab the rectangle data without creating unnecessary objects). --</body>
  </mail>
  <mail>
    <header>CAKeyframeAnimation for 'position' not working</header>
    <body>(Note: I apologize for not having this nicely formatted. My previous message - 8kB over the size limit - was rejected because it was too large, so I had to remove the extra HTML) I have a CAKeyframeAnimation that simply will not work. I have a layer. When I apply a CABasicAnimation to it, it animated exactly as expected. When I apply the keyframe animation, it doesn't... + (CAAnimation*)	randomPathAnimationWithBounds:(CGRect)inBounds keyFrames:(UInt32)inKeys layer:(CALayer*)inLayer duration:(CGFloat) // Setup our variables for keeping track of points and relative timings timing[inKeys] = CKRandomRange(1000) ;  // CKRandomRange(range) is the same as  random()%range CGFloat		total = timing[inKeys] ; // Keep a running tally of our total int count for later normalization // This will produce a random point putting the layer somewhere outside the parent's bounds, supplied to us as bounds inLayer.position = r1 ; // apply the initial position // Make our arrays for ( i=0 ; i&amp;lt;inKeys ; i++ ) // Get timing timing[i] = CKRandomRange(1000) ; // Make a new random relative timing total += timing[i] ; // add it to the running tally // Get some random point inside the bounds points[i] = CGPointMake (  CKRandomRange(inBounds.size.width) +inBounds.origin.x  ,  CKRandomRange(inBounds.size.height) // Add that point to the location array // Get an endpoint, outside the bounds, and add it to the array // Normalize the timings, comulative for ( i=1 ; i&amp;lt;inKeys+1 ; i++ ) // Fill the timing value array for ( i=0 ; i&amp;lt;inKeys+1 ; i++ ) // Make the animation CAKeyframeAnimation * animation = [CAKeyframeAnimation // Print stuff The output, from the print statements, to show that it has VALID values, is: 2009-10-01 13:11:00.887 Whose Line[56132:207] &amp;lt;CAKeyframeAnimation: 2009-10-01 13:11:00.888 Whose Line[56132:207] 	--&amp;gt; 11, 11 2009-10-01 13:11:00.888 Whose Line[56132:207] 	--&amp;gt; ( 0.178501, 0.4809336, 0.5841551, 0.8514135, 0.9723865, 1.163051, 1.258054, 1.540105, 1.850756, 1.916502, 2 ) 2009-10-01 13:11:00.888 Whose Line[56132:207] 	--&amp;gt; ( ) The timings look right - the duration was set to 2 seconds... So they're all in order and randomly spaced. The positions are also all inside the bounds (iPhone landscape, full screen @ 480x320) Any ideas would be greatly appreciated!</body>
  </mail>
  <mail>
    <header>CICrop with non 0.0, 0.0 origin problems</header>
    <body>You know sometimes I think a bunch of monkeys randomly bashing the keyboard would stumble across solutions faster than me. Anyway I sussed this one. I needed to write a ROI callback.  The problem only started when I cropped the image. The sampler sampling the 1x1 area average pixel then started coming up blank. The revolver is back in the drawer, but I'll be celebrating with the brandy.... Cheers, Max On 30 Sep 2009, at 20:07, email@hidden wrote:</body>
  </mail>
  <mail>
    <header>Re: Core Image Background Subtraction</header>
    <body>Thanks Douglas, To put the question in context I am looking to develop an effect similar to iChat's background images, i.e take an initial background image using the iSight, and then mask out parts of the video stream that are the same as the background, replacing them with a static background image. Cheers, Adam</body>
  </mail>
  <mail>
    <header>CIImages - best strategy for large images?</header>
    <body>My Cocoa-based image viewer displays multiple large bit map images (16 bit per component RGB), but now I need to change the images to CIImages so they can be visually modified with filters. I'm trying to find the best solution given a set of constraints - best meaning small image size and best drawing performance. Current implementation: The app currently uses libtiff to read in a variety of bitmap files (historical need). The data is stored in a mmap'ed file, and used to create a NSBitmapImageRep. All images are 16-bit per component, 3 component (RGB). These images can be around 2000x50000 in size (6 x 100,000,000 bytes). The images can be displayed using a zoom factor from some small percentage to some large percentage). One critical feature for the app is the ability to display the RGB value under the cursor, which must be 16 bpc (and must match the values in the original file). CCImage options: The ideal solution is one that favors drawing performance and that results in the same or only slightly larger memory footprint. [I cannot convert the images to 8 bpc since some of the filters require higher precision values.] I infer that it would be &amp;quot;a good thing&amp;quot; if Core Image can use the provided data natively and not have to reformat or recreate it. That is, suppose I were to create a CIImage with the NSBitmapImageRep I already have. It would appear that if I tried to do this that Core Image would reformat behind the scene to RGBA, or would end up having to reformat the data on the fly to RGBA. I infer this since Core Image only accepts native RGBA formatted bitmaps in either 8, 16, or 32f bpc chunks (see CIIImage imageWithBitmapData etc). For me, it appears that the RGBA 16bpc is the best option if I go with providing a NSData object directly. I know that filters receive RGBA 32f bpc pixels, and that probably is the most efficient bitmap format, but it has the severe downside of requiring me to more than double the existing image memory footprint. When images are zoomed larger, it would seem that a tiled format would result in fewer page faults, and thus provide better drawing performance (users often use scrollers to move around an image when its in a highly zoomed state). To implement tiling, I would need to develop my own tile format for the read in image, and develop my own CIImageProvider. When the CIImage needs image bits, it would call the application level data provider which would have to retrieve the required data out of the tiles to put it into a buffer as required. The benefits of this approach are a 25% smaller memory footprint with RGB 16bpc and fewer page faults, at the cost of having to copy memory at the application level. Questions: Before I go and spend a substantial amount of time experimenting with both ideas I figured I'd first solicit advice in case someone more experienced could provide direction. 1) create a NSData object with RGBA 16bpc data to use as the bitmap basis of a CIImage. 2) use private RGB 16bpc data formatted into tiles, and develop a CIImageProvider to reformat the image data as needed. It well may be that I have not correctly or completely understood the options so corrections are most welcome! David</body>
  </mail>
  <mail>
    <header>Re: CGContextFillRect - is there a faster way?</header>
    <body>On Sep 27, 2007, at 7:17 AM, Paul Tapper wrote: It sounds like you may have some other problem (e.g., perhaps you're drawing more often then you think you are), but one thing you could try would be to draw all of your like colored rectangles at once by using CGContextFillRects (note the 's' on the end). Nick</body>
  </mail>
  <mail>
    <header>Re: CGContextFillRect - is there a faster way?</header>
    <body>As far as I understand, you're using rects with a width of 1 or so, right ? In that case, using your CGBitmapContext, you have a direct access on the pixels you've allocated for it, and you can leverage the penalty of calling each CGContextFillRect. For example you subdivide a scanline into blocks of 1 height and 'x' width so that everything can stay in cache, plot the pixels, and then replicate the piece of scanlines (while they are in cache) to the height on which they should be displayed (using memcpy, which will initiate a burst as far as I remember)</body>
  </mail>
  <mail>
    <header>RE: CGContextFillRect - is there a faster way?</header>
    <body>Thanks for all the responses. My audio unit is displaying some audio-metering views, which dynamically update as the audio plays.&amp;nbsp; One example of the kind of view I'm trying to display is a correlation meter which consists of a horizontal bar mad up of lots of multi-coloured vertical lines.&amp;nbsp; You can get an idea by looking at... or any of the other images from Roughly speaking, the graph is getting updated in the order of 3-10 times a second (so not exactly a high framerate).&amp;nbsp; Each redraw is calling CGContextFillRect (for each of the little vertical lines) maybe 300 times or so (depending on the size of the view). I'm wondering whether CGContextFillRect is just the wrong function for this sort of thing.&amp;nbsp; Maybe I should make my own bmp and write into its pixel values by hand, and then draw that to the screen at the end, or something? Anyway, any further thoughts are gratefully received. Paul CC: email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGContextFillRect - is there a faster way?</header>
    <body>My code seems to spend much of its time trying to fill in a rectangle with a solid colour by calling CGContextFillRect.&amp;nbsp; Is there a quicker way of doing this sort of thing?&amp;nbsp; How much does it depend on what kind of CGContext I'm using? (I'm using a CGBitmapContext)</body>
  </mail>
  <mail>
    <header>Re: CGContextFillRect - is there a faster way?</header>
    <body>How much time do you call CGContextFillRect per second ? What is the size of the bitmap context and the size of the rectangles ? What is the type of bitmap context ? (8bits/32bits per components ?) Do you have time statistics ? I think that CGContextFillRect on a CGBitmapContext must be done on CPU, but this is a rather quick function.</body>
  </mail>
  <mail>
    <header>Re: Converting ps to pdf data for printing</header>
    <body>That did the trick! Thank you very, very much! This is a huge load off of my mind. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Converting ps to pdf data for printing</header>
    <body>Thanks a lot for the response. I'll take a look and give it a go. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: Converting ps to pdf data for printing</header>
    <body>Unfortunately PMSessionGetGraphicsContext can't return a CG context unless you do special things. We've made this easier in Tiger. Please look at the printing sample code in: This code shows how to draw with Quartz into a printing context if all you want to do is use Quartz for your drawing at print time and no QD. It uses the Tiger functions when present and the pre-Tiger technique if you are running on earlier versions of Mac OS X. Note that if you are mixing QD and Quartz drawing you should look at the QD functions QDBeginCGContext and QDEndCGContext.</body>
  </mail>
  <mail>
    <header>Re: Can CGImageDestination write 16-bit per component TIFF files?</header>
    <body>It turns out there is one magic piece knowledge that is needed and that I did not have. In Tiger, it is not possible to make a CGContextRef that is 16-bits per component.  It is possible to make a CGImageRef this is 16-bits per component. I was trying to make a CGContextRef, because that is where I have started in the past.  Making a CGImageRef is only a little more tedious.  Once you have a CGImageRef that is 16-bits per component simply adding it to an appropriate CGImageDesitnation does what I want. On Sep 18, 2007, at 9:09 PM, Michel Schinz wrote:</body>
  </mail>
  <mail>
    <header>Converting ps to pdf data for printing</header>
    <body>I tried carbon-dev and got some pointers on how to do this, but it's not working. Our app spools its own PostScript, then gives it to the Print Manager (by copying the spooled file to the destination provided by the PM). However, if the user clicks the Preview or PDF- Here's the code I have to do this. What I see in the Preview of the pdf is a blank document. Not even the huge red-filled rect added for debugging shows up. Can anyone shed some light on what's wrong? Note that nothing returns an error or fails to allocate, the CGPSConverterConvert *does* succeed. Err/nil checking has been removed for clarity. provider = CGDataProviderCreateWithData(nil, CFDataGetBytePtr(data), err = PMSessionBeginDocument(m_pmPrintSession, m_pmPrintSettings, err = PMSessionGetGraphicsContext(m_pmPrintSession, else _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: CILinearGradient positioning</header>
    <body>This is expected behavior. Apps linking on Tiger get the old (wrong) behavior in Tiger and Leopard. Apps linking on Leopard get the new (correct) behavior on Leopard.</body>
  </mail>
  <mail>
    <header>CILinearGradient positioning</header>
    <body>I just started using CILinearGradient and to my surprise have found that the positioning parameters control the wrong color. e.g. I was pretty sure I was doing something wrong unit I check and 10.5 gives the opposite (correct) behavior, has anybody else come across this? does this affect all versions of 10.4? rdar://5491421 ~ Chad</body>
  </mail>
  <mail>
    <header>Re: Can CGImageDestination write 16-bit per component TIFF files?</header>
    <body>This message might help you:</body>
  </mail>
  <mail>
    <header>CGImageRef and PSD layers</header>
    <body>I know with IOKit you can get the names of PSD layers. But can you extract them? I can't seem to find any documentation on that. If not its QT for me for easy parsing of layers. Scott Andrew</body>
  </mail>
  <mail>
    <header>Re: Can CGImageDestination write 16-bit per component TIFF files?</header>
    <body>I would like to write 16-bit per component TIFF files with CGImageDesitination.  I cannot find a way to do it.  I found the following in the archive.  Anyone know if there is any progress on this front?</body>
  </mail>
  <mail>
    <header>Core Video DisplayCallback never gets called</header>
    <body>Hi!</body>
  </mail>
  <mail>
    <header>Re: (no subject)</header>
    <body>On Sep 15, 2007, at 12:12 PM, Stanislas Polu wrote: I didn't communicate that well. What I mean is not that your particular application would be a security risk, rather that having an API in place that allowed you to get at the window buffers of another process (without that process' direct intervention) would violate the protected memory boundaries between applications.  Having that API in place would defeat specific security mechanisms and would, therefore, be a &amp;quot;risk&amp;quot;.</body>
  </mail>
  <mail>
    <header>Re: (no subject)</header>
    <body>-stan¬†</body>
  </mail>
  <mail>
    <header>Extracting embedded JPEG from RAW files</header>
    <body>Does ImageIO support extracting the embedded JPEG preview found in RAW files? I'm not referring to thumbnails, but instead the large JPEG previews that are present in Nikon's .NEF or Canon's .CRW for example. I'm getting inconsistent results with some files, which suggest that the functionality *might* be there: When I call CGImageSourceCreateThumbnailAtIndex without specifying kCGImageSourceThumbnailMaxPixelSize, I get the preview image from a sample .CRW shot with a Canon 10D, but not from a .NEF shot with a D70. Exiftool can extract the large preview from the .NEF, so I know it's in there. Is there something I'm missing? Should I be able to get the preview out of the .NEF, and any other RAW file for that matter? Mike Bernardo Green Volcano Software, LLC</body>
  </mail>
  <mail>
    <header>Re: (no subject)</header>
    <body>On Sep 14, 2007, at 9:46 PM, Stanislas Polu wrote: Are you creating the application for which you would like to get the window buffers, or are you asking to launch an arbitrary application and access it's window buffers? If it's one you own, you might be able to set up custom mechanisms for doing what you want. If you are looking to do the same for any arbitrary application... that sounds like a security risk.  The memory protection mechanisms specifically prevent that. Perhaps if we had some more information about what you are trying to do, a higher-level view of the problem you're trying to solve, we might be able to suggest other solutions.</body>
  </mail>
  <mail>
    <header>Re: (no subject)</header>
    <body>On Sep 14, 2007, at 7:46 PM, Stanislas Polu wrote: In current versions of Mac OS X no supported method exists to access window buffers (they are owned by the window server and no API exists to get at them). You can get informed of areas of change on screen and use OpenGL to grab images from the frame buffer (capture from the final desktop &amp;quot;scene&amp;quot;). Also other slower methods exists to capture from the display. Quartz_Services_Ref/Reference/reference.html#//apple_ref/c/func/ -Shawn</body>
  </mail>
  <mail>
    <header>(no subject)</header>
    <body>Hi all,-stan</body>
  </mail>
  <mail>
    <header>Re: Unsafe Patches</header>
    <body>Remember that the export function in Quartz Composer does little more than wrap the composition in a QuickTime track. The frames are still rendering through the QC engine, and since QT has the ability to run in places where sensitive data could be lost, those patches are disabled. A better place to discuss Quartz Composer is over on the email@hidden list. Feel free to describe your problem there and we'll try to find a solution. It could be that exporting to QT doesn't solve your real problem and safe patches are the least of your worries :) (QC files exported from the editor do not run on Windows for example) Troy</body>
  </mail>
  <mail>
    <header>Re: drawing with a pattern?</header>
    <body>On Sep 12, 2007, at 7:23 AM, Bill Dudney wrote: That's my understanding.  I have seen some odd stuff when you start scaling patterns but that may have more to do with the complexity of my app than the drawing library itself.  (by that I mean that the weird behavior is probably caused by a subtle problem in our app, not in Quartz)</body>
  </mail>
  <mail>
    <header>Unsafe Patches</header>
    <body>On QC 3.0, when I try to export a composition with unsafe patches in to a Quicktime movie, I lose those patches. Does anybody know how to solve this problem? Will Apple allow to export any kind of patches on Leopard? thanks for any suggestion.</body>
  </mail>
  <mail>
    <header>Re: drawing with a pattern?</header>
    <body>So if I understand, I could get what I expect by creating a transformation matrix that transforms the patterns space to the space it should stay 'consistent' as I resize the window?</body>
  </mail>
  <mail>
    <header>CVOpenGLBuffer and internal pixel format</header>
    <body>Here is some sample code that can be inserted into an NSView subclass. Can someone tell me how to make this work? (Note: the correct behavior is to draw a PURPLE&amp;nbsp;rectangle,&amp;nbsp;not a _cyan_ rectangle). Ideally, the &amp;quot;answer&amp;quot; to this question comes in the form of a working code snippet. 1 1.0 0.6</body>
  </mail>
  <mail>
    <header>Re: drawing with a pattern?</header>
    <body>On Sep 10, 2007, at 4:19 PM, Bill Dudney wrote: Thanks.  That helps a lot. OK... I understand your confusion now. Well, the pattern is not really directly based on the window (or in general terms the device),  however it is drawn in user space and the location of user space is tied to the window through the Current Transformation Matrix (CTM) of the context. When the computer wants to draw your NSView it sets up the CTM so that the origin is at the lower left corner of your view.  (the CTM defines a mapping between your view and the &amp;quot;device&amp;quot;... in this case the window).  The 0, 0 point of the user space, then is the lower left hand corner of your view.  All drawing happens in user space. When you set up a pattern, you set it up in it's own space. This is the &amp;quot;pattern space&amp;quot; talked about in the Quartz documentation.  To draw that pattern, however, then the computer needs to map from the pattern space to user space (all drawing happens in user space). When you create a pattern you specify a transformation matrix.  That very transformation is the one used to map from pattern space to user space. The computer aligns the corner of pattern space to this origin (so conceptually it puts 0,0 of the pattern space at the same location as 0,0 of user space and lines up their axes).  It then uses the pattern's matrix to adjust the coordinates of the pattern space. The important thing to realize, however, is that the pattern is set up relative to user space (in this case your view) and has nothing really to do with the geometry you also define in user space. The path doesn't really have an intrinsic 0, 0 spot of it's own. (OK, this is not entirely true... a path can be define in a drawing space all it's own... but it still must be mapped, by a transform, into user space before you can draw it). To summarize, the pattern is defined in it's own space then mapped to user space.  The path's geometry is also placed in user space, and the computer rasterizes the results of both, but in general the path and the pattern don't &amp;quot;know anything about one another&amp;quot;. Scott</body>
  </mail>
  <mail>
    <header>Re: Quartz2D drawing in AGL context</header>
    <body>http://developer.apple.com/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_texturedata/chapter_10_section_2.html It explains how to use Apple&amp;#39;s OpenGL extensions to optimize the transfer of texture information to the GPU. However, if you need to do screen-sized transfers to the GPU at high performance intervals, you might want to look into using Core Video. I do not know much about Core Video other than the fact that it uses fancy techniques to shuffle images to the GPU at regular intervals,</body>
  </mail>
  <mail>
    <header>Re: Retrieving XMP Data out of an EPS Image?</header>
    <body>Not surprising as Image IO doesn't support EPS. You can call CGImageSourceCopyTypeIdentifiers() to determine what files you can open with Image IO (with one exception - Image IO advertises PDF but won't open it). You will likely have to go this route. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Re: Retrieving XMP Data out of an EPS Image?</header>
    <body>--- Cross Post on Both Quartz and Carbon Lists.. kind of applies to both --- I'm trying to pull some XMP MetaData (Specifically the Keywords) out of an EPS file, but can't seem to do it.  For images that aren't EPS, I can do it using the following code snipet: CGImageSourceRef imgSrc = CGImageSrouceCreateWithDataProvider CFDictionaryRef ref = CGImageSourceCopyPropertiesAtIndex(imgSrc, But with EPS files, you can't just create a CGImageSourceRef with them directly, they have to be converted to PDF first... so I'm converting the EPS data into PDF using the CGPSConverterConvert function, however it seems to drop all the metadata stored in the original EPS file.... I'm just not sure what to do to get the info out using Quartz 2D.  I'd prefer not to have to use the Adobe XMP Library, but if that's the only way, then I guess I may have to resort to that.</body>
  </mail>
  <mail>
    <header>Re: Quartz2D drawing in AGL context</header>
    <body>for what it's worth, or answers any questions, maybe drawing into a seperate/overlay window saves a lot of trouble. (?) .a On Sep 5, 2007, at 15:52 , Thijs Koerselman wrote:</body>
  </mail>
  <mail>
    <header>Re: Quartz-dev Digest, Vol 6, Issue 224</header>
    <body>I'm wanting to use a CoreImage effects that uses a portion of the screen as an input parameter image. Is there a way I could copy a direct screen image then translate that into a CIImage? Perhaps there is a CGContext which contains the screen exactly as it's drawn the current time that I could get reference to and copy a CGImage from that. Any ideas? Thanks for helping.</body>
  </mail>
  <mail>
    <header>Re: Using NSTimer and CGEventPost causes problems</header>
    <body>Anyway you likely are seeing the result of a control running a runloop in a tracking mode that your timer isn't set to fire under. Change how you schedule your timer to include &amp;quot;NSEventTrackingRunLoopMode&amp;quot; (see -[NSRunLoop addTimer:forMode:]). -Shawn</body>
  </mail>
  <mail>
    <header>Re: Copying screen into CGContext</header>
    <body>You can use CGWindowListCreateImage to great a cgimage of the screen as shown in this sample code: This bit o' code will probably do it for you: And then you can trim it down to the rect you'd like, and make a CIImage out of it. -gus -- August 'Gus' Mueller Flying Meat Inc.</body>
  </mail>
  <mail>
    <header>Re: Maximum size of CIImage for offscreen drawing</header>
    <body>Don't think of a CIImage as a picture.  A CIImage is a set of instructions for drawing a picture. Because of this, CIImages can be infinite in extent. The results of some of the CIFilters, for example the Checkerboard filter, are CIImages that are infinite in size.  It is happy to draw checkerboards anywhere on the drawing plane... you usually have to combine the results of such a filter with some kind of crop to make use of them. The surface that you draw the CIImage on to may have size limits, but a CIImage itself can be infinitely large. Scott</body>
  </mail>
  <mail>
    <header>Re: Using NSTimer and CGEventPost causes problems</header>
    <body>2009/11/17 Bj√∂rn Bollensdorff &amp;lt;email@hidden&amp;gt;: You appear to be invalidating and releasing your repeating timer in the &amp;quot;timerHandle&amp;quot; method. Are you expecting it to continue repeating after the first time it fires? ...because it wont if you invalidate it, etc. -Shawn</body>
  </mail>
  <mail>
    <header>Using NSTimer and CGEventPost causes problems</header>
    <body>Hi all, I'm trying to implement a remote control for an application. The mouse data is send to the application through a socket which is checked regularly by a function triggered by a NSTimer. The data is then transfered into the corresponding CGEvent. Interestingly this works as long as I do not perform a mouse down on a NSButton or something similar. If I hit an active element the NSTimer does not fire anymore. I simplified the problem as much as possible getting to the following small application that does not use socket communication anymore. There are two buttons, a checkbox and a label. The first button triggers the mouse event and the second is the target. The checkbox sets the position of the mouseEvent. If it is unchecked the mouse down and up are performed somewhere in the applications window. If it is checked they are performed on the target button. To see if the action bound to the target button was triggered the label changes. If the first button is activated I generate a CGEvent MouseDown and start a NSTimer. The NSTimer triggers a function that generates a CGEvent MouseUp. The behavior of the small application now depends on where the MouseDown is performed. If there is no element underneath it, the NSTimer fires and the Mouse Up is performed. If an element is hit the NSTimer does not fire until I manually hit the mouse button. I'm using CGEventCreateMouseEvent to create the MouseEvent and CGEventPost to post the mouse event. I tried CGPostMouseEvent, too, but got the same result. Does anybody know how to resolve that problem? Thanks a lot Bj√∂rn I uploaded a zip-file with the project to: I'm using Mac OS X 10.5.7 and XCode 3.1.2 The source code of the small application follows: @implementation EventTest -(IBAction) startButton:(id)sender -(IBAction) testButton:(id)sender -(void) timerHandle @end</body>
  </mail>
  <mail>
    <header>Maximum size of CIImage for offscreen drawing</header>
    <body>Thanks,</body>
  </mail>
  <mail>
    <header>Re: how to refresh my paint???</header>
    <body>What API are you using for your UI... Cocoa (AppKit)? -Shawn</body>
  </mail>
  <mail>
    <header>Copying screen into CGContext</header>
    <body>Josef</body>
  </mail>
  <mail>
    <header>Re: How to wait for CGImageSourceCreateWithURL() to finish reading	image?</header>
    <body>Just keep in mind that unless you really do just want the color of exactly 1 pixel, that this is an exceptionally expensive way to get it. If you need the colors of lots of pixels, then you may as well ask for the color of all pixels rather than asking one-by-one. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: How to wait for CGImageSourceCreateWithURL() to finish reading	image?</header>
    <body>If you're just wanting the color of a pixel, you might try something along the lines of: -gus -- August 'Gus' Mueller Flying Meat Inc.</body>
  </mail>
  <mail>
    <header>Re: How to wait for CGImageSourceCreateWithURL() to finish reading 	image?</header>
    <body>Whereby 38 I meant 32... Sorry, line 38 of that code (&amp;quot;\t\t\t I&amp;#39;m attempting to use the following code to extract a CGImage from a file, but CGImageSourceGetStatusAtIndex appears to be either returning -2 (kCGImageStatusReadingHeader) or -1 (kCGImageStatusIncomplete) after I attempt to read the image. How can I make CGImageSourceCreateWithURL() synchronous, or register a callback when the reading is complete? int error(const char *str) int main(void) CFURLRef fileURL = CFURLCreateWithFileSystemPath(kCFAllocatorDefault, CFSTR(&amp;quot;foo.bmp&amp;quot;), kCFURLPOSIXPathStyle, CFDictionaryRef options = CFDictionaryCreate(kCFAllocatorDefault, (const void **)keys, (const void **)values,</body>
  </mail>
  <mail>
    <header>Re: How to wait for CGImageSourceCreateWithURL() to finish reading 	image?</header>
    <body>Sorry, line 38 of that code (&amp;quot;\t\t\t I&amp;#39;m attempting to use the following code to extract a CGImage from a file, but CGImageSourceGetStatusAtIndex appears to be either returning -2 (kCGImageStatusReadingHeader) or -1 (kCGImageStatusIncomplete) after I attempt to read the image. How can I make CGImageSourceCreateWithURL() synchronous, or register a callback when the reading is complete? int error(const char *str) int main(void) CFURLRef fileURL = CFURLCreateWithFileSystemPath(kCFAllocatorDefault, CFSTR(&amp;quot;foo.bmp&amp;quot;), kCFURLPOSIXPathStyle, CFDictionaryRef options = CFDictionaryCreate(kCFAllocatorDefault, (const void **)keys, (const void **)values,</body>
  </mail>
  <mail>
    <header>How to wait for CGImageSourceCreateWithURL() to finish reading	image?</header>
    <body>I&amp;#39;m attempting to use the following code to extract a CGImage from a file, but CGImageSourceGetStatusAtIndex appears to be either returning -2 (kCGImageStatusReadingHeader) or -1 (kCGImageStatusIncomplete) after I attempt to read the image. How can I make CGImageSourceCreateWithURL() synchronous, or register a callback when the reading is complete? int error(const char *str) int main(void) CFURLRef fileURL = CFURLCreateWithFileSystemPath(kCFAllocatorDefault, CFSTR(&amp;quot;foo.bmp&amp;quot;), kCFURLPOSIXPathStyle, CFDictionaryRef options = CFDictionaryCreate(kCFAllocatorDefault, (const void **)keys, (const void **)values,</body>
  </mail>
  <mail>
    <header>Re: how to release CGImageRef when i call CGContextDrawImage????</header>
    <body>thanks a lot,i¬†get it...¬† Are you sure you are supposed to be releasing that image at this point? Without seeing more of your code, it is difficult to know if this is correct or not. -- Reality is what, when you stop believing in it, doesn&amp;#39;t go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: CGEventKeyboardSetUnicodeString() and modifier flags?</header>
    <body>I'm trying to simulate a keypress using the CG APIs. CGEventKeyboardSetUnicodeString() appears to work great (it takes a _lot_ of the work out of trying to get the correct keycode for the current keyboard), but I can't seem to get it to work with modifier keys. E.g., say I want to simulate the keystroke "‚åò-a". My first instinct was to try the following:</body>
  </mail>
  <mail>
    <header>CGEventKeyboardSetUnicodeString() and modifier flags?</header>
    <body>I&amp;#39;m trying to simulate a keypress using the CG APIs. CGEventKeyboardSetUnicodeString() appears to work great (it takes a _lot_ of the work out of trying to get the correct keycode for the current keyboard), but I can&amp;#39;t seem to get it to work with modifier keys. E.g., say I want to simulate the keystroke &amp;quot;‚åò-a&amp;quot;. My first instinct was to try the following: int main(void) This fails to work (no keys appear to be pressed whatsoever), so I attempted to simulate the holding and lifting down of the command key instead: #include &amp;lt;Carbon/Carbon.h&amp;gt; // For kVK_KeyCodes in HIToolbox/Event.h void toggleKeyCode(CGKeyCode code, const bool down) int main(void) However, this has the same result as the first example. Could anyone give me some advice as to what&amp;#39;s going wrong? Thanks.</body>
  </mail>
  <mail>
    <header>Thread Safety Of Custom Sequential CGDataProvider</header>
    <body>I have a custom sequential CGDataProvder to render a custom image format. To increase performance of the data provider in certain instances I have the data provider machinery cache the decoded image data so I don't have to go through the decoding process each time CGDataProviderGetBytesCallback is called. However the decoded image data can be very large and it would be nice to recollect that decoded image data after some period of time. So I have implemented an NSTimer that will internally recover that memory after some amount of time has elapsed. While doing this I realized that it could be possible that the CGImage created from the custom CGDataProvider could be rendered in a background thread from say something like a CALayer. I did some simple locking inside the sequential data provider machinery to make sure that things stay in sync if the timer fires, from the main thread, while rendering occurs from a background thread. However this lead me down another road of thinking. What if a single CGImage, backed by the custom CGDataProvider, is set to multiple CALayers and even if the CGImage backs a NSBitmapImageRep that is then added to an NSImage which is then asked to be drawn from the main thread. If all of this happens at the same time, what is the thread safety of the CGDataProviderSequentialCallbacks and the internal state of the custom sequential data provider? The documentation mentions nothing so I have to assume that there is some synchronizing that occurs that prevents one thread calling to any of the CGDataProviderGetBytesCallbacks before another is completely finished gathering all data that is needed. Is this a correct assumption? Can anyone provide more information that may clarify this situation? Thanks, Michael</body>
  </mail>
  <mail>
    <header>Re: how to release CGImageRef when i call CGContextDrawImage????</header>
    <body>How do you get ahold of the image ref you are trying to release? The way you create or get the image determines if you are responsible for releasing it or not, etc. -Shawn</body>
  </mail>
  <mail>
    <header>PDFPage createDisplayList eats memory</header>
    <body>Hello,</body>
  </mail>
  <mail>
    <header>Re: how to release CGImageRef when i call CGContextDrawImage????</header>
    <body>Are you sure you are supposed to be releasing that image at this point? Without seeing more of your code, it is difficult to know if this is correct or not. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: Image from CGWindowListCreateImage causes crash when window	resizes</header>
    <body>And thanks again to the Quartz Services/Window Manager engineers for giving us the window capture functions. This simplifies my life so much not having to use skanky undocumented API's *cough* cgprivate *cough*. However, for my application, I need access to the raw pixels, I'm fine with the format of the CGImage, and can't do processing with drawing. I need to bundle up the pixels I get and pass them to another library (non-Macintosh). Maybe I could make the performance better but I'm not that concerned about it right now, over just getting this to work. I also don't want/need to use Core Image.</body>
  </mail>
  <mail>
    <header>Re: Image from CGWindowListCreateImage causes crash when window 	resizes</header>
    <body>On Wed, Nov 11, 2009 at 11:24 AM, Douglas Hill However, for my application, I need access to the raw pixels, I&amp;#39;m fine with the format of the CGImage, and can&amp;#39;t do processing with drawing. I need to bundle up the pixels I get and pass them to another library (non-Macintosh). Maybe I could make the performance better but I&amp;#39;m not that concerned about it right now, over just getting this to work. I also don&amp;#39;t want/need to use Core Image.</body>
  </mail>
  <mail>
    <header>CMGetSystemProfile Failing in Leopard</header>
    <body>I'm calling CMGetSystemProfile in my app, and have done so for years without any problem.  A few users are reporting problems after installing Leopard.  In these rare cases the function seems to be returning fnfErr (i.e., file not found).  Does this make any sense? Does it mean that somehow the display profile has been deselected in the Display panel of the system preferences? Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: Fullscreen Core Image filter (redux)</header>
    <body>I logged a bug in the 10.4 days for the ability for a NSWindow to have a 'compositeFilter', rather than the hard-coded s-over.  I suggest you do the same =) This along with the ability to have the window server call back to a window to request &amp;amp; cache some of its content at a higher resolution (a in CALayerDelegate) would allow you to make a 'loupe' window to magnify anything underneath it.  Add in the final component of my feature request; attachable extra fragment buffers on NSWindow, accessible via the composite filter, and maybe a HDR-aware window server, and you could have some *crazy* fun. This is all unlikely in the extreme, at least until the CA dust settles, so I'm not holding my breath.  The 'compositeFilter' seems the most likely, obviously and even this would probably be harder than I expect. Right now, you'll need to capture the area under your window, and put your window up with the filtered contents (it'll need to be opaque).  Be very careful that your app's window is not at such a high level that it prevents force-quit.  Be very careful that its transparent to mouse events.  In general be careful that you don't &amp;quot;lock up&amp;quot; the user's machine by putting up a busted window over their screen. On Oct 30, 2007, at 9:22 PM, Solra Bizna wrote:</body>
  </mail>
  <mail>
    <header>Fullscreen Core Image filter (redux)</header>
    <body>I've been looking into Core Animation and it seems like what I want should be possible, but I've been unsuccessful so far. Basically, I want a Core Image filter applied to the entire screen. Everything on the screen needs to be &amp;quot;behind&amp;quot; this filter. It's not a normal composite-style calculation, so I can't cheat and have a semi-opaque overlay window or something. I tried creating a transparent window (with the help of a &amp;quot;BorderlessWindow&amp;quot; class I snipped from some sample code aeons ago) and giving it a background filter, but that didn't seem to do anything at all. I also combed the docs for a few hours without any more luck. Am I doomed to fail, or is there a better approach I'm missing? -:sigma.SB</body>
  </mail>
  <mail>
    <header>CWMatchBitmap failing</header>
    <body>I am calling CWMatchBitmap() and the call fails with a -4210 error [cmInvalidSrcMap] (Source pix/bit map was invalid). I have checked the source bitmap and it appears perfectly valid as far as I can see. Is there a validate call I can make or can someone point me to a reason I am getting this error? Aaron Alpher ColorBurst</body>
  </mail>
  <mail>
    <header>Re: Rendering CoreImage in PDF Context (printing)</header>
    <body>I have problems rendering Core Image in PDF context (eg printing) in Tiger. I am not getting any particular error when creating the context using but the generated PDF file is corrupted. I tried to set the software render using kCIContextUseSoftwareRenderer options without success. It is working fine on the latest Leopard beta. Please note that a similar problem occur if you try to use the CGLayer on a PDF Context. Any suggestion for a workaround ? Many Thanks Marco Pifferi</body>
  </mail>
  <mail>
    <header>Rendering CoreImage in PDF Context (printing)</header>
    <body>I have problems rendering Core Image in PDF context (eg printing) in Tiger. I am not getting any particular error when creating the context using but the generated PDF file is corrupted. I tried to set the software render using kCIContextUseSoftwareRenderer options without success. Please note that a similar problem occur if you try to use the CGLayer on a PDF Context. Many Thanks Marco Pifferi</body>
  </mail>
  <mail>
    <header>Re: CIImage compositor filters</header>
    <body>You would usually use either a CICrop filter on the larger image or a CIAffineTransform filter on the smaller image to scale and/or center the image. You can also use any of the lanczos scaling filter for higher quality.</body>
  </mail>
  <mail>
    <header>CIImage compositor filters</header>
    <body>I am experimenting with the compositors and do not like the way they work when the input image and background images are of different sizes. Is there any options for controlling HOW this is handled. Possible values would be: stretch, center, topleft, topright etc. Currently they pin the bottom left corners together. If not, then would the best option to be to write a simple filter called &amp;quot;MatchSizes&amp;quot; that merely pads the smaller image with alpha = 0 pixels to get what I want and then pass BOTH input images through this?</body>
  </mail>
  <mail>
    <header>Re: CIImage &amp;lt;-&amp;gt; bitmap data &amp;lt;-&amp;gt; NSImage</header>
    <body>Another approach would be to create a CGBitmapContext and draw into that, if your goal is to get a bunch of pixels in memory to manipulate. (Presumably this is what NSBitmapImageRep is doing underneath, anyway.) I haven't compared the results of the two approaches.</body>
  </mail>
  <mail>
    <header>CMYK radial shading crashes</header>
    <body>to use as color chips and arbitrary and with arbitrary shaped clipping paths to fill various objects with a shading. Radial CMYK shadings crash when drawing the small rectangles. Only radial CMYK. Axial CMYK and Axial and Radial RGB work fine. Radial CMYK doesn't crash when I draw the other shapes. #0  0x904e683e in getNextRadialShadingScanline () #1  0x904e73ce in getBytesRadialShadingDataProvider () #2  0x903637e9 in CGAccessSessionGetChunks () #3  0x9037bdb7 in img_decode_read () #4  0x9037b923 in img_colormatch_read () #5  0x9037a5b6 in img_alphamerge_read () #6  0x90338fb8 in img_data_lock () #7  0x903377cd in CGSImageDataLockWithReference () #8  0x942fa318 in ripc_AcquireImage () #9  0x942f807a in ripc_DrawImage () #10 0x90335e81 in CGContextDrawImage () #11 0x904e89b9 in drawRadialShading () #12 0x9046eda2 in CGContextDrawShading () Setting CGContextSetInterpolationQuality() to different things makes no difference. N.B. &amp;quot;Wait until next week.&amp;quot; is not a usable answer - this has to work on Tiger. ...Bob Clair</body>
  </mail>
  <mail>
    <header>Re: CGContextFlush()</header>
    <body>Take a look at Technical Note TN2133 Coalesced Updates.</body>
  </mail>
  <mail>
    <header>CGContextFlush()</header>
    <body>Hi, I've a question concerning CGContextFlush(). I'm using this call in my main loop like this: (this code only for illustration; it is somewhat more complicated actually) When I open my game window on the desktop, CGContextFlush() seems to restrict the drawing speed to the refresh rate of the current display mode. I.e. the above loop would never be executed more than 60 times a second on a 60hz display mode. Is that correct? I'm asking because when I open my window on a captured display instead of the desktop, CGContextFlush() does not seem to restrict the drawing speed at all. Instead, I have to insert a wait manually when I open my window on a captured display. Otherwise the loop above would run much faster. Could someone probably explain to me why CGContextFlush() behaves differently in these two cases? Tks Andreas</body>
  </mail>
  <mail>
    <header>Re: CIImage &amp;lt;-&amp;gt; bitmap data &amp;lt;-&amp;gt; NSImage</header>
    <body>CIImage, NSImage, and a raw bitmap (and a CGImage for that matter) are all very different beasts at different abstraction levels.  Moving between them involves a series of tradeoffs and choices.  If it were the case that there was a single, &amp;quot;best&amp;quot; set of options for navigating those choices, then it would be easy to create a more generic mechanism for creating one representation from another. Unfortunately, there is not a &amp;quot;best&amp;quot; way to do it that will work for &amp;quot;most&amp;quot; applications. Scott</body>
  </mail>
  <mail>
    <header>Re: Resulting CIFilter image size</header>
    <body>On Oct 18, 2007, at 4:57 PM, Simon Raisin wrote: You have to bear in mind that a CIImage is sort of an abstract thing.  It's not at all like a data structure containing a bitmap, but more of a recipe for rendering a bitmap from the ingredients you will provide.  In that sense the extent of the image isn't telling you the size of the blurred image, but instead the limits of the area to be affected by rendering.  If you look at it that way what you're seeing makes perfect sense (at least it does to me). Nick</body>
  </mail>
  <mail>
    <header>Re: Resulting CIFilter image size</header>
    <body>On 10/18/07, It sounds to me like the filter is causing the image to grow by 18 pixels in each direction.&amp;nbsp;&amp;nbsp;When you apply a gaussian blur to an image you&amp;#39;d generally expect the edges to be blurred so your result seems reasonable to me.</body>
  </mail>
  <mail>
    <header>Re: Resulting CIFilter image size</header>
    <body>On 10/18/07, When you apply a blur (or any other convolution) to an image, the resulting image is bigger. In many cases (including, apparently, all the code that you&amp;#39;ve &amp;quot;seen that renders a CIImage&amp;quot;), you want that. In others (including, apparently, your case), you want an output image that is cropped to the size of the input image. So yes, you should use the width and height from the source image, when creating your output CGBitmapContext.</body>
  </mail>
  <mail>
    <header>Re: Resulting CIFilter image size</header>
    <body>On Oct 18, 2007, at 4:45 PM, Simon Raisin wrote: It sounds to me like the filter is causing the image to grow by 18 pixels in each direction.  When you apply a gaussian blur to an image you'd generally expect the edges to be blurred so your result seems reasonable to me. Nick</body>
  </mail>
  <mail>
    <header>Re: How to make CIContext when not drawing on screen?</header>
    <body>(CGImageRef)[myCIContext createCGImage:(CIImage *)im fromRect: (CGRect)r Hope this helps, - Ralph</body>
  </mail>
  <mail>
    <header>ATSUI text color space</header>
    <body>font-family:Arial'&gt;When drawing text with ATSUI, what color space does it assume for the text color?&amp;nbsp; Is there a way to control it? font-family:Arial'&gt;The kATSURGBAlphaColorTag only specifies the color components, as opposed to a CGColorRef. font-family:Arial'&gt; font-family:Arial'&gt;I&amp;#8217;m drawing strokes and fills with GenericRGB colors into a CGBitmapContext created with the GenericRGB space, but ATSUI is coming out different. &amp;nbsp;It seems like it&amp;#8217;s matching the main display or something.&amp;nbsp; When the Main Display is set to Generic RGB, the text colors match the other graphics. font-family:Arial'&gt; font-family:Arial'&gt; font-family:Arial'&gt;</body>
  </mail>
  <mail>
    <header>How to make CIContext when not drawing on screen?</header>
    <body>I want to take a CGImage, apply a couple of filters, and create a new CGImage.  I don't want to draw anything on the screen.  What's the best way to create a CIContext for this?  Make a dummy CGBitmapContext, use an invisible window, or what? By the way, all my windows are Carbon windows, so I don't know if the [[NSGraphicsContext currentContext] CIContext] technique would work for me. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>QCRenderer speed</header>
    <body>Hello everyone, I am trying to add QC Compositions support in my app. Doing like in ImageFX sample from Developers examples folder. It works. But.. Speed is poor comparing to CIFilters performance. Is that normal? I expected faster rendering. Things I am using: 1. Creating QCRenderer: _renderer = [[QCRenderer alloc] initWithCGLContext:[_openGLContext CGLContextObj] pixelFormat:[format 2. Setting all required parameters to QCRenderer. Input image is CIImage. 4. Getting image from renderer (CIImage format again) and displaying it. Every time parameters are changed, I am setting ALL input parameters to renderer in step 2 and then 3, 4. Would be faster, if I would set only parameter which has changed, not all? &amp;quot;_time&amp;quot; is a constant. By the way. I have played a bit with non image filters group compositions. Some of them does not have output parameter for getting image from it. I can use createSnapshotImageOfType to get image, but I am getting image with black background, instead of transparent. Tried to get snapshot as NSImage and saved its TIFFRepresentation as testImage.tiff. Am I doing something wrong, o this is expected result? I was not able to find something about this in documentation. Regards, Jonas S.</body>
  </mail>
  <mail>
    <header>Re: Using CIColorMap</header>
    <body>Looks like you forgot to include the code snippet? But the &amp;quot;rainbow gradient&amp;quot; I was talking about is, in fact, the colormap.tiff file. When I apply it to any color JPEG using DiscLabel (our software), I get a color result. Judging from your previous code snippet, we're using essentially the same code to load the images and set the filter inputs. I think the only difference I see is that we always call [filter setDefaults] before setting input parameters, but in this case, the filter only has 2 inputs, and you explicitly set both, so that should not make a difference. Perhaps your graphics context / CIContext has some kind of option setting that turns things gray?</body>
  </mail>
  <mail>
    <header>Re: Using CIColorMap</header>
    <body>I don't either.  In fact, I can't find any reference to CIAttributeTypeGradient anywhere, including the FunHouse example App. That's an interesting hypothesis. However, as you can see from the code I gave, I am using the same example gradient image (colormap.tiff) that is used in the FunHouse example, and applying it to the same sample image (copenhagen.jpg) in the FunHouse example.  So there's no grey in either image.  I used the same two images in Quartz composer, and it works there.  The only thing I can think of is that my app thinks the gradient image is grey for some reason, but I don't see why it would. I'm using this to load it:</body>
  </mail>
  <mail>
    <header>Re: Using CIColorMap</header>
    <body>It is a re-post, at least for me. I was thinking that someone more knowledgeable might answer, but I'll give it a go. The docs say that inputGradientImage is supposed to be &amp;quot;A CIImage class whose attribute type is CIAttributeTypeGradient...&amp;quot; I don't know how to get a CIImage that is a CIAttributeTypeGradient. In my experience, any CIImage can be supplied as the gradient image. What the filter seems to do is take the first row of pixels across the top of the image and use that to do an intensity value mapping. So, if your gradient image is all gray along the top row, you'll get a grayscale result. Using a rainbow gradient gives you a color result. Cheers, Brian</body>
  </mail>
  <mail>
    <header>Re: CGScreenUpdateMoveCallback never called?</header>
    <body>&amp;gt;The Move variant is currently not called by the window server. Thank you, good to have definitive knowledge. Ok:-) Kai</body>
  </mail>
  <mail>
    <header>CGImageSourceCreateWithURL() leaves file open?</header>
    <body>Has anyone one noticed that CGImageSourceCreateWithURL() will keep the image file open until the app quits?  I'm fairly certain that I have my reference counting code correct.  I have logged bug 5616988 with Apple which includes an app and code showing this. On 10.4 this isn't a problem for me, but with 10.5, it interferes with my Move to Trash feature because the users can not empty the trash afterwards because CGImageSourceCreateWithURL() is keeping the file open. Isaac</body>
  </mail>
  <mail>
    <header>Re: CGContextDrawShading performance problem on 10.5</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: CGContextDrawShading performance problem on 10.5</header>
    <body>Hi Haroon, Thanks for the info! &amp;nbsp;I tried Scott Thompson's suggestion to cache the drawing to a CGLayer instead of hitting CoreImage every time, and the user reports that it helped to some extent. The user's samples show some mysterious threads as well that aren't familiar to me: 1620 Thread_2803 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1620 thread_start &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1620 _pthread_start &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1620 glvmDoWork &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1620 pthread_cond_wait$UNIX2003 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1620 __semwait_signal &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1620 __semwait_signal &amp;nbsp;&amp;nbsp;&amp;nbsp;1620 Thread_2903 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1620 thread_start &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1620 _pthread_start &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1620 fe_fragment_thread &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1620 pthread_cond_wait$UNIX2003 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1620 __semwait_signal &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1620 __semwait_signal are those related to the glReadPixels_Exec at all? &amp;nbsp;Just curious... I didn't get around to filing a bug report yet on the performance problem; if this is expected behavior, I won't bother. thanks, Adam</body>
  </mail>
  <mail>
    <header>Re: CGContextDrawShading performance problem on 10.5</header>
    <body>Hi Haroon, Thanks for the info!  I tried Scott Thompson's suggestion to cache the drawing to a CGLayer instead of hitting CoreImage every time, and the user reports that it helped to some extent. Sadly, I can't use CGShading or NSGradient since we support Tiger.  So is this likely running on the CPU on my PowerBook and G5, even though I'm using Leopard?  That would explain why I'm not seeing the performance problems on my hardware. The user's samples show some mysterious threads as well that aren't familiar to me: 1620 Thread_2803 1620 thread_start 1620 _pthread_start 1620 glvmDoWork 1620 pthread_cond_wait$UNIX2003 1620 __semwait_signal 1620 __semwait_signal 1620 Thread_2903 1620 thread_start 1620 _pthread_start 1620 fe_fragment_thread 1620 pthread_cond_wait$UNIX2003 1620 __semwait_signal 1620 __semwait_signal are those related to the glReadPixels_Exec at all?  Just curious... I didn't get around to filing a bug report yet on the performance problem; if this is expected behavior, I won't bother. thanks, Adam</body>
  </mail>
  <mail>
    <header>Re: Crash in CGPSConverterConvert</header>
    <body>Hi! I have stumbled over a crash in CGPSConverterConvert... From the crashlog, it seems that the converter tries to return some error message, but somehow fails and crashes: [crash log removed] I have tried to find out what is going on, but since that crash happened at a customer site, and the customer has a confidentiality problem with giving me the EPS files that causes the crash, I am a bit stuck. Is there anything I could do wrong what can cause this problem? Anything I could improve? Must I provide some error callback? I only provide a progress callback in CGPSConverterCallbacks, all others are set to NULL...</body>
  </mail>
  <mail>
    <header>Re: argb32_image_mark_RGB32 vs. sseCGSBlendXXXX8888?</header>
    <body>Howdy, I'm trying to figure out what the preconditions are for CGContextDrawImage &amp;gt; argb32_image using argb32_image_mark_RGB32 instead of sseCGSBlendXXXX8888. &amp;nbsp;In both cases the source image is created via CGImageCreate, 8 bits per component, 32 bits per pixel, Device RGB color space, kCGImageAlphaPremultipliedFirst (or kCGImageAlphaSkipFirst). &amp;nbsp;In the case where argb32_image_mark_RGB32 is being used, kCGBitmapByteOrderDefault is being specified (or rather, no byte order is being specified), and in the other case kCGBitmapByteOrder32Host is -- but it doesn't matter if I specify 0 (and get inverted colors), the sseCGSBlend method is used in both cases (just a Transpose variant if I specify 0). Any ideas? &amp;nbsp;When mark_RGB32 is being used, the destination context is obtained from a QuickDraw GrafPort via QDBeginCGContext, whereas when the sse method is being used, it's the CGContext passed to a Cocoa view's drawRect handler. &amp;nbsp;(Also, am I barking up the wrong tree here? Is argb32_image_mark_RGB32 faster than sseCGSBlendXXXX8888, or is something else accounting for the speed difference? &amp;nbsp;It's about 10% here, though much of the surrounding code is different due to the switch from QD/Carbon to Cocoa.) Thanks, &amp;nbsp;&amp;nbsp;&amp;nbsp;- Vlad</body>
  </mail>
  <mail>
    <header>Re: Query max CGLayer size</header>
    <body>The CGLayer are not working properly when are bigger than a certain size (aprox 900 x 900 pixels). By not working properly I mean that the layer is restricted to that size and the extra part is not drawn: no error is returned. Is there a way to query for the maximum supported size ? Thanks Marco Pifferi</body>
  </mail>
  <mail>
    <header>Re: CGScreenUpdateMoveCallback never called?</header>
    <body>Hi, I am experimenting with CGRegisterScreenRefreshCallback and CGScreenRegisterMoveCallback from Quartz Display Services. CGRegisterScreenRefreshCallback works pretty much as expected, the registered callback of type CGScreenRefreshCallback is called for all changes on the screen. But a callback installed with CGScreenRegisterMoveCallback (of type CGScreenUpdateMoveCallback) seems to be never called. Is this because any move of a window results in recomposition due to the window shadows and therefore pure moves do not exist? Or is this function called if the arrangement of multiple displays is changed? (I did not yet test the later idea) Thanks in advance Kai Br√ºning</body>
  </mail>
  <mail>
    <header>Re: Is that possible?</header>
    <body>On Nov 27, 2007, at 3:24 AM, Harun ESUR wrote: I don't think it makes sense to try to implement a version of Quartz for generic Unix.  There'd be too much reinventing the wheel.  For example a similar project already well underway is Cairo ( ).  They have a similar graphics model, have backends for X-Windows and MS-Windows, and output to PDF and SVG.  They also have an experimental backend for Quartz.  I'd say your time would be better spent contributing to Cairo in any areas where you find it doesn't work well for you. Nick</body>
  </mail>
  <mail>
    <header>Re: CoreImage or openGL</header>
    <body>On 27 Nov 2007, at 17:31, Alexander Cohen wrote: Depends. You haven't really described what you want to do very well. Are you talking about something like painting red paint (your colour) onto a 2D surface that &amp;quot;has texture&amp;quot;, like cloth or rough paper, for example? In that case you maybe able to do something with core image by having a normal map of your surface and a light position passed in as parameters, and then using a custom kernel to do the bump mapping / light calculation. The point is the main part of the task will be writing the custom filter / shader, and that part is pretty much the same whether you're in OpenGL or CI. Playing in Quartz Composer normally gives you a good idea of what you can achieve in CI.</body>
  </mail>
  <mail>
    <header>CoreImage or openGL</header>
    <body>I'm looking into compositing a color onto a texture and it needs to look very much like that color would look in reality under lighting conditions using a surface normal. Is there any way to do this using CoreImage or do i really need to revert to openGL? All ive got going for now is a blend mode filter with an image filled with a color and an image as a texture. AC</body>
  </mail>
  <mail>
    <header>Re: Gaussian Blur or Any Blur w/ Carbon and C++</header>
    <body>On 26 Nov 2007, at 19:19, Carmen C. Cerino Jr. wrote: Although the Core Image API is Objective-C, it's easy to use it from a C++ application. You just need to either write some C wrapper functions for the bits of the Core Image API you;re using or compile your C++ files with the ObjectiveC++ compiler.</body>
  </mail>
  <mail>
    <header>Is that possible?</header>
    <body>Hello there, I and some friend of mine talked about that for days. And i want to ask that to you for having actual ideas. Question is possibility of porting quartz to any other unix platform. I know that quartz is kind of closed source, but maybe development of an implementation of it could be possible. But if it is, which ways should we follow and where should we start from? Ideas? Best regards. Harun ESUR</body>
  </mail>
  <mail>
    <header>Re: CG C API from Cocoa?</header>
    <body>On Nov 26, 2007, at 8:15 PM, Rick Mann wrote: CGContextRef cgContext = (CGContextRef) [[NSGraphicsContext</body>
  </mail>
  <mail>
    <header>Re: CG C API from Cocoa?</header>
    <body>On Nov 26, 2007, at 6:15 PM, Rick Mann wrote: I think I just answered my own question. Sorry for the noise: /apple_ref/doc/uid/TP40003290-CH211-BAAJDFGJ</body>
  </mail>
  <mail>
    <header>CG C API from Cocoa?</header>
    <body>As I understand it, I can use the C API (CGxxx methods) from Cocoa just fine. I'm trying to duplicate a view class I have in my C++ Carbon app in Cocoa. I've created a subclass of NSView, and I'm implementing the drawRect method. But it doesn't pass me a CGContextRef (which I got before via the draw CarbonEvent). TIA, Rick</body>
  </mail>
  <mail>
    <header>CGScreenUpdateMoveCallback never called?</header>
    <body>I am experimenting with CGRegisterScreenRefreshCallback and CGScreenRegisterMoveCallback from Quartz Display Services. CGRegisterScreenRefreshCallback works pretty much as expected, the registered callback of type CGScreenRefreshCallback is called for all changes on the screen. But a callback installed with CGScreenRegisterMoveCallback (of type CGScreenUpdateMoveCallback) seems to be never called. Is this because any move of a window results in recomposition due to the window shadows and therefore pure moves do not exist? Or is this function called if the arrangement of multiple displays is changed? (I did not yet test the later idea) Thanks in advance Kai Br√ºning</body>
  </mail>
  <mail>
    <header>Re: Getting QCComposition path</header>
    <body>Hello David, thank you for your tip. I have had some troubles by choosing color space. Finally decided to use Generic RGB. Looks like works in right way. With others I got wrong colors, because when drawing to screen I am matching colors to embedded image colorsync profile.</body>
  </mail>
  <mail>
    <header>Re: CALayer question</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>CALayer question</header>
    <body>If I set the contents property, the animation works. Attachment:</body>
  </mail>
  <mail>
    <header>Converting RGB to L*a*b*</header>
    <body>I'm trying to convert CGColorRef colors from RGB color spaces (generic or calibrated device, usually) to L*a*b*. No matter what I do, it seems slightly off. I notice that Digital Color Meter, for example, always gets a == 0 and b == 0 for any R == G == B (gray). No matter what method I've tried, I always get a == b, but they're not quite zero, e.g. 0.00390625. L is usually closer, but a and b are frequently a little off. I thought it might be a simple precision problem and that I should be rounding the values, but sometimes it looks like the values I get should be rounded to hundredths to get Digital Color Meter's values and sometimes it looks like they should be chopped. I've tried various methods to do the conversion involving combinations of NS, CI and CG colors and/or color spaces. Some do better, some aren't even close, but I've settled on using ColorSync directly (simplified source below). It's probably good enough for my purposes, but I wonder if there's a way that is less indirect or more accurate. CFDataRef			rgbProfileData = profileLocation.u.bufferLoc.buffer = (void*) CMFloatBitmap		rgbBitmap = CMFloatBitmapMakeChunky CMFloatBitmap		labBitmap = CMFloatBitmapMakeChunky(lab, 1, 1,</body>
  </mail>
  <mail>
    <header>Re: String constant settings</header>
    <body>Well, yes, in XCode 3 just hover the mouse over your code in the debugger and it shows the string value. Just want to be sure that its ok to USE that value in text / XML.</body>
  </mail>
  <mail>
    <header>Re: String constant settings</header>
    <body>Not sure that this answers your question but it seemed to take forever for me to find this so I'll pass it on in case it helps... If you command double click on any element in xcode it will take you to where its declared, if you alt double click it will take you to the docs. -bd-</body>
  </mail>
  <mail>
    <header>Re: CALayer animated custom property</header>
    <body>Unfotunately animating subclass properties is not supported, though it's something we would like to add in a future release, On Nov 21, 2007, at 6:56 PM, Bertrand Landry-Hetu wrote:</body>
  </mail>
  <mail>
    <header>String constant settings</header>
    <body>Any idea what the constants like &amp;quot;kCICategoryColorAdjustment&amp;quot; are defined as? I need to be able to put those into an xml property list file that is created by HAND so presumably I can just enter them as a string? I suppose I can go through using the debugger and figure it out.</body>
  </mail>
  <mail>
    <header>Query max CGLayer size</header>
    <body>The CGLayer are not working properly when are bigger than a certain size (aprox 900 x 900 pixels). By not working properly I mean that the layer is restricted to that size and the extra part is not drawn: no error is returned. Thanks Marco Pifferi</body>
  </mail>
  <mail>
    <header>Re: Getting QCComposition path</header>
    <body>You can obtain the CGLContextObj and CGLPixelFormatObj from the corresponding NSOpenGLContext and NSOpenGLPixelFormat objects and you can use CGColorspaceCreateWithName() to create a CGColorSpaceRef for the color space. Is there anything else preventing you from using initWithCGLContext:pixelFormat:colorSpace:composition:? -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>Getting QCComposition path</header>
    <body>Hello, I am wondering if is there any way to get Quartz Composer composition path (file location) from QCComposition object. I am interested exactly in that, what this object keeps in its private member '_backing'. For example:  &amp;lt;QCComposition = 0x00183C20 | identifier = &amp;quot;/lighttunnel&amp;quot; | source = File | backing = &amp;quot;/System/Library/Compositions/LightTunnel.qtz&amp;quot;, I need to know this: &amp;quot;/System/Library/Compositions/LightTunnel.qtz&amp;quot;. Why I need this information? I have all available compositions from repository and I want to create a QCRenderer by using &amp;quot;initWithOpenGLContext:pixelFormat:file:&amp;quot;. &amp;quot;initWithCGLContext:pixelFormat:colorSpace:composition:&amp;quot; is not very suitable for me due to different arguments and additional colorspace parameter. Any help is appreciate. Thanks, Jonas S.</body>
  </mail>
  <mail>
    <header>CALayer animated custom property</header>
    <body>Hi list, I'm trying to get some property of a CALayer subclass to animate and can't get it to work. Here is the code I tried: @implementation MyLayer //subclass of CALayer. -(CGFloat)myPosition -(void)setMyPosition:(CGFloat)point @end Then in a method somewhere I have: CABasicAnimation * anim = [CABasicAnimation animationWithKeyPath: I've put a break point in setMyPosition: and nothing happens. I also tried to get implicit animation working and found out that -actionForKey: does not get called, unless I don't implement my own accessors. Is there any magic to do to make it become a animatable property ?</body>
  </mail>
  <mail>
    <header>Re: high cpu use when embedding an NSView into an NSBox</header>
    <body>You might also want to use a separate CALayer to do your overlay drawing.</body>
  </mail>
  <mail>
    <header>Re: high cpu use when embedding an NSView into an NSBox</header>
    <body>You could try using the QTCaptureView delegate method: - (CIImage *)view:(QTCaptureView *)view willDisplayImage :(CIImage *)image On Nov 21, 2007, at 12:38 PM, Oriol Ferrer Mesi√† wrote:</body>
  </mail>
  <mail>
    <header>Re: high cpu use when embedding an NSView into an NSBox</header>
    <body>I'm doing some basic image analysis of the video images that affect different overlays I draw, so I find it easier to just get a CIImage or NSImage from the QTCapture, draw it and draw anything that's needed (nsbezier paths, other ciimages,etc ) on top of it. // //  Oriol Ferrer Mesi√† // // Attachment:</body>
  </mail>
  <mail>
    <header>Re: high cpu use when embedding an NSView into an NSBox</header>
    <body>About the boxing... yes, sound like a bug to me too. If no one else here confirms this is NOT a bug, you should file it. About the difference in CPU usage between your NSView and QTCaptureView. I bet QTCaptureView is using OpenGL for displaying the frames.. so it's rather optimized for performance. How are you drawing the frames? The QT Capture classes will notify you when a new frame is available... why use a timer instead? On 21.11.2007, at 13:32, Oriol Ferrer Mesi√† wrote:</body>
  </mail>
  <mail>
    <header>high cpu use when embedding an NSView into an NSBox</header>
    <body>Hi everyone, I'm playing with the new QTCapture api, and I built a small app that displays live video from camera. All good so far, but I was extremely surprised by something. I built a custom NSView, that grabs the current frame from camera and displays it, triggered by a NSTimer 25 times per second. This alone takes around 32% cpu on activity monitor, when using a QTCaptureView to display current video feed takes around 15% (I wonder if I should be subclassing the QTCaptureView somehow, but that's not the issue now). The amazing thing is that when I decide to embed my CustomNSView into a NSBox from interface builder and run the code again, it takes 55% of my cpu. QTCaptureView :	15% CPU Custom NSView with NStimer:	32% CPU Custom NSView with NStimer, embedded into a NSBox:	55% CPU What's going on here? There seems to be some SERIOUS overhead somewhere... So I went ahead and embedded the newly created box into a new box, so we have a view inside a box that's inside another box. Guess what; CPU use around 75%! I always thought NSBoxes on IB where just elements aimed at design, organization and &amp;quot;beauty&amp;quot; of your app's UI, that would never affect performance seriously.... But I'll be very careful when using them from now on! // //  Oriol Ferrer Mesi√† // // Attachment:</body>
  </mail>
  <mail>
    <header>CIAreaHistogram Filter</header>
    <body>I am looking for information on how the CIAreaHistogram filter works. There appears to be no documentation besides the CI Filter Browser widget. What I am trying to do is finding the most-often used color in a given image. This should be easy to determine from the image's histogram, however, I do not know how to interpret the output of the CIAreaHistogram filter. Simon</body>
  </mail>
  <mail>
    <header>Re: CoreImage to Bitmap Questions</header>
    <body>I don't see anything obvious in the code you've shown, but I would like to point out that using the CIImage's extent like this could cause problems.  Some combinations of filters can lead to an infinite extent - not something you'd want to create the image size from. When creating CGBitmapContexts, I usually use kCGImageAlphaPremultipliedFirst (by itself) - you could try that and see if you still get the artifacts. You could also try creating the backing store for the context yourself.  It probably won't make a difference, but it's something to try out. Something like this: How are you saving the images out?  That may shed some more light on the issue. On Nov 20, 2007, at 10:55 AM, Jim Crate wrote: -------------------------------------- Darkshadow (aka Michael Nickerson)</body>
  </mail>
  <mail>
    <header>Re: [Leopard] Warning about using CMYK color spaces in transparency layers</header>
    <body>En/na Michael Monscheuer ha escrit: I'm working in a engine PDF parser (with various pourposes, including the color sep.), and whatever particular PDF file are welcome to analyze. -- MaRC anToni Malagarriga i Picas T¬∑(34)938721642 | (34)667517069 (][ www.femfum.com PostScript&amp;amp;PDF Tool Developers Desenvolupadors d'Eines PostScript i PDF</body>
  </mail>
  <mail>
    <header>CoreImage to Bitmap Questions</header>
    <body>I have an app I'm working on which loads images, applies some CoreImage filters, and then saves the resulting image to disk.  There is an NSImageView preview in the app to show the resulting image after the filters are applied.  At this point, it is using simple filters for scaling/cropping, applying a watermark image with varying opacity, possibly making the image monochrome. If I create an NSImage with an NSCIImageRep for the NSImageView, this works well, performance is great and the image displays correctly, although there are memory leaks that have to be chased down. Googling has found several people mentioning that using that method leaks lots of memory. I also have to save these files, and since the existing code used an NSBitmapImageRep, I thought a category on CIImage might be nice: -(NSImage *)NSImageWithNSBitmapImageRep NSBitmapImageRep *ir = [[[NSBitmapImageRep alloc] NSImage *image = [[[NSImage alloc] initWithSize:NSMakeSize([self However, the images wrote out with lines and other artifacts.  I thought I had seen someone mention this before, but I cannot find that now.  If I change the code that updates the view to create the view's NSImage using this method instead of the NSCIImageRep, the image displays rendered incorrectly as well. Since my end goal is to save the images using CGImage and Image I/O, I set up the code to generate a CGImage to see if it worked any better. As a category on CIImage: - (CGImageRef)createCGImage CGColorSpaceRef colorSpace = CGBitmapInfo bitmapInfo = kCGImageAlphaPremultipliedLast | CGContextRef cgContext = CGBitmapContextCreate(NULL, width, rows, 8, // get the CIContext and render the CIImage CIContext* ciContext = [CIContext contextWithCGContext:cgContext CGImageRef cgImage = [ciContext createCGImage:self fromRect:[self I also tried using this to create the CGImage in this method: However, the file saved has the same type of artifacts.  The overlaid watermark image may not be visible at all, or the file may have anything from a few thin white lines to being completely illegible, like this one: In addition, while a large image (18000 x 14400) will render using NSCIImageRep or (incorrectly) into an NSBitmapImageRep, if I try to create a CGImage it will crash deep in the rendering code when calling [ciContext createCGImage:self fromRect:[self extent]].  Images up to 10000 x 8000 render into a CGImage, albeit similar to the test image above. I'm using MacOS 10.5.1 on a 15&amp;quot; MacBook Pro, Objective-C 2.0 with garbage collection enabled.  I continued adding the manual GC code (retain, release, autorelease) in case I had to switch back for some reason, but I'm pretty new to Cocoa development and so it is quite likely that I have done things wrong somewhere. Has anyone seen this kind of problem before, or have any pointers on where to continue searching for a solution?</body>
  </mail>
  <mail>
    <header>How to add randon transparent dots over CGPath or NSBezierPath</header>
    <body>Hello, . Here are more details about the problem. I've a solid line say width = 30(drawn using&amp;nbsp;) , now I wanted to draw transparent dots over it or transparent lines(thickness=2 or something smaller than 30). DISCLAIMER ========== This e-mail may contain privileged and confidential information which is the property of Persistent Systems Ltd. It is intended only for the use of the individual or entity to which it is addressed. If you are not the intended recipient, you are not authorized to read, retain, copy, print, distribute or use this message. If you have received this communication in error, please notify the sender and delete all copies of this message. Persistent Systems Ltd. does not accept any liability for virus infected mails.</body>
  </mail>
  <mail>
    <header>Re: Exact CGColor?</header>
    <body>What color space did you obtain your original values with respect to? That is the color space that you should be using the specify the color, assuming you don't want to change the components. The reason for the change you saw  is that the screen shot is in DeviceRGB, which isn't the same as GenericRGB (and is unpredictable because it isn't a known color space). So your basic options are either specify the color with respect to the correct color space, or convert the components you have to GenericRGB and specify them relative to GenericRGB. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Exact CGColor?</header>
    <body>hello, when I create a CGColor with the values red = 1.0 green = 0.4 blue = 0.0 I should get some bright and shiny orange. However the color looks a bit dull and when I make a screenshot and check the color values in Photoshop they are in fact something like red = 0.9 green = 0.5 blue = 0.1 I create the color with Maybe I should use with a different color space? But which one? Thanks and regards, Sebastian Mecklenburg _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>(no subject)</header>
    <body>http://cce.hcmut.edu.vn/images/aam.php</body>
  </mail>
  <mail>
    <header>Re: shouldRasterize and sublayer alignment</header>
    <body>self.view.bounds = CGRectMake(0.3, 0.3, 1000.0, 1000.0); // simulate view being scrolled zero.affineTransform = CGAffineTransformMakeScale(scale, scale); // simulate view being zoomed oneText.affineTransform = CGAffineTransformMakeScale(1.0 / scale, oneText.position = [two convertPoint:CGPointMake(400, 400) twoText.affineTransform = CGAffineTransformMakeScale(1.0 / scale, twoText.position = [two convertPoint:CGPointMake(400, 600) View (bounds shifted) The view layer bounds is shifted to simulate the scrolling of a UIScrollView. The layer Zero's transform is scaled to simulate the zooming of a UIScrollView. The inverse transform at the text layer level cancels out the transform at layer Zero, preserving the full resolution of those layers. Finally, the position is adjusted so that the converted position is on an integral boundary, when converted to Core Animation &amp;gt; Color Misaligned Images which clears the rendition of twoText. If there isn't any scrolling i.e. view layer bounds is (0, 0), the blurriness mysteriously disappears. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>shouldRasterize and sublayer alignment</header>
    <body>If I set shouldRasterize on a layer, how are the sublayers contents incorporated into the generated cached bitmap? How can we ensure that there is no unwanted interpolation or visible &amp;quot;smearing&amp;quot; when the incorporation happens? Each layer's origin and transformation may be at a fractional value instead of an integral value. But I ensure that the layer C bounds, when transformed into layer A coordinates, is always at integral values. This way, layer C appears to be crisply incorporated into the picture, in particular under Instruments it is not colored as misaligned. Now when I set shouldRasterize on layer B, layer C becomes fuzzy. I can't seem to set the right bounds for C to prevent fuzziness. I can't see if it is genuinely misaligned because Instruments doesn't appear to display internally misaligned sublayers for a rasterized layer. Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Re: Fuzzy Text in CATextLayer</header>
    <body>Hi Gerry, -Ken ¬†_______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list ¬† ¬† ¬†() Help/Unsubscribe/Update your Subscription: This email sent to</body>
  </mail>
  <mail>
    <header>Re: Fuzzy Text in CATextLayer</header>
    <body>I haven't specifically used CATextLayer, but have had similar issues with drawing on layers, which turned out to be pixel alignment issues. When I used CGContextConvertRectToDeviceSpace and CGContextConvertRectToUserSpace to ensure that the layer was pixel aligned, I was able to draw clear text. Not sure if you will need to be on the pixels or between them, but here are some methods that may help for setting your frame: HTH. Gideon</body>
  </mail>
  <mail>
    <header>Re: ImageIO, illustrator &amp;amp; thumbnails size</header>
    <body>Thanks for your answer, I'll look at the PDF APIs.</body>
  </mail>
  <mail>
    <header>Re: ImageIO, illustrator &amp;amp; thumbnails size</header>
    <body>If you want bigger thumbnails, then your best bet is to create a bitmap context yourself at the size you want and draw the content into that bitmap context. Last time I looked, Illustrator files were essentially highly marked up PDF files, so you would need to use the PDF APIs to draw them instead of ImageIO (see the CGPDFDocument APIs). -- David Duncan</body>
  </mail>
  <mail>
    <header>ImageIO, illustrator &amp;amp; thumbnails size</header>
    <body>Hello everyone, I'am playing a bit with Image IO to convert an Adobe Illustrator file (.ai) to a jpeg one with a specified size. My problem is that the maximum size for the output file seems to be limited to 1024px, no matter what value I pass to the kCGImageSourceThumbnailMaxPixelSize key. I'm new to this stuff so perhaps I am missing something, any help is welcome. Regards. _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: Subpixel functions</header>
    <body>A book I would highly recommend for all readers of this list, btw! --&amp;gt; Michael B. Johnson, PhD --&amp;gt;  (personal) --&amp;gt;  (alum) --&amp;gt; MPG Lead --&amp;gt; Pixar Animation Studios</body>
  </mail>
  <mail>
    <header>Re: Subpixel functions</header>
    <body>&amp;gt;&amp;gt;&amp;gt; What do the various subpixel functions do in iOS 4.0 etc.? From my book: When drawing text on a context attached to a color LCD display, Quartz takes advantage of the nature of LCD monitors to improve the legibility of text. This technique is called Font Smoothing. The pixels of an LCD monitor are made up of red, green, and blue sub-pixels. If you take these sub-pixels into account the screen appears to have three times the resolution commonly attributed to it, at least in one dimension. Font smoothing takes advantage of this increased resolution to improve the rendering of text. Quartz turns different sub-pixels off and on by changing the color of a pixels along the edge of letter shapes. Because your eye expects to see a hard line at the edge of the glyphs, the computer tricks it into ignoring the color in favor of perceiving a smooth edge. One disadvan- tage of font smoothing is that it relies on the fixed ordering of the sub-pixels of an LCD display. That makes the technique of limited use on other types of monitors. Font smoothing is also of limited use on offscreen bitmaps. Subpixel positioning concerns whether or not the glyphs in a line of text will be aligned to pixel boundaries or not.  If subpixel positioning is off then when glyphs are drawn their positions might be shifted slightly to take pixel boundaries in account.  This can improve the visual definition of the glyphs (making them slightly less &amp;quot;blurry&amp;quot;) at the expense of honoring the font metrics. Subpixel quantization is only enabled if subpixel positioning is enabled. Subpixel quantization improves the rendering of fonts whose glyphs are at subpixel positions by more closely examining how the shapes that make up the glyphs cover an individual pixel.  This improvement, requires additional processing so changing this value can affect text drawing performance. Scott</body>
  </mail>
  <mail>
    <header>Re: Subpixel functions</header>
    <body>Check out the docs. ¬† What do the various subpixel functions do in iOS 4.0 etc.? Specificially, CGContextSetAllowsFontSmoothing CGContextSetAllowsFontSubpixelPositioning CGContextSetAllowsFontSubpixelQuantization</body>
  </mail>
  <mail>
    <header>Re: Subpixel functions</header>
    <body>What do the various subpixel functions do in iOS 4.0 etc.? Specificially, CGContextSetAllowsFontSmoothing CGContextSetAllowsFontSubpixelPositioning CGContextSetAllowsFontSubpixelQuantization</body>
  </mail>
  <mail>
    <header>Re: Subpixel functions</header>
    <body>What do the various subpixel functions do in iOS 4.0 etc.? Specificially, CGContextSetAllowsFontSmoothing CGContextSetAllowsFontSubpixelPositioning CGContextSetAllowsFontSubpixelQuantization</body>
  </mail>
  <mail>
    <header>Subpixel functions</header>
    <body>CGContextSetAllowsFontSmoothing CGContextSetAllowsFontSubpixelPositioning CGContextSetAllowsFontSubpixelQuantization Also, why the design rationale to have both a CGContextSetAllows-xxx and a CGContextSetShould-xxx versions of these functions? Do these functions affect CoreText rendering or are there equivalent attributes for CoreText rendering? Cheers, Glen Low --- pixelglow software | simply brilliant stuff www.pixelglow.com aim: pixglen twitter: pixelglow</body>
  </mail>
  <mail>
    <header>Color shift</header>
    <body>Hello all, I'm trying to write a video analysis tool using the GPU for fast processing. So far so good except for this: I need to do some pixel wise calculation like calculating the color average and the standard deviation of a region etc.... The problem is that I can't seem to get accurate colors from my filter. I can do something simple like: and when I convert the resulting CIImage into an NSBitmapImageRep using initWithCIImage:  I look at the colors in the pixels and get the following reading for all the pixels: 0.733333 0.737255 0.733333 1.0 I know this must be related to the colorspace difference or something of the sort but I haven't been able to get an accurate reading even after converting the resulting image to all of the available colorSpaces. I really need the raw pixel information in order to do color matching for segmentation. Any help will be deeply appreciated. Best regards, Alejandro Rodr√≠guez _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>CTLineGetImageBounds origin?</header>
    <body>Hi, I'm trying to use CTLineGetImageBounds to figure out where the lines of a given string reside within an image context after frame setting. So, for example, if I frame set a string and the result spans multiple lines, and I iterate over each of these lines and use: To get the image bounds for each line in the context, all of the origin points of this rect are almost the same. For example (drawing into an 512 x 512 rect path in the context): String: (There's still time to win $10K in gear from Wired! Go here &amp;amp; tell us why you deserve a media room makeover:) Line 1: x : 265.734375 y : 430.027344 width : 467.027344 height : 23.812500 Line 2: x : 265.664062 y : 430.027344 width : 494.976562 height : 22.523438 Line 3: x : 266.308594 y : 434.796875 width : 262.476562 height : 17.460938 It looks like the widths of the lines is correct (as the last line is the shortest), but why are the origins so similar? Shouldn't they reflect the offsets of these lines in the context? Thanks.</body>
  </mail>
  <mail>
    <header>Re: CGImage floating point formats</header>
    <body>My first attempt to create and display a floating point buffer resulted in an all black result. Web search shows others have had similar result. I‚Äôm unable to find any resolution in archive or web search. Also, I‚Äôve been unable to locate an actual working example, there‚Äôs been references to a few, but when you dig into them one sees that they don‚Äôt actually using floating point images. I‚Äôve come to the point where I think the problem may be with the color space rather than the actual floating point data. The code I use to set things up: in this example I‚Äôm simply setting the buffer contents to a constant color. FloatView is a subclass of NSView. FloatView &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;g = //.009 G output is 0, .008 G output is 255 The rgb values use here give an output of Cyan (R=0, G= 255, B= 255). If I change g = .009, then the output is Blue (R = 0, G = 0, B = 255). A change of .001 in the floating point pixel value causes a full swing in the output. Running the application results in the following log: &amp;nbsp;&amp;lt;Error&amp;gt;: ApplySequenceToBitmap failed (-171) &amp;nbsp;&amp;lt;Error&amp;gt;: ColorSyncTransformConvert - failed width = 256 height = 255 dstDepth = 7 dstLayout = 0 dstBytesPerRow = 3072 srcDepth = 7 srcLayout = 0 srcBytesPerRow = 3072 &amp;nbsp;&amp;lt;Error&amp;gt;: CMMConvLut::ConvertFloat 1 input (inf) &amp;nbsp;&amp;lt;Error&amp;gt;: ApplySequenceToBitmap failed (-171) &amp;nbsp;&amp;lt;Error&amp;gt;: ColorSyncTransformConvert - failed width = 256 height = 1 dstDepth = 7 dstLayout = 0 dstBytesPerRow = 3072 srcDepth = 7 srcLayout = 0 srcBytesPerRow = 3072 This suggests the problem is in the color conversion process, it appears to be reading float values, but running into errors. One issue is with srcBytesPerRow = 3027. Quartz documentation says the floating point support is RGB 128 bits per pixel, which implies a layout that includes, but ignores, &amp;nbsp;an alpha plane. This is reflected in the  specification in the CGImageCreate parameters. I calculate srcBytesPerRow should be 4096 bytes per row, not 3027. I considered shutting off color management, but wasn‚Äôt able to figure out how to do that. You can‚Äôt specify a NULL color space. Not relevant to quartz, but still a puzzle - when I used the defines ‚Äúwidth‚Äù or ‚Äúheight‚Äù instead of ‚Äúw‚Äù and ‚Äúh‚Äù I get compile errors. Maybe I‚Äôm smoking some bad stuff, but I‚Äôll swear this didn‚Äôt use to be the case. (Currently using Snow Leopard). Eg. The line: Doesn‚Äôt compile. Any help is appreciated. Greg</body>
  </mail>
  <mail>
    <header>CGImage floating point formats</header>
    <body>My first attempt to create and display a floating point buffer resulted in an all black result. Web search shows others have had similar result. I‚Äôm unable to find any resolution in archive or web search. Also, I‚Äôve been unable to locate an actual working example, there‚Äôs been references to a few, but when you dig into them one sees that they don‚Äôt actually using floating point images. I‚Äôve come to the point where I think the problem may be with the color space rather than the actual floating point data. The code I use to set things up: in this example I‚Äôm simply setting the buffer contents to a constant color. FloatView is a subclass of NSView. FloatView &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;g = //.009 G output is 0, .008 G output is 255 The rgb values use here give an output of Cyan (R=0, G= 255, B= 255). If I change g = .009, then the output is Blue (R = 0, G = 0, B = 255). A change of .001 in the floating point pixel value causes a full swing in the output. Running the application results in the following log: &amp;nbsp;&amp;lt;Error&amp;gt;: ApplySequenceToBitmap failed (-171) &amp;nbsp;&amp;lt;Error&amp;gt;: ColorSyncTransformConvert - failed width = 256 height = 255 dstDepth = 7 dstLayout = 0 dstBytesPerRow = 3072 srcDepth = 7 srcLayout = 0 srcBytesPerRow = 3072 &amp;nbsp;&amp;lt;Error&amp;gt;: CMMConvLut::ConvertFloat 1 input (inf) &amp;nbsp;&amp;lt;Error&amp;gt;: ApplySequenceToBitmap failed (-171) &amp;nbsp;&amp;lt;Error&amp;gt;: ColorSyncTransformConvert - failed width = 256 height = 1 dstDepth = 7 dstLayout = 0 dstBytesPerRow = 3072 srcDepth = 7 srcLayout = 0 srcBytesPerRow = 3072 This suggests the problem is in the color conversion process, it appears to be reading float values, but running into errors. One issue is with srcBytesPerRow = 3027. Quartz documentation says the floating point support is RGB 128 bits per pixel, which implies a layout that includes, but ignores, &amp;nbsp;an alpha plane. This is reflected in the  specification in the CGImageCreate parameters. I calculate srcBytesPerRow should be 4096 bytes per row, not 3027. I considered shutting off color management, but wasn‚Äôt able to figure out how to do that. You can‚Äôt specify a NULL color space. Not relevant to quartz, but still a puzzle - when I used the defines ‚Äúwidth‚Äù or ‚Äúheight‚Äù instead of ‚Äúw‚Äù and ‚Äúh‚Äù I get compile errors. Maybe I‚Äôm smoking some bad stuff, but I‚Äôll swear this didn‚Äôt use to be the case. (Currently using Snow Leopard). Eg. The line: Doesn‚Äôt compile. Any help is appreciated. Greg</body>
  </mail>
  <mail>
    <header>Re: Accessing PDF font program's Encoding array</header>
    <body>On Tue, Jan 26, 2010 at 2:38 PM, Alastair Houghton I'm surprised that PDFKit offers so much more functionality than the CoreGraphics API -- I'd have expected it to be largely a wrapper. Thanks for your advice! Best wishes, Hamish</body>
  </mail>
  <mail>
    <header>Re: Accessing PDF font program's Encoding array</header>
    <body>OK, I understand (I think).  I think you probably will need to parse the font data yourself to do this---unless you can persuade Apple to make PDFKit part of the next release of the iPhone OS, anyway. Kind regards, Alastair. --</body>
  </mail>
  <mail>
    <header>Re: Accessing PDF font program's Encoding array</header>
    <body>On Tue, Jan 26, 2010 at 1:34 PM, Alastair Houghton I'd love to, but this is for iPhone OS. Which is another reason I'd like to avoid extra sythesis/parsing steps if possible. Best wishes, Hamish</body>
  </mail>
  <mail>
    <header>Re: Accessing PDF font program's Encoding array</header>
    <body>[snip] Have you considered using PDFKit?  If it can do what you want, it's going to be much easier than trying to do the text extraction yourself. Kind regards, Alastair. --</body>
  </mail>
  <mail>
    <header>Re: Accessing PDF font program's Encoding array</header>
    <body>On Tue, Jan 26, 2010 at 9:20 AM, Alastair Houghton You're quite right -- I figured that out after I'd posted. I'd still rather avoid the extra two steps of synthesising the cmap table from data and then parsing it to extract that data, if the data is more directly accessible. I have a CFDataRef containing the font program -- that's how I get a CGFontRef in the first place. That font program data comes from the FontFile/FontFile2/FontFile3 entry in the FontDescriptor dictionary of the font resource embedded in a PDF file. I hope so :) I'm trying to extract text content from PDFs. The Adobe PDF Spec, section 5.9 &amp;quot;Extraction of Text Content&amp;quot;, says: -------- A consumer application can use the following methods, in the priority given, to map a character code to a Unicode value. Tagged PDF documents, in particular, must provide at least one of these methods (see ‚ÄúUnicode Mapping in Tagged PDF‚Äù on page 892): - If the font dictionary contains a ToUnicode CMap (see Section 5.9.2, ‚ÄúToUnicode CMaps‚Äù), use that CMap to convert the character code to Unicode. - If the font is a simple font that uses one of the predefined encodings MacRomanEncoding, MacExpertEncoding, or WinAnsiEncoding, or that has an encoding whose Differences array includes only character names taken from the Adobe standard Latin character set and the set of named characters in the Symbol font (see Appendix D): 1. Map the character code to a character name according to Table D.1 on page 996 and the font‚Äôs Differences array. 2. Look up the character name in the Adobe Glyph List (see the Bibliography) to obtain the corresponding Unicode value. -------- So if the Unicode CMap is absent, we have to fall back to examining the font encoding to try to determine text content. Section 5.5.1 &amp;quot;Type 1 Fonts&amp;quot; says the following about the Encoding entry in a font dictionary: --------- (Optional) A specification of the font‚Äôs character encoding *if different from its built-in encoding* [emphasis mine]. The value of Encoding is either the name of a predefined encoding (MacRomanEncoding, MacExpertEncoding, or WinAnsiEncoding, as described in Appendix D) or an encoding dictionary that specifies differences from the font‚Äôs built-in encoding or from a specified predefined encoding (see Section 5.5.5, ‚ÄúCharacter Encoding‚Äù). -------- And in Section 5.5.5: -------- If [the BaseEncoding] entry is absent, the Differences entry describes differences from an implicit base encoding. For a font program that is embedded in the PDF file, the implicit base encoding is the font program‚Äôs built-in encoding, as described above and further elaborated in the sections on specific font types below. Otherwise, for a nonsymbolic font, it is StandardEncoding, and for a symbolic font, it is the font‚Äôs built-in encoding. -------- So if the Encoding or BaseEncoding entry is missing, I have to try to determine the encoding from the font program. I figured that creating a CGFont object and getting the information from that might be easier than parsing the font program myself, but maybe not? Thanks again for your help in this, Hamish</body>
  </mail>
  <mail>
    <header>Re: Accessing PDF font program's Encoding array</header>
    <body>No, sorry, I don't. I doubt it, actually.  I expect it would be enough to cope with one of the Unicode cmap formats (whichever the table synthesis functionality uses) and whichever post format it synthesises.  I think it's relatively unlikely that Apple would change the synthesis code, except possibly to add new formats, though obviously you'd be relying to some extent on that. Well the PostScript font data itself won't be in the fpgm table if that's what you're suggesting.  I don't know if OS X synthesises a CFF table, but if it does then you might be able to look in there to get the font program, in which case you could parse it to find the encoding data. Is there some particular reason you're after access to the PostScript encoding table?  Maybe there's another way to achieve whatever it is you're after doing? Kind regards, Alastair. --</body>
  </mail>
  <mail>
    <header>Re: Accessing PDF font program's Encoding array</header>
    <body>Hi Alastair, On Mon, Jan 25, 2010 at 12:39 PM, Alastair Houghton Thanks for the pointer. The &amp;quot;cmap&amp;quot; and &amp;quot;post&amp;quot; tables are present, but do you know of any further support functions to aid in parsing them? Otherwise it seems I would have to implement quite a few 'cmap' and 'post' parsers for various different formats; if so, perhaps the &amp;quot;Encoding array&amp;quot; is available more directly by parsing the font program instead? Best wishes, Hamish</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate, CGContextDrawImage never works</header>
    <body>Remembering how many places I had redo the math when switching from QuickDraw (top-left is 0,0) to Quartz (bottom-left is 0,0), I'm starting to wonder if this might be part of the problem. Probably not, since he said it draws as solid white. That still makes me think the bits aren't what he thinks they are and he needs to set a breakpoint and inspect them. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate, CGContextDrawImage never works</header>
    <body>There are plenty of folks that have gotten this working :). I would honestly recommend that you simplify your problem and work back to the full issue, since you seem to be having trouble getting the full solution working. This indicates non-premultiplied alpha. Unless you are sure your alpha is 1.0 (255) you may be losing your contents here. But since you say its coming from Quickdraw, I would just recommend using the SkipFirst constant instead. You should be using the SkipFirst constant here. As written, this is equivalent to SkipLast, and is probably not what you want. If you have a pointer already, then use CGDataProviderCreateWithData() instead. Then you don't have to fuss with the no-copy semantics from CF/NSData (which may choose to copy anyway). Make sure this is doing what you want. CGContextDrawImage always draws the entire image into the destination rectangle, but you seemed to indicate before that you wanted a partial (since you were calling CGImageCreateWithImageInRect). If you want a partial, clip the context first, set the image rect's width &amp;amp; height to that of the image, and set the origin to offset the image to capture the appropriate parts. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate, CGContextDrawImage never works</header>
    <body>Have you putting in a breakpoint and inspecting thebase to see if it truly is something other than all white? Right-click the variable and choose View in Memory Browser, then page through and look for something other than 0xffffffff. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate, CGContextDrawImage never works</header>
    <body>hi all, some more information after ths line: thebits = CFDataCreateWithBytesNoCopy(NULL, (unsigned char *)thebase, my buffer is cleared! why? why? i have some bits i just want to copy them to the screen! thx bill On Sun, Jan 24, 2010 at 5:16 PM, Bill Appleton</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate, CGContextDrawImage never works</header>
    <body>hi all, here is the new version with CGImageCreate as recommended the bottom section (in comments) works fine -- you see the ovals but the top section draws nothing -- white screen i am sure the buffer is good, it 4 byte 32 bit color has anyone tried to replace a simple copybits to the screen with coregraphics? thanks bill static void qdscreen2cgscreen(portinstptr theinst, screenmap *srcbits, screenmap *dstbits, rect *srcrect, rect *dstrect) // begin cg thebits = CFDataCreateWithBytesNoCopy(NULL, (unsigned char *)thebase, theimage = CGImageCreate(width, height, bitscom, bitspix, bitsrow, // end cg /***/ /***/</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate, CGContextDrawImage never works</header>
    <body>More than likely its your bitmap info, since you don't specify what the alpha is, thus it defaults to AlphaNone, which implies that there is no provision for alpha at all, that is 555 (rather than 1555) or 888 (rather than 8888), both of which are unsupported. Thus your bitmap context probably came back as NULL. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate, CGContextDrawImage never works</header>
    <body>the CGContextRef called &amp;quot;thecont&amp;quot; in the code sample is from a firefox or safari drawing surface supplied through the NPAPI interface. i can draw circles on it just fine as a test, but all my bitmaps are white i will look at CGImageCreate but can anyone see what is wrong with the existing stuff? thanks, bill</body>
  </mail>
  <mail>
    <header>Re: Accessing PDF font program's Encoding array</header>
    <body>I *think*, though I might be mistaken, that the CGFont API synthesises TrueType/OpenType tables for PostScript fonts. If so, you should be able to get the information you're after from the &amp;quot;cmap&amp;quot; and &amp;quot;post&amp;quot; tables in the font. Kind regards, Alastair. --</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate, CGContextDrawImage never works</header>
    <body>I would use CGImageCreate() to create the image then, no need to go through a bitmap context at all. May not matter much if this is a one-time thing, but if your doing it a lot you'll see a lot more overhead (since each call into CGBitmapContextCreateImage potentially needs to go into the kernel). What is 'thecont' and where do you get it from? -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate, CGContextDrawImage never works</header>
    <body>hi david, thanks i am a quartz newbie. when i do this: thebits = CGBitmapContextCreate(thebase, width, height, bitscom, thebase is a pointer to the bits i want to copy to the screen, they are ready to go so that's it: thebase is the ptr, there is a width and height, it is 32 bit rgba and i want to copy it to the CGContextRef basically this fx just does a copybits to the current context right? or does it? thx bill</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextCreate, CGContextDrawImage never works</header>
    <body>What are you trying to accomplish? For one it seems supicious that your trying to create an image from a bitmap context immediately after creating it (if you want an image of the bits you had, then just create the image, if you want something else, then you are capturing the image too early). Also, you almost never need to call CGImageCreateWithImageInRect() just to draw a subrect from an image, it is more efficient to clip the target context instead. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>CGBitmapContextCreate, CGContextDrawImage never works</header>
    <body>hi all, no matter what i do, i can' get CGBitmapContextCreate and CGContextDrawImage to work. the screen is always white. i have tried drawing ovals etc in the same coordinates and that works on the destination. I must be doing something stupid, can anyone spot it? thanks, bill // my stuff above -- gets the screen &amp;amp; bitmap information thebits = CGBitmapContextCreate(thebase, width, height, bitscom, // my stuff below -- puts it away</body>
  </mail>
  <mail>
    <header>Accessing PDF font program's Encoding array</header>
    <body>The Adobe PDF Specification (v1.7) section 5.5.5 states: &amp;quot;A Type 1 font‚Äôs built-in encoding is defined by an Encoding array that is part of the font program, not to be confused with the Encoding I can create a CGFont object from the font dictionary's FontFile/FontFile3 stream. But I can't work out how to get at the built-in encoding from the CGFont (CGFontCopyGlyphNameForGlyph() seems to use a different indexing scheme). Should I be taking a different approach? Thanks, Hamish</body>
  </mail>
  <mail>
    <header>Re: CGEventKeyboardSetUnicodeString and unicode or multiple 	characters</header>
    <body>Sorry, it turns out that the Unicode problem has a simple solution; I was simply assigning the UniChar incorrectly. It should have been inputString[0] = 0x05E8. Here&amp;#39;s the updated code:¬†</body>
  </mail>
  <mail>
    <header>CGEventKeyboardSetUnicodeString and unicode or multiple characters</header>
    <body>Hi all,</body>
  </mail>
  <mail>
    <header>Re: What is CGContextDrawPDFPage() doing that I'm not?</header>
    <body>In case anyone else ever has a similar problem: The PDF spec says the following in section 5.2.2: &amp;quot;Word spacing works the same way as character spacing but applies only I (incorrectly) took this to mean that I should apply word spacing to whatever character was mapped to /space. CGContextDrawPDFPage() (correctly) took this to mean that it should apply word spacing to the character with code 32. In the cases where there was no space after &amp;quot;s&amp;quot; (which, confusingly, was not every case on the same page) it turned out that character code 32 was mapped to /s. H</body>
  </mail>
  <mail>
    <header>What is CGContextDrawPDFPage() doing that I'm not?</header>
    <body>Hi, I have a PDF file, one of whose pages contains a TJ render operation with the following operands: &amp;quot;(of) -379 (thisc) 28 (hapter) -379 (isto) -379 (help) -379 (y) 28 (ou) -379 (comm) 28 (unicate) -379 (mathematically) -379 (b) 28 (y) In other words, the spaces are encoded as negative kerning translations... all except for two of them. Going on the above, the line effectively reads: However, CGContextDrawPDFPage() manages to render the text correctly as: Can anyone tell me why it's rendering the spaces between &amp;quot;this&amp;quot; and &amp;quot;chapter&amp;quot;, and &amp;quot;is&amp;quot; and &amp;quot;to&amp;quot;, i.e., what it might be taking into account that I'm not? Thanks, Hamish</body>
  </mail>
  <mail>
    <header>CGEventTap, worker thread</header>
    <body>I have an application that is filtering mouse and key events, and forwarding them on to another remote machine.&amp;nbsp; I noticed that if I create the event tap as an active filter on the main thread&amp;#8217;s run loop (using the following call to CGEventTapCreate), then it ends up hanging the GUI: mTapPort = CGEventTapCreate(kCGSessionEventTap, kCGTailAppendEventTap, 0x00000000,&amp;nbsp;CGEventMaskBit(kCGEventLeftMouseDown) | CGEventMaskBit(kCGEventLeftMouseUp) |&amp;nbsp;&amp;nbsp;CGEventMaskBit(kCGEventRightMouseDown) | CGEventMaskBit(kCGEventRightMouseUp) |&amp;nbsp;CGEventMaskBit(kCGEventMouseMoved) | CGEventMaskBit(kCGEventLeftMouseDragged) |&amp;nbsp;CGEventMaskBit(kCGEventRightMouseDragged) | CGEventMaskBit(kCGEventKeyDown) |&amp;nbsp;CGEventMaskBit(kCGEventKeyUp) | CGEventMaskBit(kCGEventFlagsChanged) |&amp;nbsp;CGEventMaskBit(kCGEventScrollWheel) | CGEventMaskBit(kCGEventOtherMouseDown) |&amp;nbsp;CGEventMaskBit(kCGEventOtherMouseUp) | Based on the below thread&amp;#8217;s suggestion, I now create a worker thread during startup, and register the same event tap as an active filter on the worker thread (instead of registering it on the main thread). &lt;A href="http://lists.apple.com/archives/quartz-dev/2005/Oct/msg00013.html"&gt;http://lists.apple.com/archives/quartz-dev/2005/Oct/msg00013.html&amp;#8220;In general, for fitering event tap applications, I recommend running the event tap filter in it's own thread, with it's own CFRunLoop, rather than in the main application thread. Trying to filter all the system's events in the main application thread effectively throttles the flow of events through the system to the rate at which your application can process, draw, and fetch the next I&amp;#8217;m using the following pattern in the worker thread&amp;#8217;s main function, to explicitly run the loop with 0.25 sec timeouts. &amp;nbsp;double resolution = 0.25;&amp;nbsp;&amp;nbsp;NSDate* next = [NSDate dateWithTimeIntervalSinceNow:resolution];&amp;nbsp;&amp;nbsp;isRunning = [[NSRunLoop currentRunLoop] 1) Is filtering the keyboard and mouse events on a worker thread (a thread that is NOT the main thread), safe to do?&amp;nbsp; Or must this keyboard/mouse event tap be registered on the main thread?2) Is the above pattern that I&amp;#8217;m using in the worker thread okay to use?&amp;nbsp; Or is there a better, more conventional way, to make the worker thread&amp;#8217;s run loop process the keyboard and mouse events (since my event tap is registered on the worker thread&amp;#8217;s run loop)?</body>
  </mail>
  <mail>
    <header>Re: quartz question</header>
    <body>Since you mention this in context with printing, it should be noted that this is *not* an erase, this is just painting a white rectangle over the current content, which is a subtle but distinct difference. This could cause some issues in workflow should your content be &amp;quot;printed&amp;quot; to PDF for example. The effect that you really want would generally be better achieved by setting a clip on the context rather than doing a rect fill after the fact. -- David Duncan Apple DTS Animation and Printing email@hidden</body>
  </mail>
  <mail>
    <header>Drawing On A Window Using Mouse Events</header>
    <body>I would like to draw a circle based on the location of mouse down events in a window. I have all of the pieces: creating a window, catching mouse events, finding the location of the event, drawing circles, but I just cant seem to put them together. I understand that you need a CGContext in order to draw on the window. I am just a little confused on how to go from the mouse event handler to the handler I have for updating the window. Cheers, Carmen</body>
  </mail>
  <mail>
    <header>Re: quartz question</header>
    <body>Adding to what Stephane said, if you were to add a 2nd AddRect between the 2 DrawPaths, then there is a big difference between these two workflows that should be obvious; the 1st draws the stroke over the fill, the 2nd draws the fill over the stroke. Even though you have a tiny, tiny stroke width, half of it will still get obliterated by the fill in the 2nd one. Depending on the resolution of the device it's being rendered on, it could be enough to prevent it from drawing at all. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                       www.multi-ad.com</body>
  </mail>
  <mail>
    <header>Re: quartz question</header>
    <body>For the next newbie searching the list archives, I found this mentioned in the document &amp;quot;Drawing with Quartz 2D&amp;quot;, in the section &amp;quot;Creating a Path&amp;quot;.</body>
  </mail>
  <mail>
    <header>RE: quartz question</header>
    <body>Any call to CGContextDrawPath will clear the path so you can't call it twice in a row. Use CGContextDrawPath( c, kCGPathFillStroke,) if you want to fill and stroke at the same time or use CGPath if you need to reuse the path. -----Original Message----- From: quartz-dev-bounces+smarcoui=email@hidden [] On Behalf Of Eric Slosser Sent: Friday, January 25, 2008 9:07 AM To: email@hidden Subject: quartz question Quartz newbie question: Preamble: CGContextSetRGBStrokeColor( c, 1,1,1,1);        // white, right? Now I want to erase a rect (during printing).  Is there difference between and and ?  The only difference I know about is that the last one clears the current path. I'm using the middle one, and it's not erasing.</body>
  </mail>
  <mail>
    <header>quartz question</header>
    <body>Preamble: CGContextSetRGBStrokeColor( c, 1,1,1,1);	// white, right? Now I want to erase a rect (during printing).  Is there difference between ?  The only difference I know about is that the last one clears the current path.</body>
  </mail>
  <mail>
    <header>Quartz Composer on Tablet Events</header>
    <body>Does anyone have experience with Quartz Composer and its Tablet controller? I am creating tablet events using the Quartz Services API in another application, and for some reason they are not being picked up in my QC application. However they are being detected in other apps that I have written. Thanks, Carmen</body>
  </mail>
  <mail>
    <header>Re: DPI change</header>
    <body>This is what I expected as well, but it doesn't appear to actually work (for me). Have you seen this work from an image that was loaded in with a different DPI?  In my case, I've loaded (and thumbnailed) a 200 dpi image, and I call (simplified for the purposes of example): ... CGImageDestinationAddImage(dest, processedImage, ... The destination image is still the original 200 dpi ... The docs for CGImageDestinationAddImage also don't list the DPI properties in the list of valid options you can choose for this function, either ( /apple_ref/doc/constant_group/Destination_Properties ).</body>
  </mail>
  <mail>
    <header>Re: Curious behaviour from core image components</header>
    <body>The world's most convoluted workaround  ... Hmmmmm. Thanks, DaveAttachment:</body>
  </mail>
  <mail>
    <header>Re: DPI change</header>
    <body>On Jan 24, 2008, at 12:23 PM, Mike Schrag wrote: The DPI properties can be added to the options dictionary you provide when you add an image to a destination object.  CGImageRef doesn't maintain a resolution.  The image contains a fixed number of pixels which will be imaged into whatever rectangle you provide when you draw the image.  For example, if the image contains 100x100 pixels and you draw it into a 1x1 inch rectangle your image will be rendered at 100 dpi. Nick</body>
  </mail>
  <mail>
    <header>DPI change</header>
    <body>After hunting around on Google (and there's surprisingly little discussion of this), I have not been able to figure out the proper way to do change the DPI of a CGImageRef.  There are kCGImagePropertyDPIWidth and kCGImagePropertyDPIHeight properties, but are neither documented as value properties for adding an image to a destination, nor do they appear to actually do anything when I try to use them. Roughly the workflow in my code is: CGImageSourceCreateThumbnailAtIndex CGBitmapContextCreate based on the info from the original image create a CIContext perform some CI filters CGImageDestinationCreateWithURL CGImageDestinationAddImage the resulting image from the CI filters Profit I'm wondering if those DPI keys are really only for reading metadata and not actually changing it.  Obviously Preview does this, so I suspect there's a straightforward way in the API to do it. If anyone can share some insight, I'd appreciate it. Thanks Mike Schrag email@hidden</body>
  </mail>
  <mail>
    <header>Re: Curious behaviour from core image components</header>
    <body>On 24 Jan 2008, at 11:48, Marc Epard wrote: I think all Core Image processing is done in kCGColorSpaceGenericRGBLinear. Digital Color Meter reports in the device color space. It reports as such at the bottom of the palette. For me it reads &amp;quot;Color LCD&amp;quot; (which reminds me, I haven't calibrated since installing Leopard) The colour picker works in kCGColorSpaceGenericRGB I think. It would make sense for the sliders and numbers at least. So using the constant colour filter, I would choose a colour in GenericRGB, it would get converted to LinearRGB as it entered the Core Image filter, and then finally it would be converted to ScreenRGB on it's way to the display. So if you set you're screen calibration to Generic RGB profile I think you should get the numbers you entered in the colour picker (within LinearRGB -&amp;gt; GenericRGB). If you're hand coding values into Core Image filters then they will always undergo a linear to screen conversion, the main part of which is a gamma correction. That's my understanding anyway. If anybody knows better please tell me as I've had all sorts of fun trying to work this out.</body>
  </mail>
  <mail>
    <header>Re: Curious behaviour from core image components</header>
    <body>Actually, it looks like it updates right away. I have a little patch that reports the rendering context's color profile and it reports the new display profile immediately. Another possibility is the difference between kCGColorSpaceGenericRGBLinear and kCGColorSpaceGenericRGB. I'm pretty sure Quartz Composer always uses kCGColorSpaceGenericRGBLinear as its Core Image working color space. Too bad you can't set your monitor profile to kCGColorSpaceGenericRGBLinear. If you could, I bet the numbers would match. Ah, yes, I've had no end of fun with color space conversions in Quartz Composer. Good luck.</body>
  </mail>
  <mail>
    <header>Re: Curious behaviour from core image components</header>
    <body>Did you restart Quartz Composer after changing the calibration?  I haven't tried it myself, but it wouldn't surprise me if something is hanging on to the original colour space and not paying any attention when it changes. --</body>
  </mail>
  <mail>
    <header>Re: Curious behaviour from core image components</header>
    <body>This was my first thought too, but the problem with this explanation is twofold: first that the numbers from the colour meter didn't change when I changed the monitor calibration; and the second is that the manual rendering of numbers also showed this calibration effect. However, that's not to say that there isn't something in core image applying a calibration curve to the output and, indeed, repeating the experiment now on my MacBook screen (as opposed to the external monitor at work) the numbers are significantly different  -(1,0,0,1) renders as (1,0.004,0.004,1). So, yes, device colour space has once again fallen under the spotlight. Hopefully I'll have a more successful day tomorrow. Cheers, Dave Attachment:</body>
  </mail>
  <mail>
    <header>Re: Curious behaviour from core image components</header>
    <body>AFAIK DigitalColor Meter reads device-dependent colours, but generally you specify colours (and draw images) using a device-independent colour space (usually NSCalibratedRGBColorSpace/ kCGColorSpaceGenericRGB) so that they look the same on all displays. So most probably what's happening is that the image does indeed contain the colours you specified, but when it's being displayed they're being colour corrected.  DigitalColor Meter is then reading back the device-dependent colours, which are different from the device- independent colours in your image. --</body>
  </mail>
  <mail>
    <header>Re: Curious behaviour from core image components</header>
    <body>Colour space conversion? No idea how you check this in composer but it can cause this sort of thing to be observed.</body>
  </mail>
  <mail>
    <header>Curious behaviour from core image components</header>
    <body>I've spent an unpleasant day staring at quartz composer trying to get / correct/ results out of a core image filter. Stripped down to it's bare essentials the filter just returns an RGBA vector of say (1.0,0.0,0.0,1.0). Initially the &amp;quot;Billboard&amp;quot; patch plus &amp;quot;DigitalColor Meter&amp;quot; was used to get output values but I currently have &amp;quot;Image Pixel&amp;quot; extracting each of the four components and going through a convoluted &amp;quot;Number formatter&amp;quot;-&amp;gt;&amp;quot;Image with String&amp;quot;-&amp;gt;Billboard path to see the numbers change. My problem is that between the function outputting the RGBA vector and the &amp;quot;image&amp;quot; output pin of the filter patch, something is applying a colour correction or similar transform. The colours (0,0,0,1) and (1,1,1,1) come through fine but anything else is modified. (1,0,0,1) .... for instance ... is rendered as (0.835,0.22,0.059,1). I have also repeated the experiment using a &amp;quot;Constant Color&amp;quot; patch and this gives exactly the same result. Thanks, Dave Attachment:</body>
  </mail>
  <mail>
    <header>Access to Published Outputs in Cocoa</header>
    <body>I am really not understanding how do this after reading much documentation. I have a Quartz item with Movie Loader in it. The Movie Location parameter is set as a Published Input, Movie Position and Movie Duration are set as Published Outputs. I think use QCView, but use QCPatchController to actually specify the Quartz, uh, thing. I can add a TextField in interface builder, set its binding to patch , MovieLocation.value and run it, and if I put in a MovieLocation it works fine. ALSO if add a label to Interface builder, set its binding to an output like MoviePosition.value it also works fine. However I need the MoviePosition and MovieDuration values in the backend. So I have : How do I access the values of the to Published Outputs ? Can I attach them to an outlet in IB and have the values constantly updated ? [thePatchController addObserver:self - (void) observeValueForKeyPath:(NSString*)keyPath ofObject:(id)object change:(NSDictionary*)change context:(void*)context NSLog(@&amp;quot; VALUE %@\n&amp;quot;, [change objectForKey:@&amp;quot;kind&amp;quot;]); // this is for testing By the way the docs could be cleaned up for this functionality. The main binding doc for Quartz doesn't bother to have an Output example. The only other one I found has the observeValueForKeyPath but then hilariously just puts // put your own code here.</body>
  </mail>
  <mail>
    <header>Problems rendering a CIImage to JPG in my app,	but not in CoreImage Fun House</header>
    <body>I'm having some issues rendering CoreImage instances to jpg in my application.  However, I've hacked CoreImage Fun House to save a CoreImage instance with the same source image and parameters, and it does so successfully.  So, I'm looking to the list to see if anyone can give me any ideas for troubleshooting my app. At this point, I'm working with a relatively simple case of a single source image and a CILanczosScaleTransform.  I'm using the same source image in both apps, and after the filter is applied, this is what the CIImage looks like: (gdb) po ciImg extent [0 0 8000 6422]; DOD &amp;lt;CGSRegion 0x15a73bb0&amp;gt;; filter extent [0 0 8000 4014]; DOD &amp;lt;CGSRegion 0x15a378f0&amp;gt;; filter result-format NIL; wrap-mode clamp; filter-mode nearest; image result-format NIL; wrap-mode clamp; filter-mode nearest; image In CoreImage Fun House, I've changed FunHouseDocument.m to always save a &amp;quot;JPEG File&amp;quot; instead of &amp;quot;Fun House Preset&amp;quot; in fileWrapperOfType: and added the following code in jpegData: // use a very large rect to get the whole image // now, create a bitmap context CGContextRef cgContext = CGBitmapContextCreate(NULL, width, rows, 8, CIContext* ciContext = [CIContext contextWithCGContext:cgContext CGImageRef iref = [ciContext createCGImage:ciImg fromRect:[ciImg CGImageRef iref = [context createCGImage:ciImg fromRect:[ciImg In my app, I'm using this code to create a CGImageRef to save as a file in a category on CIImage.  This also works perfectly most of the time.  However, using CILanczosScaleTransform to scale up the image, I usually end up with an empty image.  Smaller images can be scaled up about 2x before I get a white image.  Scaling down also works even with large images as long as it is scaling by a factor of 2 and doesn't have to use the upsample kernel.  I've tried allocating the memory for the CGBitmapContext myself and it doesn't make any difference. If I switch to using an affine transform for scaling it works every time, but obviously not with good quality. In addition, in my own app, rendering to screen also works all the time, both to a small preview and a fullsize view inside a scrollview.  Screen rendering is done by a subclass of NSView and is very simple: // center the image in the view float draw_x = ([self bounds].size.width - [previewImage float draw_y = ([self bounds].size.height - [previewImage [ciContext drawImage:previewImage atPoint:CGPointMake(draw_x, draw_y) Does anyone have any suggestions on how to track down this problem?</body>
  </mail>
  <mail>
    <header>ROI issue on connecting Core Image filters in QC</header>
    <body>Greetings all, I am having a bit of trouble utilizing the ROI feature of Core Image in Quartz Composer to obtain desired results,  and need some help.  For example, I can have a simple ROI extraction filter defined as follows: kernel vec4 roiExtract (sampler src, float index, float k) and the Javascript adjustment for the DoD and ROI is as follows: In this particular case the input image is defined in dimensions of k rows by kn columns where n is the number of samples, and each neighborhood (region) is k by k.     The roiExtract filter in this case will return a k by k image in the particular region desired.  If I connect two of these filters into either a multiply or add filter, I get nothing but black.    The question is what am I missing on the ROI / DOD that is cause it to miss images produced by two of these filters? The pattern that I am using for this operation is defined in the following filter definition: kernel vec4 interestingPattern(vec2 scale) I am placing a copy of the Quartz Composition at  . Thank you, Dan Beatty Ph.D. Student Texas Tech University</body>
  </mail>
  <mail>
    <header>Re: EPS Source Line Length</header>
    <body>Regarding: On Jan 22, 2008, at 10:26 AM, Nick Nallick wrote: I believe this is misleading. What is true is that DSC comment lines must be limited to 255 characters. Other than that, parsers of PostScript and EPS data should be prepared for unlimited length lines. This is obviously true because PostScript and EPS conforming documents can contain binary data (typically images) and such data cannot be required to be limited to 255 bytes between &amp;quot;line endings&amp;quot;. Note that robust parsers of DSC conforming documents should be prepared for &amp;gt;255 length comment lines as well since there are certainly generators of PostScript data that don't conform to this requirement. Hope this helps, David</body>
  </mail>
  <mail>
    <header>EPS Source Line Length</header>
    <body>When I save an EPS file via Cocoa I get a text file containing the EPS source code for my image.  If there are any fonts embedded their binary form is represented as hex strings.  According to Adobe's PostScript Document Structuring Convention (page 25 of ) lines must be limited to 255 characters.  However the fonts in an EPS file (and probably embedded images) are much longer than this.  In my test I found the lines to be over 2000 characters.  Is this a bug or am I interpreting this incorrectly? Thanks, Nick</body>
  </mail>
  <mail>
    <header>Re: CGBitmapContextGetData: invalid context</header>
    <body>Every now and then, I get the following logged out: /Programs/Live 6.0.3 OS X/Live.app/Contents/MacOS/Live: CGBitmapContextGetData: invalid context And eventually the Audio Unit will crash. Please, can any one give me any guidance on how to check whether a context is valid or not, and what sort of things might make one invalid (I suspect I have some sort of multi-threading problem).</body>
  </mail>
  <mail>
    <header>CGBitmapContextGetData: invalid context</header>
    <body>Hi, I have an Audio Unit which switches around its graphical interface according to buttons the user can click. Every now and then, I get the following logged out: /Programs/Live 6.0.3 OS X/Live.app/Contents/MacOS/Live: CGBitmapContextGetData: invalid context And eventually the Audio Unit will crash. Please, can any one give me any guidance on how to check whether a context is valid or not, and what sort of things might make one invalid (I suspect I have some sort of multi-threading problem). Any thoughts on how to go about debugging this would be gratefully received. Much thanks Paul</body>
  </mail>
  <mail>
    <header>Re: Screen Move Updates</header>
    <body>Hi Lucas, Thanks a lot for the info. Josianne Hi, Did anybody succeed in using CGScreenRegisterMoveCallback() For the latter function I&amp;#39;m only getting response on updates. I&amp;#39;m developing on Leopard so I&amp;#39;m presuming I&amp;#39;m doing something wrong since according to &amp;quot;Quartz Display Service Reference&amp;quot;&amp;nbsp; these functions are implemented in Mac OS X version 10.4.3 and later. Thanks,</body>
  </mail>
  <mail>
    <header>Re: Screen Move Updates</header>
    <body>Hi, Did anybody succeed in using CGScreenRegisterMoveCallback() updates. I'm developing on Leopard so I'm presuming I'm doing something wrong since according to "Quartz Display Service Reference"&amp;nbsp; these functions are implemented in Mac OS X version 10.4.3 and later. Thanks,</body>
  </mail>
  <mail>
    <header>Screen Move Updates</header>
    <body>Hi, Did anybody succeed in using CGScreenRegisterMoveCallback() requestedOperations = kCGScreenUpdateOperationRefresh | kCGScreenUpdateOperationMove updates. I&amp;#39;m developing on Leopard so I&amp;#39;m presuming I&amp;#39;m doing something wrong since according to &amp;quot;Quartz Display Service Reference&amp;quot;&amp;nbsp; these functions are implemented in Mac OS X version 10.4.3 and later. Thanks, Josianne</body>
  </mail>
  <mail>
    <header>Re: How to create a CGImage from a PicHandle with Image	I/O	framework?</header>
    <body>Both solution 1 and solution 2 are exactly caused by the shortage of 512 bytes with 0.</body>
  </mail>
  <mail>
    <header>Re: How to create a CGImage from a PicHandle with Image	I/O	framework?</header>
    <body>I bet that was the problem with solution 2 as well, FWIW. -- James W. Walker, Innoventive Software LLC</body>
  </mail>
  <mail>
    <header>Re: How to create a CGImage from a PicHandle with Image I/O	framework?</header>
    <body>Try providing 512 bytes worth of 0 at the beginning of your data provider. IIRC, ImageIO expects the file format version of PICT rather than the in-memory version. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try.</body>
  </mail>
  <mail>
    <header>How to create a CGImage from a PicHandle with Image I/O framework?</header>
    <body>I need to create a CGImage from a PicHandle.  I use following two solutions but they both fail to work. Solution 1: 1. Create a CGDataProvider basing on *PicHandle 2.  use kUTTypePICT hint to create CGImage from the CGDataProvider Solution 2: 1. use GraphicsImportSetDataHandle to set the hPict 2. use GraphicsImportCreateCGImage to create the CGImage err message is something like &amp;quot; data error&amp;quot;. But the following solution 3 works. 1. Create a GWorld 2. Use DrawPicture to draw the hPict into the GWorld 3. Convert the PixMap associated with GWorld to CGImage. Now I need to fix solution 1 to make it work. Does anybody have any clues on that?</body>
  </mail>
  <mail>
    <header>Re: Clarification of Quartz Events Services</header>
    <body>I didn't mean to suggest that normal application event handlers can't see the events. I just meant that you can use event taps to monitor them, as one option. -- Bill Cheeseman - email@hidden Quechee Software, Quechee, Vermont, USA www.quecheesoftware.com PreFab Software - www.prefabsoftware.com</body>
  </mail>
  <mail>
    <header>Re: Saving a PDF file with a Quartz Filter</header>
    <body>I now see that the Quartz Filter list in Preview.app is mainly made up of the items in the Filters panel of ColorSync Utility.app.  These are the entities that I wish to programmatically grab a list of and use. Anyone have a pointer of where to go next? Isaac</body>
  </mail>
  <mail>
    <header>Re: Clarification of Quartz Events Services</header>
    <body>Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Saving a PDF file with a Quartz Filter</header>
    <body>In my Cocoa application, I would like to save an image as a PDF file, and present a list of Quartz Filters that the user can choose to use. My UI would look similar to the Save As dialog in Preview.app when choosing the PDF file format.  What are the APIs that I should use to get the list of filters?  Hopefully once I know that, I will then be able to determine how to use the filter.  :-) Thanks, Isaac</body>
  </mail>
  <mail>
    <header>Re: Clarification of Quartz Events Services</header>
    <body>Use the Quartz Event Taps API both to post and to monitor Quartz events representing or simulating hardware user input activity. You can use this API to post &amp;quot;synthetic&amp;quot; events mimicking genuine hardware input events, and to create and install event taps that can detect both real and synthetic hardware input events. Instead of using Event Monitor, which has limited capability, create and install Quartz event taps to monitor hardware input events, real and synthetic. Event taps work on Tiger and Leopard. See the Quartz Event Services Reference document for details. The documentation includes a lot of detail about tablet pointer and proximity event fields. You should also go to the Wacom site and search for their technical documentation on tablet pointer and proximity events -- it supplements the Apple documentation with some very useful information. You can download my free Event Taps Testbench utility to experiment with event taps, including tablet pointer and proximity events: tablet so the tablet-oriented features of Event Taps Testbench haven't been tested in the field. -- Bill Cheeseman - email@hidden Quechee Software, Quechee, Vermont, USA www.quecheesoftware.com PreFab Software - www.prefabsoftware.com</body>
  </mail>
  <mail>
    <header>Clarification of Quartz Events Services</header>
    <body>I am attempting to use the Quartz Events Services to create tablet events to be injected into the system, so they can be detected by whatever application is in the foreground. Can the events that are created and placed into the system using this API be detected using event handlers from Carbon and Cocoa. I am assuming that they can be detected. I was able to successfully post and handle a mouse moved event using carbon event handlers, but I was unsuccessfully able to detect tablet proximity and pointer events. I have a feeling that the tablet events are more complex and they require fields in the event to be set in order for them to be detected. Also I could just be completely using the API inappropriately. Below is the code I am using to create and post a proximity event. I am using the Event Monitor application from the Carbon dev site to detect the tablet events that I have created. CGEventRef myEvent = CGEventCreateMouseEvent(NULL, CGEventSetIntegerValueField (myEvent, kCGMouseEventSubtype, I do apologize in advanced. This project I am working on is my first venture into Apple development. I am still getting adjusted to how all of the pieces of puzzle that make up everything fit and work together. Thanks, Carmen</body>
  </mail>
  <mail>
    <header>Re: Core Animation or OpenGL for 3D interface?</header>
    <body>I'm new to Cocoa and I'm developing a small app that will have a 3D UI, that is, list boxes, buttons and other views will be translated and rotated arbitrarily in a 3D world with a perspective projection applied to them. The problem that I face is how to still be able to interact with all the controls by clicking them with the mouse. I've poked around with Core Animation a little and found that doing 3D transformations on NSViews are quite easy and straight forward. By doing so though, I loose the ability click the controls.</body>
  </mail>
  <mail>
    <header>Re: Core Animation or OpenGL for 3D interface?</header>
    <body>I'm a bit biased but I think CA is the way to go. If you take a look at the UI of Front Row that is the kind of stuff that is possible in CA. There are no clicks happening in FrontRow but since CALayers respond to hitTest: you can in a straightforward way make that happen. There are some issues since when you transform the layers in 3D the hitTest stuff can get a bit wonky but no worse than doing OpenGL picking. -bd-</body>
  </mail>
  <mail>
    <header>Core Animation or OpenGL for 3D interface?</header>
    <body>Hi guys, I&amp;#39;m new to Cocoa and I&amp;#39;m developing a small app that will have a 3D UI, that is, list boxes, buttons and other views will be translated and rotated arbitrarily in a 3D world with a perspective projection applied to them. The problem that I face is how to still be able to interact with all the controls by clicking them with the mouse. I&amp;#39;ve poked around with Core Animation a little and found that doing 3D transformations on NSViews are quite easy and straight forward. By doing so though, I loose the ability click the controls. So my question to you with more experience with Core Animation and Cocoa is: Is Core Animation built to handle this or should I just do it myself using OpenGL and write my own picking code? Thanks!</body>
  </mail>
  <mail>
    <header>Re: CGEventPost and Popup menu Issue</header>
    <body>Thank you. I have tried CGPostMouseEvent, the result is the same...  :( I guess that the problem is not CGPostMouseEvent or CGEventPost, but the problem of pop up menu. If I move the mouse manually, I can click any menuitem. However, if I use CGPostMouseEvent/CGEventPost to send out an mouse move event to the menuitem position, it seems that it is too fast that the menu dose not react to this event. So then I post a mouse click event, it dose not react, either. I can see the highlight of menuitem, and it is dismissed after clicking, but it dose not takes effects! It only happens in sub submenu. It works fine on submenu of menubar. Is there any workaround for this issue? -- Best Wishes. ------------------------- sky YAN in China</body>
  </mail>
  <mail>
    <header>Re: How to highlight image in quartz?</header>
    <body>I don't know if it's the effect you want, but you might try err = HICreateTransformedCGImage( inImage, kHITransformSelected, More later, Jack</body>
  </mail>
  <mail>
    <header>Re: CI priority</header>
    <body>The process goes as follows.  A background thread uses CI to prepare the image with a handful of filters then renders it into an NSOpenGLPixelBuffer.  The main thread uses the texture from that buffer to do an animation with it while the background works on another image.  The rendering is causing the main opengl animation to studder.  The animation is capable of running at 60fps (swap interval limited) but I cap it to 20fps since 60 is over kill for what I need. But the rendering operation of the background is dropping the main animation to under 1fps for about 1-1.5 secs while the rendering occurs.  There's ample time before the new image is needed so I'd like to set the background rendering to a low priority so it doesn't harm the animation. On Jan 15, 2008, at 1:05 AM, Paul Sargent wrote:</body>
  </mail>
  <mail>
    <header>Re: Fw: On getting the image of the currently used mouse cursor</header>
    <body>Try these (undocumented) functions ... The CGRect has origin zero, zero, so you will need to use +[NSEvent mouseLocation] or similar to get the location ... extern CGError CGSGetGlobalCursorDataSize(CGSConnectionRef connection, extern CGError CGSGetGlobalCursorData(CGSConnectionRef connection, unsigned char* cursorData, int* size, int* unknown, // most likely rowbytes CGRect* cursorRect, CGPoint* hotspot,	 int* depth, int* components, --</body>
  </mail>
  <mail>
    <header>Re: CGEventPost and Popup menu Issue</header>
    <body>Hi Sky, Why not use CGPostMouseEvent? It allows you to set the cursor position and buttons at the same time. Works great on Tiger and Leopard. Thanks, David ____________________________________________________________________________________ Looking for last minute shopping deals? Find them fast with Yahoo! Search.</body>
  </mail>
  <mail>
    <header>Fw: On getting the image of the currently used mouse cursor</header>
    <body>&lt;FONT face="Times New Roman" &lt;FONT face="Times New Roman" size=3&gt;Is there any way to get system-wide programmatically the image of a currently displayed mouse cursor when the mouse is moving above the window, which doesn&amp;#8217;t belong to your application? Is there any function to get the hot spot for the currently used cursor? &lt;FONT face="Times New Roman" &lt;FONT face="Times New Roman" size=3&gt;Thanks.</body>
  </mail>
  <mail>
    <header>Re: CI priority</header>
    <body>I think you need to explain how the background rendering is being done. Quartz? OpenGL? Something else that's just CPU intensive?</body>
  </mail>
  <mail>
    <header>CGEventPost and Popup menu Issue</header>
    <body>Hi, all, I am creating a program to do automation testing on Mac. I use CGEventPost to send out mouse event to simulate user's actions. However, I encounter a problem when I want to click on a menu. The program can click the menu prompt up from menubar, but it can not select a menu prompted up from a submenu. e.g. I want to click menuitem: File--&amp;gt;New--&amp;gt;Message, I send out left mouse down and up event to click File menuitem, then click New, then click Message. However, although Message menuitem is dismissed, but it takes no efforts! If &amp;quot;Message&amp;quot; menuitem has a submenu, it can not prompt up, either. This issue really annoys me for several days. I use following code to send out click event: void PostMouseEvent(CGMouseButton button, CGEventType type, const CGEventRef theEvent = CGEventCreateMouseEvent( ...//wait for some secs It can click Button, Text... successfully, except prompted up submenu. It can only click menu prompted up from menubar, e.g. I can select File--&amp;gt;Exit menuitem, but I can not select File--&amp;gt;New--&amp;gt;Message menuitem. It only dismissed but took no effects. Is there something I'm missing? I really appreciate for all your help. -- Sky Yan</body>
  </mail>
  <mail>
    <header>CI priority</header>
    <body>I have two threads running, one is doing a realtime animation using Core Image and the other is doing some background rendering of images soon to be used.  The problem is the background rendering is slowing down the realtime animation.  Is there a way to put the background rendering on a low priority?  Using thread priorities has no effect since the core image routines are using the GPU.  So is there a way to set a priority on it?</body>
  </mail>
  <mail>
    <header>Re: Scrolling &amp;amp; screen updates</header>
    <body>It probably does have some cache which it can render to screen using the graphics card or something fancy but how can I access that without using HIViewSetNeedsDisplay? As it stands each time I scroll even 1 pixel I need to redraw the entire view and optionally cache all the contents myself in a CGLayer or similar even though Quartz seems to have this cache already... Regards, Ryan Joseph</body>
  </mail>
  <mail>
    <header>Re: Scrolling &amp;amp; screen updates</header>
    <body>Quartz copies the image in the view being scrolled and caches it , as far as I recall. Sent from my iPhone</body>
  </mail>
  <mail>
    <header>Scrolling &amp;amp; screen updates</header>
    <body>I was trying to perform some scrolling in a CGContextRef and after getting poor results I opened Quartz Debug to look at other applications screen updates. It seems NSTextView and NSTableView etc... only perform screen updates for the portion of the view which is newly exposed, instead of the entire view like I was doing. I'm confused about this because pixels in the entire view are indeed moving so how is the view moving them without a call to setNeedsDisplay (or in my case HIViewSetNeedsDisplay since I'm in a HIView) on the entire view (which would produce screen updates in Quartz Debug)? Is there a way to move pixels on screen without calling setNeedsDisplay or equivalent I'm not aware of? Maybe Apple has special access to functions that I can only use from inside a scroll view? Hope not... Thanks. Regards, Ryan Joseph</body>
  </mail>
  <mail>
    <header>Event tap code works on prior MBP, fails on latest</header>
    <body>I just got a new quad-core i7 MacBook Pro. I've migrated most of my stuff to the new machine, including a little app I wrote a few years ago that re-maps the right-side option key to &amp;quot;enter,&amp;quot; by using a CGEventTap. It also re-maps the Expose key (on the F3 key) to reveal the desktop. This works fine on 10.6.6 build 10J567, on a previous-gen unibody MBP. But with 10.6.6 build 10J3210 on the new MBP, the option-key remapping still works, but it never gets called back for the Expose key. It's possible I also did something else on the old machine, to make that key &amp;quot;live&amp;quot; (call my event tap). But if that's the case, I sure don't remember what it was. Any ideas, or is it really a change in the new machine/OS? Thanks! -- Rick</body>
  </mail>
  <mail>
    <header>CIRAWFilter consumption question</header>
    <body>-- Raphael Sebbe Creaceed ‚Äî Creative iPhone &amp;amp; Mac apps ‚Ä¢‚Ä¢‚Ä¢ Twitter: ‚Ä¢‚Ä¢‚Ä¢ Web:</body>
  </mail>
  <mail>
    <header>Creating a linear gray colorspace?</header>
    <body>Hi, Is it possible to create a linear colorspace, like the¬†kCGColorSpaceGenericRGBLinear, but only for luminance (gray values)? -- Raphael Sebbe Creaceed ‚Äî Creative iPhone &amp;amp; Mac apps ‚Ä¢‚Ä¢‚Ä¢ Twitter: ‚Ä¢‚Ä¢‚Ä¢ Web:</body>
  </mail>
  <mail>
    <header>Re: QCCompositionLayer + Garbage Collection</header>
    <body>ID: 9126523 was filed over the weekend. Is there a workaround?</body>
  </mail>
  <mail>
    <header>Re: QCCompositionLayer + Garbage Collection</header>
    <body>Bug reports on both of your issues would be very much appreciated. Just because it also occurs in the CALayerEssentials doesn't mean it's known. Thanks! Troy Koelling _______________________________________________ Do not post admin requests to the list. They will be ignored. Quartz-dev mailing list      (email@hidden) Help/Unsubscribe/Update your Subscription: This email sent to email@hidden</body>
  </mail>
  <mail>
    <header>Re: QCCompositionLayer + Garbage Collection</header>
    <body>Mon, 14 Mar 2011 10:52:36 -0400 Re: QCCompositionLayer + Garbage Collection</body>
  </mail>
  <mail>
    <header>QCCompositionLayer + Garbage Collection</header>
    <body>Kind Regards, Arvin Bhatnagar, MS, CCNP, CNE, MCSE Senior Systems Engineer (Lead) Verizon (Product Design &amp;amp; Development) 614.723.1351</body>
  </mail>
  <mail>
    <header>CG leaking?</header>
    <body>. The allocation occurs in CGPathAddLineToPoint. The history for the blocks shows nothing. Heapshot analysis doesn't seem to show any significant growth. It seems like leaks is just not seeing the free() call. Xcode 3.2.5. Am I misunderstanding something? -- Rick</body>
  </mail>
  <mail>
    <header>Not all tiles redrawn after CATiledLayer -setNeedsDisplay</header>
    <body>Hi. I'm having a problem with CATiledLayer tiles not redrawing themselves after a call to -setNeedsDisplay. I've implemented a custom zoom in my UIScrollView delegate (it stretches the content's width but keeps the height constant). After zooming, it calls -setNeedsDisplay. But, sometimes, not all tiles are refreshed. The problem only seems to happen when zooming out. More details, including screenshots, at-- (Sorry if this looks like cross-posting. I've waited a while and not gotten an answer.) -Jaymie</body>
  </mail>
  <mail>
    <header>Re: Event Tap(CFMachPortRef) problem for Hot Key- callback is not invoked</header>
    <body>Hi, Following is the code snippet: &amp;nbsp;&amp;nbsp;&amp;nbsp;-( void )startEventTapinThread //Called in a separate thread. &amp;nbsp;&amp;nbsp;&amp;nbsp;//self is the object pointer our method &amp;nbsp;&amp;nbsp;&amp;nbsp;CGEventRef myCGEventCallback(CGEventTapProxy proxy, CGEventType type, CGEventRef event, void *refcon) &amp;nbsp;&amp;nbsp;&amp;nbsp;//execute the code related to feature Thanks and Regards,</body>
  </mail>
  <mail>
    <header>Event Tap(CFMachPortRef) problem for Hot Key- callback is not	invoked</header>
    <body>Hi, Following is the code snippet: &amp;nbsp;&amp;nbsp;&amp;nbsp;-( void )startEventTapinThread //Called in a separate thread. &amp;nbsp;&amp;nbsp;&amp;nbsp;//self is the object pointer our method &amp;nbsp;&amp;nbsp;&amp;nbsp;CGEventRef myCGEventCallback(CGEventTapProxy proxy, CGEventType type, CGEventRef event, void *refcon) &amp;nbsp;&amp;nbsp;&amp;nbsp;//execute the code related to feature Thanks and Regards, Deepa Disclaimer: This email may contain confidential material. If you were not an intended recipient, please notify the sender and delete all copies. Emails to and from our network may be logged and monitored. This email and its attachments are scanned for virus by our scanners and are believed to be safe. However, no warranty is given that this email is free of malicious content or virus.</body>
  </mail>
  <mail>
    <header>Newer ImageUnitAnalyzer?</header>
    <body>Hi all, Is there a newer ImageUnitAnalyzer out there? All I could find is one from 2005. Ben</body>
  </mail>
  <mail>
    <header>Re: How do I sync my scrollbars with my IKImageView?</header>
    <body>I should say that I have seen the NSScrollViewGuide, and have seen the following method // assume that the scrollview is an existing variable newScrollOrigin=NSMakePoint(0.0,NSMaxY([[scrollview documentView] frame]) However, for some reason, in the non-flipped case, this seems to always send me to (0,1) which isn't right. I'm not exactly sure what that subtraction of NSHeight is supposed to do, but in any case, I can't seem to find the correct Y value when my image is really big... Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: flipped bitmaps with CGBitmapContextCreate and 	CGBitmapContextCreateImage</header>
    <body>Hi All, So to draw like QuickDraw with the origin at top left of the window, it is much faster to NOT flip the coordinate system and to instead map the source and destination rectangles to the new core graphics coordinate system. This prevents bitmaps from needing any kind of correction. To clarify, the problems start when you flip the destination context. So this is an alternative way to do it. Thx, Bill On Wed, Mar 31, 2010 at 11:46 AM, Bill Appleton You seem to be confused about flippedness or perhaps transforms in general. Read the AppKit documentation about flippedness. Even though it talks about NSImage rather than Quartz, it should clarify things for you. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>How do I sync my scrollbars with my IKImageView?</header>
    <body>I have an IKIMageView with scrollbars, via a workaround that someone sent me. However, when I rotate the image, the scrollbars are no longer accurate. Also, I'd like to be able to programatically manipulate the scrollbars and have the image scroll appropriately... Just setting the scroll bar value doesn't seem to have any effect, and I don't see any relevant methods in the NSScrollView class. thanks Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
  <mail>
    <header>Re: flipped bitmaps with CGBitmapContextCreate and 	CGBitmapContextCreateImage</header>
    <body>On Wed, Mar 31, 2010 at 11:46 AM, Bill Appleton You seem to be confused about flippedness or perhaps transforms in general. Read the AppKit documentation about flippedness. Even though it talks about NSImage rather than Quartz, it should clarify things for you. --Kyle Sluder</body>
  </mail>
  <mail>
    <header>Re: flipped bitmaps with CGBitmapContextCreate and	CGBitmapContextCreateImage</header>
    <body>What isn't good about these solutions is the speed it will take to blit twice or swap the rows.</body>
  </mail>
  <mail>
    <header>Re: flipped bitmaps with CGBitmapContextCreate and 	CGBitmapContextCreateImage</header>
    <body>Hi All, What isn&amp;#39;t good about these solutions is the speed it will take to blit twice or swap the rows. If I don&amp;#39;t scale the destination context, the bit map is right side up But I can&amp;#39;t figure out the right translation for the destination context. It seems to require some kind of fudge factor to get the CDContext to align right under the title bar. Does anyone understand these issues? thanks, bill Steve gave you a solution, but I&amp;#39;ll explain what is happening here. The calls to change the CTM don&amp;#39;t affect the current contents of the context, they only change the coordinate system that you are using to draw. Since you aren&amp;#39;t drawing anything, you get nothing changed. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: flipped bitmaps with CGBitmapContextCreate and	CGBitmapContextCreateImage</header>
    <body>Steve gave you a solution, but I'll explain what is happening here. The calls to change the CTM don't affect the current contents of the context, they only change the coordinate system that you are using to draw. Since you aren't drawing anything, you get nothing changed. -- David Duncan Apple DTS Animation and Printing</body>
  </mail>
  <mail>
    <header>Re: flipped bitmaps with CGBitmapContextCreate and	CGBitmapContextCreateImage</header>
    <body>Create another bitmap context of the same size/depth, flip that one, and draw the image to it. Or, since you started off with your own pointer to the bits, just loop through and swap the rows yourself. _________________________________________________________ Steve Mills                              Me: 952-401-6255 Senior Software Architect                         MultiAd email@hidden                        www.multiad.com</body>
  </mail>
  <mail>
    <header>flipped bitmaps with CGBitmapContextCreate and	CGBitmapContextCreateImage</header>
    <body>hi all, calling translate &amp;amp; scale between CGBitmapContextCreate and CGBitmapContextCreateImage doesn&amp;#39;t seem to do anything (thebase, is there a way to flip my bitmap image vertically? thanks in advance</body>
  </mail>
  <mail>
    <header>Re: Drawing via Quartz</header>
    <body>Quartz is a software rendering engine, and on the iPhone platform is typically less viable for creating realtime games by itself. Combining it intelligently with either Core Animation or OpenGL can allow you to get some pretty nice looking results, but basically as you note you have to be careful to avoid redrawing. When you have maskToBounds=YES and the bounds of a layer is non-rectangular (cornerRadius != 0) or when you mask to another layer, Core Animation needs to render to an offscreen before it can place your content onscreen, which is noticeably slower as you realized. You can see this in Instruments with the Core Animation tool by turning on the &amp;quot;Color Offscreen-Rendered Yellow&amp;quot; flag. I'm not certain about the gradient layer issue however. -- Reality is what, when you stop believing in it, doesn't go away. Failure is not an option. It is a privilege reserved for those who try. David Duncan</body>
  </mail>
  <mail>
    <header>Drawing via Quartz</header>
    <body>Hello, Im Lukas from Czech Republic and I have a question.. Im sorry for my English.. I read a lot of articles, examples... I saw all graphic WWDC 2009 videos.. But I still haven't my answer.. What quartz 2d for? Is't possible to develop cool 2d games via quartz? Quartz core have many primitives, many cool things such bezier curves, gradient etc.. but how I can use it for my purpose.. for example if I need redraw my bezier curve (position animation of points) so quickly for some cool effect.. I have only one option, setNeedsDisplay.. I think this isn't much optional.. maybe Im wrong.. can you help me? I know almost everything around OpenGL, but many my ideas aren't realisable so easy like via quartz.. many of them are dead ways via OpenGL... Thank you very much PS: last question.. Core Animation is by definition very perform.. I tried to use and my performance of app downgrade so big.. I used cornerRadius and mask on to my UIImage, because I needed round corner images in cells, and scrolling was terrible slow.. The same slow made gradient used via CAGradientLayer.. Both problems I solved via drawRect view callback, I made quickly rounder corners and gradient.. table scroll much better... Is there a special mechanism to use CA? Ing. Lukas Korba, Czech Republic</body>
  </mail>
  <mail>
    <header>Creating simple images</header>
    <body>I am using ImageIO to write a serie of small jpeg/JFIF data that I have a CGImageRef. I really want to get JFIF, no EXIF here. The problem I have is the following: as soon as I specify some properties like DPI width , there is an EXIF part created. Also if I      set some JFIF properties like x density they seem to be ignored or set to a wrong value. I also tried to copy all properties from an image that is like I want, and then apply it to an image destination, but again it creates undesired properties like a TIFF dictionay. How can I say to ImageIO that I want only a given set of properties of a given kind? Or should I use NSImageRep or a lower level JPEG libray?</body>
  </mail>
  <mail>
    <header>Re: Turning NSImage* into CGImageRef?</header>
    <body>Actually, No. it can&amp;#39;t that&amp;#39;s evidently a bug in the documentation. there are no public methods that allow you to put an NSImage into an IKImageView. (There *IS* a private setImage:(id) which can take an NSImage*, but that&amp;#39;s private, and so might disappear without warning...) I&amp;#39;ve asked Apple Tech support, and this is the answer that they gave me. Similarly with the rotate:(id) method that is used in IKImageViewDemo. It&amp;#39;s a bug in the demo... I&amp;#39;ve filed bug reports. Not that anyone will ever fix them... Dangit. The problem with this is that I need to know the colorspace. Now, I can get an imageRep from my NSImage, and find out it&amp;#39;s colorspace, but it seems to be NSDeviceBlackColorSpace, which doesn&amp;#39;t appear to have an exact duplicate in CGColorspaces... I guess I can assume that it&amp;#39;s either black, or RGB, and figure out all the other stuff from the NSImageRep... ¬†but that just seems REALLY REALLY clunky. Also, converting to bitmap seems to lose a LOT of my resolution.</body>
  </mail>
  <mail>
    <header>Re: Turning NSImage* into CGImageRef?</header>
    <body>Actually, No. it can't that's evidently a bug in the documentation. there are no public methods that allow you to put an NSImage into an IKImageView. (There *IS* a private setImage:(id) which can take an NSImage*, but that's private, and so might disappear without warning...) I've asked Apple Tech support, and this is the answer that they gave me. Similarly with the rotate:(id) method that is used in IKImageViewDemo. It's a bug in the demo... I've filed bug reports. Not that anyone will ever fix them... Dangit. The problem with this is that I need to know the colorspace. Now, I can get an imageRep from my NSImage, and find out it's colorspace, but it seems to be NSDeviceBlackColorSpace, which doesn't appear to have an exact duplicate in CGColorspaces... I guess I can assume that it's either black, or RGB, and figure out all the other stuff from the NSImageRep...  but that just seems REALLY REALLY clunky. Also, converting to bitmap seems to lose a LOT of my resolution. I'm sorely tempted to just say screw it, and stay with the hidden NSImage* way, but then that doesn't help me at all when I need to copy a region of the image, because there doesn't seem to be an equivalent to  CGImageCreateWithImageInRect for NSImage... Brian Postow Senior Software Engineer Acordex Imaging Systems</body>
  </mail>
</mails>

