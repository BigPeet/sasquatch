<?xml version="1.0" encoding="UTF-8"?>
<mails>
  <mail>
    <header>[jpa-spec users] Welcome to the jsr338-experts@jpa-spec.java.net mailing list!</header>
    <date>Mon Feb 28 22:24:05 CET 2011</date>
    <body>This is a confirmation email that you have subscribed to the  jsr338-experts@... mailing list. Your subscription email is users@... Welcome! Expert Group Mailing List Java.net developer mailing lists</body>
  </mail>
  <mail>
    <header>[jpa-spec users] CriteriaBuilder outside of the JPA spec</header>
    <date>Tue Jul 05 15:05:58 CEST 2011</date>
    <body>Hi all, I'm just wondering, is it feasible to move the CriteriaBuilder API to outside of the JPA spec, to allow other EE services to reuse it?  One thought I've had is to allow JMS to use the criteria builder for generating selectors, but since it's in the JPA spec it makes it difficult to integrate (since it requires the JPA technology to use it).  HOwever, if Crtieria were outside JPA it would be smoother to use it (since it would require the full JPA stack to use). John</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Integration with CDI</header>
    <date>Wed Jul 13 10:57:30 CEST 2011</date>
    <body>Dear Linda Demichiel, I contacted you a few weeks ago to find out about any plans the JPA 2.1 spec team had about specifying integration with CDI, because it's a real sore-point in development of EE 6 apps at present. I thought I'd follow up and see what you thought, as I haven't heard back from you or seen any further discussion on the -experts mailing list after Adam Bien reposted my message to him on that list. I'm really concerned that JPA 2.1 might come out without any kind of CDI integration, and it's such a big usability issue on any kind of DI-based app (ie: any standard Java EE 6 app using EJB 3.1 or plain CDI) that it'd be a big shame. If there's any way I can help with this I'd be happy to, though I don't feel particularly well qualified when it comes to the details. More recently I've noticed another interesting hiccup. According to the Glassfish folks, JTA datasource names in persistence.xml must be looked up only in the container global JNDI context, not the component/module context. That means datasources for persistence.xml cannot be mapped using a web.xml resource-ref and a glassfish-web.xml or jboss-web.xml resource-ref mapping. This seems like an ... interesting ... quirk, given that resources are mapped in almost all other contexts and _need_ to be mappable for inter-container portability. Is there any chance JPA 2.1 could specify that persistence contexts be initialized within module context, applying any mappings defined for that context? -- Craig Ringer POST Newspapers 276 Onslow Rd, Shenton Park Ph: 08 9381 3088     Fax: 08 9388 2258 ABN: 50 008 917 717 http://www.postnewspapers.com.au/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Integration with CDI</header>
    <date>Wed Jul 13 21:51:38 CEST 2011</date>
    <body>Hi Craig, Thanks for the feedback.  I do apologize for having let your previous email sent to me lapse. Unfortunately it got buried hundreds of emails deep in my inbox. Actually, one of the things I have been discussing behind the scenes is how we might support CDI injection into entity listeners.  In the meantime, if you or Adam have a concrete proposal that you would like to make, that is of course welcome.  As you may have noticed, however, the group as a whole is not currently supportive of injection into JPA entities. If that changes, it could also be considered further. With regard to the datasource mapping item you mention (as well as the GlassFish issue you filed) -- that has triggered some interesting internal discussion here.  We think your point is valid, and I will add this item to the queue for consideration for JPA 2.1. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Integration with CDI</header>
    <date>Thu Jul 14 03:51:26 CEST 2011</date>
    <body>Linda, I realize you're busy and juggling a lot of things, so don't worry about letting my mail get lost in the noise. Thanks for taking the time to reply now. It sounds like it might be necessary to bash together a couple of example apps to show why injection into entity listeners is necessary (showing how problematic the BeanManager access hacks currently required are), along with a more definite proposal for how this might work. I'll see if I can interest anybody in collaborating on that. Personally, I think injection into JPA entities themselves is a marginal idea at best; if anything I think a warning should be emitted if @Inject is found in an @Entity. It's entity listeners where I think injection support and lifecycle annotations are vital. I'll work on bashing out a demo and description. Time and other committments permitting I might even be able to have a look at how hard it'd be to retrofit support into EclipseLink as a proof of concept. I'm glad to hear the DS mapping issue got some attention. It's one of those interesting little holes in the spec that I'm always tripping over, making Java EE 6 usability and productivity far from what it could be. I keep a list here: http://blog.ringerc.id.au/2011/07/java-ee-6-traps-pitfalls-and-warts-list.html POST Newspapers 276 Onslow Rd, Shenton Park Ph: 08 9381 3088     Fax: 08 9388 2258 ABN: 50 008 917 717 http://www.postnewspapers.com.au/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Integration with CDI</header>
    <date>Thu Jul 14 21:21:21 CEST 2011</date>
    <body>Craig, Thanks for the pointer to your blog -- more useful feedback! regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Integration with CDI</header>
    <date>Fri Jul 15 07:03:37 CEST 2011</date>
    <body>Hi Craig,  Linda,    I realize you're busy and juggling a lot of things, so don't worry about   letting my mail get lost in the noise. Thanks for taking the time to reply   now.    It sounds like it might be necessary to bash together a couple of example   apps to show why injection into entity listeners is necessary (showing how   problematic the BeanManager access hacks currently required are), along   with a more definite proposal for how this might work. I'll see if I can   interest anybody in collaborating on that.    Personally, I think injection into JPA entities themselves is a marginal   idea at best; if anything I think a warning should be emitted if @Inject is   found in an @Entity. It's entity listeners where I think injection support   and lifecycle annotations are vital. @Inject inside entities should not work at all. Entities are managed by the  EM and not by CDI. BUT: I would like to have a standard way to access an  EntityManager from an entity. E.g. passing the EM as a parameter during  attachment or obtaining it through current transaction.  Currently we often use ThreadLocal as a hack to pass entities between an EJB  and JPA-Entities...    I'll work on bashing out a demo and description. Time and other   committments permitting I might even be able to have a look at how hard   it'd be to retrofit support into EclipseLink as a proof of concept.    I'm glad to hear the DS mapping issue got some attention. It's one of those   interesting little holes in the spec that I'm always tripping over, making   Java EE 6 usability and productivity far from what it could be. I keep a   list here:     http://blog.ringerc.id.au/2011/07/java-ee-6-traps-pitfalls-and-warts-list.html Great post!, thanks!, adam       http://www.postnewspapers.com.au/      POST Newspapers  276 Onslow Rd, Shenton Park  Ph: 08 9381 3088     Fax: 08 9388 2258  ABN: 50 008 917 717   http://www.postnewspapers.com.au/ Independent Consultant, Speaker, Java Champion    Weblog: blog.adam-bien.com  press: press.adam-bien.com  eMail:  abien@...  twitter: twitter.com/AdamBien  Mobile: 0049(0)170 280 3144  Author of:   "Real World Java EE Night Hacks", "Real World Java EE Patterns--Rethinking  Best Practices"      </body>
  </mail>
  <mail>
    <header>[jpa-spec users] JPA specification feature requests?</header>
    <date>Thu Oct 13 21:26:33 CEST 2011</date>
    <body>Where would be the best place to enter feature requests for future revisions of the JPA specification? I don't mean for 2.1; I mean "bigger" than that.  For example: standardized, configurable auditing.  EclipseLink and OpenJPA do this (maybe Hibernate does too); it would be nice to have it standardized. I didn't see a JIRA link from http://java.net/projects/jpa-spec , and the associated Wiki is a stub ( http://java.net/projects/jpa-spec/pages/Home ). Also this email list has a grand total of 39 subscribers which makes me worry that perhaps I have the wrong list.  :-) Best, Laird -- http://about.me/lairdnelson</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: JPA specification feature requests?</header>
    <date>Thu Oct 13 21:42:59 CEST 2011</date>
    <body>Hi Laird, In attention to being a reflector of the expert group email list, this list is also intended to be used for the discussion of such topics. Members of the expert group are expected to monitor it. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: JPA specification feature requests?</header>
    <date>Thu Oct 13 21:55:17 CEST 2011</date>
    <body>On Thu, Oct 13, 2011 at 3:42 PM, Linda DeMichiel linda.demichiel@... In attention to being a reflector of the expert group email list, this list is also intended to be used for the discussion of such topics. Members of the expert group are expected to monitor it.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: JPA specification feature requests?</header>
    <date>Thu Oct 13 22:12:52 CEST 2011</date>
    <body>Yes, unfortunately, the way the infrastructure works is that if you are  *subscribed* to the users list, you will receive both email sent to the EG list as well as  email sent to the users list.  The users list *archive*, however, only contains  email sent directly to the users list. Thanks! -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: JPA specification feature requests?</header>
    <date>Thu Oct 13 22:15:42 CEST 2011</date>
    <body>On Thu, Oct 13, 2011 at 4:12 PM, Linda DeMichiel linda.demichiel@... Yes, unfortunately, the way the infrastructure works is that if you are *subscribed* to the users list, you will receive both email sent to the EG list as well as email sent to the users list.  The users list *archive*, however, only contains email sent directly to the users list.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Feature request: portably-specified custom generators</header>
    <date>Tue Oct 18 16:58:25 CEST 2011</date>
    <body>I can't remember if this was brought up at the JavaOne JPA 2.1 talk or not. Every JPA project I work on I really miss the lack of the ability to specify a primary key generator of my own devising.  I can do it in Hibernate, I can do it in EclipseLink, and I can do it in OpenJPA, but the way I do it in all of them is different. If this is on the roadmap already, my apologies. Best, Laird -- http://about.me/lairdnelson</body>
  </mail>
  <mail>
    <header>[jpa-spec users] JPA spec position on mutability of query results?</header>
    <date>Tue Oct 25 17:06:38 CEST 2011</date>
    <body>Hello.  Does the JPA 2.0 specification have an implicit position (I couldn't find an explicit one) on the mutability of query result lists? I stumbled across some code in our application that adds an item to the end of a Query#getResultList() return value.  Leaving aside the question of whether this is a wise thing to do, is it permitted by the specification? Thanks, Laird -- http://about.me/lairdnelson</body>
  </mail>
  <mail>
    <header>[jpa-spec users] JPA spec on EntityManager#createNamedQuery(String, Class)</header>
    <date>Wed Oct 26 20:34:00 CEST 2011</date>
    <body>Page 72 of the JPA 2.0 specification spells out the requirements of the EntityManager#createNamedQuery(String, Class) method: /**  * Create an instance of TypedQuery for executing a  * Java Persistence query language named query.  * The select list of the query must contain only a single  * item, which must be assignable to the type specified by  * the resultClass argument.[27] Footnote 27 reads as follows: The semantics of this method may be extended in a future release of this specification to support other result types. Applications that specify other result types (e.g., Tuple.class) will not be portable. Suppose I have an interface, A .  And I have a JPA-compliant @Entity class, B , that implements A . Is the following construct permitted by the specification? Both EclipseLink and Hibernate accept this construction.  OpenJPA rejects it ( https://issues.apache.org/jira/browse/OPENJPA-2065 ). It seems to me that this is a legal construct.  What say the spec authors? Best, Laird -- http://about.me/lairdnelson</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: JPA spec on EntityManager#createNamedQuery(String, Class)</header>
    <date>Wed Oct 26 21:41:27 CEST 2011</date>
    <body>On Wed, Oct 26, 2011 at 2:34 PM, Laird Nelson ljnelson@... Page 72 of the JPA 2.0 specification spells out the requirements of the EntityManager#createNamedQuery(String, Class) method: /**  * Create an instance of TypedQuery for executing a  * Java Persistence query language named query.  * The select list of the query must contain only a single  * item, which must be assignable to the type specified by  * the resultClass argument.[27] Footnote 27 reads as follows: The semantics of this method may be extended in a future release of this specification to support other result types. Applications that specify other result types (e.g., Tuple.class) will not be portable.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Proposal: allow @PersistenceContext/@PersistenceUnit on method parameters</header>
    <date>Sun Nov 13 13:29:19 CET 2011</date>
    <body>Hi all, I'd like to propose to allow @PersistenceContext/@PersistenceConstructor on  constructor parameters to be able to build immutable clients for it. As it's  currently only allowed to use these annotations you usually end up with  something like this:   @PersistenceContext    @Inject so the class is half immutable in the sense of me having to rely on the  infrastructure to inject the EntityManager via reflection or introducing an  additional constructor to instantiate the class with MyDependency *and*  EntityManager. So what I actually would like to be able to write is this:   @Inject As the container has to hold an EntityManager as injectable resource anyway  it could already pipe it into the client on object construction instead of  applying it later on. Regards, Ollie --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Proposal: allow JPA entities not to have a default no-arg constructor</header>
    <date>Sun Nov 13 13:30:11 CET 2011</date>
    <body>Hi all, I'd like to propose to allow JPA entities not to have a default constructor.  Section 2.1 of the JPA 2.0 spec states.  The entity class must have a no-arg constructor. The entity class may have   other constructors  as well. The no-arg constructor must be public or protected. as well as   The entity class must not be final. No methods or persistent instance   variables of the entity  class may be final. Both of theses constraints prevent entities to be designed as strong domain  classes that enforce some constraints and thus allow safer programming. Given  a User class I might want make sure always has a a not-null, not-empty email  address and I want to make that sure on the type level. Thus the class I'd  like to write is the following Due to the current restrictions of the spec I have to write:    knowing that persistence providers will use the no-arg constructor pretty  much breaking my domain constraints and populating the emailAddress property  via reflection. So here's what I am proposing: 0. Allow entities not to provide a no.arg constructor and use final fields. 1. Define a constructor resolution strategy such as:    - if a nor-arg constructor is found, use this one    - if a single complex constructor is found, use this one (see more on  parameter binding below)    - if multiple single complex constructors are found, insist on one being  selected as the one to  be used via an annotation (e.g.  @PersistenceConstructor) or the appropriate XML equivalent 2. Resolve constructor parameters by either inspecting parameter names (req.  compilation with debug flag), inspecting @Column information on a property  with same name as parameter name or define an annotation to define the column  the parameter object shall be read from. 3. If relations are passed into the constructor this requires the relation to  be resolved eagerly. 4. The so defined constructor is used for queries not explicitly using a  constructor expression (see 4.8.2 of the 2.0 spec). Another question that plays into this is that I am wondering what the  rationale behind limiting the results of constructor expressions in queries  to be in new or detached state? I can perfectly understand why this is the  case for (non-entity) DTOs. Nevertheless I think it might make sense to let  an entity in its entirety be constructed from such an expression. Why  shouldn't it be in a managed state? E.g. @Entity   @Id   @GeneratedValue      – Why shouldn't I be able to define a query "select new User(u.emailAddress)  from User u" forcing the persistence provider to use a particular constructor  in that scenario and just get the entity back in the same state as if I had  queried "select u from User u"? Regards, Ollie --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Proposal: allow JPA entities not to have a default no-arg constructor</header>
    <date>Wed Nov 16 11:31:47 CET 2011</date>
    <body>Hi all, as a follow up on this one…  Another question that plays into this is that I am wondering what the   rationale behind limiting the results of constructor expressions in queries   to be in new or detached state? I can perfectly understand why this is the   case for (non-entity) DTOs. Nevertheless I think it might make sense to let   an entity in its entirety be constructed from such an expression. Why   shouldn't it be in a managed state? E.g.    @Entity     @Id   @GeneratedValue           –    Why shouldn't I be able to define a query "select new User(u.emailAddress)   from User u" forcing the persistence provider to use a particular   constructor in that scenario and just get the entity back in the same state   as if I had queried "select u from User u"? I re-read up the spec on entity states. The 2.1 draft says, that the entity  is in detached state if the primary key is read. So I am assuming the entity  is in the detached state due to the assumption that actual fields of the  entity about to be created are not expected to be populated, right? So IMHO  it would be cool if the spec was a bit more precise about that area and  actually defines the expected behavior as "use the constructor but populate  the entity's properties nevertheless". I think if you really need strong  projections to a set of fields only you'd rather introduce a dedicated DTO  anyway as you might carry around the entity half populated and potentially  violate object invariants. Regards, Ollie --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Typo in page 425 JPA 2.0 FR</header>
    <date>Fri Nov 18 16:49:46 CET 2011</date>
    <body>Hello, I would like to inform that there is a typo on page 425. In the Project entity, I think there is a missing '@' before the 'DiscriminatorValue' annotation. Regards, Piotr</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Typo in page 425 JPA 2.0 FR</header>
    <date>Fri Nov 18 19:15:45 CET 2011</date>
    <body>Thanks!  Now fixed. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] changes for 7.6.2 Container-managed Transaction-scoped Persistence Context</header>
    <date>Fri Nov 25 17:09:05 CET 2011</date>
    <body>Could we change 7.6.2 Container-managed Transaction-scoped Persistence Context to be more clear in the following paragraph: " If the entity manager is invoked outside the scope of a transaction, any entities loaded from the database will immediately become detached at the end of the method call. " Which method call do we mean exactly?  I think we mean each entity manager call (e.g. EntityManager.find).  Perhaps changing to: " If the entity manager is invoked outside the scope of a transaction, any entities loaded from the database will immediately become detached at the end of the entity manager method call. " Which rules out other possible interpretations of which method is referred to. Related to this, could the previous paragraph: " The persistence context ends when the associated JTA transaction commits or rolls back, and all entities that were managed by the EntityManager become detached.[80] " Also be made more specific about when the persistence context ends for the transaction scoped entity manager that is invoked outside of a JTA transaction.  Detaching the loaded entities is different than closing the persistence context. Perhaps we could cover this with an additional change to the last paragraph in this section: " If the entity manager is invoked outside the scope of a transaction, any entities loaded from the database will immediately become detached at the end of the entity manager method call.  The persistence context may end after each entity manager method call (some implementations could release the persistence context at the end of the container invocation). " Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: changes for 7.6.2 Container-managed Transaction-scoped Persistence Context</header>
    <date>Fri Nov 25 20:20:35 CET 2011</date>
    <body>I probably have interpreted the wording incorrectly, I think it should be at the end of the container level method invocation.  All the more reason to be explicit with the wording.  ;)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Javadoc problem</header>
    <date>Sat Nov 26 14:35:25 CET 2011</date>
    <body>Howdy, I was just looking at Oracle's Javadoc for JPA and it seems that LockModeType section is a bit messed up ( http://docs.oracle.com/javaee/6/api/javax/persistence/LockModeType.html ). I'm not sure if it's JPA Expert Group or Oracle responsibility to build up the Javadoc, but just wanted to sign a problem. Most likely it's because of unclosed &amp;lt;code&amp;gt; tag here:  following. Regards, Piotr</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: changes for 7.6.2 Container-managed Transaction-scoped Persistence Context</header>
    <date>Sat Nov 26 18:03:59 CET 2011</date>
    <body>Update to the last paragraph in 7.6.2: " If the entity manager is invoked outside the scope of a transaction, any entities loaded from the database will immediately become detached at the end of the container method invocation (persistence context will also end). " Or another way to state this could be: " If the entity manager is invoked outside the scope of a transaction, the persistence context will end when the container level invocation completes.  In which case, any entities loaded from the database will immediately become detached. "</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Javadoc problem</header>
    <date>Mon Nov 28 20:11:40 CET 2011</date>
    <body>Thanks, Piotr.  We'll fix the next time we rev the javadocs. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: changes for 7.6.2 Container-managed Transaction-scoped Persistence Context</header>
    <date>Mon Nov 28 20:40:28 CET 2011</date>
    <body>Hi Scott, Please see below..... Perhaps I am failing to grasp your concern here, but I see the intention of  the last paragraph of section 7.6.2 as specifying the user-visible behavior that is  required to result.  Your paragraph above describes a possible implementation strategy  to achieve it, but I don't think the spec needs to include that. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: changes for 7.6.2 Container-managed Transaction-scoped Persistence Context</header>
    <date>Tue Nov 29 20:06:59 CET 2011</date>
    <body>Hi Linda, I think that the current 7.6.2 wording "detached at the end of the method call", is ambiguous about which method call is intended (the entity manager method invocation or the container level method invocation.) I think that clarifying it with one of the following changes, would eliminate the ambiguity. Updated proposal for last paragraph in 7.6.2: " If the entity manager is invoked outside the scope of a transaction, any entities loaded from the database will immediately become detached at the end of the container level method invocation call. " Regards, Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft: unsynchronized persistence contexts</header>
    <date>Tue Nov 29 20:08:47 CET 2011</date>
    <body>Hi Linda and all, Here are some comments on the changes for chapter 3, 7 and section 10.4.1. I  apologize for taking so long. I'd also like to say that it ends up being much  more readable and understandable than what I had anticipated so kudos to  Linda. 3.2.4  OPEN ISSUE: The wording above now implies that an application-managed   persistence context that has not been joined   to the current transaction must not be flushed to the database. I believe   this was either unspecified (or permitted) previously.   Is this the semantics we want? Yes it looks appropriate and more consistent to me. 3.3.1 I'd like your opinion on how we treat application managed persistence  contexts. According to 3.3.1, we treat them as SYNCHRONIZED all the time.  There is no way to make an application-managed persistence context simulate  what a container-managed persistence context. This lack of symmetry makes me  uneasy in and of itself but also in the perspective of extending this to the  SE world. Any reason why we disallow UNSYNCHRONIZED application-managed persistence  contexts? In particular section 7.6.1 seems generic to me and not specific to container  managed persistence contexts 7.4 EntityManagerFactory.addNamedQuery As I raised initially, I'm still unclear how a user would properly use this  feature. Where in his code would it add a new named query and make sure this  has not been added before. To me it'd be preferable to have some  initialization hook to set such queries. I'm happy to change my mind but I  looked up my emails and I don't think anyone came up with a use case for the  current proposed form. Note that this also avoid a bunch of concurrent safe structures as well but  that's impl details. See the thread named Re: named queries and my exchange with Gordon from April  28th forward. 7.6.3  OPEN ISSUE: Support for allowing components other than stateful session   beans to initiate container-managed extended persistence contexts. What do you have in mind precisely?  I was thinking that CDI would want this kind of feature but I remembered that  an application managed PC is by design an extended PC and that other  programmatic models (than EJB) can piggyback on this. Still we could think of  a nicer user experience if we manage to standardize this somehow or at least  think of it as cross JSR EGs. 7.6.3.1  If the stateful session beans differ in declared synchronization type, the   EJBException is thrown by the container. Wild thought, should we try and make sure an deployment time exception  occurs? That would be much better and nicer than waiting for the first call  to blow up on the user. I don't think the spec defines what's going on if a SfulSB defines an EPC and  called upon a SlessSB which defines a PC. It might be implied but that was  not clear from my readings: - if a JTA tx is present, is the PC propagated? Or is a new PC created for  the SlessSB? - what happens if no Tx is present? 7.6.4  OPEN ISSUE: Whether to add support for the propagation of extended   persistence contexts when no transaction exists. Does this use case applies currently ? Let me know if I derail. EPC are propagated along Stateful Session Beans regardless of a transaction  so this side is covered. We don't have EPC outside Stateful Session Beans so the open question does  not apply. Am I correct? Say we open up EPC for non SfulSB, then, I think it would be nice to get EPC  propagation even without transaction to mimic what SfulSB do. 7.6.4.1  OPEN ISSUE: Should this be permitted if the former persistence context has   been joined to the JTA transaction? I can see why users would expect that to work. Unfortunately that's not  something we can catch as a deployment time error and thus, I'd rather allow  that to limit bugs in production. What would be the downside of allowing such thing (gotchas or limit for  future evolutions)? I can't see any right now. 7.9.2  OPEN ISSUE: This contract will presumably need to change, as the provider   needs to know which type of persistence   context is being created (SYNCHRONIZED or UNSYNCHRO- NIZED) so that it   knows whether to register for synchronization   notifications or not. We will presumably need to define a standard property   that the container passes in as part of the map passed   to the createEntityManager invocation. +1 for using the Map and not using an overloaded method. Note that this  weight in favor of allowing app-managed PC to be UNSYNCHRONIZED if they wish  to. Emmanuel    I've uploaded a new specification draft to the project Documents area  ( http://java.net/projects/jpa-spec/downloads )    This draft includes changes in support of unsynchronized persistence  contexts along the lines of the proposal submitted two months ago.    These changes affect Chapters 3 and 7 and section 10.4.1.  There are a  number of open issues flagged in the text, including some pertaining  to ways in which we might extend this functionality.    Please review these changes carefully, and post any comments or corrections.    If the group is supportive of this direction, I would like to submit a   version  of this draft to the JCP for purposes of Early Draft Review so that we can   get  feedback from the broader community.  Please let me know if you think that  anything stands in the way of that.    thanks,    -Linda    p.s.  The changebars are additive from the last draft, so that all changes  since JPA 2.0 will be flagged when we submit a JCP draft.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: changes for 7.6.2 Container-managed Transaction-scoped Persistence Context</header>
    <date>Tue Nov 29 20:19:17 CET 2011</date>
    <body>Just to be very clear.... What are you referring to by "container level method invocation call"?   The call to the entity manager proxy? thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: changes for 7.6.2 Container-managed Transaction-scoped Persistence Context</header>
    <date>Tue Nov 29 21:15:27 CET 2011</date>
    <body>Not the call to the entity manager proxy.  I mean the code that is calling the entity manager proxy (e.g. a session bean calling the entity manager proxy.) Perhaps a clearer proposal could be: " If the entity manager is invoked outside the scope of a transaction, any entities loaded from the database will immediately become detached at the end of the container level method invocation call (e.g. when the session bean method ends). "</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft: unsynchronized persistence contexts</header>
    <date>Tue Nov 29 21:25:57 CET 2011</date>
    <body>OK, thanks.  Unless anyone sees any problems with this, I plan to remove the "open  issue" note. Not really.  Actually, I'd like to hear from the group as to whether we  should extend the spec in that direction. OK.  Unfortunately, we don't have a well-defined initialization sequence, and  the EMF may be created by the container before application code gets to execute.   However, a startup singleton EJB's postConstruct could be used for adding named queries,  or a servlet init method.  In the mean time, do you think addNamedQuery has so  little value that we should remove it? This is a placeholder that was triggered by a more general CDI-related  discussion I was having with Pete.   Creation of a JIRA is a to-do item that has been on  my list for some time, and I should really move this issue there as it is more of an  RFE. PC is propagated to the SLSB No propagation Sort of.  The terminology the spec uses is "inheritance", since it is pegged  to creation rather than to transactional propagation.  After the EPC is created in this way, it  propagates normally. Correct Other than in the abstract, do you see a compelling use case for this? I think it could work, but not sure what the right thing to do here is. Agree. thanks for the feedback -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: changes for 7.6.2 Container-managed Transaction-scoped Persistence Context</header>
    <date>Wed Nov 30 00:48:53 CET 2011</date>
    <body>Ah, thanks.  That is not the intended semantics.  Rather it is that the persistence context is created to service the method call that is made on the entity manager only.  It doesn't have the duration of a business method invocation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft: unsynchronized persistence contexts</header>
    <date>Wed Nov 30 00:54:12 CET 2011</date>
    <body>[snip] Cancel that last question.  I've just been trading emails with one of the  members of the users list, who immediately urged me not to even think of taking this  away! -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft: unsynchronized persistence contexts</header>
    <date>Wed Nov 30 15:28:43 CET 2011</date>
    <body>   OK.  Unfortunately, we don't have a well-defined initialization sequence,   and the EMF  may be created by the container before application code gets to execute.    However, a  startup singleton EJB's postConstruct could be used for adding named   queries, or a  servlet init method.  In the mean time, do you think addNamedQuery has so   little  value that we should remove it? I don't think the feature should be removed but I'm still questioning the  form. Today to use it, you need the following nasty piece of code         em.getEntityManagerFactory().addNamedQuery("foo", namedQuery);     That's a horrible code idiom for something that should be as trivial as I understand the lifecycle problem, that's a shame. An alternative may be a named query request listener that has the option to  build and enlist the query if it does not exist yet.     /**      * called if a given named query is not found and before raising an  IllegalArgumentException      */         /**          * return null if the query name is unexpected          * or return a query object that ought to be associated to the given  name          */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft: unsynchronized persistence contexts</header>
    <date>Wed Nov 30 15:29:03 CET 2011</date>
    <body>   Not really.  Actually, I'd like to hear from the group as to whether we   should extend the spec  in that direction. Of course I'm +1 for allowing it :)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft: unsynchronized persistence contexts</header>
    <date>Wed Nov 30 15:33:05 CET 2011</date>
    <body>   PC is propagated to the SLSB      No propagation OK cool, maybe we should add this a bit more explicitly in the spec.      Sort of.  The terminology the spec uses is "inheritance", since it is   pegged to creation rather  than to transactional propagation.  After the EPC is created in this way,   it propagates normally.      Correct      Other than in the abstract, do you see a compelling use case for this? I guess that will depend on the discussion you and Pete started on having EPC  on some CDI components. If that discussion goes nowhere, we don't need this  feature in the current spec.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: changes for 7.6.2 Container-managed Transaction-scoped Persistence Context</header>
    <date>Wed Nov 30 16:26:30 CET 2011</date>
    <body>   Ah, thanks.  That is not the intended semantics.  Rather it is that the  persistence context is created to service the method call that is made  on the entity manager only.  It doesn't have the duration of a business  method invocation. Is it really? I have to admit I am amongst people that read this as bound to the duration  of the business method call. The main problem with this approach is that many  em.find() calls are essentially useless as they would not load (lazy)  associations.  Upon detachment, the provider then raises a NotLoadedException of some sort.  The only way to work around that is to write a query with the specific fetch  pattern that ought to be used. If entities are detached after the business call, the user can navigate and  use PersistenceUtil contracts to load the appropriate part of the graph  before ending the method. Note that for a PC that detaches entities upon EM method calls, Hibernate has  a specific API with specialized contracts to tailor that kind of use case. I  don't think our existing EM API is necessarily well suited for that per se. Emmanuel</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft: unsynchronized persistence contexts</header>
    <date>Wed Nov 30 19:42:07 CET 2011</date>
    <body>Following up on your points here -- Is the issue that you want to know if a query of the given name exists:     a) So you don't clobber it by defining one with the same name?     b) Because you might want to use it, even though you may not know what it  does? What about adding a replaceIfExists argument to addNamedQuery, which seems a simpler approach?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: changes for 7.6.2 Container-managed Transaction-scoped Persistence Context</header>
    <date>Wed Nov 30 19:53:48 CET 2011</date>
    <body>Yes.  See also section 3.8.7, which is quite explicit with regard to queries. BTW, these decisions date back to JPA 1.0 (aka EJB 3.0).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft: unsynchronized persistence contexts</header>
    <date>Wed Nov 30 21:21:43 CET 2011</date>
    <body>Linda,   Following up on your points here --  Is the issue that you want to know if a query of the given   name exists:       a) So you don't clobber it by defining one with the same name?       b) Because you might want to use it, even though you may   not know what it does? I would have thought the issue is that addNamedQuery might be slow, so you want to avoid calling it again when you don't need to.    What about adding a replaceIfExists argument to   addNamedQuery, which seems a  simpler approach? It seems to me the primary use case for addNamedQuery is for a stanadlone (Java SE) application. Do we really need to cater for defining named queries programmatically in a Java EE environment?  -----Original Message-----  From: Linda DeMichiel [ mailto:linda.demichiel@...]   Sent: Thursday, 1 December 2011 7:42 a.m.  To: jsr338-experts@...  Cc: Emmanuel Bernard  Subject: [jsr338-experts] Re: updated spec draft:   unsynchronized persistence contexts        &amp;gt;&amp;gt;&amp;gt; 7.4  &amp;gt;&amp;gt;&amp;gt; EntityManagerFactory.addNamedQuery  &amp;gt;&amp;gt;&amp;gt; As I raised initially, I'm still unclear how a user would   properly use this feature. Where in his code would it add a   new named query and make sure this has not been added before.   To me it'd be preferable to have some initialization hook to   set such queries. I'm happy to change my mind but I looked up   my emails and I don't think anyone came up with a use case   for the current proposed form.  &amp;gt;&amp;gt;&amp;gt; Note that this also avoid a bunch of concurrent safe   structures as well but that's impl details.  &amp;gt;&amp;gt;&amp;gt; See the thread named Re: named queries and my exchange   with Gordon from April 28th forward.  &amp;gt;&amp;gt; OK.  Unfortunately, we don't have a well-defined   initialization sequence, and the EMF  &amp;gt;&amp;gt; may be created by the container before application code   gets to execute.  However, a  &amp;gt;&amp;gt; startup singleton EJB's postConstruct could be used for   adding named queries, or a  &amp;gt;&amp;gt; servlet init method.  In the mean time, do you think   addNamedQuery has so little  &amp;gt;&amp;gt; value that we should remove it?  &amp;gt; I don't think the feature should be removed but I'm still   questioning the form.  &amp;gt; Today to use it, you need the following nasty piece of code  &amp;gt;          em.getEntityManagerFactory().addNamedQuery("foo",   &amp;gt; That's a horrible code idiom for something that should be   as trivial as  &amp;gt; I understand the lifecycle problem, that's a shame.  &amp;gt; An alternative may be a named query request listener that   has the option to build and enlist the query if it does not exist yet.  &amp;gt;      /**  &amp;gt;       * called if a given named query is not found and   before raising an IllegalArgumentException  &amp;gt;       */  &amp;gt;          /**  &amp;gt;           * return null if the query name is unexpected  &amp;gt;           * or return a query object that ought to be   associated to the given name  &amp;gt;           */      Following up on your points here --  Is the issue that you want to know if a query of the given   name exists:       a) So you don't clobber it by defining one with the same name?       b) Because you might want to use it, even though you may   not know what it does?    What about adding a replaceIfExists argument to   addNamedQuery, which seems a  simpler approach?      </body>
  </mail>
  <mail>
    <header>[jpa-spec users] Read-Only entities feature</header>
    <date>Thu Dec 01 11:12:54 CET 2011</date>
    <body>Hello, I'm trying to create a Read-Only entity relationship. Assuming that Entity E1 has a OneToMany relationship with E2, I would like **not** to be able to edit E2 from E1 (so E1.getE2().setSomeAttribute(-) should not be reflected in the database). By default, when I fetch E1 from the EntityManager it becomes managed, as well as E2, so every changes made are passed to the database. For simple types I can just do "@Column(updatable=false)", but for entities it doesn't seem to work. Does "@JoinColumn(updatable=false)" for entities means that the **FK cannot change**, but the **state of entity can**? If so, then in "Pro JPA 2 - Mastering the JAva Persistence API" book from Apress, Chapter 10 - Advanced Object-Relational Mapping, "Advanced Mapping Elements" there is the following text: "No new entities will be persisted, and existing entities will never be updated.". It firmly refers to the "entities" but should I understand it as "entity state" rather than "entity" itself? Nevertheless, the book also mentions: "JPA does not really define any kind of read-only entity, although **it will likely show up in a future release.**" Does the JPA 2.1 Expert Group have some @ReadOnly spec-compliant feature in plans? Thanks in advance for your answer. Regards, Piotr -- http://piotrnowicki.com</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft: unsynchronized persistence contexts</header>
    <date>Thu Dec 01 12:36:35 CET 2011</date>
    <body>   Following up on your points here --  Is the issue that you want to know if a query of the given name exists:     a) So you don't clobber it by defining one with the same name?     b) Because you might want to use it, even though you may not know what   it does?    What about adding a replaceIfExists argument to addNamedQuery, which seems a  simpler approach? I have a few reasons in mind - a) is definitely one - I don't have b) in mind - I'd like to promote a programming model where metadata and definitions is  not mixed or lost within business code and dilute readability - building the query might be slow or complex and adding it to the pool of  named queries will require some synchronization, I'd rather avoid these costs  My view is that we can offer something as nicely isolated than existing named  queries but as flexible as programmatically written dynamic queries. replaceIfExists is solving a) but not the rest.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: changes for 7.6.2 Container-managed Transaction-scoped Persistence Context</header>
    <date>Fri Dec 02 05:37:55 CET 2011</date>
    <body>My updated proposal for the 7.6.2 wording is: " If the entity manager is invoked outside the scope of a transaction, any entities loaded from the database will immediately become detached at the end of the entity manager method call. " I was thinking more about the lazy associations problem.  Is there room in the JPA 2.1 specification, to make an enhancement that improves what applications experience when the entity manager is invoked outside the scope of a transaction?  I'm thinking that the container could have a way to give a hint to the persistence provider, that lazy fetching should not be used for the entity manager invocation.  The advantage of this enhancement, is that applications will successfully be able to access the loaded entity(s) that currently fail (due to lazy fetching being used).  I think that the application can also make changes to work around this (e.g. use FetchType.EAGER) but I would like the same (FetchType.LAZY) entity/association to work with both container/app managed entity managers (that don't run in a JTA transaction). Perhaps this could be a (new) standard property that the container could pass to EntityManagerFactory.createEntityManager(Map) that suggests that lazy fetching should be disabled.  Or something more generic that covers other possible problems.  e.g. some hint like "javax.persistence.auto-detach" that lets the provider know that loaded entities will be detached (so the provider can avoid using lazy fetching).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: changes for 7.6.2 Container-managed Transaction-scoped Persistence Context</header>
    <date>Fri Dec 02 22:27:13 CET 2011</date>
    <body>OK, sure + "... or query invocation"</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] lazy associations [was Re: Re: Re: changes for 7.6.2 Container-managed Transaction-scoped Persistence Context</header>
    <date>Fri Dec 02 22:33:15 CET 2011</date>
    <body>Thanks, Scott. Changing the subject and snipping out the previous discussion here so that the Scott's suggestion doesn't get buried in the thread.... Folks, I'd like to get more opinions on this.   Should we pursue this  direction? Should we try to address this issue via fetch groups/profiles?  Other? thanks, Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: lazy associations [was Re: Re: Re: changes for 7.6.2 Container-managed Transaction-scoped</header>
    <date>Mon Dec 05 10:32:59 CET 2011</date>
    <body>Hi Linda, all, me, I think we should have a look into the fetch groups/profile  stuff. From my point of view it's awfully needed to establish a  contract on what has been loaded. Best regards Rainer Am 2 Dec 2011 um 13:33 hat Linda DeMichiel geschrieben:  Thanks, Scott.    Changing the subject and snipping out the previous discussion here so that   the  Scott's suggestion doesn't get buried in the thread....        &amp;gt; I was thinking more about the lazy associations problem. Is there room in   &amp;gt; the JPA 2.1 specification, to make an  &amp;gt; enhancement that improves what applications experience when the entity   &amp;gt; manager is invoked outside the scope of a  &amp;gt; transaction? I'm thinking that the container could have a way to give a   &amp;gt; hint to the persistence provider, that lazy  &amp;gt; fetching should not be used for the entity manager invocation. The   &amp;gt; advantage of this enhancement, is that applications  &amp;gt; will successfully be able to access the loaded entity(s) that currently   &amp;gt; fail (due to lazy fetching being used). I think  &amp;gt; that the application can also make changes to work around this (e.g. use   &amp;gt; FetchType.EAGER) but I would like the same  &amp;gt; (FetchType.LAZY) entity/association to work with both container/app   &amp;gt; managed entity managers (that don't run in a JTA  &amp;gt; transaction).  &amp;gt; Perhaps this could be a (new) standard property that the container could   &amp;gt; pass to  &amp;gt; EntityManagerFactory.createEntityManager(Map) that suggests that lazy   &amp;gt; fetching should be disabled. Or something more  &amp;gt; generic that covers other possible problems. e.g. some hint like   &amp;gt; "javax.persistence.auto-detach" that lets the provider  &amp;gt; know that loaded entities will be detached (so the provider can avoid   &amp;gt; using lazy fetching).    Folks, I'd like to get more opinions on this.   Should we pursue this   direction?  Should we try to address this issue via fetch groups/profiles?  Other?    thanks,    Linda   --- Rainer Schweigkoffer                      SAP AG Walldorf Business Solution &amp;amp; Technology            TD Core JI Technology Development                    Dietmar-Hopp-Allee 16 Java Server Core                          D-69190 Walldorf JEE Implementation Group           phone: +49 6227 7 45305 Building 3, I.3.14                 fax:   +49 6227 7 821177 rainer.schweigkoffer@... Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Bill McDermott (Co-CEO),  Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse oder sonstige vertrauliche Informationen enthalten. Sollten Sie diese E-Mail  irrtuemlich erhalten haben, ist Ihnen eine Verwertung des Inhalts,  eine Vervielfaeltigung oder Weitergabe der E-Mail ausdruecklich  untersagt. Bitte benachrichtigen Sie uns und vernichten Sie die  empfangene E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged, undisclosed, or otherwise confidential information. If you have received this e-mail  in error, you are hereby notified that any review, copying, or  distribution of it is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft: unsynchronized persistence contexts</header>
    <date>Mon Dec 05 10:39:05 CET 2011</date>
    <body>Hi Linda, all, Am 29 Nov 2011 um 12:25 hat Linda DeMichiel geschrieben:  &amp;gt; 3.3.1  &amp;gt; I'd like your opinion on how we treat application managed persistence   &amp;gt; contexts. According to 3.3.1, we treat them as SYNCHRONIZED all the time.   &amp;gt; There is no way to make an application-managed persistence context   &amp;gt; simulate what a container-managed persistence context. This lack of   &amp;gt; symmetry makes me uneasy in and of itself but also in the perspective of   &amp;gt; extending this to the SE world.  &amp;gt; Any reason why we disallow UNSYNCHRONIZED application-managed persistence   &amp;gt; contexts?    Not really.  Actually, I'd like to hear from the group as to whether we   should extend the spec  in that direction. To my perception this extension could make the concept more easy to  capture. Best regards Rainer --- Rainer Schweigkoffer                      SAP AG Walldorf Business Solution &amp;amp; Technology            TD Core JI Technology Development                    Dietmar-Hopp-Allee 16 Java Server Core                          D-69190 Walldorf JEE Implementation Group           phone: +49 6227 7 45305 Building 3, I.3.14                 fax:   +49 6227 7 821177 rainer.schweigkoffer@... Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Bill McDermott (Co-CEO),  Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse oder sonstige vertrauliche Informationen enthalten. Sollten Sie diese E-Mail  irrtuemlich erhalten haben, ist Ihnen eine Verwertung des Inhalts,  eine Vervielfaeltigung oder Weitergabe der E-Mail ausdruecklich  untersagt. Bitte benachrichtigen Sie uns und vernichten Sie die  empfangene E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged, undisclosed, or otherwise confidential information. If you have received this e-mail  in error, you are hereby notified that any review, copying, or  distribution of it is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: lazy associations [was Re: Re: Re: changes for 7.6.2 Container-managed Transaction-scoped Persistence Context</header>
    <date>Mon Dec 05 16:23:28 CET 2011</date>
    <body>   Folks, I'd like to get more opinions on this.   Should we pursue this   direction?  Should we try to address this issue via fetch groups/profiles?  Other? Yes the fetch groups/profile is the right place to address this problem.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft: unsynchronized persistence contexts</header>
    <date>Mon Dec 05 21:11:52 CET 2011</date>
    <body>Hi Rainer, Thanks for the feedback.  I'll add this to the items to consider after I get  the EDR out. best regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft: unsynchronized persistence contexts</header>
    <date>Mon Dec 05 21:34:06 CET 2011</date>
    <body>We should allow Application Managed Persistence contexts to be specified as  UNSYNCHRONIZED to allow those created within the context of a transaction to  remain unsynchronized. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: lazy associations [was Re: Re: Re: changes for 7.6.2 Container-managed Transaction-scoped</header>
    <date>Mon Dec 05 22:19:54 CET 2011</date>
    <body>Hello All,    "fetch groups" or "operation groups" do have their usefulness for  pre-serialization processing and merging.  Whatever structures are created it would be valuable  to define them a generic way allowing them to be applied to other operations other than just  attribute loading. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: lazy associations [was Re: Re: Re: changes for 7.6.2 Container-managed Transaction-scoped Persistence Context</header>
    <date>Mon Dec 05 23:33:45 CET 2011</date>
    <body>Hi, I agree, fetch groups allow to control the relationships being loaded together with an entity, so this would address the the lazy associations problem. Regards Michael I was thinking more about the lazy associations problem. Is there room in the JPA 2.1 specification, to make an enhancement that improves what applications experience when the entity manager is invoked outside the scope of a transaction? I'm thinking that the container could have a way to give a hint to the persistence provider, that lazy fetching should not be used for the entity manager invocation. The advantage of this enhancement, is that applications will successfully be able to access the loaded entity(s) that currently fail (due to lazy fetching being used). I think that the application can also make changes to work around this (e.g. use FetchType.EAGER) but I would like the same (FetchType.LAZY) entity/association to work with both container/app managed entity managers (that don't run in a JTA transaction). Perhaps this could be a (new) standard property that the container could pass to EntityManagerFactory.createEntityManager(Map) that suggests that lazy fetching should be disabled. Or something more generic that covers other possible problems. e.g. some hint like "javax.persistence.auto-detach" that lets the provider know that loaded entities will be detached (so the provider can avoid using lazy fetching).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft: unsynchronized persistence contexts</header>
    <date>Mon Dec 05 23:46:38 CET 2011</date>
    <body>+1 Regards Michael We should allow Application Managed Persistence contexts to be specified as UNSYNCHRONIZED to allow those created within the context of a transaction to remain unsynchronized. --Gordon 3.3.1 I'd like your opinion on how we treat application managed persistence contexts. According to 3.3.1, we treat them as SYNCHRONIZED all the time. There is no way to make an application-managed persistence context simulate what a container-managed persistence context. This lack of symmetry makes me uneasy in and of itself but also in the perspective of extending this to the SE world. Any reason why we disallow UNSYNCHRONIZED application-managed persistence contexts?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] appendix B open issues moved to JIRA</header>
    <date>Mon Dec 05 23:51:13 CET 2011</date>
    <body>As part of preparing the Early Draft of the spec, I've removed appendix B  (Open Issues), and have created a JIRA, to which I have moved the items previously tracked  there. There is now also an issues@... mailing list to which you can subscribe. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] unsynchronized application-managed PCs</header>
    <date>Tue Dec 06 00:40:09 CET 2011</date>
    <body>It looks like the consensus is that we should add the ability to allow application-managed persistence contexts to be specified as unsynchronized. The next issue, of course, is to agree on the API for this. Does this warrant adding to EntityManagerFactory a createEntityManager method that takes a SynchronizationType argument?  Or just provide this via a property that is passed in the Map argument to createEntityManager?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: unsynchronized application-managed PCs</header>
    <date>Tue Dec 06 14:08:13 CET 2011</date>
    <body>Hi Linda, all, for the EDR I'd define a property and wait for comments.  To my understanding an AMPC with a JTA EM created outside a JTA  transaction defaults to unsynchronised, an AMPC with JTA EM created  inside a JTA transaction and an AMPC with an RL EM to synchronised. Best regards Rainer Am 5 Dec 2011 um 15:40 hat Linda DeMichiel geschrieben:  It looks like the consensus is that we should add the ability to  allow application-managed persistence contexts to be specified as  unsynchronized.    The next issue, of course, is to agree on the API for this.    Does this warrant adding to EntityManagerFactory a createEntityManager   method  that takes a SynchronizationType argument?  Or just provide this via a  property that is passed in the Map argument to createEntityManager?   --- Rainer Schweigkoffer                      SAP AG Walldorf Business Solution &amp;amp; Technology            TD Core JI Technology Development                    Dietmar-Hopp-Allee 16 Java Server Core                          D-69190 Walldorf JEE Implementation Group           phone: +49 6227 7 45305 Building 3, I.3.14                 fax:   +49 6227 7 821177 rainer.schweigkoffer@... Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Bill McDermott (Co-CEO),  Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse oder sonstige vertrauliche Informationen enthalten. Sollten Sie diese E-Mail  irrtuemlich erhalten haben, ist Ihnen eine Verwertung des Inhalts,  eine Vervielfaeltigung oder Weitergabe der E-Mail ausdruecklich  untersagt. Bitte benachrichtigen Sie uns und vernichten Sie die  empfangene E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged, undisclosed, or otherwise confidential information. If you have received this e-mail  in error, you are hereby notified that any review, copying, or  distribution of it is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: unsynchronized application-managed PCs</header>
    <date>Tue Dec 06 21:45:49 CET 2011</date>
    <body>This is a well defined component of the annotation; it should be an argument  of the API as well. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: unsynchronized application-managed PCs</header>
    <date>Tue Dec 06 23:24:14 CET 2011</date>
    <body>Hi Rainer, all, Hmmmm....  Please see section 7.9.2. When a JTA application-managed persistence context is created inside the scope of a transaction, it is synchronized with regard to that transaction. However, after that transaction ends, it is not synchronized with a subsequent transaction unless joinTransaction is invoked.  Because this is an application-managed PC, this is an application's responsibility rather than a requirement on the container.  So, we can't default it to have the behavior of SynchronizationType.SYNCHRONIZED and maintain backward compatibility. When a JTA application-managed persistence context is created outside the scope of a transaction, it is not synchronized with regard to the next tx, and is not otherwise synchronized with regard to subsequent transactions, so in that case we could claim that an application-managed persistence context created outside the scope of a transaction defaults to SynchronizationType.UNSYNCHRONIZED. That said, if we do extend the notion of SynchronizationType to JTA application-managed PCs, it seems clearest to me to define that there is no default. best regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: addNamedQuery was Re: updated spec draft: unsynchronized persistence contexts</header>
    <date>Tue Dec 06 21:39:58 CET 2011</date>
    <body>At this point the specification can add this functionality so that every time  addNamedQuery is called any previous queries registered with the same name  are replaced.  The EntityManagerFactory API is intended to be concurrently  accessed and that contract would be extended to this API as well.  Any  namedQuery management can safely be deferred to the client code.  For  instance if a client application wants to prevent calling this API multiple  times using the same name the client code can easily maintain a list of added  query names.  In the future if the need arrives we can add additional API to  check for the existence of a named query. We should still add EMF lifecycle events as otherwise it would be very  difficult for an client application to ensure it's named query was created on  the EMF provided by the container as the container is managing the EMF  internally and this behaviour is not specified. A simple structure would be the following:      /**       * The persistence provider is responsible for invoking this method on  any registered implementors of EntityManagerFactoryListener after the  EntityManagerFactory has been created just prior to returning the  EntityManagerFactory to the requester.       */ This is an easy point to define within the EMF lifecycle.  More complicated  points (like post metadata processing stage) can be added later as needed. The EntityManagerFactoryListener could be registered through a EMF property. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: unsynchronized application-managed PCs</header>
    <date>Wed Dec 07 12:46:52 CET 2011</date>
    <body>Hi Linda, all, Am 6 Dec 2011 um 14:24 hat Linda DeMichiel geschrieben:  That said, if we do extend the notion of SynchronizationType to JTA  application-managed PCs, it seems clearest to me to define that there  is no default. Well, there is, in the sense that if I don't set a  SynchronizationType I'll get the old behaviour for backwards  compatibility.  Thus, I think we have to describe the differences clearly in one  place in the spec. We should also, I believe, say what happens if  someone tries to set a SynchronisationType for a resource-local  entity manager. Always throw an exception ? Only for  SynchronizationType.UNSYNCHRONIZED ? Kind regards Rainer --- Rainer Schweigkoffer                      SAP AG Walldorf Business Solution &amp;amp; Technology            TD Core JI Technology Development                    Dietmar-Hopp-Allee 16 Java Server Core                          D-69190 Walldorf JEE Implementation Group           phone: +49 6227 7 45305 Building 3, I.3.14                 fax:   +49 6227 7 821177 rainer.schweigkoffer@... Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Bill McDermott (Co-CEO),  Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse oder sonstige vertrauliche Informationen enthalten. Sollten Sie diese E-Mail  irrtuemlich erhalten haben, ist Ihnen eine Verwertung des Inhalts,  eine Vervielfaeltigung oder Weitergabe der E-Mail ausdruecklich  untersagt. Bitte benachrichtigen Sie uns und vernichten Sie die  empfangene E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged, undisclosed, or otherwise confidential information. If you have received this e-mail  in error, you are hereby notified that any review, copying, or  distribution of it is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Proposal: Introduce @Query annotation for fields</header>
    <date>Thu Dec 08 11:06:09 CET 2011</date>
    <body>Hello everyone, I would like to propose a new feature for JPA. To make it more clear, I will show you my use case: I have got a customer object with a list of offers: @OneToMany So far, so good. As soon as I want a List with all undelivered orders, I have to create a separate business method, call the EntityManager with a specific query and set the resultset in my POJO. From my point of view, this is overhead! My proposal: @Transient @Query("FROM Order WHERE delivered = false") But I don't want it only to apply for collections, but also for other field types: @Transient @Query("FROM Order WHERE status = ? ORDER BY ", OrderStatus.Failed) Additionally a FetchType would be good to specify. Regards, Daniel</body>
  </mail>
  <mail>
    <header>[jpa-spec users] AutoCloseable EntityManager</header>
    <date>Fri Dec 09 06:51:21 CET 2011</date>
    <body>I've wondered about will Java EE 7 include a JPA specification that updates the EntityManager interface so that it extends Java 7's AutoCloseable. I've posted a question regarding this (in a generalized way) at StackOverflow ( http://stackoverflow.com/q/8431755/433835 ). Please see that for fancy links. I've checked out Java EE 7's JCP page to get some information on this, but I didn't find a thing. Then I did the same with the early draft for JPA 2.1 and it doesn't mentions such a thing also. My first thought was that JPA 2.1 will try to be compatible with Java 6, so AutoCloseable is a big no (correct me if I'm wrong). Is JPA 2.1 a candidate to be included in Java EE 7? If yes, does it mean that it has to / will focus on Java 7 features like automatic resource management? Other than this, how do *you* like the idea of AutoCloseable EntityManagers, EntityTransaction, et cetera?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] mapping conversion</header>
    <date>Fri Dec 09 22:02:24 CET 2011</date>
    <body>Hi everyone, One of the most common feature requests from people who are mapping entities using JPA is a way to transform or convert the state from the way it is stored in the entity attribute to the way it is stored in the database, and vice versa. A “converter” provides this functionality. Below is a proposal for converters. A converter is an object that implements the javax.persistence.mappings.EntityAttributeConverter interface, defined as: (*** OPEN ISSUE: Should the package just be javax.persistence or should a subpackage be used in anticipation of other mapping extensions? Some people have asked for a mapping API and this class would probably fit in with that.) (*** OPEN ISSUE: Is there a better classname and method pair?) /**   * A class that implements this interface can be used to convert entity attribute state   * into database column representation and back again.  Note that the X and Y types may  * be the same Java type.  *  * @param X The type of the entity attribute  * @param Y The type of the database column   */     /**      * Converts the object stored in the entity attribute into the data representation      * to be stored in the database.      *      * @param attributeObject The object in the entity attribute to be converted      * @return The converted data to be stored in the database column      */     /**      * Converts the data stored in the database column into an object      * to be stored in the entity attribute.      *      * @param dbData The data from the database column to be converted      * @return The converted state to be stored in the entity attribute      */ A converter is defined using a javax.persistence.Converter annotation. It is similar to a TableGenerator and SequenceGenerator in that it can be defined on any entity class, mapped superclass, or entity attribute, and given a name, but the name must be unique within the scope of the persistence unit. The annotation to use on the attribute is defined as: @Retention(RUNTIME)         In order to make use of a converter the @Convert annotation is used on an entity or mapped superclass attribute. The @Convert annotation is defined as: @Retention(RUNTIME)     It annotates an entity attribute and specifies the name of the converter. The same converter can be referenced by any number of @Convert annotated attributes. Example: @Entity @Converter(name="booleanToInteger",            converterClass=BooleanToIntegerConverter.class)         @Convert("booleanToInteger")         ... public class BooleanToIntegerConverter implements     public Integer convertToDatabaseColumn (Boolean                 public Boolean convertToEntityAttribute ( Integer         return (dbData &amp;gt; 0)     The Container is responsible for invoking the corresponding method when loading a converted entity attribute from the database, or before saving that entity attribute state to the database. The @Convert annotation may be used on default or explicitly mapped basic attributes. It is not portably supported to use @Convert on mappings marked with @Enumerated, @Temporal, @Id, @Version or on embedded, relationship or element collection attributes. (*** OPEN ISSUE: Should we require providers to support some of these mappings? I can see supporting the conversion of temporal and enumerated attribute types, but in those cases @Convert would be used on its own, not in combination with either @Enumerated or @Temporal.) Schema Generation When using schema generation a provider may choose to assume the column type matches the entity attribute type if no additional @Column.columnDefinition is supplied, or a provider may have an alternate way of deciding the column type for an entity attribute. Queries Instances of attribute values in JP QL or criteria queries (such as in comparisons or bulk assignments, etc) must be converted to database types before the queries are executed. If the result of a query includes one or more specific entity attributes then the results must be converted to entity attribute form before being returned. XML Converters can be defined in XML at method, class, mapping file or persistence unit level, similar to generators, and will override the same named converters if such converters were defined in annotation form. Enhancements This proposal covers converting data from a single column to a single entity attribute. The assumptions are: &lt;p class="MsoNormal" style="margin-left: 0.5in; text-indent: -0.25in;"&gt; a)       This will cover the 90% case &lt;p class="MsoNormal" style="margin-left: 0.5in; text-indent: -0.25in;"&gt; b)       More advanced cases, such as multiple columns to a single attribute, can be covered by a more advanced mapping or a more advanced converter type that can be added later if necessary &lt;p class="MsoNormal" style="margin-left: 0.5in; text-indent: -0.25in;"&gt; c)   It is not worth convoluting the simple case to handle a much less likely complex case &lt;p class="MsoNormal" style="margin-left: 0.5in; text-indent: -0.25in;"&gt; Comments? &lt;p class="MsoNormal" style="margin-left: 0.5in; text-indent: -0.25in;"&gt; -Mike</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] updated spec draft / EDR</header>
    <date>Mon Dec 19 21:54:38 CET 2011</date>
    <body>Hi all, I've uploaded to the Documents area the spec draft I just submitted to the JCP for Early Draft.   Sorry this has taken so long, but there was more process-wise than I had anticipated. As planned, this contains a few minor corrections and cleanups to Draft 4, and move of the open issue notes out of the spec and into the JIRA. New features and enhancements will now be targeted to Draft 6. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: mapping conversion</header>
    <date>Mon Dec 19 22:22:28 CET 2011</date>
    <body>Hi all, It would be good to get some feedback from you on this proposal! As I hope you might expect, I don't have any major feedback since Mike and I have discussed this beforehand :-) when he was kind enough to volunteer to drive this feature. The few suggestions I do have are embedded below.  Otherwise, I think we should proceed to add this feature to the spec. thanks, -Linda I would keep this in javax.persistence, as it is pretty basic functionality. We can revisit later on (i.e. until JPA 2.1 PFD) if we change our mind. I would drop "Entity" from the interface name Since the converter is a class (unlike TableGenerator and SequenceGenerator), I'm thinking it would be nicer if we could apply the annotation to the converter class rather than to an entity, mapped superclass, or attribute in order to define the class as a converter. We could then default the name of the converter to the lowercased unqualified class name, or allow a name to be specified. e.g., @Converter  //name defaults to booleanToInteger or @Converter(name="booleanToInt") Yes If we follow the suggestion above, I would do this at mapping file or  persistence unit levels only.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: mapping conversion</header>
    <date>Tue Dec 20 02:15:12 CET 2011</date>
    <body>Linda, Do you have to use @Convert on every attribute to be converted? In:  &amp;gt; @Convert("booleanToInteger") should id be 'int', not 'long'? I'm on vacation so probably won't read any reply until next month :-)  -----Original Message-----  From: Linda DeMichiel [ mailto:linda.demichiel@...]   Sent: Tuesday, 20 December 2011 10:22 a.m.  To: jsr338-experts@...  Cc: michael keith  Subject: [jsr338-experts] Re: mapping conversion    Hi all,    It would be good to get some feedback from you on this proposal!    As I hope you might expect, I don't have any major feedback since  Mike and I have discussed this beforehand :-) when he was kind enough  to volunteer to drive this feature.    The few suggestions I do have are embedded below.  Otherwise, I  think we should proceed to add this feature to the spec.    thanks,    -Linda      &amp;gt; Hi everyone,  &amp;gt; One of the most common feature requests from people who are   mapping entities using JPA is a way to transform or convert  &amp;gt; the state from the way it is stored in the entity attribute   to the way it is stored in the database, and vice versa. A  &amp;gt; "converter" provides this functionality. Below is a   proposal for converters.  &amp;gt; A converter is an object that implements the   javax.persistence.mappings.EntityAttributeConverter   interface, defined as:  &amp;gt; (*** OPEN ISSUE: Should the package just be   javax.persistence or should a subpackage be used in   anticipation of other  &amp;gt; mapping extensions? Some people have asked for a mapping   API and this class would probably fit in with that.)    I would keep this in javax.persistence, as it is pretty basic   functionality.  We can revisit later on (i.e. until JPA 2.1 PFD) if we change   our mind.    &amp;gt; (*** OPEN ISSUE: Is there a better classname and method pair?)    I would drop "Entity" from the interface name    &amp;gt; /**  &amp;gt; * A class that implements this interface can be used to   convert entity attribute state  &amp;gt; * into database column representation and back again.  Note   that the X and Y types may  &amp;gt;   * be the same Java type.  &amp;gt;   *  &amp;gt;   * @param X The type of the entity attribute  &amp;gt;   * @param Y The type of the database column  &amp;gt; */  &amp;gt;      /** ||  &amp;gt;       * Converts the object stored in the entity attribute   into the data representation||  &amp;gt; ||     * to be stored in the database.  &amp;gt;       *  &amp;gt;       * @param attributeObject The object in the entity   attribute to be ||converted  &amp;gt;       * @return The converted data to be stored in the   database column||  &amp;gt;       */||  &amp;gt;      public Y convertToDatabaseColumn (X attributeObject);||  &amp;gt; ||  &amp;gt;      /** ||  &amp;gt;       * Converts the data stored in the database column   into an object ||  &amp;gt;       * to be stored in the entity attribute.||  &amp;gt;       *||  &amp;gt;       * @param dbData The data from the database column to   be converted||  &amp;gt;       * @return The converted state to be stored in the   entity attribute||  &amp;gt;       */||  &amp;gt;      public X convertToEntityAttribute (Y dbData);||  &amp;gt; |A converter is defined using a javax.persistence.Converter   annotation. It is similar to a TableGenerator and  &amp;gt; SequenceGenerator in that it can be defined on any entity   class, mapped superclass, or entity attribute, and given a  &amp;gt; name, but the name must be unique within the scope of the   persistence unit.  &amp;gt; The annotation to use on the attribute is defined as:  &amp;gt; @Retention(RUNTIME)  &amp;gt; |  &amp;gt; In order to make use of a converter the @Convert annotation   is used on an entity or mapped superclass attribute. The  &amp;gt; @Convert annotation is defined as:  &amp;gt; @Retention(RUNTIME)  &amp;gt; |  &amp;gt; It annotates an entity attribute and specifies the name of   the converter. The same converter can be referenced by any  &amp;gt; number of @Convert annotated attributes.  &amp;gt; Example:  &amp;gt; |@Entity  &amp;gt; @Converter(name="booleanToInteger",  &amp;gt;             converterClass=BooleanToIntegerConverter.class)  &amp;gt; @Convert("booleanToInteger")  &amp;gt; ...  &amp;gt; |  &amp;gt; |public class BooleanToIntegerConverter implements   &amp;gt; return (dbData &amp;gt; 0)    Since the converter is a class (unlike TableGenerator and   SequenceGenerator),  I'm thinking it would be nicer if we could apply the annotation to the  converter class rather than to an entity, mapped superclass,   or attribute  in order to define the class as a converter.    We could then default the name of the converter to the lowercased  unqualified class name, or allow a name to be specified.    e.g.,    @Converter  //name defaults to booleanToInteger    or  @Converter(name="booleanToInt")        &amp;gt; |The Container is responsible for invoking the   corresponding method when loading a converted entity   attribute from the  &amp;gt; database, or before saving that entity attribute state to   the database.  &amp;gt; The @Convert annotation may be used on default or   explicitly mapped basic attributes. It is not portably supported to  &amp;gt; use @Convert on mappings marked with @Enumerated,   @Temporal, @Id, @Version or on embedded, relationship or element  &amp;gt; collection attributes.  &amp;gt; (*** OPEN ISSUE: Should we require providers to support   some of these mappings? I can see supporting the conversion of  &amp;gt; temporal and enumerated attribute types, but in those cases   @Convert would be used on its own, not in combination with  &amp;gt; either @Enumerated or @Temporal.)    Yes    &amp;gt; Schema Generation  &amp;gt; When using schema generation a provider may choose to   assume the column type matches the entity attribute type if no  &amp;gt; additional @Column.columnDefinition is supplied, or a   provider may have an alternate way of deciding the column type for  &amp;gt; an entity attribute.  &amp;gt; Queries  &amp;gt; Instances of attribute values in JP QL or criteria queries   (such as in comparisons or bulk assignments, etc) must be  &amp;gt; converted to database types before the queries are   executed. If the result of a query includes one or more specific  &amp;gt; entity attributes then the results must be converted to   entity attribute form before being returned.  &amp;gt; XML  &amp;gt; Converters can be defined in XML at method, class, mapping   file or persistence unit level, similar to generators, and  &amp;gt; will override the same named converters if such converters   were defined in annotation form.    If we follow the suggestion above, I would do this at mapping   file or persistence unit levels only.    &amp;gt; Enhancements  &amp;gt; This proposal covers converting data from a single column   to a single entity attribute. The assumptions are:  &amp;gt; a)This will cover the 90% case  &amp;gt; b)More advanced cases, such as multiple columns to a single   attribute, can be covered by a more advanced mapping or a  &amp;gt; more advanced converter type that can be added later if necessary  &amp;gt; c)It is not worth convoluting the simple case to handle a   much less likely complex case  &amp;gt; Comments?  &amp;gt; -Mike    </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: mapping conversion</header>
    <date>Tue Dec 20 16:26:10 CET 2011</date>
    <body>Hi Linda, Mike, all, +1 for adding this feature to the spec. Am 19 Dec 2011 um 13:22 hat Linda DeMichiel geschrieben:  &amp;gt; (*** OPEN ISSUE: Is there a better classname and method pair?)    I would drop "Entity" from the interface name What about TypeConverter / convertToDBType() / convertToJavaType() ? Kind regards Rainer --- Rainer Schweigkoffer                      SAP AG Walldorf Business Solution &amp;amp; Technology            TD Core JI Technology Development                    Dietmar-Hopp-Allee 16 Java Server Core                          D-69190 Walldorf JEE Implementation Group           phone: +49 6227 7 45305 Building 3, I.3.14                 fax:   +49 6227 7 821177 rainer.schweigkoffer@... Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Bill McDermott (Co-CEO),  Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse oder sonstige vertrauliche Informationen enthalten. Sollten Sie diese E-Mail  irrtuemlich erhalten haben, ist Ihnen eine Verwertung des Inhalts,  eine Vervielfaeltigung oder Weitergabe der E-Mail ausdruecklich  untersagt. Bitte benachrichtigen Sie uns und vernichten Sie die  empfangene E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged, undisclosed, or otherwise confidential information. If you have received this e-mail  in error, you are hereby notified that any review, copying, or  distribution of it is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] (Fwd) Proposal: Introduce @Query annotation for fields</header>
    <date>Tue Dec 20 16:38:23 CET 2011</date>
    <body>Hi all, how do you feel about Daniel Sachse's proposal on the observer list ? Having talked to a couple of users I got the feedback that they'd  consider it a quite useful feature. Precise name of the annotation would have to be discussed. I'd rather  expect standard JPQL syntax for the query string (selects only, of  course) or preferably names of named Queries. In the latter case one  might also dare to think of a load attribute for the @Transient  annotation. Support of fetch types would be helpful. Comments ? Best regards Rainer ------- Weitergeleitete Nachricht / Forwarded message ------- Datum:          Thu, 8 Dec 2011 11:06:09 +0100 An:                     users@... Betreff:                [jpa-spec users] Proposal: Introduce @Query  annotation for fields Antwort an:             users@... Hello everyone, I would like to propose a new feature for JPA. To make it more clear, I will show you my use case: I have got a customer object with a list of offers: @OneToMany So far, so good. As soon as I want a List with all undelivered  orders, I have to create a separate business method, call the EntityManager with a specific query and set the resultset in my POJO. From my point of view, this is overhead! My proposal: @Transient @Query("FROM Order WHERE delivered = false") But I don't want it only to apply for collections, but also for other field types: @Transient @Query("FROM Order WHERE status = ? ORDER BY ", OrderStatus.Failed) Additionally a FetchType would be good to specify. Regards, Daniel --- Ende der weitergeleiteten Nachricht / End of forwarded message --- --- Rainer Schweigkoffer                      SAP AG Walldorf Business Solution &amp;amp; Technology            TD Core JI Technology Development                    Dietmar-Hopp-Allee 16 Java Server Core                          D-69190 Walldorf JEE Implementation Group           phone: +49 6227 7 45305 Building 3, I.3.14                 fax:   +49 6227 7 821177 rainer.schweigkoffer@... Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Bill McDermott (Co-CEO),  Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse oder sonstige vertrauliche Informationen enthalten. Sollten Sie diese E-Mail  irrtuemlich erhalten haben, ist Ihnen eine Verwertung des Inhalts,  eine Vervielfaeltigung oder Weitergabe der E-Mail ausdruecklich  untersagt. Bitte benachrichtigen Sie uns und vernichten Sie die  empfangene E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged, undisclosed, or otherwise confidential information. If you have received this e-mail  in error, you are hereby notified that any review, copying, or  distribution of it is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] new expert group member</header>
    <date>Tue Dec 20 22:36:19 CET 2011</date>
    <body>Please welcome Bernd Mueller as a new expert to the group. Bernd is an iJUG representative and author of a book on JPA. Some of you may already know him from the JSF expert group. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Multicolumn IN-Subselect</header>
    <date>Thu Dec 22 14:15:59 CET 2011</date>
    <body>I'd like to see JPA 2.1 to support queries like this: SELECT x  FROM MyEntity x  WHERE    (x.p1, x.p2)      IN        (         SELECT y.p1, MAX(y.p2)          FROM MyEntity y          GROUP BY y.p1       )</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft / EDR</header>
    <date>Sat Dec 24 01:37:39 CET 2011</date>
    <body>The Early Draft has now been posted:  http://jcp.org/en/jsr/summary?id=338 -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Custom Sequence Generation</header>
    <date>Tue Dec 27 22:20:46 CET 2011</date>
    <body>Hi Expert Group Are there plans to standardise the mechanism of building custom sequence generators? I could not find any sequence generator enhancements in Early Draft 5. Existing tools provide proprietary extensions to accomplish this.  EclipseLink has org.eclipse.persistence.sequencing.Sequence http://wiki.eclipse.org/EclipseLink/Examples/JPA/CustomSequencing Hibernate has org.hibernate.id.SequenceGenerator http://www.georgestragand.com/jpaseq.html For example, you can imagine people wanting to build sequences that include check digits, or custom formatting. Many thanks Andrew Ward</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Map Enum to Coded Values</header>
    <date>Tue Dec 27 22:57:11 CET 2011</date>
    <body>Hi Expert Group The current JPA spec provides @Enumerated, with EnumType being limited to  ORDINAL and STRING. ORDINAL is not good long term, as enumerations may be expanded, and ordinal  positions will change if new literals are inserted. Seems too brittle. STRING is better than ORDINAL, but creates a conflict between the desire for  short codes in the database, and meaningful literal names. What users really want is the ability to map an enum literal to a code (a  short string, or number).  The design problem you have, is how to best specify the custom enum attribute  to be used as the code. EclipseLink and Hibernate understand this requirement, and provide  proprietary solutions. http://wiki.eclipse.org/EclipseLink/Examples/JPA/EnumToCode http://stackoverflow.com/questions/2751733/map-enum-in-jpa-with-fixed-values Is there any chance of standardising this in JPA 2.1? Thanks Andrew Ward</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Map Enum to Coded Values</header>
    <date>Wed Dec 28 01:06:21 CET 2011</date>
    <body>My preference would be to add a new option to @EnumType, say ATTRIBUTE, and  additional parameters to @Enumerated, called say, AttributeGetter and  AttributeType. Example:  public Suit(String code)  public String getCode() @Enumerated(EnumType.ATTRIBUTE, AttributeGetter="getCode",  AttributeType.STRING) Perhaps we could infer AttributeType from the return type of AttributeGetter. Thanks Andrew  Hi Expert Group    The current JPA spec provides @Enumerated, with EnumType being limited to   ORDINAL and STRING.    ORDINAL is not good long term, as enumerations may be expanded, and ordinal   positions will change if new literals are inserted. Seems too brittle.  STRING is better than ORDINAL, but creates a conflict between the desire   for short codes in the database, and meaningful literal names.    What users really want is the ability to map an enum literal to a code (a   short string, or number).     The design problem you have, is how to best specify the custom enum   attribute to be used as the code.    EclipseLink and Hibernate understand this requirement, and provide   proprietary solutions.     http://wiki.eclipse.org/EclipseLink/Examples/JPA/EnumToCode     http://stackoverflow.com/questions/2751733/map-enum-in-jpa-with-fixed-values    Is there any chance of standardising this in JPA 2.1?    Thanks    Andrew Ward</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Specification improvements for compound ids</header>
    <date>Mon Jan 09 17:13:37 CET 2012</date>
    <body>Hi all, when working with compound ids in entities, esp. when using @IdClass various  JPA implementations behave quite differently which makes a consistent usage  very hard. Here are some of the issues I've identified. 1) the JavaDoc of IdentifiableType.getIdType() seems to a bit vague in terms  of what clients can expect when calling this method. Thus, various  implementations handle that case differently. Hibernate does not return the  type defined in an @IdClass annotation [0], whereas OpenJPA and EclipseLink  return the annotated type correctly. 2) Generation of partial id values does not work with EclipseLink and  Hibernate. EclipseLink rejects the class while building the meta model  already saying only one id attribute can be auto-generated. Hibernate seems  to have trouble assigning values to the attributes in question [1]. The discussion on whether these are critical bugs aside, I'd argue that the  spec could be a bit more precise (especially for 1) or leave a few ore words  on to what degree one can rely on auto-generation of id values for compound  identifiers. Ollie [0]  https://hibernate.onjira.com/browse/HHH-6951 [1]  https://hibernate.onjira.com/browse/HHH-6044 --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Improving the TCK</header>
    <date>Mon Jan 09 17:23:43 CET 2012</date>
    <body>Hi all, working with JPA I've stumbled across quite a few ([0], [1]) issues in  implementations that are seem to be violations of the spec which could/should  be discovered by the TCK. So what is the general way to get access to the TCK and how does one propose  improvements? The JSR-317 website [2] states in section 2.16 that there will  be a TCK made available but I had a hard time finding except other voices  being disappointed about not getting access to it [3]. Cheers, Ollie [0]  https://bugs.eclipse.org/bugs/show_bug.cgi?id=322579 [1]  https://hibernate.onjira.com/browse/HHH-4259 [2]  http://jcp.org/en/jsr/detail?id=317 [3]  http://datanucleus.blogspot.com/2011/01/jpa-tck-request-and-jpa21.html --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: mapping conversion</header>
    <date>Thu Jan 12 01:49:58 CET 2012</date>
    <body>Are there any additional comments on this proposal? If so, please post them now. If not, I will proceed to add this to the spec. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: mapping conversion</header>
    <date>Thu Jan 12 19:04:44 CET 2012</date>
    <body>1. On the queries section, it may be worthwhile to mention that the binding parameters or projected terms referring to a converted attribute are of type &amp;lt;X&amp;gt; i.e. the type of the attribute. 2. It may be useful to allow static method of a "converter" class be invoked for conversion. Typical conversion (e.g. boolean to integer) can then be handled more centrally/uniformly. Regards -- Pinaki Poddar Chair, Apache OpenJPA Project            http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware To:     jsr338-experts@... Date:   01/11/2012 04:50 PM Subject:        [jsr338-experts] Re: mapping conversion Are there any additional comments on this proposal? If so, please post them now. If not, I will proceed to add this to the spec. thanks, -Linda  Hi all,  It would be good to get some feedback from you on this proposal!  As I hope you might expect, I don't have any major feedback since  Mike and I have discussed this beforehand :-) when he was kind enough  to volunteer to drive this feature.  The few suggestions I do have are embedded below. Otherwise, I  think we should proceed to add this feature to the spec.  thanks,  -Linda entities using JPA is a way to transform or convert it is stored in the database, and vice versa. A converters. javax.persistence.mappings.EntityAttributeConverter interface, defined as: a subpackage be used in anticipation of other class would probably fit in with that.)  I would keep this in javax.persistence, as it is pretty basic functionality.  We can revisit later on (i.e. until JPA 2.1 PFD) if we change our mind.  I would drop "Entity" from the interface name attribute state and Y types may representation|| converted It is similar to a TableGenerator and superclass, or entity attribute, and given a unit. an entity or mapped superclass attribute. The converter. The same converter can be referenced by any  Since the converter is a class (unlike TableGenerator and SequenceGenerator),  I'm thinking it would be nicer if we could apply the annotation to the  converter class rather than to an entity, mapped superclass, or attribute  in order to define the class as a converter.  We could then default the name of the converter to the lowercased  unqualified class name, or allow a name to be specified.  e.g.,  @Converter //name defaults to booleanToInteger  or  @Converter(name="booleanToInt") loading a converted entity attribute from the basic attributes. It is not portably supported to @Version or on embedded, relationship or element mappings? I can see supporting the conversion of would be used on its own, not in combination with  Yes type matches the entity attribute type if no an alternate way of deciding the column type for comparisons or bulk assignments, etc) must be result of a query includes one or more specific form before being returned. persistence unit level, similar to generators, and in annotation form.  If we follow the suggestion above, I would do this at mapping file or persistence unit levels only. entity attribute. The assumptions are: can be covered by a more advanced mapping or a likely complex case</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: mapping conversion</header>
    <date>Fri Jan 13 11:44:14 CET 2012</date>
    <body>Hi all, comments inline… Am 12.01.2012 um 01:49 schrieb Linda DeMichiel: Why shouldn't it be allowed for embeddables? Does this mean the converters would be instantiated by the persistence  provider then? This would expose it to the same drawbacks EntityListeners  currently face: injecting objects into them is quite complicated. Is there some notion of a "global converter"? Given Mike's examples one would  have to annotate each and every boolean attribute in entity classes with  @Convert("booleanToInteger") to get it converted. What if the converters  registered would be applied globally to all attributes of the appropriate  type (discoverable from generics) and could be explicitly overridden at the  field level. Alternatively we could have a "global" flag in the @Converter  annotation that triggers this behavior. Regards, Ollie --  /** * @author Oliver Gierke - Senior Member Technical Staff * * @param email ogierke@... * @param phone +49-351-30929001 * @param fax   +49-351-418898439 * @param skype einsdreizehn * @see  http://www.olivergierke.de */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: mapping conversion</header>
    <date>Fri Jan 13 16:45:15 CET 2012</date>
    <body>Thanks, Oliver. See comments below. Hi all, comments inline… Am 12.01.2012 um 01:49 schrieb Linda DeMichiel: The @Convert annotation may be used on default or explicitly mapped basic attributes. It is not portably supported to use @Convert on mappings marked with @Enumerated, @Temporal, @Id, @Version or on embedded, relationship or element collection attributes.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: mapping conversion</header>
    <date>Fri Jan 13 16:49:52 CET 2012</date>
    <body>Sure, this feature will need some coverage in the queries section, including parameter binding. Why would it need to be static? Applications can instantiate their own concrete class and use it for conversion of anything they want if they need to do so.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: mapping conversion</header>
    <date>Fri Jan 13 17:35:33 CET 2012</date>
    <body> Why would it need to be static? I did not mean that it *must* be static. Having a mechanics to *also* invoke static method of a user-supplied implementation for conversion will have some runtime benefit (e.g. less instantiation of the converter instances). Regards -- Pinaki Poddar Chair, Apache OpenJPA Project            http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware To:     jsr338-experts@... Cc:     Pinaki Poddar/Dallas/IBM@IBMUS Date:   01/13/2012 07:53 AM Subject:        [jsr338-experts] Re: mapping conversion  1. On the queries section, it may be worthwhile to mention that the binding  parameters or projected terms referring to a converted attribute are of  type&amp;lt;X&amp;gt;  i.e. the type of the attribute. Sure, this feature will need some coverage in the queries section, including parameter binding.  2. It may be useful to allow static method of a "converter" class be  invoked for conversion. Typical conversion (e.g. boolean to integer) can  then be handled more centrally/uniformly. Why would it need to be static? Applications can instantiate their own concrete class and use it for conversion of anything they want if they need to do so.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: mapping conversion</header>
    <date>Fri Jan 13 17:46:51 CET 2012</date>
    <body>Given that there probably isn't much call for injection or dynamic state in these converters, a provider could just keep a single instance around to use for all of its conversions of that type, right?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Enhancement suggestion: Relationships to non-primary key columns (or alternatively alternative key columns)</header>
    <date>Fri Jan 13 19:48:06 CET 2012</date>
    <body>Dear JPA 2 expert group, I have something to share with you about a possible JPA feature that I'm  pondering over for quite a long time. A use case I frequently encounter is when trying to reference a (sub) table  that has a super table (mapped via InheritanceType.JOINED). Due to the fact  that JPA requires all sub entities to use the identifier of the inheritance  root  class results in the fact that *other* tables/entities (external to the  inheritance tree) are supposed to / forced to reference the primary key  columns of an inheritance entity (at least not without a performance hit). Take, for example, geographic areas: continents, countries, states etc. These  could be modeled and mapped as following (a parent_id is in GeoAreas for the  parent-child relationship of geo areas): CREATE TABLE GeoAreas (   id        INTEGER     NOT NULL,   parent_id INTEGER     NULL,   name      VARCHAR(50) NOT NULL,   PRIMARY KEY (id),   FOREIGN KEY (parent_id) REFERENCES GeoAreas (id) CREATE TABLE Continents (   id       INTEGER NOT NULL,   iso_code CHAR(2) NOT NULL,   PRIMARY KEY (id),   UNIQUE (iso_code),   FOREIGN KEY (id) REFERENCES GeoAreas (id) )      CREATE TABLE Countries (   id        INTEGER  NOT NULL,   iso_code  CHAR(2)  NOT NULL,   dial_code SMALLINT NULL,   PRIMARY KEY (id),   UNIQUE (iso_code),   FOREIGN KEY (id) REFERENCES GeoAreas (id) ) CREATE TABLE States (   id       INTEGER    NOT NULL,   iso_code VARCHAR(5) NOT NULL,   PRIMARY KEY (id),   FOREIGN KEY (id) REFERENCES GeoAreas (id) ) Note the two UNIQUE constraints for continents and countries, especially the  latter one. The mapped entity classes then could look like the following: @Entity @Table(name = "GeoAreas") @Inheritance(strategy = InheritanceType.JOINED) public abstract class GeoArea implements Serializable     @Id     @Column(name = "id")     @Column(name = "name")     @ManyToOne     @JoinColumn(name = "parent_id", referencedColumnName = "id")     ... @Entity @Table(name = "Continents") public class Continent extends GeoArea     @Column(name = "iso_code")          ... @Entity @Table(name = "Countries") public class Country extends GeoArea     @Column(name = "iso_code")     @Column(name = "dial_code")     ... @Entity @Table(name = "States") public class State extends GeoArea     @Column(name = "iso_code")          ... Now there's basically no problem with the above. However, now imagine you  wanted to use the well-known ISO country codes in another table to construct  a composite primary key from them, e.g. you could construct the Zips table's  primary key by using the UNIQUE NOT NULL column iso_code from the Countries  table like this: CREATE TABLE Zips (   country_code CHAR(2)     NOT NULL,   code         VARCHAR(10) NOT NULL,   PRIMARY KEY (country_code, code),   FOREIGN KEY (country_code) REFERENCES Countries (iso_code) ) According to the JPA however, relationships to non-primary keys aren't per se  supported (it's up to the JPA provider to support such a mapping or not). The  JPA portably only allows using the ID of the root class AFAIK: CREATE TABLE Zips (   country_id   INTEGER     NOT NULL,   code         VARCHAR(10) NOT NULL,   PRIMARY KEY (country_id, code),   FOREIGN KEY (country_id) REFERENCES Countries (id) ) The problem I have with the above is that the primary key actually isn't a  natural one. Due to the fact that inheritance trees force the use of the root  table's primary key almost always results in an artificial ID to be  propagated into all sub tables and there's actually no portable/specified way  to "return" to the sub tables' natural components, here the ISO country codes. This is really limiting regarding database design (and often you're not  allowed to make changes to a DB). There's no guarantee any JPA provider will  support relationships to non-primary key columns and this might break with  any next version of the JPA software or when changing the JPA provider. Is there any chance for JPA to support some kind of way to "return" to the  natural components as in the example given? For my part it would suffice to  require the existence of an alternative key (UNIQUE, NOT NULL) for caching  reasons and others I don't know/understand (Hibernate already has an  annotation named @NaturalId which seems to be close to what I'm asking for). -- Final note: JPA providers do "kind of" support relationships to non-primary key columns,  but they do this very inconsistently. All-new JPA 2 derived identities mostly  fail in contrast to JPA 1-style mappings, effectively making the newer  mappings *less* powerful. I put up a series of tests a while ago to see how  EclipseLink 2.2.0 and Hibernate 3.6.0 fare using all possible relationship  constellations in practice. The results can be found here (rather lengthy  read): http://kawoolutions.com/Technology/JPA,_Hibernate,_and_Co./Relationships_to_Non-Primary_Key_Columns As you can see, the matrices near the bottom of that page show that JPA 1.0  mappings always work, whereas derived identies cause all sorts of mapping  exceptions and others (the X's), which proves that for relationships to  non-primary key columns JPA 1.0 style mappings are more powerful than JPA 2.0  style mappings. -- Sorry for the long explanation, but it's rather hard for me to explain what I  mean. You probably wouldn't have needed such a long use case as you already  know the details. So, to summarize my RFE: 1. add support for relationships to non-primary key columns, where  relationships to alternative key columns would *probably* suffice 2. add support for alternative keys (optional) Please let me know what you think. Thank you very much and best regards Karsten Wutzke PS: all I want to do is return to the natural key columns, so if there's  currently an (official!) way to do this please let me know. ___________________________________________________________ SMS schreiben mit WEB.DE FreeMail - einfach, schnell und kostenguenstig. Jetzt gleich testen!  http://f.web.de/?mc=021192</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: mapping conversion</header>
    <date>Mon Jan 16 06:05:54 CET 2012</date>
    <body>Hi, I would advocate for Lindas proposal, giving some additional unique/global/local arguments Am 12.01.2012 01:49, schrieb Linda DeMichiel:  Are there any additional comments on this proposal?    If so, please post them now.    If not, I will proceed to add this to the spec.    thanks,    -Linda   I see a problem: the name of the converter is unique in the pu but the usage mimics locality for an entity. In JSF there is also a converter annotation called FacesConverter doing similar things. However it is used to annotate the converter (as the name implies ?) The refactored example: @Converter("booleanToInteger") public class BooleanToIntegerConverter implements   ... @Entity  @Convert("booleanToInteger")  ... The Converter annotation uses "value" instead of "name" and is therefore a little bit less verbose. And the usage of "booleanToInteger" reflects its uniqueness instead of doubling the string literal in one single entity class. To demand for global application of the converter there is a second attribute "forClass" in @Converter defining the Class to apply for. Example @Converter(forClass=SomeBusinessPropertyType.class) ... @Entity ...   SomeBusinessPropertyType sbpt; // Converter applied automatically ... regards Bernd</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: mapping conversion</header>
    <date>Mon Jan 16 09:33:07 CET 2012</date>
    <body>Hi all, trying to reply Bernd's and Mike's replies.    There is nothing stopping someone from using it on mappings *within* an   embeddable, but an embeddable as a whole entails a number of mapped fields   and we are not supporting multiple columns in this round. Point taken. Do the types of the properties to be converted need to be known  as JPA-special types (@Entity, @MappedSuperclass) at all then? The  availability of a converter of property type X to natively mappable "column"  type Y pretty much hands responsibility of mapping the object, right?    Yes, they would be similar to EntityListeners in the fact that they would   be instantiated by the provider. Injection is not as useful, though, since   the point of converters is simply to transform attribute state from one   form to another, not to perform domain operations. That doesn't mean we   can't allow it, it's just not a priority IMO. I didn't think of injecting an EntityManager or other sort of application  component but rather of a way to configure a converter implementation by some  means, e.g. configuring a date pattern a Date object shall be read and  written into. I don't think it's a good idea to defer this question to a  later stage as it requires quite some changes to the API and implementations  if introduced later. I also think that the instance creation of user defined  extensions (what the EntityAttributeConverter actually is) should not be  hardcoded into the persistence provider. Other comments on Mike's suggestions regarding converter annotations and  EntityManagerFactory API see below. Am 16.01.2012 um 06:05 schrieb Bernd Müller:  I see a problem: the name of the converter is unique in the pu but  the usage mimics locality for an entity.    In JSF there is also a converter annotation called FacesConverter  doing similar things. However it is used to annotate the converter (as  the name implies ?)    The refactored example:    @Converter("booleanToInteger")  public class BooleanToIntegerConverter implements   ...    @Entity  @Convert("booleanToInteger")  ... +1. Although there's one caveat to this. As the flip switch to define whether  a converter is a global one or not is now tied to the implementation (as it  resides in the annotation) the implementation cannot be reused for the exact  opposite use case. Besides this flag the @Converter annotation is just what  @Named is. So I wonder if it might make sense to simply consider managed  beans of a given type (EntityAttributeConverter&amp;lt;X, Y&amp;gt;) as converters? But  that might be an implementation detail of the EntityAttributeConverterFactory  (see below). So generally I think a "global-by-default" approach would remove the need to  have this special flag at the implementation side of things. We could then  have the @Convert annotation look something like this: @Retention(RUNTIME)     boolean disabled() default false; // This would result in an opt-out model like this: 1. if a property is not annotated we look for an EntityAttributeConverter&amp;lt;X,  Y&amp;gt; with X as the property type or less concrete 2. if a property is annotated use the converter defined in value() or skip  conversion if disabled() is set to true The @Converter annotation is entirely obsolete then and the decision which  converter to be used is not split between two annotations. The according  EntityManagerFactory methods could look like this:   void setEntityAttributeConverterFactory(EntityAttributeConverterFactory    void addEntityAttributeConverter(Class&amp;lt;? extends      &amp;lt;X, Y&amp;gt; EntityAttributeConverter&amp;lt;X, Y&amp;gt; getConverter(Class&amp;lt;? extends   The Converter annotation uses "value" instead of "name" and is therefore  a little bit less verbose. +1  And the usage of "booleanToInteger" reflects its uniqueness instead  of doubling the string literal in one single entity class.    To demand for global application of the converter there is a second  attribute "forClass" in @Converter defining the Class to apply for.    Example    @Converter(forClass=SomeBusinessPropertyType.class)  ... I don't think this is necessary here as we can use a generic interface the  type parameters can be extracted from. I think using the interface based  approach trumps an annotation based one here as the metadata reflected in the  annotation attributes can be used in the method signatures to make the SPI a  bit neater to implement. Cheers, Ollie --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: mapping conversion</header>
    <date>Mon Jan 16 22:31:46 CET 2012</date>
    <body>I am fine with annotating the converter class instead of adding the @Converter annotation to the entity class, although I don't see that there are too many use cases for needing to define the target convertable class name. The type parameter of the converter will define the one that it is intended to convert, and for the most part will be enough. Two cases that I can think of that *might* warrant an additional type parameter are: a) You want to map both the primitive and wrapper types (e.g. boolean and Boolean). Not sure how often this would occur in practice. b) You want to convert a class and all its subclasses. A subclass might actually have additional state, though, so the conversion might not be sufficient. I don't believe (a) is all that useful, really. It is probably reasonable for people to either add another converter for the primitive type or put a @Convert annotation on it. Similarly for (b), an empty converter subclass could be added to handle each convertible type subclass. I was also thinking that string names for the converters are shorter but less typesafe than just using the class name of the converter. Example: @Converter public class BooleanToIntegerConverter implements @Converter(autoApply=true) public class EmployeeDateConverter implements @Entity    @Convert(com.acme.BooleanToInteger.class)    ...     // Automatically applied</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: mapping conversion</header>
    <date>Mon Jan 16 22:33:57 CET 2012</date>
    <body>Not sure what you mean. A converter converts only a single attribute in an entity, mapped superclass, or embeddable, not the entity/mapped superclass/embeddable itself. Is a date pattern something that is unique to the converter and that could be defined in some static state within it? I guess it's possible that you might need some externally defined state in order to do the conversion, it just feels like making converters something like CDI beans might be overkill. Converter implementation classes would not be hardcoded into the provider any more than entity classes or any other user class would. The provider discovers the converter class just like it does entity classes, listeners, or other user-defined classes, and instantiates them reflectively on demand. No hardcoding necessary. I think it's reasonable to state declaratively in the converter whether it is a global conversion or not. That is probably where you would want to do it. If you don't want to put it there, though, you could always put it in an XML entry instead to decouple it from the code entirely. Hmmm, a factory seems like an unnecessary addition to the model. This is just removing the autoApply option at the expense of the non-default case (which becomes uglier). Yes.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: mapping conversion</header>
    <date>Tue Jan 17 15:31:11 CET 2012</date>
    <body>There was apparently a typo in the example in my earlier message. The @Convert annotation should have been ... @Convert(com.acme.BooleanToInteger Converter .class) ... -Mike I am fine with annotating the converter class instead of adding the @Converter annotation to the entity class, although I don't see that there are too many use cases for needing to define the target convertable class name. The type parameter of the converter will define the one that it is intended to convert, and for the most part will be enough. Two cases that I can think of that *might* warrant an additional type parameter are: a) You want to map both the primitive and wrapper types (e.g. boolean and Boolean). Not sure how often this would occur in practice. b) You want to convert a class and all its subclasses. A subclass might actually have additional state, though, so the conversion might not be sufficient. I don't believe (a) is all that useful, really. It is probably reasonable for people to either add another converter for the primitive type or put a @Convert annotation on it. Similarly for (b), an empty converter subclass could be added to handle each convertible type subclass. I was also thinking that string names for the converters are shorter but less typesafe than just using the class name of the converter. Example: @Converter public class BooleanToIntegerConverter implements @Converter(autoApply=true) public class EmployeeDateConverter implements @Entity    @Convert(com.acme.BooleanToInteger.class)    ...     // Automatically applied Hi, I would advocate for Lindas proposal, giving some additional unique/global/local arguments Am 12.01.2012 01:49, schrieb Linda DeMichiel: Are there any additional comments on this proposal? If so, please post them now. If not, I will proceed to add this to the spec. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: mapping conversion</header>
    <date>Tue Jan 17 16:33:49 CET 2012</date>
    <body>Am 16.01.2012 um 21:33 schrieb michael keith:  Not sure what you mean. A converter converts only a single attribute in   an entity, mapped superclass, or embeddable, not the entity/mapped   superclass/embeddable itself. That was more of a question actually. Let's say I have a @Entity This should be enough, right? The Wrapped class doesn't have to be annotated  beyond that, does it?  Is a date pattern something that is unique to the converter and that   could be defined in some static state within it?  I guess it's possible that you might need some externally defined state   in order to do the conversion, it just feels like making converters   something like CDI beans might be overkill. Assume the following:     // use format to create the string   // … same for reading How would I get this converter working without a factory in between? I am  fine if we go for allowing users registering converter instances but  registering class names and requiring an empty constructor to create  instances by reflection feels so 2000 when you had to teach people the values  of DI. I also don't necessarily vote for deep CDI integration. The default  mechanism might be exactly that (reflection, no-arg constructor) but please  allow getting more detailed control about converter instantiation. Working  with EntityListeners (which are stuck to that model) is usually either not  gaining you much (if you stick to the standard instantiation mechanism, no  injection available) or very complex (including AspectJ).  Converter implementation classes would not be hardcoded into the   provider any more than entity classes or any other user class would. The   provider discovers the converter class just like it does entity classes,   listeners, or other user-defined classes, and instantiates them   reflectively on demand. No hardcoding necessary. Nono, I referred to the *instance creation* (requiring a no-arg constructor  to be able to instantiate the converter by reflection) being hard coded into  the persistence providers. That's how it works with EntityListeners as  described above. Doing so effectively turns persistence providers into  factories for components like EntityListeners, AttributeConverters etc. and  usually don't permit the flexibility necessary (as it's not their  responsibility to). Hibernate e.g. allows EventListener instances being registered which is a  much more powerful approach.  I think it's reasonable to state declaratively in the converter whether   it is a global conversion or not. That is probably where you would want   to do it. If you don't want to put it there, though, you could always   put it in an XML entry instead to decouple it from the code entirely. But imagine a library like JodaTime coming up with implementations of  EntityAttributeConverter. If the global-non-global config is done at the  converter implementation they already have to do it. This would result in the  need to annotate all JodaTime properties with @Convert("foo") or declare em  in XML. If it's global by default, I just register the implementations with  the EMF and opt-out at properties I don't want to have converted with the  default global one. I even might want to have two different instances of the  implementation class using different date formats: one global one plus one  only used for a single property. I think we have to consider the converters not only being written alongside  the application but rather the type they convert. Thus they can be part of a  library (such as JodaTime) and thus we should have as little configuration as  possible at the implementation class.  Hmmm, a factory seems like an unnecessary addition to the model. For what reason do you think it's unnecessary? What's the downside of having  it? It allows user side control over the converter instantiation (which I  think can be crucial) plus might then allow multiple instances of the same  converter implementation used for different conversion purposes (global,  local).  This is just removing the autoApply option at the expense of the   non-default case (which becomes uglier). What exactly becomes uglier? Actually you have less annotations to use and do  not spread the information about which converter to use over multiple places: 1. If there's a converter registered for the property type use that by  default. 2. If the property has a custom convert declared (annotation or XML), use  that. 3. If the property has conversion disabled (annotation or XML) do not convert. Actually the entire @Converter annotation becomes obsolete then if I don't  oversee any particular scenario as all the necessary meta information can  either be derived from the type or be considered when designing the  registration API at the EMF, ConverterFactory. Cheers, Ollie --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: mapping conversion</header>
    <date>Tue Jan 17 17:07:05 CET 2012</date>
    <body>I agree. I missed that the interface to implement determines the type already and the autoApply attribute therefore realizes this case. regards Bernd Am 16.01.2012 22:31, schrieb michael keith:  I am fine with annotating the converter class instead of adding the  @Converter annotation to the entity class, although I don't see that  there are too many use cases for needing to define the target  convertable class name. The type parameter of the converter will define  the one that it is intended to convert, and for the most part will be  enough. Two cases that I can think of that *might* warrant an additional  type parameter are:  a) You want to map both the primitive and wrapper types (e.g. boolean  and Boolean). Not sure how often this would occur in practice.  b) You want to convert a class and all its subclasses. A subclass might  actually have additional state, though, so the conversion might not be  sufficient.    I don't believe (a) is all that useful, really. It is probably  reasonable for people to either add another converter for the primitive  type or put a @Convert annotation on it.    Similarly for (b), an empty converter subclass could be added to handle  each convertible type subclass.    I was also thinking that string names for the converters are shorter but  less typesafe than just using the class name of the converter.    Example:    @Converter  public class BooleanToIntegerConverter implements  @Converter(autoApply=true)  public class EmployeeDateConverter implements    @Entity     @Convert(com.acme.BooleanToInteger.class)     ...      // Automatically applied  </body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: mapping conversion</header>
    <date>Tue Jan 17 22:46:20 CET 2012</date>
    <body>The Wrapped class would never need to be annotated. However, the converter class would need to be annotated with @Converter, and if you wanted it to be applied to every attribute of type Wrapped then autoApply=true would also need to be set in the @Converter annotation. There's not really much of a win in your example above. The code is basically going to be a switch statement to support each format (all of which must be known ahead of time) and I don't really see the value in having a single super converter that is really just squishing the code for multiple converters into a single class. If the caller has to know enough to call a factory to instantiate a converter with the right format argument then they will know enough to call the correct specific converter directly. Each converter is very lightweight and does a very specific conversion, i.e. there is nothing in a converter class over and above the two (typically short) conversion methods. I guess the term "hardcoding" is a subjective thing, but I wouldn't call a reflective operation on an unknown class hardcoding :-). Yes, the provider is responsible for creating the converter instances of whatever converter classes need to be used. I don't see that the additional complexity of a factory merits the flexibility that you say is necessary. In fact, I have never had anyone ask for that specific flexibility. Sure, every provider has their own design bell or whistle that someone (or in some cases nobody) uses, but we can't be adding all these to the spec. We have to draw the line somewhere reasonable, and given that I have never had anyone need the ability to get their own instances of these objects I think this is the place to draw it. If I understand your use case correctly you are saying that they have some canned converters that are *not* set to autoApply and they now want to reuse them in a different app but set them to autoApply? It's possible but I would say less likely.  A converter is going to generally be one of three types: a) Convert a domain-specific object (e.g. EmployeeStatus). These are going to be set to autoApply and applied whenever that object type is mapped b) Convert a known Java type that does not have a corresponding DB type (e.g. a URL). These will also be set to autoApply since all mapped objects of that type will typically be converted the same way. c) Convert an existing Java type to be stored a very specific way in a specific column (e.g. a Date is stored as a Number in a specific DB column). This is used by a very limited number of mappings and will not be set to autoApply. Pre-existing converters will generally have an autoApply setting that makes sense, and when reused that setting will still make the most sense. If, as you suggest, we made all converters autoApply by default to all attributes of that type then we save the implementer adding a single attribute in case a) and b), but if ever someone wants the default to not be autoApply (case c) then you will require that they use @Convert for every single attribute that *doesn't* need conversion. In our case c) above that means every Date other than the one that I want to convert needs a @Convert just to disable the default conversion. If, in the unlikely case that somebody does want to use a converter that has an autoApply setting other than the one defined on it then they would have two choices: 1) Add an overriding entry in the XML file 2) Use the EMF API that I brought up earlier to specify/override the autoApply option at runtime API bloat for little benefit. See my explanation above about how the inverse case becomes ugly because of all the annotations needed to cause the converter not to be applied.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: mapping conversion</header>
    <date>Wed Jan 18 11:18:56 CET 2012</date>
    <body>What use-cases should be supported? So far, I have only seen the need for an enumeration-to-code-conversation in real-world-projects. However, there could be more: * List of Strings to be converted into a single string with comma-separated values * Array of Strings to be converted into a comma-separated-values string * Array of String to be stored as java.sql.Array (Where to get the java.sql.Connection from?) * JAXB2: Marshalling/Unmarshalling a graph of objects into a byte[]. (Where to provide the contextPath for the JAXBContext?) * Auditing: Replacing a stale java.util.Date value with the current time. * Encryption: Use the currently logged-in user's key to encrypt/decrypt a String (How to obtain the currently logged-in user?) Some of these use cases raise a question that is probably only easy to answer for the enumeration-to-code conversation: When will such a converted field/property be updated in the database? * Always? * When the java representation changes? * When the database representation changes? What is then the criteria for detecting a change? Kind regards Frank</body>
  </mail>
  <mail>
    <header>[jpa-spec users] @OrderColumn: "non-sparse" and smallest value</header>
    <date>Wed Jan 18 18:27:57 CET 2012</date>
    <body>Hello, I've just read http://docs.oracle.com/javaee/6/api/javax/persistence/OrderColumn.html There are the sentences: "The persistence provider maintains a contiguous (non-sparse) ordering of the  values of the order column when updating the association or element  collection. The order column value for the first element is 0." What does "non-sparse" mean? What if I inserted entities into the table, that  is with its order column starting with 1, would list.get(0) return that  entity even though the actual DB order value is 1? Thanks Karsten ___________________________________________________________ SMS schreiben mit WEB.DE FreeMail - einfach, schnell und kostenguenstig. Jetzt gleich testen!  http://f.web.de/?mc=021192</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] javadocs</header>
    <date>Thu Jan 19 19:31:49 CET 2012</date>
    <body>I've uploaded the javadocs that correspond to the Early Draft version of the spec to  http://java.net/projects/jpa-spec/downloads . Please let me know if you find anything amiss. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] new expert group member</header>
    <date>Sat Jan 21 00:22:24 CET 2012</date>
    <body>I'd like to welcome Christian Romberg to the JPA 2.1 expert group.  Christian represents Versant Corporation, and has long experience with O/R mapping and  persistence. Welcome, Christian! -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Fwd: Features for the next JPA</header>
    <date>Tue Jan 24 20:39:39 CET 2012</date>
    <body>Hello Expert group and users, I'm forwarding a conversation I started with the JSR 317 group back in April 2010 about a manual flush mode idea. Those of us who have been following JPA have known this has been a requested feature since JPA 1.0. I've scanned the archives of the expert mailing list and the user list and didn't see anything that seemed like it reflected this conversation, if this has already been addressed, my apologies. Forwarded conversation Subject: Features for the next JPA ------------------------ From: Jason Porter lightguard.jp@... Date: Wed, Apr 14, 2010 at 12:25 To: jsr-317-feedback@... FlushMode.MANUAL Reasoning:  Imagine I have a wizard like form that takes you through a series of steps. You can cancel the wizard at any time. If you cancel and don't complete the process either the records that were persisted need to be removed, or with a manual flush option I can flush once at the end of the wizard (keeping state in a conversation or a session) and just write to the database once. That's the power of having a manual flush. I'm not done with my unit of work, so don't write anything until I tell you to, when I'm done with the unit of work. It would be the same in a desktop / fat client application, why commit things to the database and have to possibly clean it out if I don't need to? Standardization of audit log / versioning API Reasoning: Both Hibernate (via Envers) and Eclipselink have this ability.  It also comes up very frequent in enterprise applications, and we have to look to non-standard solutions or roll our own.  A standard API would be very helpful. -- Jason Porter Software Engineer Open Source Advocate PGP key id: 926CCFF5 PGP key available at: keyserver.net , pgp.mit.edu ---------- From: Evan Ireland eireland@... Date: Wed, Apr 14, 2010 at 14:41 lightguard.jp@... Cc: jsr-317-feedback@... Jason, Agreed on FlushMode.MANUAL. I'm not convinced regharding auditing. Yes it comes up in many enterprise applications, but the requirements vary considerably from system to system. What did you have in mind with regard to a versioning API? ---------- From: Jason Porter lightguard.jp@... Date: Wed, Apr 14, 2010 at 15:16 eireland@... jsr-317-feedback@... jsr-317-feedback@... I'm only familiar with Envers, so I'm not sure what other solutions do. Envers creates a duplicate of your table(s) with additional columns such as change date. Whenever a change happens to your table(s) an entry is put into the other table, this also allows for a rudimentary audit log (if other data such as user that made the change could be added) as well. Another solution (though it kinda reinvents what some dbs already do) would be to store the SQL that was run, that would allow a replay log to some degree. It also would allow for a restore of previous states if needed (not sure if that would/should be part of a spec atm). Sent from my iPhone ---------- From: Mike Keith michael.keith@... Date: Wed, Apr 14, 2010 at 17:39 lightguard.jp@... Cc: jsr-317-feedback@... Hi Jason, While I agree with the use case of a longer lived application transaction, a Flushmode.Manual option is not the right solution for the problem, particularly in the context of multiple resource JTA transactions. A longer-lived pseudo-transactional context should be transactionally consistent and have expected transactional semantics. A FlushMode.Manual option (of the variety promoted by some people/products) on the persistence context is not the solution to what you are describing and does not exhibit the correct transaction behavior (in your example multiple JTA transactions may have been committed in the interim since the wizard started, and deciding to simply not persist the entities at any time can render a potentially inconsistent result in those transactions). The solution is to have a proper application transaction mechanism that behaves as one might expect an application transaction to behave. This should involve more than just the persistence layer, although if the primary use case is that only a single persistence resource is needed/used then we might be able to come up with a special transaction flag, or add a wonky option to JTA to allow for that. Not convinced that would be very nice, though. ---------- From: Evan Ireland eireland@... Date: Wed, Apr 14, 2010 at 17:45 To: michael.keith@... lightguard.jp@... Cc: jsr-317-feedback@... Mike, If you are working with versioned entities, then an application transaction that spans multiple JTA transactions can still ensure repeatable read semantics for any updated entities. So can other OCC schemes such as "compare all columns" at update time. If you use non-versioned entities, then I would agree that you may be asking for trouble. This technique of using version (timestamp) or all-columns verification for updates has been very successful and well-proven in PowerBuilder for many years now. Perhaps you could elaborate on the kinds of inconsistency that you are concerned about that could crop up for versioned entities? ---------- From: Mike Keith michael.keith@... Date: Wed, Apr 14, 2010 at 18:00 eireland@... lightguard.jp@... jsr-317-feedback@... The audit usecase was given, so let's take that as an example. Say the audit is being written to an entirely different database using JDBC, or even sent to a remote audit log system through JMS. At some intermediate phase the wizard causes some entitiy changes to an extended persistence context that is set to only flush MANUALly. In the same transaction the audit entry indicating the change to that entity gets sent off through a transactional JMS queue to the remote audit system. The JTA transaction commits (successfully) and the JMS message gets sent off. Further downstream of the wizard (after X more steps and Y more JTA transactions successfully committing) Joe user decides he doesn't really want to finish what he started so he hits the cancel button in the wizard, causing all of the changes in the persistence context to be discarded and not be flushed. The transactional no-no is that JTA transactions are being committed assuming that the entity changes have also been committed, but FlushMode.Maanual is delinquently ignoring the commit and assuming its own control over the persistent data, ignoring the real transaction scope in favour of its own aritificially created scope. Atomicity, consistency, durability -- all gone. ---------- From: Jason Porter lightguard.jp@... Date: Wed, Apr 14, 2010 at 19:05 To: " michael.keith@... michael.keith@... eireland@... jsr-317-feedback@... jsr-317-feedback@... I can understand your point Mike, though the idea (at least as I use it) of a MANUAL flush mode is my entity/entities being used in the wizard would never touch a persist until the full use case is complete. IOW there would be a bunch of POJO entities around in memory ready to be persisted, but wouldn't be persisted until told to. If I'm dealing with persisted entities that will be updated, then yes there is a risk of all the transaction goodness being thrown out the window, but like Evan was saying versioned entities, or compare all columns should take care of that. Might need to do a merge first, or similar operation to guarantee the data hasn't been changed since the initial find. The point I keep thinking about: is the spec and the impementation smarter than I am about when I want/need to persist to the database for my application? I would really question an answer of yes. Kind of like framework x saying it knows better than I do when to send something to a view layer. Sent from my iPhone ---------- From: Gavin King gavin.king@... Date: Wed, Apr 14, 2010 at 19:12 To: michael.keith@... eireland@... lightguard.jp@... jsr-317-feedback@... I don't see how a "proper application transaction mechanism" would solve the problem you just described, unless you're planning on re-engineering JTA and JDBC. JDBC will commit any SQL you send it directly either immediately (in autocommit mode), or when the JDBC connection is committed. So assuming that you're not going to hold the JDBC transaction open across a user interaction (which is the whole point of the usecase) then any "proper application transaction mechanism" you can come up with is going to suffer from the problem that inserts/updates/deletes executed directly against the JDBC connection are going to be immediately executed. That's JDBC. If you want some other behavior, you're going to have to wrap JDBC in some other technology that implements write-behind. Of course, the solution to the problem is to simply *not* execute inserts/updates/deletes directly against JDBC until the end of the application transaction. You queue them until the end of the application transaction. It turns out that this is very easy to do with JDBC because JDBC is a very simple technology that doesn't do too many things automagically. The problem is that today it is NOT easy to do using JPA (due to automatic dirty checking and automatic flushing), and that is why people want FlushMode.MANUAL. I don't have any problem with disguising FlushMode.MANUAL as a "proper application transaction mechanism", but let's not deceive ourselves that there's any deep underlying difference there. It's the same thing, just characterized slightly differently from the user point of view. (That is, unless you want to re-engineer JTA/JDBC and build a JPA-ish notion of write-behind into these technologies. Which I imagine is pretty much not going to happen.) -- Gavin King gavin.king@... http://in.relation.to/Bloggers/Gavin http://hibernate.org http://seamframework.org ---------- From: Gavin King gavin.king@... Date: Wed, Apr 14, 2010 at 19:16 lightguard.jp@... Cc: " michael.keith@... michael.keith@... eireland@... jsr-317-feedback@... jsr-317-feedback@... The thing is, a LOT of the time, JPA is smarter (or at least as smart) as you. But there simply are *enough( cases where the JPA "magic" (automatic dirty checking and automatic flushing) really do get in the way of people who know what they are trying to do, and today JPA offers no recourse for those people, other than "go off and work directly against JDBC". FlushMode.MANUAL would go a *long* way to giving you the extra control they need for the cases where JPA isn't smarter than you. ---------- From: Evan Ireland eireland@... Date: Wed, Apr 14, 2010 at 19:21 gavin.king@... michael.keith@... lightguard.jp@... jsr-317-feedback@... I concur with this. Assuming any JMS operations might need to be executed (in Mike's example) one would expect they would also be deferred until the flush occurs. It doesn't make sense to me that someone would defer an entity update until flush, but not defer a JMS message send that is driven by the deferred update. ---------- From: Gavin King gavin.king@... Date: Wed, Apr 14, 2010 at 19:25 eireland@... Cc: michael.keith@... lightguard.jp@... jsr-317-feedback@... Yeah, I wrote about JDBC, but I think the JMS case is even easier to see. The user is deliberately delaying their flush. I don't see why they would not also deliberately delay sending the message. And the same reasoning applies that a "proper application transaction mechanism" would not solve the problem unless you were able to build some kind of "delayed send" mechanism into JMS. ---------- From: Jason Porter lightguard.jp@... Date: Wed, Apr 14, 2010 at 19:31 gavin.king@... Cc: " michael.keith@... michael.keith@... eireland@... jsr-317-feedback@... jsr-317-feedback@... Sure, a lot of the automagic that happens in JPA (like you said dirty checking, cascading persist / update, etc) is smarter than what I'd come up with on my own, but like you also said, it doesn't fit all the time, there very well may be, and certainly are, cases where it is not.  As a user, I would like to be given the chair and enough rope to hang myself should I desire.  Of course I also understand we want to protect users from doing so most of the time, but reverting back to JDBC really shouldn't be the final answer, IMO. ---------- From: Mike Keith michael.keith@... Date: Wed, Apr 14, 2010 at 20:24 lightguard.jp@... eireland@... jsr-317-feedback@... jsr-317-feedback@... Although the optimistic locking problem is certainly accentuated it will not cause problems in the database or in the transaction consistency. The problem is that the persistence context is pretending to be participating in the JTA transaction, but is not really doing so. It is enlisted, and listening for synchronization, but unless someone happens to have manually flushed the data contained in the PC that data will not be written out or be committed as part of the tx that it was changed in. This violates the rules of the tx resource contract and can cause tx inconsistency. ---------- From: Mike Keith michael.keith@... Date: Wed, Apr 14, 2010 at 20:27 gavin.king@... eireland@... lightguard.jp@... jsr-317-feedback@... An application transaction would solve it because it would provide correct application transaction semantics. This is not a new concept, and has been talked about in the past. As I mentioned, it could involve some additional features in JTA (e.g. a nested transaction feature) as well as some additional support from the EJB layer, but I wouldn't call it re-engineering. Well, I know that you don't have any problem with disguising FlushMode.MANUAL as an application transaction since that is what you have been doing :-). I claim that there is a very real difference, though, and described why in my last email. It simply does not exhibit application transaction semantics in conjunction with the the rest of the transactional resources on the server. They just aren't playing the same transactional game. ---------- From: Mike Keith michael.keith@... Date: Wed, Apr 14, 2010 at 20:37 gavin.king@... eireland@... lightguard.jp@... jsr-317-feedback@... That is essentially proposing that people should not even use the existing JTA transaction in the EJB where they are making the changes to the entities. They should all be aware that the JTA transaction is fake and they should queue up all of their operations to additional transactional resources (like JMS) until some magic application signal occurs, at which time they must let it all out. That isn't an application transaction, it's just a description of how someone is supposed to only partly use the existing JTA transactions, but not really use them because if they do then they will be hosed ;-). Maybe it comes down to what one's view of an EntityManager/persistence context is. If someone feels that it is an application structure then it might make sense to control whether it gets sent to the DB. However, I think it is more of an integrated framework with the system, if you don't want to write something out then don't keep it in the transactional persistence context (that is advertised as being integrated with JTA) when the transaction commits. ---------- From: Evan Ireland eireland@... Date: Wed, Apr 14, 2010 at 20:44 To: michael.keith@... lightguard.jp@... Cc: jsr-317-feedback@... Mike, Not writing data does not cause tx inconsistency. A read-only transaction for example doesn't write data, and that doesn't cause tx inconsistency. If we were to allow FlushMode.MANUAL, we would also need to stipulate that if you delete or update (via flush) a versioned object in one transaction that was loaded in a previous transaction, that an appropriate version check is done by the flushing transaction. That version check (if done properly) will ensure repeatable read semantics, just as if the data had been loaded in the current (flushing) transaction. You might want to point out an actual anomaly that would be possible in this case rather than claiming inconsistency without supporting evidence. michael.keith@... ] ---------- From: Emmanuel Bernard ebernard@... Date: Thu, Apr 15, 2010 at 01:54 eireland@... gavin.king@... michael.keith@... lightguard.jp@... jsr-317-feedback@... Note that if we add a post flush event listener concept in JPA, we can indeed control when to release all these heterogeneous operations. ---------- From: Mike Keith michael.keith@... Date: Thu, Apr 15, 2010 at 06:36 eireland@... lightguard.jp@... jsr-317-feedback@... Evan, I'm not sure why you keep bringing up entity versioning issues. There are no problems with entity versioning, and even though keeping them cached for longer before writing them out will increase the likelihood of getting an optlock exception our version checking will still work. I am not talking about data inconsistency, but transactional inconsistency or integrity due to the atomicity property being violated. I gave evidence in my previous email when I described a JMS message that got written and commmitted, but the persistence context writes were suppressed so data modified in the same transaction did not get committed. ---------- From: Mike Keith michael.keith@... Date: Thu, Apr 15, 2010 at 07:18 lightguard.jp@... eireland@... jsr-317-feedback@... jsr-317-feedback@... I just thought of another potential solution to this problem. Currently we generally require that container-managed persistence contexts be synchronized with the transaction. We allow regular transactional PCs, or PCs of the extended variety, but if we added a new option for non-synchronized PCs then all of this stuff could occur outside of the JTA transactions and then when the time comes the joinTransaction call could be used (there are already cases when it is required of C-M PCs) to register the PC with the transaction. This might solve the problem for everybody so that the objects stay managed, multiple JTA transactions may occur and transactional integrity is not compromised in any way because the EM is essentially non-transactional while it is non-synchronized. One could achieve this by doing something like the following: @PersistenceContext(type=NON_ SYNCHRONIZED) Anyway, my point is that I believe there are better ways to solve this problem (that clearly needs to be solved) and I am happy to include it on our list if other members of the group are as well. ---------- From: Jason Porter lightguard.jp@... Date: Thu, Apr 15, 2010 at 07:53 To: " michael.keith@... michael.keith@... eireland@... jsr-317-feedback@... jsr-317-feedback@... That sounds like a reasonable solution. Sent from my iPhone ---------- From: Emmanuel Bernard ebernard@... Date: Thu, Apr 15, 2010 at 09:35 To: michael.keith@... lightguard.jp@... eireland@... jsr-317-feedback@... jsr-317-feedback@... If in the NON_SYNCHRONIZED semantic we also imply that flush must not occur then that could work. But from your wording it seems that only the Tx registerSync would be affected which unfortunately is not enough as the PC could decide to do early flushed and ruin the whole party. ---------- From: Gordon Yorke gordon.yorke@... Date: Thu, Apr 15, 2010 at 09:50 ebernard@... Cc: michael.keith@... lightguard.jp@... eireland@... jsr-317-feedback@... jsr-317-feedback@... This EntityManager would not be in a transaction so the current rules would apply (TransactionRequiredException). We would also want to follow the propagation rules of an Extended Persistence Context. --Gordon ---------- From: Mike Keith michael.keith@... Date: Fri, Apr 16, 2010 at 07:00 ebernard@... lightguard.jp@... eireland@... jsr-317-feedback@... jsr-317-feedback@... Yes, that was the point. A manual flush mode could make sense in this new kind of persistence context. ---------- From: Mike Keith michael.keith@... Date: Fri, Apr 16, 2010 at 07:16 gordon.yorke@... ebernard@... lightguard.jp@... eireland@... jsr-317-feedback@... jsr-317-feedback@... The TransactionRequiredException is conveniently defined to be thrown on a persistence context type of TRANSACTION, so operations like persist() will be fine on a NON_SYNCHRONIZED type. Agreed on the propagation. We can discuss the details, though, when the group starts up in earnest. --  Jason Porter http://lightguard-jp.blogspot.com http://twitter.com/lightguardjp Software Engineer Open Source Advocate Author of Seam Catch - Next Generation Java Exception Handling PGP key id: 926CCFF5 PGP key available at: keyserver.net , pgp.mit.edu</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Fwd: Features for the next JPA</header>
    <date>Tue Jan 24 20:45:42 CET 2012</date>
    <body>Hi Jason, Thanks for the feedback.  With regard to the request for a manual flushmode, please see the new functionality we have added in the JPA 2.1 Early Draft for unsynchronized persistence contexts.  This allows the application to control the synchronization of the persistence context with the JTA transaction. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] converters</header>
    <date>Mon Feb 06 23:30:27 CET 2012</date>
    <body>Here's a more formal writeup of the converter proposal, which factors in results of the recent discussion. In conjunction with the consensus around extending the original proposal to "autoapply" converters, I've also extended the syntax to allow @Convert to be specified more broadly as well as to allow for disabling autoapply converters. Mike and I have already gone through the details here several times. Please review and post to the group if you think we should make any changes or if there are any corrections. I have left as an open issue whether we should also add a capability to add converters dynamically or whether we should defer this pending further feedback from developers. -Linda --------------------------- Converters may be specified to provide conversion between the entity attribute representation and the database representation for attributes of basic types.  Converters may be used to convert attributes defined by entity classes, mapped superclasses, or embeddable classes. The conversion of all basic types are supported except for the following: Id attributes, version attributes, relationship attributes, and attributes explicitly annotated (or designated via XML) as Enumerated or Temporal.  Auto-apply converters will not be applied to such attributes, and applications that apply converters to such attributes through use of the Convert annotation will not be portable. The persistence provider runtime is responsible for invoking the corresponding conversion method when loading the entity attribute from the database and before storing the entity attribute state to the database.  Instances of attribute values used within JPQL or criteria queries (such as in comparisons, bulk updates, etc.) must be converted to their database types by the provider before being sent to the database for the query execution.  If the result of a JPQL or criteria query includes one or more specific entity attributes that have been designated for conversion, then the results must be converted to their entity attribute representation before being returned. An attribute converter must implement the  javax.persistence.mapping.AttributeConverter interface. /**  * A class that implements this interface can be used to convert entity  * attribute state into database column representation and back again.  * Note that the X and Y types may be the same Java type.  *  * @param X  the type of the entity attribute  * @param Y  the type of the database column  */     /**      * Converts the value stored in the entity attribute into the data      * representation to be stored in the database.      *      * @param attribute  the entity attribute value to be converted      * @return  the converted data to be stored in the database column      */     /**      * Converts the data stored in the database column into the value      * to be stored in the entity attribute.      * Note that it is the responsibility of the converter writer to      * specify the correct dbData type for the corresponding column for      * use by the JDBC driver, i.e., persistence providers are not expected      * to do such type conversion.      *      * @param dbData  the data from the database column to be converted      * @return  the converted value to be stored in the entity attribute      */ A converter class must be annotated with the Converter annotation or defined in the object/relational mapping descriptor as a converter. /**  *  Specifies that the annotated class is a converter and defines its scope  */ @Retention(RUNTIME)    /**     * If set to true, specifies that the converter will automatically     * be applied to all mapped attributes of the specified     * target type pair for the entities in the persistence unit,     * unless overridden by means of the Convert annotation (or XML  equivalent).     * In determining whether a converter is applicable to an attribute,     * the provider must treat primitive types and wrapper types as equivalent.     *     * Note that Id attributes, version attributes, relationship     * attributes, and attributes explicitly annotated as Enumerated     * or Temporal (or designated as such via XML) will not be converted.     *     * If autoApply is false, only those attributes of the target type pair     * for which the Convert annotation (or corresponding XML element) has     * been specified will be converted.     *     * The behavior is undefined if there are two converters defined for     * the same target types and the Convert annotation is not used     * to explicitly specify use of a converter.     *     * Note that if autoApply is true, the Convert annotation may be used to     * override or disable auto-apply conversion on a per-attribute basis.     */ Type conversion may be specified at the level of individual attributes by means of the Convert annotation.  The Convert annotation may also be used to override or disable an auto-apply conversion. The Convert annotation may be applied directly to an attribute of an entity, mapped superclass, or embeddable class to specify conversion of the attribute or to override the use of a converter that has been specified as autoApply=true.  When persistent properties are used, the Convert annotation is applied to the getter method. The Convert annotation may be applied to an entity that extends a mapped superclass to specify or override the conversion mapping for an inherited basic or embedded attribute. @Retention(RUNTIME)   /**    * Specifies the converter to be applied.  A value for this    * element must be specified if multiple converters would    * otherwise apply.    */   /**    * The attributeName must be specified unless the Convert annotation    * is on an attribute of basic type or on an element collection of basic    * type.  In these cases, attributeName must not be specified.    */   /**    * Used to disable an auto-apply or inherited converter.    * If disableConversion is true, the converter element should    * not be specified.    */ /**  * Used to group Convert annotations  */ @Retention(RUNTIME) The Convert annotation is used to specify the conversion of a Basic (whether explicit or default) field or property.  The Convert annotation should not be used to specify conversion of the following: Id attributes, version attributes, relationship attributes, and attributes explicitly annotated (or designated via XML) as Enumerated or Temporal. Applications that specify such conversions will not be portable. The Convert annotation may be applied on a basic attribute or an element collection of basic type (in which case the converter is applied to the elements of the collection).  In these cases, the attributeName element must not be specified. Examples: @Converter public class BooleanToIntegerConverter implements AttributeConverter&amp;lt;Boolean,  @Converter(autoApply=true) public class EmployeeDateConverter implements  @Entity    @Convert(BooleanToIntegerConverter.class)    ...     // EmployeeDateConverter is applied automatically // Apply a converter to an element collection of basic type @ElementCollection @Convert(NameConverter.class)  // applies to each element in the collection // Apply a converter to an element collection that is a map of basic values // The converter is applied to the map *value* @ElementCollection @Convert(EmployeeNameConverter.class) When the Convert annotation is applied to a map to specify conversion of a map key of basic type, "key" must be used to specify that it is the map key that is to be converted. // Apply a converter to a Map key of basic type (relationship) @OneToMany @Convert(converter=ResponsibilityCodeConverter.class, attributeName="key") // Apply a converter to a Map key of basic type (element collection) @ElementCollection @Convert(converter=ResponsibilityCodeConverter.class, attributeName="key") // Disable conversion in the presence of an autoApply converter @Convert(disableConversion=true) The Convert annotation may be applied on an embedded attribute or on a map collection whose key or value is of embeddable type (in which case the converter is applied to the specified attribute of the embeddables contained in the collection).  In these cases the attributeName element must be specified. To override conversion mappings at multiple levels of embedding, a dot (".")  notation form must be used in the attributeName element to indicate an attribute within an embedded attribute.  The value of each identifier used with the dot notation is the name of the respective embedded field or property. When the Convert annotation is applied to a map containing embeddables, the attributeName element must be specified, and "key." or "value." must be used to prefix the name of the attribute that is to be converted in order to specify it as part of the map key or map value. // Apply a converter to an embeddable attribute @Embedded @Convert(converter=CountryConverter.class, attributeName="country") // Apply a converter to a nested embeddable attribute: @Embedded @Convert(converter=CityConverter.class, attributeName="region.city")   ...   // Apply a converter to a nested attribute of an embeddable that is   // a map key of an element collection   @Convert(name="key.region.city", converter=CityConverter.class)   @ElementCollection @OneToMany // Apply to an embeddable that is a map key for a relationship @Convert(attributeName="key.type",  converter=ResponsibilityTypeConverter.class) The Convert annotation may be applied to an entity class that extends a mapped superclass to specify or override a conversion mapping for an inherited basic or embedded attribute. // Override conversion mappings for attributes inherited from a mapped  superclass @Entity   @Convert(attributeName="startDate", converter=DateConverter.class), Open Issue: Do we need an EntityManagerFactory method to dynamically add converters?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converters</header>
    <date>Wed Feb 08 18:07:38 CET 2012</date>
    <body>One thing that we might still consider adding is a comment about schema generation. The provider should use the database type from the converter for an attribute that is slated for conversion. If a @Column.columnDefinition is specified it should take precedence, though. -Mike</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converters</header>
    <date>Wed Feb 08 18:13:47 CET 2012</date>
    <body>On second thought, we might need to be a little more discriminating about autoApply and schema generation. Perhaps the autoApply would need to be defined in XML if the converter does type changing? Seems unkind to require that the provider rifle through all of the classes looking for every converter before it can process any entity. What do others think?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converters</header>
    <date>Wed Feb 08 19:08:17 CET 2012</date>
    <body>Am 08.02.2012 18:13, schrieb michael keith:  On second thought, we might need to be a little more discriminating  about autoApply and schema generation.  Perhaps the autoApply would need to be defined in XML if the converter  does type changing? Seems unkind to require that the provider rifle  through all of the classes looking for every converter before it can  process any entity. What do others think?   JSF has to scan an archive too,because of the @FacesConverter annotation. CDI has to scan an archive too, because all beans are injectable anywhere. But: this has to be done only one time, at application start. I think this is justifiable. Probably, it should be done only for archives with a persistence.xml, like beans.xml for CDI. Otherwise additional jars can break an application. Should this be added to the spec? regards Bernd</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converters</header>
    <date>Fri Feb 10 02:57:20 CET 2012</date>
    <body>Some comments inline: --Gordon I understand the goal of this wording but this will be a barrier to us  supporting such conversion in the future as any support would not be backward  compatible.   It should be suffecient to just have the list of supported and  unsupported. "target type pair" is not well defined and implies the database column type is used to  evaluate applicability of converters but it is only the attribute which is (and should be)  considered.  "target type" would be better. Overriding conversion through the Convert annotation adds complexity and duplicates an overriding  facility that is already present in the Specification.  It would be far clearer to add a  "converter" attribute to the AttributeOverride annotation.  There are already well  defined rules for how AttributeOverrides are applied and these rules are applicable to the  converter override as well.  If the type needs to be overridden it is likely that an  AttributeOverride will already be specified.   If the override rules remain in the Convert  annotations users may consider the "attribute" path is required for every  MappedSuperclass or Embeddable. If we use the AttributeOverride this attribute could be renamed "key".  This  would eliminate the empty string and the requirement not to specify the attribute name. This annotation should no longer be needed if we place convert in  AttributeOverrides Alternatively : @Convert(converter=ResponsibilityCodeConverter.class,  key=true) Or: @AttributeOverride(name="country",  convert=@Convert(converter=CountryConverter.class)) This would require changing "column" to be no longer required but I see not  issue with that. This is where a customization event during EMF initialization would be very  helpful.  Adding converters dynamically at runtime would have unpredictable  behaviour.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converters</header>
    <date>Fri Feb 10 06:06:34 CET 2012</date>
    <body>Hi Gordon, Thanks for the comments.  More below.... If others could provide feedback on these issues, it would be helpful. -Linda I think we have to say something in terms of whether these attributes will be  converted or not. In specifying it this way,  I had figured that the use of the Converter  annotation would give us the flexibility to support such conversion in the future if we were to augment it with another element (e.g., "extended") to indicate that conversion should be extended to such attributes. I had assumed that this level of specificity/flexibility would be needed in  determining whether a converter was applicable or not. I'd like to get further feedback on this point. Yes.  Mike and I discussed this approach as well, and I agree with your  points, but I'm not convinced. One of the things in the back of my mind is the issue of how we might grow  this facility, e.g., to extend to multiple attributes, which was one of the reasons I  thought it best to decouple from AttributeOverride.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converters</header>
    <date>Fri Feb 10 15:07:22 CET 2012</date>
    <body>Hello Linda,   Perhaps a call about this would help. --Gordon By saying Enumerated or Temporal is not supported it is clear that Converters  of those types would not be expected to be applied globally as they are not  supported types.   Without this change autoApply can not be extended in the  future. This becomes ambiguous when DDL is involved.  If you have multiple Converts  defined with the same annotation type which type is created on the database?   Having multiple converters for a particular attribute is advanced flexibility  that is not required for this round and would be easy for providers to add if  there was a real need. Extend what to multiple attributes?  Multiple attribute overrides are already  specified. I will withdraw this comment, Converts may still be required in the case of  Map ElementCollections.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converters</header>
    <date>Fri Feb 10 20:24:03 CET 2012</date>
    <body>First off thanks for the very nice work by Mike and Linda, many of my initial  concerns have been addressed in the formal writeup. We might want to be explicit in the spec that it's a first step to address  80% of the use cases and not a complete replacement of what specific vendors  propose in this area. I fear some backslash otherwise esp for fairly common  uses cases like: - one java type to multiple columns - use of database specific types / operation or JDBC specific code upon  read/write of a type. More in line. Emmanuel  Here's a more formal writeup of the converter proposal, which factors  in results of the recent discussion.    In conjunction with the consensus around extending the original  proposal to "autoapply" converters, I've also extended the syntax  to allow @Convert to be specified more broadly as well as to allow  for disabling autoapply converters.    Mike and I have already gone through the details here several times.  Please review and post to the group if you think we should make any  changes or if there are any corrections.    I have left as an open issue whether we should also add a capability  to add converters dynamically or whether we should defer this pending  further feedback from developers.    -Linda    ---------------------------    Converters may be specified to provide conversion between the entity  attribute representation  and the database representation for  attributes of basic types.  Converters may be used to convert  attributes defined by entity classes, mapped superclasses, or  embeddable classes.    The conversion of all basic types are supported except for the  following: Id attributes, version attributes, relationship attributes,  and attributes explicitly annotated (or designated via XML) as  Enumerated or Temporal.   Why not allowing @Id and @Version attributes provided the converted type is  within the acceptable list of types for id and version?  Auto-apply converters will not be applied to  such attributes, and applications that apply converters to such  attributes through use of the Convert annotation will not be portable. Have you considered `implicit` instead of  `auto-apply`. I feel it flows  nicely with the word converter.    The persistence provider runtime is responsible for invoking the  corresponding conversion method when loading the entity attribute from  the database and before storing the entity attribute state to the  database.  Instances of attribute values used within JPQL or criteria  queries (such as in comparisons, bulk updates, etc.) must be converted  to their database types by the provider before being sent to the  database for the query execution.  If the result of a JPQL or criteria  query includes one or more specific entity attributes that have been  designated for conversion, then the results must be converted to their  entity attribute representation before being returned.    An attribute converter must implement the   javax.persistence.mapping.AttributeConverter interface. +1 on removing Entity from the name    /**  * A class that implements this interface can be used to convert entity  * attribute state into database column representation and back again.  * Note that the X and Y types may be the same Java type.  *  * @param X  the type of the entity attribute  * @param Y  the type of the database column  */      /**     * Converts the value stored in the entity attribute into the data     * representation to be stored in the database.     *     * @param attribute  the entity attribute value to be converted     * @return  the converted data to be stored in the database column     */ An alternative and more neutral name could be: - convertForDatabase - convertFromDatabase or read / write.      /**     * Converts the data stored in the database column into the value     * to be stored in the entity attribute.     * Note that it is the responsibility of the converter writer to     * specify the correct dbData type for the corresponding column for     * use by the JDBC driver, i.e., persistence providers are not expected     * to do such type conversion.     *     * @param dbData  the data from the database column to be converted     * @return  the converted value to be stored in the entity attribute     */      A converter class must be annotated with the Converter annotation or  defined in the object/relational mapping descriptor as a converter.    /**  *  Specifies that the annotated class is a converter and defines its scope  */  @Retention(RUNTIME)     /**    * If set to true, specifies that the converter will automatically    * be applied to all mapped attributes of the specified    * target type pair for the entities in the persistence unit, I agree with Gordon, I was expecting X to be the sole element taken into  account. What about attributes that have a subtype of X? If we allow subtypes, which  rules do we have for "specialization"? Do we reuse the overloading rules of the JVM? What about potential conflicts,  is that an exception at deployment time?    * unless overridden by means of the Convert annotation (or XML   equivalent).    * In determining whether a converter is applicable to an attribute,    * the provider must treat primitive types and wrapper types as equivalent.    *    * Note that Id attributes, version attributes, relationship    * attributes, and attributes explicitly annotated as Enumerated    * or Temporal (or designated as such via XML) will not be converted.    *    * If autoApply is false, only those attributes of the target type pair    * for which the Convert annotation (or corresponding XML element) has    * been specified will be converted.    *    * The behavior is undefined if there are two converters defined for    * the same target types and the Convert annotation is not used    * to explicitly specify use of a converter. I'd rather have an exception here. Any reason to leave it undefined?    *    * Note that if autoApply is true, the Convert annotation may be used to    * override or disable auto-apply conversion on a per-attribute basis.    */      Type conversion may be specified at the level of individual attributes  by means of the Convert annotation.  The Convert annotation may also be  used to override or disable an auto-apply conversion.    The Convert annotation may be applied directly to an attribute of an  entity, mapped superclass, or embeddable class to specify conversion of  the attribute or to override the use of a converter that has been  specified as autoApply=true.  When persistent properties are used, the   Convert  annotation is applied to the getter method.    The Convert annotation may be applied to an entity that extends a mapped  superclass to specify or override the conversion mapping for an inherited  basic or embedded attribute.      @Retention(RUNTIME)    /**   * Specifies the converter to be applied.  A value for this   * element must be specified if multiple converters would   * otherwise apply.   */    /**   * The attributeName must be specified unless the Convert annotation   * is on an attribute of basic type or on an element collection of basic   * type.  In these cases, attributeName must not be specified.   */    /**   * Used to disable an auto-apply or inherited converter.   * If disableConversion is true, the converter element should   * not be specified.   */    /**  * Used to group Convert annotations  */  @Retention(RUNTIME)   The plural is unfortunate here :( In Bean validation we use an inner annotation @Convert.List but that's not in  the spirit of this spec.      The Convert annotation is used to specify the conversion of a Basic  (whether explicit or default) field or property.  The Convert annotation  should not be used to specify conversion of the following:  Id attributes, version attributes, relationship attributes, and attributes  explicitly annotated (or designated via XML) as Enumerated or Temporal.  Applications that specify such conversions will not be portable.      The Convert annotation may be applied on a basic attribute or  an element collection of basic type (in which case the converter  is applied to the elements of the collection).  In these cases, the  attributeName element must not be specified. So that excludes the possibility to convert a collection object into a flat  representation in String. do we want to cover this use case? I'm tempted to say that's a fair use case. A way around that would be to make use of the attributeName to decide whether  or not the collection itself or its elements are targeted. That has the  benefit of rendering @Convert(attributeName="key") symmetric with  @Convert(attributeName="value") (example below).    Examples:    @Converter  public class BooleanToIntegerConverter implements     @Converter(autoApply=true)  public class EmployeeDateConverter implements     @Entity     @Convert(BooleanToIntegerConverter.class)   ...    // EmployeeDateConverter is applied automatically      // Apply a converter to an element collection of basic type  @ElementCollection  @Convert(NameConverter.class)  // applies to each element in the collection      // Apply a converter to an element collection that is a map of basic values  // The converter is applied to the map *value*  @ElementCollection  @Convert(EmployeeNameConverter.class)    When the Convert annotation is applied to a map to specify conversion  of a map key of basic type, "key" must be used to specify that it  is the map key that is to be converted.    // Apply a converter to a Map key of basic type (relationship)  @OneToMany  @Convert(converter=ResponsibilityCodeConverter.class, attributeName="key")    // Apply a converter to a Map key of basic type (element collection)  @ElementCollection  @Convert(converter=ResponsibilityCodeConverter.class, attributeName="key")      // Disable conversion in the presence of an autoApply converter  @Convert(disableConversion=true)      The Convert annotation may be applied on an embedded attribute or on a  map collection whose key or value is of embeddable type (in which case  the converter is applied to the specified attribute of the embeddables  contained in the collection).  In these cases the attributeName  element must be specified.    To override conversion mappings at multiple levels of embedding, a dot  (".")  notation form must be used in the attributeName element to  indicate an attribute within an embedded attribute.  The value of each  identifier used with the dot notation is the name of the respective  embedded field or property.    When the Convert annotation is applied to a map containing  embeddables, the attributeName element must be specified, and "key."  or "value." must be used to prefix the name of the attribute that is  to be converted in order to specify it as part of the map key or map  value.    // Apply a converter to an embeddable attribute  @Embedded  @Convert(converter=CountryConverter.class, attributeName="country")      // Apply a converter to a nested embeddable attribute:  @Embedded  @Convert(converter=CityConverter.class, attributeName="region.city")    ...  // Apply a converter to a nested attribute of an embeddable that is  // a map key of an element collection  @Convert(name="key.region.city", converter=CityConverter.class)  @ElementCollection    @OneToMany  // Apply to an embeddable that is a map key for a relationship  @Convert(attributeName="key.type",   converter=ResponsibilityTypeConverter.class)      The Convert annotation may be applied to an entity class that extends  a mapped superclass to specify or override a conversion mapping  for an inherited basic or embedded attribute.    // Override conversion mappings for attributes inherited from a mapped   superclass  @Entity  @Convert(attributeName="startDate", converter=DateConverter.class),        Open Issue:    Do we need an EntityManagerFactory method to dynamically add converters? Does it mean I can also define an attribute as @Convert? What happens to  auto-apply converters, would that reconsider existing entities? Besides all these comments, I have a few additional proposals for the group: - do we want converters to accept parameters to help people make them more  generic. Using the boolean to string example, one could feed the converter  with the string values it expect as true and false (y/N, 0/1 etc). - we should support the listing of converters in persistence.xml (just like  classes) in case people chose to disable scanning.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converters</header>
    <date>Fri Feb 10 20:57:28 CET 2012</date>
    <body>Hello all, Just a couple comments: This case is covered through the @Basic annotation.  Marking the collection attribute as @Basic and @Convert would allow the collection to be converted to a string. Yes. First off thanks for the very nice work by Mike and Linda, many of my initial concerns have been addressed in the formal writeup. We might want to be explicit in the spec that it's a first step to address 80% of the use cases and not a complete replacement of what specific vendors propose in this area. I fear some backslash otherwise esp for fairly common uses cases like: - one java type to multiple columns - use of database specific types / operation or JDBC specific code upon read/write of a type. More in line. Emmanuel Why not allowing @Id and @Version attributes provided the converted type is within the acceptable list of types for id and version? Have you considered `implicit` instead of  `auto-apply`. I feel it flows nicely with the word converter. +1 on removing Entity from the name An alternative and more neutral name could be: - convertForDatabase - convertFromDatabase or read / write. I agree with Gordon, I was expecting X to be the sole element taken into account. What about attributes that have a subtype of X? If we allow subtypes, which rules do we have for "specialization"? Do we reuse the overloading rules of the JVM? What about potential conflicts, is that an exception at deployment time? I'd rather have an exception here. Any reason to leave it undefined? The plural is unfortunate here :( In Bean validation we use an inner annotation @Convert.List but that's not in the spirit of this spec. So that excludes the possibility to convert a collection object into a flat representation in String. do we want to cover this use case? I'm tempted to say that's a fair use case. A way around that would be to make use of the attributeName to decide whether or not the collection itself or its elements are targeted. That has the benefit of rendering @Convert(attributeName="key") symmetric with @Convert(attributeName="value") (example below). Does it mean I can also define an attribute as @Convert? What happens to auto-apply converters, would that reconsider existing entities? Besides all these comments, I have a few additional proposals for the group: - do we want converters to accept parameters to help people make them more generic. Using the boolean to string example, one could feed the converter with the string values it expect as true and false (y/N, 0/1 etc). - we should support the listing of converters in persistence.xml (just like classes) in case people chose to disable scanning.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: criteria API bulk update/delete</header>
    <date>Fri Feb 10 22:46:33 CET 2012</date>
    <body>Please see below... To return to this specific point.  We took this path back in April (and it is  reflected in the current javadocs).  However, while this worked when I tested it back then  (jdk 1.6.0_25), it now results in compiler errors.  The obvious fix is to move the from()  methods in CommonAbstractQuery back down into AbstractQuery.   At this point then, I think that  CommonAbstractQuery serves little purpose, and my plan is to refactor to remove it.  There should  otherwise be no user-visible changes. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converters</header>
    <date>Fri Feb 10 23:24:50 CET 2012</date>
    <body>Yes, there are hacky ways to do these kinds of things using the simple converters below, but proper support cwould be introduced by future converter types if enough people ask for it. I prefer autoApply since converters are "applied" to attributes, and implicit doesn't really describe much to me, but names are so subjective... I am open to new names for these methods, but having "Database" in both method names is less clear, I think, and not as easily parsed. Reading and writing is ambiguous because on one hand you are reading from one side and writing to the other, and vice versa in the other direction. Agreed. Yes, I have flip-flopped on the specialization question. I can think of a case or two where it would be nice for a converter to apply to all attributes of all subclasses of the target type, but I can also see cases where it would be a pain if that were true. Some obvious options, here, are: 1) Restrict it to the exact type 2) Apply it to all subclasses 3) Apply it to all assignable types If we decide it is worthwhile either now or at some point in the future to support more than one of 1), 2) and 3) then some ideas are to: a) Make autoApply an enum type, with values of TargetClassOnly, ... (whatever options we think are worthwhile) b) Make autoApply an internal annotation with separate elements that can be filled in with additional options/rules now or later c) Add an additional element to @Convert (yuck) to add more information (e.g. the set of classes to apply it to if autoApply is true) Just in case providers wanted to adopt their own rules for converter precedence. I have no problem with an exception, here, though. I expect most providers would throw one, anyway. Yes, unfortunately. Yes, Gordon's suggestion seems to be a simple way to achieve that. No, an API to apply mappings is not on the table right now (much as I would like it to be, there just isn't enough time). There are so many different ways to generalize converters that I hesitate to go down that path, but having these be standardized leaves the door open to people creating standard converter routines/libraries that would be portable across all providers and applications. Absolutely. For every annotation yin there must be an XML yang :-)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: criteria API bulk update/delete</header>
    <date>Sat Feb 11 01:35:15 CET 2012</date>
    <body>I've uploaded updated javadocs to the project Downloads area. Please let me know if you find anything amiss. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] new expert group member</header>
    <date>Sat Feb 11 02:12:45 CET 2012</date>
    <body>I'd like to welcome Oliver Gierke as a new member of the JPA 2.1 expert group. Oliver joins us as the representative from VMware. Please welcome him to the group! thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new expert group member</header>
    <date>Sat Feb 11 11:11:26 CET 2012</date>
    <body>welcome Oliver Bernd Am 11.02.2012 02:12, schrieb Linda DeMichiel:  I'd like to welcome Oliver Gierke as a new member of the JPA 2.1 expert  group.  Oliver joins us as the representative from VMware.    Please welcome him to the group!    thanks,    -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] new expert group member</header>
    <date>Sat Feb 11 21:12:02 CET 2012</date>
    <body>Nicolas Seyvet joins us as the expert group representative from Ericsson. Please welcome him to the group! thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new expert group member</header>
    <date>Sun Feb 12 15:05:55 CET 2012</date>
    <body>Welcome Nicolas Am 11.02.2012 21:12, schrieb Linda DeMichiel:  Nicolas Seyvet joins us as the expert group representative from Ericsson.    Please welcome him to the group!    thanks,    -Linda Bernd</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new expert group member</header>
    <date>Sun Feb 12 15:46:05 CET 2012</date>
    <body>Welcome to the group! Am 12.02.2012 um 15:05 schrieb Bernd Müller:  Welcome Nicolas    Am 11.02.2012 21:12, schrieb Linda DeMichiel:      Bernd</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converters</header>
    <date>Mon Feb 13 13:58:53 CET 2012</date>
    <body>     Yes, I have flip-flopped on the specialization question. I can think of a   case or two where it would be nice for a converter to apply to all   attributes of all subclasses of the target type, but I can also see cases   where it would be a pain if that were true. Some obvious options, here, are:    1) Restrict it to the exact type  2) Apply it to all subclasses  3) Apply it to all assignable types    If we decide it is worthwhile either now or at some point in the future to   support more than one of 1), 2) and 3) then some ideas are to:    a) Make autoApply an enum type, with values of TargetClassOnly, ...   (whatever options we think are worthwhile)  b) Make autoApply an internal annotation with separate elements that can be   filled in with additional options/rules now or later  c) Add an additional element to @Convert (yuck) to add more information   (e.g. the set of classes to apply it to if autoApply is true) I like option a.      Just in case providers wanted to adopt their own rules for converter   precedence.  I have no problem with an exception, here, though. I expect most providers   would throw one, anyway. Can any implementation that would like custom rules here speak up? I'll try  and think about it on my side. We could also make it an open question on the  first draft to gather feedback.        Yes, Gordon's suggestion seems to be a simple way to achieve that. Yes, I had not realized that option. </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converters</header>
    <date>Mon Feb 13 21:01:34 CET 2012</date>
    <body>1. Using conversion to change mapping cardinality such as a @ElementCollection List&amp;lt;String&amp;gt; to a encoded single String is possible, but such facility/technique seems to encroach upon responsibilities of mapping primitives dictating the database schema. It may also impact how attribute) can be executed. 2. how the converted attributes are used in query context (in expressions, as parameter values) can be clarified further. The strict rule that they are only be used as their original declared type will be simpler at the cost of prohibiting some useful facility. For example, if a field declared as java.net.URL is converted to a java.lang.String, then a query that uses LIKE operator or orders the result on the (stringfied) URLs can be useful at the expense of deviating from such a strict rule. Regards -- Pinaki Poddar To:     jsr338-experts@...,             jsr338-experts@... Date:   02/10/2012 11:58 AM Subject:        [jsr338-experts] Re: converters Hello all, Just a couple comments: So that excludes the possibility to convert a collection object into a flat representation in String. do we want to cover this use case? I'm tempted to say that's a fair use case. This case is covered through the @Basic annotation. Marking the collection attribute as @Basic and @Convert would allow the collection to be converted to a string.  - we should support the listing of converters in persistence.xml (just like classes) in case people chose to disable scanning. Yes.   First off thanks for the very nice work by Mike and Linda, many of my   initial concerns have been addressed in the formal writeup.   We might want to be explicit in the spec that it's a first step to   address 80% of the use cases and not a complete replacement of what   specific vendors propose in this area. I fear some backslash otherwise   esp for fairly common uses cases like:   - one java type to multiple columns   - use of database specific types / operation or JDBC specific code upon   read/write of a type.   More in line.   Emmanuel   &amp;gt; Here's a more formal writeup of the converter proposal, which factors   &amp;gt; in results of the recent discussion.   &amp;gt; In conjunction with the consensus around extending the original   &amp;gt; proposal to "autoapply" converters, I've also extended the syntax   &amp;gt; to allow @Convert to be specified more broadly as well as to allow   &amp;gt; for disabling autoapply converters.   &amp;gt; Mike and I have already gone through the details here several times.   &amp;gt; Please review and post to the group if you think we should make any   &amp;gt; changes or if there are any corrections.   &amp;gt; I have left as an open issue whether we should also add a capability   &amp;gt; to add converters dynamically or whether we should defer this pending   &amp;gt; further feedback from developers.   &amp;gt; -Linda   &amp;gt; ---------------------------   &amp;gt; Converters may be specified to provide conversion between the entity   &amp;gt; attribute representation   &amp;gt; and the database representation for   &amp;gt; attributes of basic types.  Converters may be used to convert   &amp;gt; attributes defined by entity classes, mapped superclasses, or   &amp;gt; embeddable classes.   &amp;gt; The conversion of all basic types are supported except for the   &amp;gt; following: Id attributes, version attributes, relationship attributes,   &amp;gt; and attributes explicitly annotated (or designated via XML) as   &amp;gt; Enumerated or Temporal.   Why not allowing @Id and @Version attributes provided the converted type   is within the acceptable list of types for id and version?   &amp;gt; Auto-apply converters will not be applied to   &amp;gt; such attributes, and applications that apply converters to such   &amp;gt; attributes through use of the Convert annotation will not be portable.   Have you considered `implicit` instead of  `auto-apply`. I feel it flows   nicely with the word converter.   &amp;gt; The persistence provider runtime is responsible for invoking the   &amp;gt; corresponding conversion method when loading the entity attribute from   &amp;gt; the database and before storing the entity attribute state to the   &amp;gt; database.  Instances of attribute values used within JPQL or criteria   &amp;gt; queries (such as in comparisons, bulk updates, etc.) must be converted   &amp;gt; to their database types by the provider before being sent to the   &amp;gt; database for the query execution.  If the result of a JPQL or criteria   &amp;gt; query includes one or more specific entity attributes that have been   &amp;gt; designated for conversion, then the results must be converted to their   &amp;gt; entity attribute representation before being returned.   &amp;gt; An attribute converter must implement the   javax.persistence.mapping.AttributeConverter interface.   +1 on removing Entity from the name   &amp;gt; /**   &amp;gt; * A class that implements this interface can be used to convert entity   &amp;gt; * attribute state into database column representation and back again.   &amp;gt; * Note that the X and Y types may be the same Java type.   &amp;gt; *   &amp;gt; * @param X  the type of the entity attribute   &amp;gt; * @param Y  the type of the database column   &amp;gt; */   &amp;gt;   /**   &amp;gt;    * Converts the value stored in the entity attribute into the data   &amp;gt;    * representation to be stored in the database.   &amp;gt;    *   &amp;gt;    * @param attribute  the entity attribute value to be converted   &amp;gt;    * @return  the converted data to be stored in the database column   &amp;gt;    */   An alternative and more neutral name could be:   - convertForDatabase   - convertFromDatabase   or read / write.   &amp;gt;   /**   &amp;gt;    * Converts the data stored in the database column into the value   &amp;gt;    * to be stored in the entity attribute.   &amp;gt;    * Note that it is the responsibility of the converter writer to   &amp;gt;    * specify the correct dbData type for the corresponding column for   &amp;gt;    * use by the JDBC driver, i.e., persistence providers are not   expected   &amp;gt;    * to do such type conversion.   &amp;gt;    *   &amp;gt;    * @param dbData  the data from the database column to be converted   &amp;gt;    * @return  the converted value to be stored in the entity attribute   &amp;gt;    */   &amp;gt; A converter class must be annotated with the Converter annotation or   &amp;gt; defined in the object/relational mapping descriptor as a converter.   &amp;gt; /**   &amp;gt; *  Specifies that the annotated class is a converter and defines its   scope   &amp;gt; */   &amp;gt; @Retention(RUNTIME)   &amp;gt;  /**   &amp;gt;   * If set to true, specifies that the converter will automatically   &amp;gt;   * be applied to all mapped attributes of the specified   &amp;gt;   * target type pair for the entities in the persistence unit,   I agree with Gordon, I was expecting X to be the sole element taken into   account.   What about attributes that have a subtype of X? If we allow subtypes,   which rules do we have for "specialization"?   Do we reuse the overloading rules of the JVM? What about potential   conflicts, is that an exception at deployment time?   &amp;gt;   * unless overridden by means of the Convert annotation (or XML   equivalent).   &amp;gt;   * In determining whether a converter is applicable to an attribute,   &amp;gt;   * the provider must treat primitive types and wrapper types as   equivalent.   &amp;gt;   *   &amp;gt;   * Note that Id attributes, version attributes, relationship   &amp;gt;   * attributes, and attributes explicitly annotated as Enumerated   &amp;gt;   * or Temporal (or designated as such via XML) will not be converted.   &amp;gt;   *   &amp;gt;   * If autoApply is false, only those attributes of the target type   pair   &amp;gt;   * for which the Convert annotation (or corresponding XML element) has   &amp;gt;   * been specified will be converted.   &amp;gt;   *   &amp;gt;   * The behavior is undefined if there are two converters defined for   &amp;gt;   * the same target types and the Convert annotation is not used   &amp;gt;   * to explicitly specify use of a converter.   I'd rather have an exception here. Any reason to leave it undefined?   &amp;gt;   *   &amp;gt;   * Note that if autoApply is true, the Convert annotation may be used   to   &amp;gt;   * override or disable auto-apply conversion on a per-attribute basis.   &amp;gt;   */   &amp;gt; Type conversion may be specified at the level of individual attributes   &amp;gt; by means of the Convert annotation.  The Convert annotation may also be   &amp;gt; used to override or disable an auto-apply conversion.   &amp;gt; The Convert annotation may be applied directly to an attribute of an   &amp;gt; entity, mapped superclass, or embeddable class to specify conversion of   &amp;gt; the attribute or to override the use of a converter that has been   &amp;gt; specified as autoApply=true.  When persistent properties are used, the   Convert   &amp;gt; annotation is applied to the getter method.   &amp;gt; The Convert annotation may be applied to an entity that extends a   mapped   &amp;gt; superclass to specify or override the conversion mapping for an   inherited   &amp;gt; basic or embedded attribute.   &amp;gt; @Retention(RUNTIME)   &amp;gt; /**   &amp;gt;  * Specifies the converter to be applied.  A value for this   &amp;gt;  * element must be specified if multiple converters would   &amp;gt;  * otherwise apply.   &amp;gt;  */   &amp;gt; /**   &amp;gt;  * The attributeName must be specified unless the Convert annotation   &amp;gt;  * is on an attribute of basic type or on an element collection of   basic   &amp;gt;  * type.  In these cases, attributeName must not be specified.   &amp;gt;  */   &amp;gt; /**   &amp;gt;  * Used to disable an auto-apply or inherited converter.   &amp;gt;  * If disableConversion is true, the converter element should   &amp;gt;  * not be specified.   &amp;gt;  */   &amp;gt; /**   &amp;gt; * Used to group Convert annotations   &amp;gt; */   &amp;gt; @Retention(RUNTIME)   The plural is unfortunate here :(   In Bean validation we use an inner annotation @Convert.List but that's   not in the spirit of this spec.   &amp;gt; The Convert annotation is used to specify the conversion of a Basic   &amp;gt; (whether explicit or default) field or property.  The Convert   annotation   &amp;gt; should not be used to specify conversion of the following:   &amp;gt; Id attributes, version attributes, relationship attributes, and   attributes   &amp;gt; explicitly annotated (or designated via XML) as Enumerated or Temporal.   &amp;gt; Applications that specify such conversions will not be portable.   &amp;gt; The Convert annotation may be applied on a basic attribute or   &amp;gt; an element collection of basic type (in which case the converter   &amp;gt; is applied to the elements of the collection).  In these cases, the   &amp;gt; attributeName element must not be specified.   So that excludes the possibility to convert a collection object into a   flat representation in String. do we want to cover this use case?   I'm tempted to say that's a fair use case.   A way around that would be to make use of the attributeName to decide   whether or not the collection itself or its elements are targeted. That   has the benefit of rendering @Convert(attributeName="key") symmetric with   @Convert(attributeName="value") (example below).   &amp;gt; Examples:   &amp;gt; @Converter   &amp;gt; public class BooleanToIntegerConverter implements   &amp;gt; @Converter(autoApply=true)   &amp;gt; public class EmployeeDateConverter implements   &amp;gt; @Entity   &amp;gt;  @Convert(BooleanToIntegerConverter.class)   &amp;gt;  ...   &amp;gt;   // EmployeeDateConverter is applied automatically   &amp;gt; // Apply a converter to an element collection of basic type   &amp;gt; @ElementCollection   &amp;gt; @Convert(NameConverter.class)  // applies to each element in the   collection   &amp;gt; // Apply a converter to an element collection that is a map of basic   values   &amp;gt; // The converter is applied to the map *value*   &amp;gt; @ElementCollection   &amp;gt; @Convert(EmployeeNameConverter.class)   &amp;gt; When the Convert annotation is applied to a map to specify conversion   &amp;gt; of a map key of basic type, "key" must be used to specify that it   &amp;gt; is the map key that is to be converted.   &amp;gt; // Apply a converter to a Map key of basic type (relationship)   &amp;gt; @OneToMany   &amp;gt; @Convert(converter=ResponsibilityCodeConverter.class,   attributeName="key")   &amp;gt; // Apply a converter to a Map key of basic type (element collection)   &amp;gt; @ElementCollection   &amp;gt; @Convert(converter=ResponsibilityCodeConverter.class,   attributeName="key")   &amp;gt; // Disable conversion in the presence of an autoApply converter   &amp;gt; @Convert(disableConversion=true)   &amp;gt; The Convert annotation may be applied on an embedded attribute or on a   &amp;gt; map collection whose key or value is of embeddable type (in which case   &amp;gt; the converter is applied to the specified attribute of the embeddables   &amp;gt; contained in the collection).  In these cases the attributeName   &amp;gt; element must be specified.   &amp;gt; To override conversion mappings at multiple levels of embedding, a dot   &amp;gt; (".")  notation form must be used in the attributeName element to   &amp;gt; indicate an attribute within an embedded attribute.  The value of each   &amp;gt; identifier used with the dot notation is the name of the respective   &amp;gt; embedded field or property.   &amp;gt; When the Convert annotation is applied to a map containing   &amp;gt; embeddables, the attributeName element must be specified, and "key."   &amp;gt; or "value." must be used to prefix the name of the attribute that is   &amp;gt; to be converted in order to specify it as part of the map key or map   &amp;gt; value.   &amp;gt; // Apply a converter to an embeddable attribute   &amp;gt; @Embedded   &amp;gt; @Convert(converter=CountryConverter.class, attributeName="country")   &amp;gt; // Apply a converter to a nested embeddable attribute:   &amp;gt; @Embedded   &amp;gt; @Convert(converter=CityConverter.class, attributeName="region.city")   &amp;gt; ...   &amp;gt; // Apply a converter to a nested attribute of an embeddable that is   &amp;gt; // a map key of an element collection   &amp;gt; @Convert(name="key.region.city", converter=CityConverter.class)   &amp;gt; @ElementCollection   &amp;gt; @OneToMany   &amp;gt; // Apply to an embeddable that is a map key for a relationship   &amp;gt; @Convert(attributeName="key.type",   converter=ResponsibilityTypeConverter.class)   &amp;gt; The Convert annotation may be applied to an entity class that extends   &amp;gt; a mapped superclass to specify or override a conversion mapping   &amp;gt; for an inherited basic or embedded attribute.   &amp;gt; // Override conversion mappings for attributes inherited from a mapped   superclass   &amp;gt; @Entity   &amp;gt; @Convert(attributeName="startDate", converter=DateConverter.class),   &amp;gt; Open Issue:   &amp;gt; Do we need an EntityManagerFactory method to dynamically add   converters?   Does it mean I can also define an attribute as @Convert? What happens to   auto-apply converters, would that reconsider existing entities?   Besides all these comments, I have a few additional proposals for the   group:   - do we want converters to accept parameters to help people make them   more generic. Using the boolean to string example, one could feed the   converter with the string values it expect as true and false (y/N, 0/1   etc).   - we should support the listing of converters in persistence.xml (just   like classes) in case people chose to disable scanning.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converters</header>
    <date>Tue Feb 14 09:15:53 CET 2012</date>
    <body>Just for the record, I have not really asked for a way to change cardinality.  I just wanted to be able to use converters on Collection&amp;lt;X&amp;gt;.  1. Using conversion to change mapping cardinality such as a  @ElementCollection List&amp;lt;String&amp;gt; to a encoded single String is possible, but  such facility/technique seems to encroach upon responsibilities of mapping  primitives dictating the database schema. It may also impact how  attribute) can be executed.    2. how the converted attributes are used in query context (in expressions,  as parameter values) can be clarified further. The strict rule that they  are only be used as their original declared type will be simpler at the  cost of prohibiting some useful facility. For example, if a field declared  as java.net.URL is converted to a java.lang.String, then a query that uses  LIKE operator or orders the result on the (stringfied) URLs can be useful  at the expense of deviating from such a strict rule.      Regards --    Pinaki Poddar                  To:   jsr338-experts@...,             jsr338-experts@...  Date: 02/10/2012 11:58 AM  Subject:      [jsr338-experts] Re: converters        Hello all,  Just a couple comments:    flat representation in String. do we want to cover this use case? I'm  tempted to say that's a fair use case.  This case is covered through the @Basic annotation. Marking the collection  attribute as @Basic and @Convert would allow the collection to be converted  to a string.    like classes) in case people chose to disable scanning.  Yes.     First off thanks for the very nice work by Mike and Linda, many of my   initial concerns have been addressed in the formal writeup.     We might want to be explicit in the spec that it's a first step to   address 80% of the use cases and not a complete replacement of what   specific vendors propose in this area. I fear some backslash otherwise   esp for fairly common uses cases like:   - one java type to multiple columns   - use of database specific types / operation or JDBC specific code upon   read/write of a type.     More in line.     Emmanuel         Why not allowing @Id and @Version attributes provided the converted type   is within the acceptable list of types for id and version?         Have you considered `implicit` instead of  `auto-apply`. I feel it flows   nicely with the word converter.     javax.persistence.mapping.AttributeConverter interface.     +1 on removing Entity from the name       An alternative and more neutral name could be:   - convertForDatabase   - convertFromDatabase     or read / write.     expected   scope     I agree with Gordon, I was expecting X to be the sole element taken into   account.   What about attributes that have a subtype of X? If we allow subtypes,   which rules do we have for "specialization"?   Do we reuse the overloading rules of the JVM? What about potential   conflicts, is that an exception at deployment time?     equivalent).   equivalent.   pair     I'd rather have an exception here. Any reason to leave it undefined?     to   Convert   mapped   inherited   basic     The plural is unfortunate here :(   In Bean validation we use an inner annotation @Convert.List but that's   not in the spirit of this spec.     annotation   attributes     So that excludes the possibility to convert a collection object into a   flat representation in String. do we want to cover this use case?   I'm tempted to say that's a fair use case.     A way around that would be to make use of the attributeName to decide   whether or not the collection itself or its elements are targeted. That   has the benefit of rendering @Convert(attributeName="key") symmetric with   @Convert(attributeName="value") (example below).     collection   values   attributeName="key")   attributeName="key")   converter=ResponsibilityTypeConverter.class)   superclass   converters?     Does it mean I can also define an attribute as @Convert? What happens to   auto-apply converters, would that reconsider existing entities?     Besides all these comments, I have a few additional proposals for the   group:   - do we want converters to accept parameters to help people make them   more generic. Using the boolean to string example, one could feed the   converter with the string values it expect as true and false (y/N, 0/1   etc).   - we should support the listing of converters in persistence.xml (just   like classes) in case people chose to disable scanning.  </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converters</header>
    <date>Tue Feb 14 18:05:35 CET 2012</date>
    <body>Yes, Emmanuel, as long as we restrict the generic type arguments of Convert&amp;lt;X,Y&amp;gt; to retain their cardinality, we will be maintaining "separation of concern" between mapping and conversion. What are your thoughts about relaxing the usage of "converted fields" in query expressions? Should we allow a URL field converted as String be used as String in queries? Regards -- Pinaki To:     jsr338-experts@... Date:   02/14/2012 12:16 AM Subject:        [jsr338-experts] Re: converters Just for the record, I have not really asked for a way to change cardinality. I just wanted to be able to use converters on Collection&amp;lt;X&amp;gt;.  1. Using conversion to change mapping cardinality such as a  @ElementCollection List&amp;lt;String&amp;gt; to a encoded single String is possible, but  such facility/technique seems to encroach upon responsibilities of mapping  primitives dictating the database schema. It may also impact how  attribute) can be executed.  2. how the converted attributes are used in query context (in expressions,  as parameter values) can be clarified further. The strict rule that they  are only be used as their original declared type will be simpler at the  cost of prohibiting some useful facility. For example, if a field declared  as java.net.URL is converted to a java.lang.String, then a query that uses  LIKE operator or orders the result on the (stringfied) URLs can be useful  at the expense of deviating from such a strict rule.  Regards --  Pinaki Poddar  To:            jsr338-experts@...,             jsr338-experts@...  Date:          02/10/2012 11:58 AM  Subject:               [jsr338-experts] Re: converters  Hello all,  Just a couple comments:  flat representation in String. do we want to cover this use case? I'm  tempted to say that's a fair use case.  This case is covered through the @Basic annotation. Marking the collection  attribute as @Basic and @Convert would allow the collection to be converted  to a string.  like classes) in case people chose to disable scanning.  Yes.   First off thanks for the very nice work by Mike and Linda, many of my   initial concerns have been addressed in the formal writeup.   We might want to be explicit in the spec that it's a first step to   address 80% of the use cases and not a complete replacement of what   specific vendors propose in this area. I fear some backslash otherwise   esp for fairly common uses cases like:   - one java type to multiple columns   - use of database specific types / operation or JDBC specific code upon   read/write of a type.   More in line.   Emmanuel   Why not allowing @Id and @Version attributes provided the converted type   is within the acceptable list of types for id and version?   Have you considered `implicit` instead of  `auto-apply`. I feel it flows   nicely with the word converter.   javax.persistence.mapping.AttributeConverter interface.   +1 on removing Entity from the name   An alternative and more neutral name could be:   - convertForDatabase   - convertFromDatabase   or read / write.   expected   scope   I agree with Gordon, I was expecting X to be the sole element taken into   account.   What about attributes that have a subtype of X? If we allow subtypes,   which rules do we have for "specialization"?   Do we reuse the overloading rules of the JVM? What about potential   conflicts, is that an exception at deployment time?   equivalent).   equivalent.   pair   I'd rather have an exception here. Any reason to leave it undefined?   to   Convert   mapped   inherited   basic   The plural is unfortunate here :(   In Bean validation we use an inner annotation @Convert.List but that's   not in the spirit of this spec.   annotation   attributes   So that excludes the possibility to convert a collection object into a   flat representation in String. do we want to cover this use case?   I'm tempted to say that's a fair use case.   A way around that would be to make use of the attributeName to decide   whether or not the collection itself or its elements are targeted. That   has the benefit of rendering @Convert(attributeName="key") symmetric with   @Convert(attributeName="value") (example below).   collection   values   attributeName="key")   attributeName="key")   converter=ResponsibilityTypeConverter.class)   superclass   converters?   Does it mean I can also define an attribute as @Convert? What happens to   auto-apply converters, would that reconsider existing entities?   Besides all these comments, I have a few additional proposals for the   group:   - do we want converters to accept parameters to help people make them   more generic. Using the boolean to string example, one could feed the   converter with the string values it expect as true and false (y/N, 0/1   etc).   - we should support the listing of converters in persistence.xml (just   like classes) in case people chose to disable scanning.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Newbie</header>
    <date>Thu Feb 16 08:12:39 CET 2012</date>
    <body>Hi, My name is Nicolas, I work for Ericsson. And just joined this JSR. How should I get started?  Best regards /Nicolas</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Minor clarification suggestion for 3.2.6 Evicting an Entity Instance from the Persistence Context</header>
    <date>Mon Feb 20 15:51:52 CET 2012</date>
    <body>Dear group, While reading through our Versant JPA internal state diagram (which contains refinements of the states and transitions as needed by our implementation), I noticed that we still had a minor question open: The javadoc for EntityManager.detach() says:  ... Unflushed changes made * to the entity if any (including removal of the entity), * will not be synchronized to the database. ... Section 3.2.6 says: ... If X is a managed entity, the detach operation causes it to become detached. The detach operation is cascaded... ... If X is a removed entity, the detach operation is cascaded to entities ... Based on the javadoc comment, I presume, that the last part should read as: If X is a removed entity, the detach operation causes it to become detached. The detach operation is cascaded to entities ... Unless this was not the intended semantics, I suggest to reword that sentence for more clarity. Thank you! Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Minor clarification suggestion for 3.2.6 Evicting an Entity Instance from the Persistence Context</header>
    <date>Tue Feb 21 19:37:17 CET 2012</date>
    <body>Hi Christian, I believe that what you suggest is correct. Folks, please let me know if any of you disagree. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] question about transactional scoped versus extended persistence context in section 7.9.1 (container responsibilities)</header>
    <date>Wed Feb 22 03:44:07 CET 2012</date>
    <body>Hi, In section 7.9.1, we currently have the wording: " For the management of a transaction-scoped persistence context, if there is no EntityManager already associated with the JTA transaction: *  The container creates a new entity manager by calling EntityManagerFactory.createEntityManager when the first invocation of an entity manager with PersistenceContextType.TRANSACTION occurs within the scope of a business method executing in the JTA transaction. ... For stateful session beans with extended persistence contexts: ... * When a business method of the stateful session bean is invoked, if the stateful session bean uses container managed transaction demarcation, and the entity manager is not already associated with the current JTA transaction, the container associates the entity manager with the current JTA transaction and, if the persistence context is of type SynchronizationType.SYNCHRONIZED, the container calls EntityManager.joinTransaction. If there is a different persistence context already associated with the JTA transaction, the container throws the EJBException. ... " My question is about the extended persistence context part.  Do we require the container to associate the entity manager with the JTA transaction, at the time that the stateful session bean is invoked or instead when the entity manager is invoked in the stateful session bean? I just wanted to make sure that this section is clearly stating the intended container requirement. Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question about transactional scoped versus extended persistence context in section 7.9.1 (container responsibilities)</header>
    <date>Wed Feb 22 05:53:12 CET 2012</date>
    <body>Hi Scott, I think the spec is pretty clear that it should be the former.  I think this  is also the correct behavior -- i.e., if managed entities were updated without a call to the  entity manager, updates could otherwise be lost. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converters</header>
    <date>Wed Feb 22 11:15:28 CET 2012</date>
    <body>I don't have a specific opinion. I think I like the current approach which is  to keep the original field type at the query level and apply conversion  underneath. That also works best for the type safe criteria API.  Yes, Emmanuel, as long as we restrict the generic type arguments of  Convert&amp;lt;X,Y&amp;gt; to retain their cardinality, we will be maintaining  "separation of concern" between mapping and conversion.  What are your thoughts about relaxing the usage of "converted fields" in  query expressions? Should we allow a URL field converted as String be used  as String in queries?      Regards --    Pinaki                  To:   jsr338-experts@...  Date: 02/14/2012 12:16 AM  Subject:      [jsr338-experts] Re: converters        Just for the record, I have not really asked for a way to change  cardinality. I just wanted to be able to use converters on Collection&amp;lt;X&amp;gt;.      but  mapping  expressions,  declared  uses  collection  converted  with      </body>
  </mail>
  <mail>
    <header>[jpa-spec users] Clarification on JPA spec</header>
    <date>Wed Feb 29 16:33:53 CET 2012</date>
    <body>Hi all, Please forgive me if this email is inappropriate for this list. I have recently read the JPA 2.1 draft specification and I don't understand part of section 3.2.7.1 (Merging Detached Entity State).  I wonder if it could use some clarification. From the spec, the semantics of the merge operation: • If X is a detached entity, the state of X is copied onto a pre-existing managed entity instance X' of the same identity or a new managed copy X' of X is created. • If X is a new entity instance, a new managed entity instance X' is created and the state of X is copied into the new managed entity instance X'. • If X is a removed entity instance, an IllegalArgumentException will be thrown by the merge operation (or the transaction commit will fail). • If X is a managed entity, it is ignored by the merge operation, however, the merge operation is cascaded to entities referenced by relationships from X if these relationships have been anno- tated with the cascade element value cascade=MERGE or cascade=ALL annotation. • For all entities Y referenced by relationships from X having the cascade element value cascade=MERGE or cascade=ALL, Y is merged recursively as Y'. For all such Y refer- enced by X, X' is set to reference Y'. (Note that if X is managed then X is the same object as X'.)  • If X is an entity merged to X', with a reference to another entity Y, where cascade=MERGE or cascade=ALL is not specified, then navigation of the same association from X' yields a reference to a managed object Y' with the same persistent identity as Y.  The last bullet point is my pickle.   If Entity Y was stored, I read this to say that it'll be retrieved from the DB as Y' and X' will reference Y', but Y's state will not be copied to Y'. But what if Y is a new object, not managed nor persisted?  What should JPA to do produce Y'?   I have done some playing around with this, and I have found that at least one implementation (eclipselink 2.3.0) cascades the merge to Y in order to produce Y', but I'm not sure that's the intended outcome, especially given that cascade=MERGE is not specified. For what it's worth, my expectation is that if Y was stored, it'll be retrieved but the merge not cascaded.  If Y was a new object an exception would be thrown (IllegalArgumentException, perhaps?). Does this part need clarification in the new spec, or am I too noob to understand it? =) Cheers, --Royce</body>
  </mail>
  <mail>
    <header>[jpa-spec users] JPA 1.0 style mappings: insertable = false, updatable = false on redundant @Column fields</header>
    <date>Fri Mar 02 15:52:59 CET 2012</date>
    <body>Hello all, I've recently reported a bug for Hibernate 4 here: https://hibernate.onjira.com/browse/HHH-7139 As it turns out, the bug has been rejected. The reason Mr Ebersole gave on the bug report isn't very satisfying as I have often found the JPA 1.0 style mappings to be a lot more robust over JPA 2.0 style derived identities. However, I'm not a JPA expert. I'd like to hear a few people's opinion on the reasons why this has been rejected to not being fixed. Is he right? My opinion is that even if the JPA spec isn't clear about allowing insertable = false / updatable = false on @Column the read-only properties are better off on the redundant @Column fields rather than the relationships. Is there anybody who can help me with this? Karsten    Ihr WEB.DE Postfach immer dabei: die kostenlose WEB.DE Mail App für iPhone und Android.    https://produkte.web.de/freemail_mobile_startseite/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converters</header>
    <date>Mon Mar 05 22:41:05 CET 2012</date>
    <body>So Linda has pointed out that the "column" attribute in AttributeOverride is of type @Column with it's own defaults.  As such we are unable to make @Column optional in a clean way, so please ignore my suggestion on using @AttributeOverride for @Convert overrides. Thanks, Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] converters (updated)</header>
    <date>Tue Mar 06 02:26:47 CET 2012</date>
    <body>Here's an update of my original writeup.  This attempts to incorporate the results of the discussion so far, and cleans up some of the description.  There are a couple of open issues at the end. -Linda --------------------------- Converters may be specified to provide conversion between the entity attribute representation and the database representation for attributes of basic types.  Converters may be used to convert attributes defined by entity classes, mapped superclasses, or embeddable classes. The conversion of all basic types are supported except for the following: Id attributes, version attributes, relationship attributes, and attributes explicitly annotated (or designated via XML) as Enumerated or Temporal.  Auto-apply converters will not be applied to such attributes, and applications that apply converters to such attributes through use of the Convert annotation will not be portable. The persistence provider runtime is responsible for invoking the corresponding conversion method when loading the entity attribute from the database and before storing the entity attribute state to the database.  The persistence provider must apply any conversion mappings to instances of attribute values used within JPQL or criteria queries (such as in comparisons, bulk updates, etc.)  before sending them to the database for the query execution.  If the result of a JPQL or criteria query includes one or more entity attributes for which conversion mappings have been specified, the persistence provider must apply the specified conversions to the corresponding values in the query result before returning them to the application. An attribute converter must implement the  javax.persistence.mapping.AttributeConverter interface. /**  * A class that implements this interface can be used to convert entity  * attribute state into database column representation and back again.  * Note that the X and Y types may be the same Java type.  *  * @param X  the type of the entity attribute  * @param Y  the type of the database column  */     /**      * Converts the value stored in the entity attribute into the data      * representation to be stored in the database.      *      * @param attribute  the entity attribute value to be converted      * @return  the converted data to be stored in the database column      */     /**      * Converts the data stored in the database column into the value      * to be stored in the entity attribute.      * Note that it is the responsibility of the converter writer to      * specify the correct dbData type for the corresponding column for      * use by the JDBC driver, i.e., persistence providers are not expected      * to do such type conversion.      *      * @param dbData  the data from the database column to be converted      * @return  the converted value to be stored in the entity attribute      */ A converter class must be annotated with the Converter annotation or defined in the object/relational mapping descriptor as a converter. /**  *  Specifies that the annotated class is a converter and defines its scope  */ @Retention(RUNTIME)    /**     * If set to true, specifies that the converter will automatically     * be applied to all mapped attributes of the specified     * target type for all entities in the persistence unit     * unless overridden by means of the Convert annotation (or XML  equivalent).     * In determining whether a converter is applicable to an attribute,     * the provider must treat primitive types and wrapper types as equivalent.     *     * Note that Id attributes, version attributes, relationship     * attributes, and attributes explicitly annotated as Enumerated     * or Temporal (or designated as such via XML) will not be converted.     *     * If autoApply is false, only those attributes of the target type     * for which the Convert annotation (or corresponding XML element) has     * been specified will be converted.     *     * If there is more than one converter defined for the same target     * type, the Convert annotation should be used to explicitly specify     * which converter to use.     *     * Note that if autoApply is true, the Convert annotation may be used to     * override or disable auto-apply conversion on a per-attribute basis.     */ Type conversion may be specified at the level of individual attributes by means of the Convert annotation.  The Convert annotation may also be used to override or disable an auto-apply conversion. The Convert annotation may be applied directly to an attribute of an entity, mapped superclass, or embeddable class to specify conversion of the attribute or to override the use of a converter that has been specified as autoApply=true.  When persistent properties are used, the Convert annotation is applied to the getter method. The Convert annotation may be applied to an entity that extends a mapped superclass to specify or override the conversion mapping for an inherited basic or embedded attribute. @Retention(RUNTIME)   /**    * Specifies the converter to be applied.  A value for this    * element must be specified if multiple converters would    * otherwise apply.    */   /**    * The attributeName must be specified unless the Convert annotation    * is on an attribute of basic type or on an element collection of basic    * type.  In these cases, attributeName must not be specified.    */   /**    * Used to disable an auto-apply or inherited converter.    * If disableConversion is true, the converter element should    * not be specified.    */ /**  * Used to group Convert annotations  */ @Retention(RUNTIME) The Convert annotation is used to specify the conversion of a Basic (whether explicit or default) field or property.  The Convert annotation should not be used to specify conversion of the following: Id attributes, version attributes, relationship attributes, and attributes explicitly annotated (or designated via XML) as Enumerated or Temporal. Applications that specify such conversions will not be portable. The Convert annotation may be applied to a basic attribute or to an element collection of basic type (in which case the converter is applied to the elements of the collection).  In these cases, the attributeName element must not be specified. Examples: @Converter public class BooleanToIntegerConverter implements AttributeConverter&amp;lt;Boolean,  @Converter(autoApply=true) public class EmployeeDateConverter implements  @Entity    @Convert(BooleanToIntegerConverter.class)    ...     // EmployeeDateConverter is applied automatically // Apply a converter to an element collection of basic type @ElementCollection @Convert(NameConverter.class)  // applies to each element in the collection // Apply a converter to an element collection that is a map of basic values // The converter is applied to the map *value* @ElementCollection @Convert(EmployeeNameConverter.class) When the Convert annotation is applied to a map to specify conversion of a map key of basic type, "key" must be used to specify that it is the map key that is to be converted. // Apply a converter to a Map key of basic type (relationship) @OneToMany @Convert(converter=ResponsibilityCodeConverter.class, attributeName="key") // Apply a converter to a Map key of basic type (element collection) @ElementCollection @Convert(converter=ResponsibilityCodeConverter.class, attributeName="key") // Disable conversion in the presence of an autoApply converter @Convert(disableConversion=true) The Convert annotation may be applied to an embedded attribute or to a map collection attribute whose key or value is of embeddable type (in which case the converter is applied to the specified attribute of the embeddable instances contained in the collection).  In these cases the attributeName element must be specified. To override conversion mappings at multiple levels of embedding, a dot (".")  notation form must be used in the attributeName element to indicate an attribute within an embedded attribute.  The value of each identifier used with the dot notation is the name of the respective embedded field or property. When the Convert annotation is applied to a map containing embeddables, the attributeName element must be specified, and "key." or "value." must be used to prefix the name of the attribute that is to be converted in order to specify it as part of the map key or map value. // Apply a converter to an embeddable attribute @Embedded @Convert(converter=CountryConverter.class, attributeName="country") // Apply a converter to a nested embeddable attribute: @Embedded @Convert(converter=CityConverter.class, attributeName="region.city")   ...   // Apply a converter to a nested attribute of an embeddable that is   // a map key of an element collection   @Convert(name="key.region.city", converter=CityConverter.class)   @ElementCollection @OneToMany // Apply to an embeddable that is a map key for a relationship @Convert(attributeName="key.type",  converter=ResponsibilityTypeConverter.class) The Convert annotation may be applied to an entity class that extends a mapped superclass to specify or override a conversion mapping for an inherited basic or embedded attribute. // Override conversion mappings for attributes inherited from a mapped  superclass @Entity   @Convert(attributeName="startDate", converter=DateConverter.class), OPEN ISSUES: There are still a couple of open issues: Open Issue:  Conversion of @Id and @Version. Open Issue:  Explicit listing of converters in persistence.xml file. I'm not sure I understand what is being proposed here. Open Issue: What to do about the "specialization" issue that was raised.  Would someone care to provide some use cases and/or flesh out that part of the proposal further? Please let me know if I have missed anything or if there are any other corrections.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] updated spec draft (converters)</header>
    <date>Wed Mar 07 23:33:13 CET 2012</date>
    <body>I've uploaded a draft of the spec with the attribute converter additions to  the document downloads area,  http://java.net/projects/jpa-spec/downloads . The converter changes can be found in sections 3.7, 10.5, and 11.1.10-11. The following open issues are pending: * Conversion of @Id and @Version. * Explicit listing of converters in persistence.xml file.   I'm not sure I  understand what   was being proposed here. * What to do about the "specialization" issue that was raised.  Would someone  care to provide   some use cases and/or flesh out that part of the proposal further? * Whether these APIs belong in the javax.persistence package or whether we  should introduce   a new package (e.g., javax.persistence.mapping). Please let me know if I have missed anything or if there are any other corrections, additions, etc. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] new expert group member</header>
    <date>Thu Mar 08 20:01:32 CET 2012</date>
    <body>I'd like to welcome Steve Ebersole as a new member of the JPA 2.1 Expert  Group. Steve joins us as a representative from Red Hat. Please welcome him to the group! thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new expert group member</header>
    <date>Thu Mar 08 20:18:20 CET 2012</date>
    <body>welcome Steve nice to have you on board Bernd Am 08.03.2012 20:01, schrieb Linda DeMichiel:  I'd like to welcome Steve Ebersole as a new member of the JPA 2.1 Expert  Group.  Steve joins us as a representative from Red Hat.    Please welcome him to the group!    thanks,    -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new expert group member</header>
    <date>Thu Mar 08 20:30:03 CET 2012</date>
    <body>Welcome, Steve! :) Am 08.03.2012 um 20:01 schrieb Linda DeMichiel:  I'd like to welcome Steve Ebersole as a new member of the JPA 2.1 Expert   Group.  Steve joins us as a representative from Red Hat.    Please welcome him to the group!    thanks,    -Linda --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */ Attachment: signature.asc Description: Message signed with OpenPGP using GPGMail</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Map Enum to Coded Values and Converters</header>
    <date>Thu Mar 08 21:57:14 CET 2012</date>
    <body>Hi Will it be possible to use the proposed  AttributeConverter  class to address the limitation with Enums below? I was hoping this class would enable the selection of a custom enum attribute when persisting, without using the proprietary extensions in EclipseLink and Hibernate. Section 3.7 of the updated spec indicates that @Enumerated will be excluded, which would be a shame. Thanks Andrew andreww100@... Hi Expert Group The current JPA spec provides @Enumerated, with EnumType being limited to ORDINAL and STRING. ORDINAL is not good long term, as enumerations may be expanded, and ordinal positions will change if new literals are inserted. Seems too brittle. STRING is better than ORDINAL, but creates a conflict between the desire for short codes in the database, and meaningful literal names. What users really want is the ability to map an enum literal to a code (a short string, or number). The design problem you have, is how to best specify the custom enum attribute to be used as the code. EclipseLink and Hibernate understand this requirement, and provide proprietary solutions. http://wiki.eclipse.org/EclipseLink/Examples/JPA/EnumToCode http://stackoverflow.com/questions/2751733/map-enum-in-jpa-with-fixed-values Is there any chance of standardising this in JPA 2.1? Thanks Andrew Ward</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] proposal : @Entity on interfaces</header>
    <date>Fri Mar 09 19:04:48 CET 2012</date>
    <body>I'd like to propose that JPA 2.1 allow @Entity on Java interfaces not just classes.  The main reason is typing in spec contracts.  For domain models that leverage interfaces, it is usually desirable to deal with the interfaces over the implementation classes.  For example, such applications would generally prefer to attempt to load or get a reference to an instance based on the interface name as opposed to the class name.  E.g.     ... @Entity     ... But this does not work today in a portable manner.  To work in the most portable manner, I think the @Entity annotated interface also would need to name the "persistent implementation class": @Entity( impl = PersonImpl.class )     ...     ... It could be up to each provider whether or not to support @Entity on an interface that did not specify a "persistent implementation class". Another way to look at this is as basically "aliasing" the entity type metadata using the interface name instead of the implementation class name. -Steve</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new expert group member</header>
    <date>Fri Mar 09 19:18:30 CET 2012</date>
    <body>Thanks everyone for the welcomes!</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Fri Mar 09 20:50:31 CET 2012</date>
    <body>Hi all, great idea. Would it make sense to invert the mapping to point from the  implementation class to the interface as a mapping the other way round would  effectively create a bidirectional dependency. The interface referring to  it's implementation feels a bit weird. What about   … @Entity(typeAliasFor = Person.class)   … The persistence provider would then reject mapping constellations where more  than one implementation class refers to the interface as type alias. With a  mapping this way round one could even have interface and implementation in  separate JARs being brought together at runtime. Cheers, Ollie Am 09.03.2012 um 19:04 schrieb Steve Ebersole:  I'd like to propose that JPA 2.1 allow @Entity on Java interfaces not   just classes.  The main reason is typing in spec contracts.  For domain   models that leverage interfaces, it is usually desirable to deal with   the interfaces over the implementation classes.  For example, such   applications would generally prefer to attempt to load or get a   reference to an instance based on the interface name as opposed to the   class name.  E.g.        ...    @Entity      ...      But this does not work today in a portable manner.  To work in the most   portable manner, I think the @Entity annotated interface also would need   to name the "persistent implementation class":    @Entity( impl = PersonImpl.class )      ...        ...    It could be up to each provider whether or not to support @Entity on an   interface that did not specify a "persistent implementation class".    Another way to look at this is as basically "aliasing" the entity type   metadata using the interface name instead of the implementation class name.    -Steve --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */ Attachment: signature.asc Description: Message signed with OpenPGP using GPGMail</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Fri Mar 09 21:05:15 CET 2012</date>
    <body>Exactly what you mention was the reason I thought to define it on the interface itself.  That way you would enforce that it was used only once.  But I totally understand about the awkwardness of the "bi-directionality". Good point about the separating.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Map Enum to Coded Values and Converters</header>
    <date>Fri Mar 09 22:57:18 CET 2012</date>
    <body>Hi Andrew, Yes, converters will be able to handle that case. When we said that @Enumerated was not permitted on a converted mapping that only meant that there was no reason for the two annotations to co-reside on the same attribute. You can convert attributes of any enumerated type in any way that you wish, including your suggested mappings below. Regards, -Mike Hi Will it be possible to use the proposed  AttributeConverter  &lt;span class="Apple-style-span" style="-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: "&gt; class to address the limitation with Enums below? I was hoping this class would enable the selection of a custom enum attribute when persisting, without using the proprietary extensions in EclipseLink and Hibernate. &lt;span class="Apple-style-span" style="-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469); "&gt; &lt;span class="Apple-style-span" style="-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469); "&gt; Section 3.7 of the updated spec indicates that @Enumerated will be excluded, which would be a shame. &lt;span class="Apple-style-span" style="-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469); "&gt; &lt;span class="Apple-style-span" style="-webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);"&gt; Thanks &lt;span class="Apple-style-span" style="-webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);"&gt; Andrew andreww100@... Hi Expert Group The current JPA spec provides @Enumerated, with EnumType being limited to ORDINAL and STRING. ORDINAL is not good long term, as enumerations may be expanded, and ordinal positions will change if new literals are inserted. Seems too brittle. STRING is better than ORDINAL, but creates a conflict between the desire for short codes in the database, and meaningful literal names. What users really want is the ability to map an enum literal to a code (a short string, or number). The design problem you have, is how to best specify the custom enum attribute to be used as the code. EclipseLink and Hibernate understand this requirement, and provide proprietary solutions. http://wiki.eclipse.org/EclipseLink/Examples/JPA/EnumToCode http://stackoverflow.com/questions/2751733/map-enum-in-jpa-with-fixed-values Is there any chance of standardising this in JPA 2.1? Thanks Andrew Ward</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Sun Mar 11 17:23:27 CET 2012</date>
    <body>I think, this would make many things more complex to specify in a consistent way. In general, interfaces are a way to describe contracts in an implementation independent way and therefore should be as abstract as possible. JPA is a mapping between VM-objects and databases and therefore as close as possible to the implementation level. Here I see some problems from a conceptual point of view. I also see many problems in practice. How to map fields in classes, which are eventually named different than the annotated getter in the interface? How to map interface hierarchies with multiple super interfaces to eventually DIFFERENT class hierarchies with single inheritance? There are more, I think. We have to balance if it's worth to get such new problems which have to be resolved in the spec and on the other hand the benefit is less typing (how much?). Bernd Am 09.03.2012 19:04, schrieb Steve Ebersole:  I'd like to propose that JPA 2.1 allow @Entity on Java interfaces not  just classes.  The main reason is typing in spec contracts.  For domain  models that leverage interfaces, it is usually desirable to deal with  the interfaces over the implementation classes.  For example, such  applications would generally prefer to attempt to load or get a  reference to an instance based on the interface name as opposed to the  class name.  E.g.        ...    @Entity      ...      But this does not work today in a portable manner.  To work in the most  portable manner, I think the @Entity annotated interface also would need  to name the "persistent implementation class":    @Entity( impl = PersonImpl.class )      ...        ...    It could be up to each provider whether or not to support @Entity on an  interface that did not specify a "persistent implementation class".    Another way to look at this is as basically "aliasing" the entity type  metadata using the interface name instead of the implementation class name.    -Steve</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Sun Mar 11 18:27:20 CET 2012</date>
    <body>Let me first say that I don't like the model of interfaces on entities. It is quite rarely requested and when it is, often the user wants multiple entities to be able to implement the same interface (e.g. polymorphism w/o inheritance) to do something like this: and then in some entity:          ...          @ManyToOne The feature being proposed, if I understood it correctly, would not support this. Having said that, if we think that aliasing is one of those things that could stop large companies from using JPA we might decide to close our eyes, hold our noses and go ahead and add some kind of support. While I understood and sympathized with the reasons why Steve first suggested that the annotation be on the interface (to facilitate interface-to-single-entity enforcement) I tend to agree with Oliver and Bernd that it would make more sense to just add an "alias" element to the @Entity annotation and continue to apply it to the impl class. It would be the responsibility of the provider to disallow two entities from creating an alias to the same class. Example:     @Entity(alias=Employee.class)        // mappings ... Where does one draw the line? Would we assume that a single alias is enough and that an entity would not be allowed to specify a Set? For example, that we would not need to support something like: @Entity(aliasInterfaces=true)        // mappings ... -Mike</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Mon Mar 12 06:31:31 CET 2012</date>
    <body>If we choose to take this on, we could let users express simply through simple java "implements" statements which persistent interfaces a given class implements, and we could allow persistent fields of persistent interface types and persistent collections of interfaces types, as well as queries through them. @Entity // or @MappedSuperclass? // or new @EntityInterface?   // default mapping info allowed here? ===== @Entity public class Person sufficient */   @Column(name="emp_num") ==== @Entity   @OneToMany   @OneToOne   @AllowedClasses(Person.class)   /* Note:   Any assignment of a class other than those whitelisted in @AllowedClasses would   cause the JPA implementation to throw ClassCastException   */ /* @AllowedClasses could also be an attribute of relationship annotations instead of its own annotation. */ ===== The hard part, IMHO, is portably mapping them.  Wherever you have a reference to a persistent interface, you also need at the point the class of the implementation.  Further, I would say that persistent interfaces could only be mapped using property access, as interfaces have no instance fields. One possibility is to allow the use of persistent interfaces, but just say that their mappings are not portable.  It would be a nontrivial body of work to bite off the additional specification of portably mapping persistent interfaces. -matthew On Sun, Mar 11, 2012 at 12:27 PM, michael keith  Let me first say that I don't like the model of interfaces on entities. It  is quite rarely requested and when it is, often the user wants multiple  entities to be able to implement the same interface (e.g. polymorphism w/o  inheritance) to do something like this:  and then in some entity:          ...          @ManyToOne  The feature being proposed, if I understood it correctly, would not support  this.  Having said that, if we think that aliasing is one of those things that  could stop large companies from using JPA we might decide to close our eyes,  hold our noses and go ahead and add some kind of support.  While I understood and sympathized with the reasons why Steve first  suggested that the annotation be on the interface (to facilitate  interface-to-single-entity enforcement) I tend to agree with Oliver and  Bernd that it would make more sense to just add an "alias" element to the  @Entity annotation and continue to apply it to the impl class. It would be  the responsibility of the provider to disallow two entities from creating an  alias to the same class.  Example:     @Entity(alias=Employee.class)        // mappings ...  Where does one draw the line? Would we assume that a single alias is enough  and that an entity would not be allowed to specify a Set? For example, that  we would not need to support something like:   @Entity(aliasInterfaces=true)        // mappings ...  -Mike --  @matthewadams12 mailto:matthew@ ... skype:matthewadams12 yahoo:matthewadams aol:matthewadams12 google-talk:matthewadams12@... msn:matthew@... http://matthewadams.me http://www.linkedin.com/in/matthewadams</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Mon Mar 12 09:38:36 CET 2012</date>
    <body>First, I would like to say that I find the proposal to be quite complex  compared to the gains.  And I agree with Bernd and Michael. However, I have to say that I did not understand the following comment: "Having said that, if we think that aliasing is one of those things that  could stop large companies from using JPA we might decide to close our eyes,  hold our noses and go ahead and add some kind of support." What do you mean by aliasing? IMO, the biggest problem with JPA is that it is seen as being slow, and to  encourage bad DB schema designs. /Nicolas -----Original Message----- From: michael keith [ mailto:michael.keith@...]  Sent: Sunday, March 11, 2012 6:27 PM To: jsr338-experts@... Cc: Bernd Müller Subject: [jsr338-experts] Re: proposal : @Entity on interfaces Let me first say that I don't like the model of interfaces on entities.  It is quite rarely requested and when it is, often the user wants multiple  entities to be able to implement the same interface (e.g.  polymorphism w/o inheritance) to do something like this: and then in some entity:           ...           @ManyToOne The feature being proposed, if I understood it correctly, would not support  this. Having said that, if we think that aliasing is one of those things that could  stop large companies from using JPA we might decide to close our eyes, hold  our noses and go ahead and add some kind of support. While I understood and sympathized with the reasons why Steve first suggested  that the annotation be on the interface (to facilitate  interface-to-single-entity enforcement) I tend to agree with Oliver and Bernd  that it would make more sense to just add an "alias" element to the @Entity  annotation and continue to apply it to the impl class. It would be the  responsibility of the provider to disallow two entities from creating an  alias to the same class. Example:      @Entity(alias=Employee.class)         // mappings ... Where does one draw the line? Would we assume that a single alias is enough  and that an entity would not be allowed to specify a Set? For example, that  we would not need to support something like: @Entity(aliasInterfaces=true)         // mappings ... -Mike  I think, this would make many things more complex to specify in a   consistent way.  In general, interfaces are a way to describe contracts in an   implementation independent way and therefore should be as abstract as   possible. JPA is a mapping between VM-objects and databases and   therefore as close as possible to the implementation level.  Here I see some problems from a conceptual point of view.  I also see many problems in practice. How to map fields in classes,   which are eventually named different than the annotated getter in the   interface? How to map interface hierarchies with multiple super   interfaces to eventually DIFFERENT class hierarchies with single   inheritance? There are more, I think.  We have to balance if it's worth to get such new problems which have   to be resolved in the spec and on the other hand the benefit is less   typing (how much?).  Bernd  Am 09.03.2012 19:04, schrieb Steve Ebersole:</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Mon Mar 12 12:33:52 CET 2012</date>
    <body>Hi Nicolas, See below. The key part of what was being proposed (at least how I interpreted the feature, Steve can correct me if I misinterpreted) was the ability to "alias" an entity class. This just means that one might be able register an entity class in the providers entity list keyed not only by the concrete entity class, but also an (or more than one?) interface, so that when the provider encounters that interface it gets loaded/saved by the provider as the concrete entity class. The performance question is more about perception than of specifying, I think. I'm not sure we can do much about that other than make sure we get the message out that persistence provider caching makes JPA tend to be faster than raw JDBC. On the bad schema side that is an argument for making sure we do a better job at warning developers not to use schema generation for production schemas. -Mike</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Mon Mar 12 12:37:39 CET 2012</date>
    <body>I would really prefer we not go down this path... -Mike</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Mon Mar 12 13:08:26 CET 2012</date>
    <body>+1 on Mike's statement here. My initial intention (and I think Steve's as  well) was what Mike explained in his other email: aliasing access to a JPA  entity through an interface for the purpose of being able to separate code  and e.g. have the interfaces in one JAR and the implementation class in  another one.   The key part of what was being proposed (at least how I interpreted the   feature, Steve can correct me if I misinterpreted) was the ability to   "alias" an entity class. This just means that one might be able register   an entity class in the providers entity list keyed not only by the   concrete entity class, but also an (or more than one?) interface, so   that when the provider encounters that interface it gets loaded/saved by   the provider as the concrete entity class. No additional metadata, simply metadata aliasing. I think ideas like this  originate from the trend for developers to build rich domain models, follow  Domain Driven Design and don't treat domain classes as pure data containers.  Although one might argue this is not what the JPA spec is about I think we  should make sure we don't get in the way when following this approach so that  devs don't get into a "I can't do this because JPA can't deal with it"  situation from a class' design perspective. Looking at the extended proposal @Entity on an interface feels a bit weird as  well, at least given the current semantics in the spec. I could imagine an  @EntityAlias though with the following usage model: %&amp;lt;---------------------------------- @EntityAlias @Entity %&amp;lt;---------------------------------- Not sure I favor this over explicitly listing the alias interfaces on the  proposal you could implement the interface with a second class without  creating the conflict of two implementation classes using the interface as  alias.  Having said that, if we think that aliasing is one of those things that   could stop large companies from using JPA we might decide to close our   eyes, hold our noses and go ahead and add some kind of support. Could you elaborate on that? I don't quite get why this feature should stop  companies from using JPA.      @Entity(alias=Employee.class)         // mappings ...    Where does one draw the line? Would we assume that a single alias is   enough and that an entity would not be allowed to specify a Set? For   example, that we would not need to support something like:        @Entity(aliasInterfaces=true)         // mappings ... The latter doesn't seem to complicate things, does it? At least given the  assumption there has to be a single implementation class to be selected to  back the type alias, right? Cheers, Ollie Am 12.03.2012 um 12:37 schrieb michael keith:  I would really prefer we not go down this path...    -Mike   --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */ Attachment: signature.asc Description: Message signed with OpenPGP using GPGMail</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Mon Mar 12 15:40:52 CET 2012</date>
    <body>No, I didn't say that very well. I meant the opposite, i.e. if the fact that there was no aliasing could prompt them to decide that... Every feature addition adds some non-zero amount of complexity. Our task is to weigh the value and usefulness of the feature with its complexity, with the time that we have to release the spec, and with its relative priority w.r.t. other potential features that may be more useful and more worth our time to add. -Mike</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Mon Mar 12 15:53:53 CET 2012</date>
    <body>JPA is about mapping concrete classes to relational databases.  I never claimed otherwise.  If that was the mis-assumption from suggesting @Entity be allowed on interfaces, I apologize for not being more clear. No, as Oliver points out, the proposal is more about aliasing.  There is a thread on the Hibernate developer list[1] that discuss this in detail. Basically I think that trends in development prefer interface-driven design.  Whether you agree with that or not for a JPA model is not really the discussion.  The proposal is more about working with that more seamlessly. one-for-one.  That and another more esoteric reason is what led me to propose @Entity on the interface itself, with a attribute pointing to the class being physically mapped.  But I also see the benefit in Oliver's suggestion to having that "link" on the entity *class*.  This is a point we specifically discussed on that Hibernate developer discussion.  In retrospect, I think the approach Oliver mentions is better approach.  It allows the jar separation he mentions, plus other benefits. [1]  http://lists.jboss.org/pipermail/hibernate-dev/2012-January/007627.html</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Mon Mar 12 15:58:39 CET 2012</date>
    <body>Am 12.03.2012 um 15:40 schrieb michael keith:  No, I didn't say that very well. I meant the opposite, i.e. if the fact  that there was no aliasing could prompt them to decide that... Gotcha.  Every feature addition adds some non-zero amount of complexity. Our task  is to weigh the value and usefulness of the feature with its complexity,  with the time that we have to release the spec, and with its relative  priority w.r.t. other potential features that may be more useful and  more worth our time to add. +1 on that. The question I wanted to raise if there's a significant  difference in complexity by allowing multiple aliases over just one. Cheers, Ollie --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */ Attachment: signature.asc Description: Message signed with OpenPGP using GPGMail</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Mon Mar 12 17:01:44 CET 2012</date>
    <body>Sorry, most of the pertinent discussion actually happened on IRC, not the email list.  Just for completeness: http://transcripts.jboss.org/channel/irc.freenode.org/%23hibernate-dev/2012/%23hibernate-dev.2012-01-26.log.html</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Mon Mar 12 17:44:45 CET 2012</date>
    <body>++1 The notion of interface in Java and  type that has persistent state just do not mix. The "contract:" of interface is not a state-oriented contract. Trying to mix them will not be a step in a right direction. Regards -- Pinaki Poddar Chair, Apache OpenJPA Project            http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware To:     jsr338-experts@... Cc:     Matthew Adams &amp;lt;matthew@...&amp;gt;, Bernd Müller Date:   03/12/2012 04:39 AM Subject:        [jsr338-experts] Re: proposal : @Entity on interfaces I would really prefer we not go down this path... -Mike  If we choose to take this on, we could let users express simply  through simple java "implements" statements which persistent  interfaces a given class implements, and we could allow persistent  fields of persistent interface types and persistent collections of  interfaces types, as well as queries through them.  @Entity  // or @MappedSuperclass?  // or new @EntityInterface?     // default mapping info allowed here?  =====  @Entity  public class Person  sufficient */     @Column(name="emp_num")  ====  @Entity     @OneToMany     @OneToOne     @AllowedClasses(Person.class)     /* Note:     Any assignment of a class other than those whitelisted in  @AllowedClasses would     cause the JPA implementation to throw ClassCastException     */  /*  @AllowedClasses could also be an attribute of relationship annotations  instead of its own annotation.  */  =====  The hard part, IMHO, is portably mapping them.  Wherever you have a  reference to a persistent interface, you also need at the point the  class of the implementation.  Further, I would say that persistent  interfaces could only be mapped using property access, as interfaces  have no instance fields.  One possibility is to allow the use of persistent interfaces, but just  say that their mappings are not portable.  It would be a nontrivial  body of work to bite off the additional specification of portably  mapping persistent interfaces.  -matthew  On Sun, Mar 11, 2012 at 12:27 PM, michael keith It w/o support eyes, the be creating an enough that domain most need an</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Mon Mar 12 17:57:52 CET 2012</date>
    <body>I fear we get into a philosophical discussion here. Objects have state *and*  behavior, which is what especially proponents of Domain Driven Design  emphasize. Why should JPA get in the way of using objects this way? I always  considered JPA an API to allow mapping objects onto a relational store, not  creating Java representations of data tables. Cheers, Ollie Am 12.03.2012 um 17:44 schrieb Pinaki Poddar:  ++1    The notion of interface in Java and  type that has persistent state just do  not mix.  The "contract:" of interface is not a state-oriented contract.  Trying to mix them will not be a step in a right direction.      Regards --    Pinaki Poddar  Chair, Apache OpenJPA Project            http://openjpa.apache.org/  JPA Expert Group Member  Application &amp;amp; Integration Middleware                  To:   jsr338-experts@...  Cc:   Matthew Adams &amp;lt;matthew@...&amp;gt;, Bernd Müller  Date: 03/12/2012 04:39 AM  Subject:      [jsr338-experts] Re: proposal : @Entity on interfaces        I would really prefer we not go down this path...    -Mike    It  w/o  support  eyes,  the  be  creating an  enough  that  domain  most  need  an       --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */ Attachment: signature.asc Description: Message signed with OpenPGP using GPGMail</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Mon Mar 12 18:19:15 CET 2012</date>
    <body> I've uploaded a draft of the spec with the attribute converter additions to   the document  downloads area,  http://java.net/projects/jpa-spec/downloads .    The converter changes can be found in sections 3.7, 10.5, and 11.1.10-11.    The following open issues are pending:    * Conversion of @Id and @Version.    * Explicit listing of converters in persistence.xml file.   I'm not sure I   understand what   was being proposed here. When the archive is not scanned (it's an option in persistence.xml), the  provider has no way of finding the list of converters unless the user  explicitly list them in persistence.xml like it does list classes.    * What to do about the "specialization" issue that was raised.  Would   someone care to provide   some use cases and/or flesh out that part of the proposal further?    * Whether these APIs belong in the javax.persistence package or whether we   should introduce   a new package (e.g., javax.persistence.mapping).    Please let me know if I have missed anything or if there are any other  corrections, additions, etc.      -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Mon Mar 12 18:51:30 CET 2012</date>
    <body>OK, thanks -- so would you propose that they simply be listed using the class  element?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Converters and Enum</header>
    <date>Tue Mar 13 03:46:23 CET 2012</date>
    <body>Hi, I have just joined the list, tried to search the archives for this, but haven't found any message that address this directly. So Linda asked about the case about Converters and inheritance. The main use case would be enums. For my current project, with a legacy database, I've created two annotations: @PersistentId (with a int value) and @Acronym (with a String value). I have an annotation processor that ensures each Enum class is consistently annotated with only of them and that ids are unique. Then I have a custom UserType (with Hibernate) to implements the basic support in both cases (and a few helper classes, but that is irrelevant). The problem is while the mechanism is the same, I need to know the target class in order to convert to and from the actual enum class. So today I have to either parametrize my type for each class or creating a subclass, the latter being the approach I've chosen. So I end up with a single UserType subclass per enum and have to declare a type for each of them. Therefore, not only Converters would need to work based on the inheritance chain, but also need to have access to the target mapped class. I can provide more information if needed. Regards, Michael Nascimento Santos http://threeten.sf.net/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Tue Mar 13 21:58:57 CET 2012</date>
    <body>+1000</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Tue Mar 13 23:55:45 CET 2012</date>
    <body>    http://java.net/projects/jpa-spec/downloads .    OK, thanks -- so would you propose that they simply be listed using the   class element? I would not mind. Potential future annoatted objects would fit as well.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Wed Mar 14 01:43:21 CET 2012</date>
    <body> I fear we get into a philosophical discussion here. That fear is misplaced. It has nothing to do with philosophy. Using interfaces as model for persistent data and their relation for mapping them to relational database is a poor idea. Most likely rooted in confusion about distinct role of interfaces and stateful objects. That's all. Ducktaping may accommodate such ideas -- but it will not improve the quality of JPA specification. Regards -- Pinaki Poddar Chair, Apache OpenJPA Project            http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware To:     jsr338-experts@... Date:   03/14/2012 02:29 AM Subject:        [jsr338-experts] Re: proposal : @Entity on interfaces +1000  I fear we get into a philosophical discussion here. Objects have state *and* behavior, which is what especially proponents of Domain Driven Design emphasize. Why should JPA get in the way of using objects this way? I always considered JPA an API to allow mapping objects onto a relational store, not creating Java representations of data tables.  Cheers,  Ollie  Am 12.03.2012 um 17:44 schrieb Pinaki Poddar: entities. do http://openjpa.apache.org/ entities. multiple that and would to not with the type class</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converters (updated)</header>
    <date>Wed Mar 14 15:10:05 CET 2012</date>
    <body>Sorry, catching up on this thread.  There is one point about which I have a question. Often, the use of annotations is to not require an implementation class of some behavior to implement an interface, like the lifecycle callback annotations (@PrePersist, etc).  Then, the developer has an often mutually exclusive choice to either implement the interface or use the annotations. In this case, it looks like we're offering the interface, AttributeConverter&amp;lt;X,Y&amp;gt;, and the annotation, @Converter.  It seems to me that these should be mutually exclusive or, if both used, only be allowed to used compatibly. To abide by the spirit of the entity lifecycle callback methods, I expected the proposal to look more like this: @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD)     TO_DATABASE,     FROM_DATABASE So that it could be used like this: @AttributeConverter // types inferred from methods below     @Converter(Direction.TO_DATABASE)     @Converter(Direction.FROM_DATABASE) It's behaviorally equivalent and in the same spirit as the entity callback annotations, except that multiple methods would be required within the class.  I'd say users could use the interface or the annotations, but not both. I also feel that the persistence provider could scan the classpath for either annotated classes or classes that implement the interface. Thoughts? -matthew On Mon, Mar 5, 2012 at 7:26 PM, Linda DeMichiel  Here's an update of my original writeup.  This attempts to incorporate  the results of the discussion so far, and cleans up some of the  description.  There are a couple of open issues at the end.  -Linda  ---------------------------  Converters may be specified to provide conversion between the entity  attribute representation and the database representation for  attributes of basic types.  Converters may be used to convert  attributes defined by entity classes, mapped superclasses, or  embeddable classes.  The conversion of all basic types are supported except for the  following: Id attributes, version attributes, relationship attributes,  and attributes explicitly annotated (or designated via XML) as  Enumerated or Temporal.  Auto-apply converters will not be applied to  such attributes, and applications that apply converters to such  attributes through use of the Convert annotation will not be portable.  The persistence provider runtime is responsible for invoking the  corresponding conversion method when loading the entity attribute from  the database and before storing the entity attribute state to the  database.  The persistence provider must apply any conversion mappings  to instances of attribute values used within JPQL or criteria queries  (such as in comparisons, bulk updates, etc.)  before sending them to  the database for the query execution.  If the result of a JPQL or  criteria query includes one or more entity attributes for which  conversion mappings have been specified, the persistence provider must  apply the specified conversions to the corresponding values in the  query result before returning them to the application.  An attribute converter must implement the  javax.persistence.mapping.AttributeConverter interface.  /**   * A class that implements this interface can be used to convert entity   * attribute state into database column representation and back again.   * Note that the X and Y types may be the same Java type.   *   * @param X  the type of the entity attribute   * @param Y  the type of the database column   */     /**      * Converts the value stored in the entity attribute into the data      * representation to be stored in the database.      *      * @param attribute  the entity attribute value to be converted      * @return  the converted data to be stored in the database column      */     /**      * Converts the data stored in the database column into the value      * to be stored in the entity attribute.      * Note that it is the responsibility of the converter writer to      * specify the correct dbData type for the corresponding column for      * use by the JDBC driver, i.e., persistence providers are not expected      * to do such type conversion.      *      * @param dbData  the data from the database column to be converted      * @return  the converted value to be stored in the entity attribute      */  A converter class must be annotated with the Converter annotation or  defined in the object/relational mapping descriptor as a converter.  /**   *  Specifies that the annotated class is a converter and defines its scope   */  @Retention(RUNTIME)    /**     * If set to true, specifies that the converter will automatically     * be applied to all mapped attributes of the specified     * target type for all entities in the persistence unit     * unless overridden by means of the Convert annotation (or XML  equivalent).     * In determining whether a converter is applicable to an attribute,     * the provider must treat primitive types and wrapper types as  equivalent.     *     * Note that Id attributes, version attributes, relationship     * attributes, and attributes explicitly annotated as Enumerated     * or Temporal (or designated as such via XML) will not be converted.     *     * If autoApply is false, only those attributes of the target type     * for which the Convert annotation (or corresponding XML element) has     * been specified will be converted.     *     * If there is more than one converter defined for the same target     * type, the Convert annotation should be used to explicitly specify     * which converter to use.     *     * Note that if autoApply is true, the Convert annotation may be used to     * override or disable auto-apply conversion on a per-attribute basis.     */  Type conversion may be specified at the level of individual attributes  by means of the Convert annotation.  The Convert annotation may also be  used to override or disable an auto-apply conversion.  The Convert annotation may be applied directly to an attribute of an  entity, mapped superclass, or embeddable class to specify conversion of  the attribute or to override the use of a converter that has been  specified as autoApply=true.  When persistent properties are used, the  Convert  annotation is applied to the getter method.  The Convert annotation may be applied to an entity that extends a mapped  superclass to specify or override the conversion mapping for an inherited  basic or embedded attribute.  @Retention(RUNTIME)   /**    * Specifies the converter to be applied.  A value for this    * element must be specified if multiple converters would    * otherwise apply.    */   /**    * The attributeName must be specified unless the Convert annotation    * is on an attribute of basic type or on an element collection of basic    * type.  In these cases, attributeName must not be specified.    */   /**    * Used to disable an auto-apply or inherited converter.    * If disableConversion is true, the converter element should    * not be specified.    */  /**   * Used to group Convert annotations   */  @Retention(RUNTIME)  The Convert annotation is used to specify the conversion of a Basic  (whether explicit or default) field or property.  The Convert annotation  should not be used to specify conversion of the following:  Id attributes, version attributes, relationship attributes, and attributes  explicitly annotated (or designated via XML) as Enumerated or Temporal.  Applications that specify such conversions will not be portable.  The Convert annotation may be applied to a basic attribute or to  an element collection of basic type (in which case the converter  is applied to the elements of the collection).  In these cases, the  attributeName element must not be specified.  Examples:  @Converter  public class BooleanToIntegerConverter implements  @Converter(autoApply=true)  public class EmployeeDateConverter implements  @Entity    @Convert(BooleanToIntegerConverter.class)    ...     // EmployeeDateConverter is applied automatically  // Apply a converter to an element collection of basic type  @ElementCollection  @Convert(NameConverter.class)  // applies to each element in the collection  // Apply a converter to an element collection that is a map of basic values  // The converter is applied to the map *value*  @ElementCollection  @Convert(EmployeeNameConverter.class)  When the Convert annotation is applied to a map to specify conversion  of a map key of basic type, "key" must be used to specify that it  is the map key that is to be converted.  // Apply a converter to a Map key of basic type (relationship)  @OneToMany  @Convert(converter=ResponsibilityCodeConverter.class, attributeName="key")  // Apply a converter to a Map key of basic type (element collection)  @ElementCollection  @Convert(converter=ResponsibilityCodeConverter.class, attributeName="key")  // Disable conversion in the presence of an autoApply converter  @Convert(disableConversion=true)  The Convert annotation may be applied to an embedded attribute or to a  map collection attribute whose key or value is of embeddable type (in  which case the converter is applied to the specified attribute of the  embeddable instances contained in the collection).  In these cases the  attributeName element must be specified.  To override conversion mappings at multiple levels of embedding, a dot  (".")  notation form must be used in the attributeName element to  indicate an attribute within an embedded attribute.  The value of each  identifier used with the dot notation is the name of the respective  embedded field or property.  When the Convert annotation is applied to a map containing  embeddables, the attributeName element must be specified, and "key."  or "value." must be used to prefix the name of the attribute that is  to be converted in order to specify it as part of the map key or map  value.  // Apply a converter to an embeddable attribute  @Embedded  @Convert(converter=CountryConverter.class, attributeName="country")  // Apply a converter to a nested embeddable attribute:  @Embedded  @Convert(converter=CityConverter.class, attributeName="region.city")   ...   // Apply a converter to a nested attribute of an embeddable that is   // a map key of an element collection   @Convert(name="key.region.city", converter=CityConverter.class)   @ElementCollection  @OneToMany  // Apply to an embeddable that is a map key for a relationship  @Convert(attributeName="key.type",  converter=ResponsibilityTypeConverter.class)  The Convert annotation may be applied to an entity class that extends  a mapped superclass to specify or override a conversion mapping  for an inherited basic or embedded attribute.  // Override conversion mappings for attributes inherited from a mapped  superclass  @Entity   @Convert(attributeName="startDate", converter=DateConverter.class),  OPEN ISSUES:  There are still a couple of open issues:  Open Issue:  Conversion of @Id and @Version.  Open Issue:  Explicit listing of converters in persistence.xml file.  I'm not sure I understand what is being proposed here.  Open Issue: What to do about the "specialization" issue that was  raised.  Would someone care to provide some use cases and/or flesh out  that part of the proposal further?  Please let me know if I have missed anything or if there are any other  corrections. --  @matthewadams12 mailto:matthew@ ... skype:matthewadams12 yahoo:matthewadams aol:matthewadams12 google-talk:matthewadams12@... msn:matthew@... http://matthewadams.me http://www.linkedin.com/in/matthewadams</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Wed Mar 14 16:19:09 CET 2012</date>
    <body>FTR, if we think adding *better* support for using interfaces (we already have some interface support, it just requires that the concrete class be specified in the mapping metadata) is going to satisfy the DDD crowd we are deluding ourselves. Their problems with JPA go beyond that simple issue.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Wed Mar 14 16:24:33 CET 2012</date>
    <body>These are O/R mapping classes. Why would they be listed in persistence.xml? The flag to not scan for annotations (the xml-mapping-metadata-complete option) is in the orm.xml mapping file, that is where I would expect to specify these when not using annotations, right?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Wed Mar 14 16:38:22 CET 2012</date>
    <body>but the flag that controls scanning for persistence related classes is in the persistence.xml file as is the list of persistence related classes.  By default the provider is not allowed to scan for classes. Many deployments occur without any orm.xml.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Wed Mar 14 16:59:19 CET 2012</date>
    <body>To me its not about appeasing crowds.  Its about removing barriers to popular development models.  Whether you agree with it or not, interface driven development is a popular development model even in the domain model. I am not sure what the end result of such discussions are supposed to be in terms of a proposal making it into the spec or not.  But this is starting to feel like its going in circles.  If this is something the rest of the group feels should not be in JPA thats fine.  I can easily do this in Hibernate.  But thats yet another provider portability concern.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Wed Mar 14 17:00:25 CET 2012</date>
    <body> Using interfaces as model for persistent data and their relation for  mapping them to relational database is a poor idea. Most likely rooted in  confusion about  distinct role of interfaces and stateful objects. That's all. Taken without context, I agree with you, Pinaki.  It is odd to think about using interfaces within domain object models, especially if you think of them in the context of dependency injection and the like. However, now that I've written some behaviorally rich, persistent domain object models using Java &amp;amp; AspectJ, that the use of interfaces within an object model does have some benefit, but it takes a more mature attitude than just black &amp;amp; white towards the use of interfaces.  As in life, everything is a shade of grey. The kind of interfaces I'm talking about using in a persistent domain model are not "servicy", but "traity".  That is, the interfaces that I've found have a place within the domain object model layer are not coarse-grained like a service-oriented interface.  They are more like stateful traits (a la Scala) that an entity may exhibit.  For example, I was working in the legal domain, and it turned out that many of the entities we were modeling had court orders associated with them.  We naturally identified a HasOrders interface, which was the contract for any entity that had court orders associated with it, which was simply a unidirectional collection of documents.  We wrote an AspectJ ITD that was applied to any class declaring that it implemented the HasOrders interface.  With default mapping metadata inside the ITD, it was a breeze to add this behavior basically as a stateful trait, to any of our entities that needed it.  Programmatically, we could treat collections of HasOrders instances just fine, but we did run into issues when some other entity needed to persist a field of type HasCourtOrders or a collection thereof. In these cases, and with the directions that Java appears to be going toward in JDK 8 (basically, virtual extension methods as stateless traits), I think that stateful traits might be in our future--not our near future, mind you, but sometime, I think. To me, the allowance for the persistence of interfaces is not a question of ability, but one of difficulty.  To me, the difficulty is that much is evident simply by annotating an interface as persistent and observing which @Entity classes implement it.  Also, the ability to query over extents of persistent interfaces and through fields of interface types doesn't seem too difficult, either, because the metadata contains sufficient information to formulate a query that includes all candidate instances that implement the interface type. The difficulty lies in the mapping of polymorphic fields of interface types (and of type java.lang.Object), because you not only have to map the id of the referenced entity, but also its class. If we were to identify a portable means by which to map the implementation class of the polymorphic reference along with its id, we'd be able to fully support persistent interfaces.  Perhaps we use the value of the "name" attribute of the concrete class's @Entity attribute as an additional field.  Perhaps we identify some other way. The bigger question, to me, is simply should we specify it, and if we do, should it be done in the 2.1 timeframe? -matthew</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Wed Mar 14 17:11:16 CET 2012</date>
    <body>There is an exclude-unlisted-classes setting in the persistence.xml file that *disables* scanning for entities, embeddables and mapped superclasses, but by default the provider is supposed to scan (when running in a container - outside the container it is not defined). This option was only added to enable a persistence unit to disable some classes from being considered even though they were in the same JAR. In any case, my point was that this is an object-relational mapping class and by design we have always tried to keep the O/R mapping concepts in the orm.xml file (when we are talking about not relying upon the annotation sensing, which I believe was the point).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Wed Mar 14 17:18:36 CET 2012</date>
    <body>exclude-unlisted-classes is true by default and so by default scanning by the provider must be *enabled*.  The scanning the container performs is defined elsewhere.  The only thing being discussed here is the auto scanning and how the provider should be notified of the converter classes when scanning is not enabled.  Requiring an orm.xml file just to list the converter classes seems out of place.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Wed Mar 14 17:27:44 CET 2012</date>
    <body>Sure, interface driven development is popular in general, but I've not seen that doing so with entities is. That's just my experience. The end results come after the discussion happens, and the discussion is necessary to get as much information about the feature as possible. Let's let all of the opinions get expressed and the value of the proposal get assessed. This spec stuff takes time for a reason :-) Are you saying this would be the only feature that Hibernate does beyond the spec? ;-)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Wed Mar 14 17:42:47 CET 2012</date>
    <body>Thats probably just my newness to the spec game :) Well I did specifically say "yet another" ;)  This one is just more troubling to me for some reason which I can't quite verbalize yet.  I guess its the nature of an explicit JPA API call behaving totally different.  javax.persistence.metamodel.Metamodel#entity( MyInterface.class ) working or not working.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Wed Mar 14 17:49:59 CET 2012</date>
    <body>No, by default it is false (but I hope that is what you mean?) Correct. We're not talking about container scanning, just provider scanning. Yes. So far you have mostly just restated what I said above, so we're good up to this point :-) Putting mapping information in our mapping file is exactly the right place, and is the way we designed it from the beginning. Listing them in with the entities/embeddables and mapped superclasses is clearly more convenient, but not architecturally the correct place for them to be. I guess we need to decide whether we want to go with architectural consistency or convenience. Both have merits and are valid arguments.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Converters and Enum</header>
    <date>Wed Mar 14 18:28:55 CET 2012</date>
    <body>My first question would be to ask why you can't use Bean Validation for some of this. Emmanuel would probably be interested in finding out more about your use case and what kinds of things validation might need to add to accommodate it :-) Not sure if I understand your scenario, but I took away from it that you will either need to have access to the mapped class in which the converted attribute resides, or be able to override the converter for a given type to be different for the subtypes. Not sure if that is close, but maybe some code snippets would help... -Mike</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Wed Mar 14 18:31:15 CET 2012</date>
    <body>Am 14.03.2012 um 17:27 schrieb michael keith:  Sure, interface driven development is popular in general, but I've not   seen that doing so with entities is. That's just my experience. Well, you currently can't do it, that's probably why :). I've seen tons of  project where this would have been helpful in structuring and modularizing a  codebase but people had to live with a monolithic "core" which contained all  entities. Cheers, Ollie Attachment: signature.asc Description: Message signed with OpenPGP using GPGMail</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Wed Mar 14 18:37:50 CET 2012</date>
    <body>No, the default is true : "&amp;lt;xsd:element name="exclude-unlisted-classes" type="xsd:boolean" default="true" minOccurs="0"&amp;gt;" and portable Java SE applications must list all managed persistence classes (section 8.2.1.6.4) . No, we disagree on whether @Converter classes will always be discovered.  Emmanuel's point and my point is that by default in Java SE there is no way for a provider to find the *annotated* auto-apply Converter classes.   Users should have the option of listing the annotated Converter classes within the list of managed classes in the persistence.xml file. I do not believe anyone is proposing listing converter classes within other managed classes (ie .entities/embeddables and mapped superclasses).  That seems out of place to me. If it is architecturally inconsistent then why have the list of annotated managed classes at all in the persistence.xml file ?  There is nothing inconsistent about continuing a configuration pattern that is already defined in the specification. For clarity, an element for converters should still be added to the persistence-unit element in orm.xml for un-annotated classes but I assumed that would be added later when the xsds are updated.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Wed Mar 14 19:36:38 CET 2012</date>
    <body>Currently @Converter has only one attribute, autoApply, although we have open issues around scoping (extending to subclasses, etc). It occurs to me that we could approach autoApply converters as we do default entity listeners, and require them to be listed in the orm.xml.  If we did this, we could dispense with the @Converter annotation until such time as we needed it to further refine the semantics. Opinions?  Fire away :-) -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: updated spec draft (converters)</header>
    <date>Wed Mar 14 19:47:37 CET 2012</date>
    <body>When I say the default value I am talking about what the value is assumed to be when the element is not specified at all in the persistence.xml file in a container environment. In that case a false value is assumed. The "default" element in the XML schema only applies if you say &amp;lt;exclude-unlisted-classes/&amp;gt;. It is set to true for convenience, so one doesn't have to say Yes, to be portable in SE  outside the container an application must list all of the classes. We did not define whether a provider does or doesn't do scanning. A portable application just can't assume that it does. The "exclude-unlisted-classes" option that started this whole discussion was, as I mentioned earlier, targeted at the container scenario, so that is what I was discussing primarily. In a container, annotation scanning *does* take place by default, and having the entries in the XML mapping file is only required if this option was turned on. If we can agree on that then we can go on to discussing the situation outside the container (when the exclude-unlisted-classes option is undefined, but when the providers are minimally expected to use the classes listed in persistence.xml). Neither was I. I said "in with", meaning "in" the &amp;lt;classes&amp;gt; element "with" the other entities and embeddables. It was where the entity objects were listed in Java SE. I guess now we are having the discussion that you thought we were having earlier but that I didn't know we were having? :-) The comparison still exists between the entities and other objects, like entity listeners which needed to be in the mapping file. The only difference is that converters are identified by annotations. Sure.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Wed Mar 14 20:09:31 CET 2012</date>
    <body>That would certainly be consistent. Not as convenient, but definitely consistent :-) Another approach would be to go back to defining the annotation on an entity (i.e. like named queries, id generators and many other global things that we already define) and point to the converter class. Example: @Converter(class=MyConverter.class  autoApply=true) @Entity</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Wed Mar 14 20:14:04 CET 2012</date>
    <body>That seems to me much less convenient.  Unlike named queries (which are just  strings, rather than objects), we don't need a class to attach converters to in order  to define them outside of XML.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Wed Mar 14 21:07:02 CET 2012</date>
    <body>It's true, but there's something to be said for having a path to the class from a managed class.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converters (updated)</header>
    <date>Wed Mar 14 21:57:11 CET 2012</date>
    <body>Hi Matthew, I know what you mean and went through a similar thought process to yours. In the end, the typing of the converter interface was compelling enough to warrant using it. Describing the method constraints would be a lot more work that just seemed unnecessary and more error-prone during development. On the scanning end, providers have not thus far had to do any class-scanning based on interface implementation, so the annotation made some sense. It also gives us a nicer place to add the extra autoApply option (although in the absence of the annotation I suppose we could just add an autoApply() method to the interface). -Mike</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Thu Mar 15 01:21:52 CET 2012</date>
    <body>As I read this thread, I keep thinking of package annotations in package-info.java for global (or near-global, maybe for the annotated package and optionally subpackages) configuration.  Could this help out here?  It's true, but there's something to be said for having a path to the class  from a managed class. http://java.net/projects/jpa-spec/downloads . --  @matthewadams12 mailto:matthew@ ... skype:matthewadams12 yahoo:matthewadams aol:matthewadams12 google-talk:matthewadams12@... msn:matthew@... http://matthewadams.me http://www.linkedin.com/in/matthewadams</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Fri Mar 16 16:20:18 CET 2012</date>
    <body> It occurs to me that we could approach autoApply converters as we  do default entity listeners, and require them to be listed in the  orm.xml.  If we did this, we could dispense with the @Converter  annotation until such time as we needed it to further refine the  semantics. Every time I write configuration in XML, I die a little.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Fri Mar 16 16:20:16 CET 2012</date>
    <body>It could, Hibernate uses something like that but you still need to list the  packages when scanning is disable. Back to square one :)  As I read this thread, I keep thinking of package annotations in  package-info.java for global (or near-global, maybe for the annotated  package and optionally subpackages) configuration.  Could this help  out here?   http://java.net/projects/jpa-spec/downloads .        --   @matthewadams12   mailto:matthew@ ...  skype:matthewadams12  yahoo:matthewadams  aol:matthewadams12  google-talk:matthewadams12@...  msn:matthew@...   http://matthewadams.me   http://www.linkedin.com/in/matthewadams</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Fri Mar 16 22:52:55 CET 2012</date>
    <body>Steve, all, As you might surmise from the responses on this thread, I'm not seeing enough consensus to pursue this proposal further.  It is something that could be reconsidered in JPA 2.2 should opinions change in the interim. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] @Where Annotation for filtering a to-many relationship</header>
    <date>Sat Mar 17 14:43:16 CET 2012</date>
    <body>Hi, some JPA provider support a @Where annotation, e.g Hibernate, for filtering a to-many relationship. This annotation is very useful if you are using soft deletes instead of physical deletes. Without the annotation, it is not possible to use a getter to retrieve a collection of entities without all soft-deleted entities from the database. So I would like to propose this annotation for the JPA specification. Thanks a lot, Heinz -- Heinz Wilming Diplom Wirtschaftsinformatiker (FH)  akquinet tech@spree GmbH Bülowstrasse 66, D-10783 Berlin Fon:    +49 (0)30 235 520 - 0 Fax:    +49 (0)30 217 520 - 12 E-Mail:   heinz.wilming@... www.akquinet.de akquinet tech@spree GmbH, Berlin Geschäftsführung: Martin Weber, Dr. Torsten Fink Amtsgericht Berlin-Charlottenburg HRB 86780 B USt.-Id. Nr.: DE 225 964 680</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] BNF question: how to parse the example containing KEY, VALUE (and LIKE)</header>
    <date>Mon Mar 19 14:54:51 CET 2012</date>
    <body>Dear group, While investigating how to parse queries like this (all page/chapter references refer to EDR 2.1, Dec 19th, 2011): SELECT p FROM Person p, in(pc.firstnames) n WHERE n = 'Joe' (firstnames is a List&amp;lt;String&amp;gt;) I came across the following example provided in the spec, which is related in structure: section 4.4.4, p. 153: SELECT i.name , VALUE(p) FROM Item i JOIN i.photos p WHERE KEY(p) LIKE ‘%egret’ I looked at the BNF how this is parsed, and did not find any derivation for the FROM clause: where_clause ::= WHERE conditional_expression conditional_expression ::= conditional_term conditional_term ::= conditional_factor conditional_factor ::= conditional_primary conditional_primary ::= simple_cond_expression simple_cond_expression ::= like_expression like_expression ::= string_expression LIKE pattern_value So eventually I end up with 'string_expression', from which I can not derive a 'qualified_identification_variable' string_expression ::= state_field_path_expression | string_literal | input_parameter | functions_returning_strings | aggregate_expression | case_expression | function_invocation | (subquery) Did I miss something, or is there actually a rule missing in the grammar for this? Thank you! Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: BNF question: how to parse the example containing KEY, VALUE (and LIKE)</header>
    <date>Mon Mar 19 20:55:40 CET 2012</date>
    <body>Hi Christian, Yes, you are right.  The problem is that  composable_qualified_identification_variable is currently assumed to not designate a state field, which it should. Not sure yet what the best fix for this is in the grammar yet. Thanks for pointing this out. -Linda \</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: BNF question: how to parse the example containing KEY, VALUE (and LIKE)</header>
    <date>Mon Mar 19 23:26:23 CET 2012</date>
    <body>Hi Linda, Hi Christian, Dear group, While investigating how to parse queries like this (all page/chapter references refer to EDR 2.1, Dec 19th, 2011): SELECT p FROM Person p, in(pc.firstnames) n WHERE n = 'Joe' (firstnames is a List&amp;lt;String&amp;gt;) I came across the following example provided in the spec, which is related in structure: section 4.4.4, p. 153: SELECT i.name , VALUE(p) FROM Item i JOIN i.photos p WHERE KEY(p) LIKE ‘%egret’ I looked at the BNF how this is parsed, and did not find any derivation for the FROM clause: where_clause ::= WHERE conditional_expression conditional_expression ::= conditional_term conditional_term ::= conditional_factor conditional_factor ::= conditional_primary conditional_primary ::= simple_cond_expression simple_cond_expression ::= like_expression like_expression ::= string_expression LIKE pattern_value So eventually I end up with 'string_expression', from which I can not derive a 'qualified_identification_variable' string_expression ::= state_field_path_expression | string_literal | input_parameter | functions_returning_strings | aggregate_expression | case_expression | function_invocation | (subquery) Did I miss something, or is there actually a rule missing in the grammar for this?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: BNF question: how to parse the example containing KEY, VALUE (and LIKE)</header>
    <date>Mon Mar 19 23:47:56 CET 2012</date>
    <body>Hi Michael, I believe so, but I think we have to go further.  I.e., I think we need to  support this also in in_expression, function_arg, case_operand, aggregate_expression, and  entity_or_value_expression. Do you agree? regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Converters and Enum</header>
    <date>Tue Mar 20 03:39:55 CET 2012</date>
    <body> My first question would be to ask why you can't use Bean Validation for some  of this. Emmanuel would probably be interested in finding out more about  your use case and what kinds of things validation might need to add to  accommodate it :-) It has absolutely nothing to do with Bean Validation. Believe me, I am on the EG for that one ;-)  Not sure if I understand your scenario, but I took away from it that you  will either need to have access to the mapped class in which the converted  attribute resides, or be able to override the converter for a given type to  be different for the subtypes. Not sure if that is close, but maybe some  code snippets would help... Basically, for when you read data from the PreparedStatement instance, you need to convert it to the proper Enum instance. For that, you need to use Enum.valueOf(Class&amp;lt;T&amp;gt; enumType, String name) . Therefore, access to the class will be needed somehow. Currently, the only way is either to build one instance per Enum class, which is somewhat ridiculous giving everything else is the same. It would make much more sense to have access to the actual target class as a parameter so this wouldn't be needed. I hope my use case is clear now. Michael Nascimento Santos http://threeten.sf.net/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Custom Sequence Generation</header>
    <date>Wed Mar 21 00:10:28 CET 2012</date>
    <body>Hi Are there any plans to standardise the API to register a custom sequence generator? I can't find a reference in draft 6 that explains how custom sequences are registered with the JPA provider, and the EclipseLink code below uses a proprietary API.  It would be great if there was a portable sequence/id generator Interface. Thanks Andrew Ps: thanks for responding to my Enum question Mike. andreww100@... Hi Expert Group Are there plans to standardise the mechanism of building custom sequence generators? I could not find any sequence generator enhancements in Early Draft 5. Existing tools provide proprietary extensions to accomplish this.  EclipseLink has org.eclipse.persistence.sequencing.Sequence http://wiki.eclipse.org/EclipseLink/Examples/JPA/CustomSequencing Hibernate has org.hibernate.id.SequenceGenerator http://www.georgestragand.com/jpaseq.html For example, you can imagine people wanting to build sequences that include check digits, or custom formatting. Many thanks Andrew Ward</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: BNF question: how to parse the example containing KEY, VALUE (and LIKE)</header>
    <date>Thu Mar 22 23:50:40 CET 2012</date>
    <body>Title: Per.book Hi Linda, Hi Michael, Hi Linda, Hi Christian, Dear group, While investigating how to parse queries like this (all page/chapter references refer to EDR 2.1, Dec 19th, 2011): SELECT p FROM Person p, in(pc.firstnames) n WHERE n = 'Joe' (firstnames is a List&amp;lt;String&amp;gt;) I came across the following example provided in the spec, which is related in structure: section 4.4.4, p. 153: SELECT i.name , VALUE(p) FROM Item i JOIN i.photos p WHERE KEY(p) LIKE ‘%egret’ I looked at the BNF how this is parsed, and did not find any derivation for the FROM clause: where_clause ::= WHERE conditional_expression conditional_expression ::= conditional_term conditional_term ::= conditional_factor conditional_factor ::= conditional_primary conditional_primary ::= simple_cond_expression simple_cond_expression ::= like_expression like_expression ::= string_expression LIKE pattern_value So eventually I end up with 'string_expression', from which I can not derive a 'qualified_identification_variable' string_expression ::= state_field_path_expression | string_literal | input_parameter | functions_returning_strings | aggregate_expression | case_expression | function_invocation | (subquery) Did I miss something, or is there actually a rule missing in the grammar for this?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: BNF question: how to parse the example containing KEY, VALUE (and LIKE)</header>
    <date>Fri Mar 23 01:27:28 CET 2012</date>
    <body>Hi Michael, Thanks for following up on this.  More below.... Yes, orderBy_item needs to be covered as well. I am hesitating about adding composable_qualified_identification_variable to state_field_path_expression though, because a key may often not be a state field of the object that is the map value.  I'm also concerned about the order-by rules.  E.g., @Entity   ...   @OneToMany  // map key is division name The following query shouldn't work, because the key is not part of the result. SELECT vp FROM Company c JOIN c.organization vp WHERE vp.reports &amp;gt; 100 ORDER BY key(vp) So, I was thinking of just either adding  composable_qualified_identification_variable to the needed places (ugly) or defining another non-terminal, e.g., generalized_state_expression::= state_field_path_expression |  composable_qualified_identification_variable. What do you think? regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] lockModes and eager fetching</header>
    <date>Fri Mar 23 14:30:37 CET 2012</date>
    <body>Dear group, Maybe this has been discussed already -- my apologies, if this is the case: Is there any propagation of lockModes along with eager fetching? From my understanding of the spec it seems, that the lockMode used in most operations (e.g. find, refresh) is only applied to the entities which are the direct argument of these operations and not to any entities which are loaded via eager fetching along with these objects. (Also, if an referred object was not eagerly fetched, but is eventually loaded (e.g. by navigation), the current lockMode of the owner of the reference is not applied for this lazily fetched object.) Please let me know, whether my understanding is correct. Thank you! Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: lockModes and eager fetching</header>
    <date>Fri Mar 23 21:55:12 CET 2012</date>
    <body>Hi Christian, Yes. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: BNF question: how to parse the example containing KEY, VALUE (and LIKE)</header>
    <date>Mon Mar 26 23:14:25 CEST 2012</date>
    <body>Hi Linda, more below ... Hi Michael, Thanks for following up on this.  More below.... Hi Linda, Hi Michael, Hi Linda, Hi Christian, Dear group, While investigating how to parse queries like this (all page/chapter references refer to EDR 2.1, Dec 19th, 2011): SELECT p FROM Person p, in(pc.firstnames) n WHERE n = 'Joe' (firstnames is a List&amp;lt;String&amp;gt;) I came across the following example provided in the spec, which is related in structure: section 4.4.4, p. 153: SELECT i.name , VALUE(p) FROM Item i JOIN i.photos p WHERE KEY(p) LIKE ‘%egret’ I looked at the BNF how this is parsed, and did not find any derivation for the FROM clause: where_clause ::= WHERE conditional_expression conditional_expression ::= conditional_term conditional_term ::= conditional_factor conditional_factor ::= conditional_primary conditional_primary ::= simple_cond_expression simple_cond_expression ::= like_expression like_expression ::= string_expression LIKE pattern_value So eventually I end up with 'string_expression', from which I can not derive a 'qualified_identification_variable' string_expression ::= state_field_path_expression | string_literal | input_parameter | functions_returning_strings | aggregate_expression | case_expression | function_invocation | (subquery) Did I miss something, or is there actually a rule missing in the grammar for this?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: BNF question: how to parse the example containing KEY, VALUE (and LIKE)</header>
    <date>Mon Mar 26 23:20:42 CEST 2012</date>
    <body>Hi Michael, Yes.  I agree that this should be legal. Yes, only I would cover this as a separate rule (to make it very clear). Yes. Thanks again!  It looks like we're on the same page here. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] support for multitenancy</header>
    <date>Tue Mar 27 01:54:33 CEST 2012</date>
    <body>One of the main items on the agenda for the JPA 2.1 release is support for multitenancy in Java EE 7 cloud environments. In Java EE 7, an application can be submitted into a cloud environment for use by multiple tenants in what can be viewed as a basic form of software as a service (SaaS).  The application is customized and deployed on a per-tenant basis.  At runtime, there is a separate application instance (or set of instances, e.g., in a clustered environment) per tenant.  The instances used by different tenants are isolated from one another.  The resources used by a tenant's application may also be isolated from one another, or may be shared. In general, however, it is assumed that a tenant's data is isolated from that other tenants. There are three well-known approaches to support for multitenancy at the database level:   (1)  separate database approach   (2)  shared database / separate schema approach   (3)  shared schema / shared table approach To get the discussion started, this is a high-level strawman sketch of how the 3 approaches might be used with JPA in keeping with the Java EE 7 approach.  At the same time, however, we also want to be sure that what we specify in JPA 2.1 can be extended to encompass a more general approach to SaaS in the future in which a single application instance serves multiple tenants and in which multitenancy is managed by the Java EE environment. For further information on how Java EE 7 is approaching PaaS/SaaS, you might find the documents on the javaee-spec.java.net project useful, particularly  http://java.net/projects/javaee-spec/downloads/download/PaaS.pdf and the latest draft of the Java EE 7 Platform spec, http://java.net/projects/javaee-spec/downloads/download/JavaEE_Platform_Spec.pdf . Note that the identifier for the tenant will be made available to the application in JNDI as java:comp/tenantId.  The tenantId will be a string, whose max length should allow it to be portably stored in a single database column. APPROACHES: (1) Separate database approach In this approach, each tenant's persistence unit is mapped to a separate database.  This approach provides the greatest isolation between tenants and does not impose any additional constraints over the object/relational mapping of the persistence unit or over the operations that can be performed.  In particular, the use of multiple database schemas or catalogs are supported as are native queries. In some cloud environments, use of this approach might not be available, as a tenant might be allocated storage within a database rather than a separate database. (2) Shared database / separate schema  approach In this approach, each tenant's data is stored in database tables that are isolated from those of any other tenant.  In databases that support schemas, this will typically be achieved by allocating a separate schema per tenant.  The database's permissions facility is used to confine a tenant's access to the designated schema, thus providing isolation between tenants at the schema level. Support for this approach is straightforward if the persistence unit uses only the default schema or catalog (i.e., if it does not specify schema names or catalogs in the object/relational mapping metadata). A native query that attempts to access data in a schema other than that assigned to the tenant by the platform provider will be trapped by the database authorization mechanisms and will result in an exception. [While the case where the persistence unit metadata explicitly specifies one or more schemas could potentially be handled by the persistence provider by remapping schema names and native queries that embed schema names, I would not propose that we specify or require support for this case, although a more sophisticated persistence provider might choose to support it.] (3) Shared table approach In this approach, database tables are shared ("striped") across tenants. It is the reponsibility of the persistence provider to provide per-tenant isolation in accessing data.  This will typically be done by mapping and maintaining a tenant ID column in the respective tables, and augmenting data retrieval and query operations, updates, and inserts with tenant IDs.  The use of native queries would need to be trapped by the persistence provider and not allowed unless the persistence provider were able to modify them to provide isolation of tenant data. Ideally, the management of the tenant id should be transparent to the application, although we should revisit this in Java EE 8 as we move further into support for SaaS. I believe that the main use case for the shared table approach is in SaaS environments in which a single application instance is servicing multiple tenants.  This is outside the scope of Java EE 7, so I don't think that we need to standardize on support for this approach now, although we should not lose sight of it as we standardize on other aspects. DETERMINING THE MULTITENANCY STORAGE MAPPING STRATEGY: We see two general approaches to determining the multitenancy storage mapping strategy that should be used for a persistence unit.  In some cases, these approaches might be combined. Again, note that a cloud platform provider might use a single strategy for all tenants in allocating database storage.  For example, each tenant might be allocated a separate database, or each tenant might only be allocated a schema within a database. (A)  The Application Specifies Its Requirements In this approach, the application specifies its functional requirements (in terms of need for named, multiple schemas and/or use of native queries) in the persistence.xml descriptor, and the deployer and/or cloud platform provider determine the storage strategy that is used for the tenant.  This metadata serves as input to the deployer for the tenant or as input into the automated provisioning of the application by the cloud platform provider (if automated provisioning is supported by the platform instance). For example, an application might specify that it requires support for multiple schemas and native queries.  In general, such requirements would mean that a separate database would need to be provisioned for the tenant.  If this is not possible, then unless the platform provider supported a persistence provider that could perform schema remapping and/or modification of native queries, the application might fail to deploy or fail to initialize.  On the other hand, if an application specifies that it uses only the default schema and native queries, then either the separate database or separate schema approach could be used. (B)  The Application Specifies the Multitenancy Storage Mapping Strategy An alternative approach is that the application specifies the required (or preferred) multitenancy storage mapping strategy in the persistence.xml. For example, a multitenant application that is designed with the intention that separate databases be used might indicate this in the persistence.xml as multitenancy = SEPARATE_DATABASE. An application that is designed with the intention that databases may be shared by partitioning at the database schema level might indicate this in the persistence.xml as multitenancy = SHARED_DATABASE.  [A portable application that specifies this strategy should not specify schema or catalog names, as it might otherwise fail to deploy or fail to initialize.] An application that is designed with the intention that tables be shared might indicate this in the persistence.xml as multitenancy = SHARED_SCHEMA.  An app that uses explicit multitenant mapping metadata would be expected to specify this. [Open Issue: Is it useful to specify requirements along the lines of those used in approach (A) with this approach?  If so, is the platform provider allowed to choose a different mapping strategy as long as that approach is more isolated?  If no functional requirements are specified as in approach (A) and if a mapping strategy is specified in the persistence.xml that is provided by the application submitter, then if this information is not observed, the risk is that the app will fail.  For example, observation of the specified mapping strategy might be required for the case where explicit multitenant mapping metadata is supplied for the striped mapping approach.] With both the approaches (A) and (B), different storage mapping strategies may be used for different tenants of the same application if the cloud platform provider supports a range of storage mapping choices. REQUIREMENTS FOR PORTABLE APPLICATIONS Applications that are intended to be portable in cloud environments should not specify schema or catalog names. DEPLOYMENT When an application instance is deployed for a tenant, the container needs to make the tenant identifier and tenant-related configuration information available to the persistence provider.  The container needs to pass to the persistence provider a data source that is configured with appropriate credentials for the tenant, and which will provide isolation between that tenant and other tenants of the application.  We should probably also define an interface to capture the tenant identifier and tenant-related metadata and configuration information that the container needs to pass to the persistence provider, e.g., a TenantContext. OTHER OPEN ISSUES 1. Additional metadata to support schema generation. 2. Do we need metadata to indicate whether an application supports    multitenant use -- i.e., whether it is "multitenant enabled"?    Do we need this information specifically for JPA? 3. Specification of resources that are shared across tenants--e.g.,    a persistence unit for reference data that can be accessed by    multiple tenants.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Tue Mar 27 03:06:14 CEST 2012</date>
    <body>Is the proposal allowing mixing of strategies within a deployment?  For example, can an application use a combination of SHARED_SCHEMA and SHARED_TABLE within a single persistence unit?  I did not see that specifically addressed (apologize if I missed it) and just wanted to make sure.  Specifically it is the mixing of "discriminated" (what you call "shared table") multi-tenancy and "non-discriminated" (separate database or shared database) I am concerned about. With regard to "DETERMINING THE MULTITENANCY STORAGE MAPPING STRATEGY", I think there should be an option for the developer/deployer to chose an explicit strategy.  Whether that means in addition to the ability to name "functional requirements", I am not sure.  If JPA goes the functional requirement route and allows platform providers to choose which strategy to use, also makes sense to allow the platform provider to query the JPA provider to see what it supports, which requires adding that information as well.  That way the platform provider can choose between the numerous JPA providers it might have available. Even though we don't want to focus on "shared table" with this rev, we need to consider it.  At a minimum we will need metadata support to name the column that indicates which rows belong to which tenants. In my experience with implementing this in Hibernate, it is nice to have a flag saying whether multitenancy is enable for the unit and what strategy.  But this latter part ties in with your strategy (A) and (B) question.  Another option is to let the application specify (a) which strategies it can use and (b) which strategy it prefers.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: BNF question: how to parse the example containing KEY, VALUE (and LIKE)</header>
    <date>Tue Mar 27 09:14:40 CEST 2012</date>
    <body>Hi Linda and Michael, We do also need to cover the special short-cut case, that "VALUE(identification_variable)" is equivalent to just "identification_variable" Cheers, Christian On Mon, Mar 26, 2012 at 11:20 PM, Linda DeMichiel linda.demichiel@... Hi Michael, Hi Linda, more below ... Hi Michael, Thanks for following up on this.  More below.... Hi Linda, Hi Michael, Hi Linda, Hi Christian, Dear group, While investigating how to parse queries like this (all page/chapter references refer to EDR 2.1, Dec 19th, 2011): SELECT p FROM Person p, in(pc.firstnames) n WHERE n = 'Joe' (firstnames is a List&amp;lt;String&amp;gt;) I came across the following example provided in the spec, which is related in structure: section 4.4.4, p. 153: SELECT i.name http://i.name FROM Item i JOIN i.photos p WHERE KEY(p) LIKE ‘%egret’ I looked at the BNF how this is parsed, and did not find any derivation for the FROM clause: where_clause ::= WHERE conditional_expression conditional_expression ::= conditional_term conditional_term ::= conditional_factor conditional_factor ::= conditional_primary conditional_primary ::= simple_cond_expression simple_cond_expression ::= like_expression like_expression ::= string_expression LIKE pattern_value So eventually I end up with 'string_expression', from which I can not derive a 'qualified_identification_ variable' string_expression ::= state_field_path_expression | string_literal | input_parameter | functions_returning_strings | aggregate_expression | case_expression | function_invocation | (subquery) Did I miss something, or is there actually a rule missing in the grammar for this?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Tue Mar 27 19:38:45 CEST 2012</date>
    <body>Hi Steve, It isn't preventing it.  However, in this release I am not proposing that we  address this, as I see the main rationale for usage of the SHARED_TABLE approach to be in the  single-instance SaaS case. Yes, I did consider this, but didn't include it in the writeup, because it  wasn't clear to me how realistic it was, and it would definitely add complexity.  I'd like to  get feedback from the group on this aspect though.  Do we really expect both that more than one  provider would be available in a PaaS environment, and, if so, also that the application  wouldn't be dependent on a specific provider (and therefore wire that provider choice  into the persistence.xml)? Hmmmm.....  Specifying SHARED_SCHEMA means that SEPARATE_DATABASE would work  as well, so you wouldn't even need to state that.  But if SHARED_TABLE were specified you'd  still need to know whether you need a separate database or just a schema, so you'd need  the additional SHARED_SCHEMA or SEPARATE_DATABASE to be specified along with it.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: BNF question: how to parse the example containing KEY, VALUE (and LIKE)</header>
    <date>Tue Mar 27 20:08:04 CEST 2012</date>
    <body>Hi Christian, I had intended that we would treat KEY and VALUE the same in the below. Did you mean more than that? regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Tue Mar 27 20:47:18 CEST 2012</date>
    <body>Hi Linda, Thanks for writing all this up. Some comments inline. -Mike That seems like the right default assumption to make. A config option made to/by the resource consumer (in our case the JPA provider) would override that assumption, I suppose? So this is basically what JPA assumes today. So, in summary,  portable apps may not specify a schema or catalog at any level: mapping (annotation or XML), mapping file, persistence unit default,  or in a native query. So, portable applications could not use either schemas or native queries in this mode, and there will be an opportunity for the application to be able to map the tenant id column in each table. For the application to not have to manage tenant ids, I guess the tenant identifier would need to be available to the provider on a per-invocation basis (in a thread context set by the container)? As you mention, not something that we necessarily have to worry about now,  but just so we know what we will need in the future if this is what we want. Yes, there is some value in this being available today, though, given that some people are doing multitenancy in their own environment, outside the cloud. I guess it just depends how far we want to go to enable SaaS in JPA in this round. I'm less enamored with this approach. Although many cloud platforms are going to support both an internally hosted DBaaS as well as access to an external DB, my guess is that they won't have multiple different ways of implementing their internally hosted database services (e.g. one as a separate DB and one with striped data). I could be wrong, but realistically I don't think a cloud provider is ever going to implement a db service using striping. As was mentioned above, a SaaS application might decide to use its database that way. Basically, the restriction that schemas not be used in portable cloud apps is enough, I think, for cloud applications. Any additional requirements or relaxations are cloud specific. This is a preferable approach, and even though it may not be *necessary* for cloud deployment, it would be nice to have these options so the provider can do some checking at deployment time rather than the app failing at runtime. It would also provide a standard way of configuring for striping in SaaS apps. In general I don't think they would even need to specify this, since this is what we already assume, isn't it? This probably doesn't matter, but although I find the terminology easy to understand, from a PaaS user perspective the line between 1 and 2 might be a little fuzzy because most of the cloud providers have some kind of "database service", but the capabilities of those services differ. In some cases one can create db instances and schemas (SEPARATE DB), yet and in other cases the tenant "database" is just a place to store data, with a default schema and no ability to create a new one (SHARED DB). This sounds very reasonable to me and solves 99% of the cloud JPA app scenario. Again, this would definitely help to enable JPA in SaaS apps. We might want to rename this to what it actually does -- table generation :-) Again, it is not strictly required for PaaS, but it would be really nice to have it so SaaS cound be enabled, even though it is not formally supported. I'm not sure we need to solve this problem at this stage. Multiple tenants accessing a shared read-only resource through identical JPA configurations is one thing, but having a single shared persistence unit spanning multiple applications seems out of scope.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Wed Mar 28 00:15:14 CEST 2012</date>
    <body>Hi Mike, Whether a database was used by multiple tenants (with isolated data) would be determined by the tenant (e.g., an SLA might stipulate a separate database, assuming that option were available, or the tenant's deployer's might) or  would be determined by the cloud platform provider.   I don't see the JPA provider overriding, at least in Java EE 7 case, but maybe I'm missing what you  intended. Right Right Right Right.  For shared application instances we will need this. Right.  This would be "application-managed SaaS" in my terminology :-) I'm not opposed to considering it in this release, although it depends on time constraints.  But it might be advantageous to get more vendor experience first before we standardize. I don't think a cloud provider would do this either. But in a cloud environment that may not be the default storage option, so this would indicate that the application was written to *require* use of that strategy. That would certainly be more precise for the separate schema case :-) But if the  application instance "owned" the database, the persistence  provider might be creating schemas. I was assuming the former -- i.e., the tenants had identical persistence unit  configurations for the resource, but no sharing at the EMF level -- i.e., separate EMF per  application instance.  Things certainly do get more interesting when we move in the SaaS  case where application instances are multitenant :-)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: BNF question: how to parse the example containing KEY, VALUE (and LIKE)</header>
    <date>Wed Mar 28 09:30:45 CEST 2012</date>
    <body>Hi Linda, Yes, a bit more: All terms derived from qualified_identification_variable or composable_qualified_identification_variable start with KEY, VALUE or ENTRY, respectively. However, section 4.4.4, footnote 56 and the referred section 4.4.2 state "the use of VALUE is optional" E.g. consider this query (modfied example from the spec, VALUE instead of KEY) SELECT i.name , p FROM Item i JOIN i.photos p WHERE VALUE(p) LIKE ‘%egret’ which would now be parsable after your suggested BNF modifications, is equivalent to: SELECT i.name , p FROM Item i JOIN i.photos p WHERE p LIKE ‘%egret’ which might still not be parsable, because we do only have a 'identification_variable' here, and not a 'qualified_identification_variable'. Thank you! Christian On Tue, Mar 27, 2012 at 8:08 PM, Linda DeMichiel linda.demichiel@... Hi Christian, I had intended that we would treat KEY and VALUE the same in the below. Did you mean more than that? regards, -Linda Hi Linda and Michael, We do also need to cover the special short-cut case, that "VALUE(identification_ variable)" is equivalent to just "identification_variable" Cheers, Christian linda.demichiel@... linda.demichiel@ oracle.com    Hi Michael,        Hi Linda,        more below ...            Hi Michael,            Thanks for following up on this.  More below....                Hi Linda,                    Hi Michael,                        Hi Linda,                            Hi Christian,                                Dear group,                                While investigating how to parse queries like this (all page/chapter references refer to                                EDR 2.1, Dec 19th, 2011):                                SELECT p                                FROM Person p, in(pc.firstnames) n                                WHERE n = 'Joe'                                (firstnames is a List&amp;lt;String&amp;gt;)                                I came across the following example provided in the spec, which is related in structure:                                section 4.4.4, p. 153:                                SELECT i.name http://i.name http://i.name                                FROM Item i JOIN i.photos p                                WHERE KEY(p) LIKE ‘%egret’                                I looked at the BNF how this is parsed, and did not find any derivation for the FROM clause:                                where_clause ::= WHERE conditional_expression                                conditional_expression ::= conditional_term                                conditional_term ::= conditional_factor                                conditional_factor ::= conditional_primary                                conditional_primary ::= simple_cond_expression                                simple_cond_expression ::= like_expression                                like_expression ::= string_expression LIKE pattern_value                                So eventually I end up with 'string_expression', from which I can not derive a                                'qualified_identification___ variable'                                string_expression ::=                                state_field_path_expression |                                string_literal |                                input_parameter |                                functions_returning_strings |                                aggregate_expression |                                case_expression |                                function_invocation |                                (subquery)                                Did I miss something, or is there actually a rule missing in the grammar for this?                            Yes, you are right.  The problem is that composable_qualified___ identification_variable                            is currently assumed to not designate a state field, which it should.                            Not sure yet what the best fix for this is in the grammar yet.                        would it work if we add composable_qualified___ identification_variable as an additional                        alternative in string_expression                        (and datetime_expressio etc.):                    I believe so, but I think we have to go further.  I.e., I think we need to support this also                    in in_expression, function_arg, case_operand, aggregate_expression, and entity_or_value_expression.                    Do you agree?                yes agreed. The same holds true for orderby_item, correct?            Yes, orderBy_item needs to be covered as well.            I am hesitating about adding composable_qualified___ identification_variable            to state_field_path_expression though, because a key may often not be a            state field of the object that is the map value.  I'm also concerned            about the order-by rules.  E.g.,            @Entity              ...              @OneToMany  // map key is division name            The following query shouldn't work, because the key is            not part of the result.        I agree, the above query should not work. But how about returning the key that is used in the order by:        SELECT key(vp)        FROM Company c JOIN c.organization vp        WHERE vp.reports &amp;gt; 100        ORDER BY key(vp)    Yes.  I agree that this should be legal.        I think this query should be legal. This means the orderby_item needs to be covered the same as all the other        places we        discussed where composable_qualified___ identification_variable needs to be added. But maybe we should extend the        rules for        the orderby_item as specified in chapter 4.9 on pages 183/184. We could add        composable_qualified___ identification_variable        to the second rule that covers state_field_path_expression already.    Yes, only I would cover this as a separate rule (to make it very clear).            SELECT vp            FROM Company c JOIN c.organization vp            WHERE vp.reports &amp;gt; 100            ORDER BY key(vp)            So, I was thinking of just either adding composable_qualified___ identification_variable to            the needed places (ugly) or defining another non-terminal, e.g.,            generalized_state_expression:: __= state_field_path_expression | composable_qualified___ identification_variable.            What do you think?        I like the idea of having another non-terminal because a composable_qualified___ identification_variable like        key(vp) is        not really a state_field_path_expression.        That would mean the new non-terminal generalized_state_expression is used in all the places we discussed before,        correct?    Yes.    Thanks again!  It looks like we're on the same page here.    regards,    -Linda        Regards Michael            regards,            -Linda                So maybe we extend state_field_path_expression and add composable_qualified___ identification_variable as                an alternative:                state_field_path_expression ::=                            general_subpath.state_field |                            composable_qualified___ identification_variable                Regards Michael                    regards,                    -Linda                        string_expression ::=                           state_field_path_expression |                        composable_qualified___ identification_variable |                           string_literal |                           ...                        Regards Michael                            Thanks for pointing this out.                            -Linda                            \                                Thank you!                                Christian                                --                                Christian Romberg                                Chief Engineer| Versant GmbH                                (T) +49 40 60990-0                                (F) +49 40 60990-113                                (E) cromberg@... cromberg@... cromberg@...                                &amp;lt;mailto: cromberg@...                                 www.versant.com http://www.versant.com http://www.google.com/url?q=_ _http%3A%2F%2Fwww.versant.com% __2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=__ AFrqEzeeEBc_gN___ 8mxtt8xDB0tjXDXQVlw http://www.google.com/url?q= http%3A%2F%2Fwww.versant.com% 2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg= AFrqEzeeEBc_gN_ 8mxtt8xDB0tjXDXQVlw                                 www.db4o.com http://www.db4o.com http://www.google.com/url?q=_ __sa=D&amp;amp;sntz=1&amp;amp;usg=__ AFrqEzdo3Q40RwKQPBtnPIuBYQd1di __FxJQ http://www.google.com/url?q= sa=D&amp;amp;sntz=1&amp;amp;usg= AFrqEzdo3Q40RwKQPBtnPIuBYQd1di FxJQ                                --                                Versant                                GmbH is incorporated in Germany. Company registration number: HRB                                54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359                                Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John                                CONFIDENTIALITY                                NOTICE: This e-mail message, including any attachments, is for the sole                                use of the intended recipient(s) and may contain confidential or                                proprietary information. Any unauthorized review, use, disclosure or                                distribution is prohibited. If you are not the intended recipient,                                immediately contact the sender by reply e-mail and destroy all copies of                                the original message.                        --                        *Michael Bouschen*                        *Prokurist*                        akquinet tech@spree GmbH                        Bülowstr. 66, D-10783 Berlin                        Fon: +49 30 235 520-33                        Fax: +49 30 217 520-12                        Email: michael.bouschen@... michael.bouschen@ akquinet.de                        Web: www.akquinet.de http://www.akquinet.de http://www.akquinet.de                        akquinet tech@spree GmbH, Berlin                        Geschäftsführung: Martin Weber, Dr. Torsten Fink                        Amtsgericht Berlin-Charlottenburg HRB 86780 B                        USt.-Id. Nr.: DE 225 964 680                --                *Michael Bouschen*                *Prokurist*                akquinet tech@spree GmbH                Bülowstr. 66, D-10783 Berlin                Fon: +49 30 235 520-33                Fax: +49 30 217 520-12                Email: michael.bouschen@... michael.bouschen@ akquinet.de                Web: www.akquinet.de http://www.akquinet.de http://www.akquinet.de                akquinet tech@spree GmbH, Berlin                Geschäftsführung: Martin Weber, Dr. Torsten Fink                Amtsgericht Berlin-Charlottenburg HRB 86780 B                USt.-Id. Nr.: DE 225 964 680        --        *Michael Bouschen*        *Prokurist*        akquinet tech@spree GmbH        Bülowstr. 66, D-10783 Berlin        Fon: +49 30 235 520-33        Fax: +49 30 217 520-12        Email: michael.bouschen@... michael.bouschen@ akquinet.de        Web: www.akquinet.de http://www.akquinet.de http://www.akquinet.de        akquinet tech@spree GmbH, Berlin        Geschäftsführung: Martin Weber, Dr. Torsten Fink        Amtsgericht Berlin-Charlottenburg HRB 86780 B        USt.-Id. Nr.: DE 225 964 680 -- Christian Romberg Chief Engineer| Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... cromberg@... www.versant.com http://www.google.com/url?q= http%3A%2F%2Fwww.versant.com% 2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg= AFrqEzeeEBc_gN_ 8mxtt8xDB0tjXDXQVlw www.db4o.com http://www.google.com/url?q= sa=D&amp;amp;sntz=1&amp;amp;usg= AFrqEzdo3Q40RwKQPBtnPIuBYQd1di FxJQ -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Wed Mar 28 16:12:23 CEST 2012</date>
    <body>Yes, I didn't mean that the JPA provider would actually do the overriding, but that some config option passed to it might dictate the strategy. I agree that the tenant, deployer, or cloud provider would determine that. I like the term application-managed SaaS. Best way to describe what we are talking about. What does the absence of the property imply? That it would work with any storage option, or that it is undefined? I figured for the sake of backward compatibility of existing applications that did not specify this property (but, for example, used a schema) that means they would require a separate database, but I guess this property would need to be set to deploy such an app into a cloud?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Thu Mar 29 20:37:05 CEST 2012</date>
    <body>Cool.  In my experience it gets quite dicey supporting both discriminated (SHARED_TABLE) and non-discriminated (other 2) simultaneously within the same deployment anyway. You are right, its probably not a practical expectation that most deployments are persistence provider agnostic.  So this could certainly be addressed later at such a time when it is more feasible to assume that deployments are more provider agnostic. Yes.  In my experience, just as you mention, there are really 2 distinct categories here.  SHARED_SCHEMA and SEPARATE_DATABASE both are largely interchangeable, and then you have SHARED_TABLE.  Really the question here lies in which of these 2 categories the deployment can handle. What you say in regards to SHARED_SCHEMA and SEPARATE_DATABASE is absolutely correct, if the app can handle using one strategy it can handle the other; but that same app may or may not be able to (or want to) handle SHARED_TABLE.  Conversely, an application that is built for SHARED_TABLE could conceivably operate in SHARED_SCHEMA or SEPARATE_DATABASE situations as well (native SQL aside, potentially).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Thu Mar 29 21:06:49 CEST 2012</date>
    <body>Actually in the SHARED_TABLE approach, the use of schemas and catalogs are actually OK.  The idea is that the schema layout is the same for all tenants. Native-sql can be tricky because of the need for handling the tenant-discrimination fragment by either the app developer or by injection from the persistence provider.  Maybe we want to allow for "placeholder" for this? You had said this would be available via ENC/JNDI before I think? I would at least like to see the annotation for naming the "tenant discriminator" column standardized.  I think that's the most crucial piece from app developer perspective. Perhaps I missed this in the initial reading also, but I did not see specifics on how JDBC Connections were expected to be made available as part of this. I think maybe the realization I am starting to come to is that this proposal is really just assuming that a given instance of the jpa provider would only be dealing with one tenant.  Reading back, I guess that is what you meant with the "single-instance SaaS case" comments. Am I correct in this new-found realization? If that is the case, I actually see no difference between SEPERATE_DATABASE (and even SHARED_SCHEMA for most databases) and what JPA does today.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Thu Mar 29 23:07:03 CEST 2012</date>
    <body>Yes, but the difference here is whether the app is expecting to use multiple  schemas (or named schemas).  If so, then SEPARATE_DATABASE and SEPARATE_SCHEMA wouldn't be  interchangeable, although the former could be used in place of the latter.  With SHARED_TABLE the same  issue holds --i.e., if the app were expecting multiple schemas, then the persistence provider  would have needed to have been allocated a database (rather than just a schema).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Thu Mar 29 23:12:56 CEST 2012</date>
    <body>I wouldn't expect to require that a provider support native sql queries in  the striped approach. A provider might support it, but I wouldn't expect the spec to require that. Right But if the app itself is truly managing multitenancy (rather than the  provider), then this column could be named anything.   If the provider is "multitenancy-aware",  the provider could plausibly make this column transparent to the app. Yes. You mean SHARED_DATABASE rather than SHARED_SCHEMA, right?   Except today,  JPA assumes that the app has access to more than just the default schema.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Fri Mar 30 00:49:06 CEST 2012</date>
    <body>Totally.  That is why I said "largely interchangeable" originally.  I guess my follow up loses the "potentially" slant.  But you are absolutely right. There are other concrete differences, but those mostly come more into play in the SaaS domain, which you have stated is outside the scope of this for 2.1 (how you get Connections, how you ALTER Connections, etc). I was just slanted to thinking the SaaS point-of-view since that is what I did in implementing multi-tenancy for Hibernate.  Luckily for me the SaaS solution encompasses the PaaS domain :D</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Fri Mar 30 01:07:24 CEST 2012</date>
    <body>Sure, but the persistence provider needs to be made ware of that name. That would be the point of this annotation - it would name that column. Well supporting the SaaS case is certainly more complicated than the PaaS case... Ah, yes.  These are not the terms I am familiar with... Well I think there is another assumption here about why people choose SEPERATE_DATABASE over SHARED_DATABASE.  Some, I am sure, do desire there to be separate schemas available.  But I think plenty of others chose it simply because it is more secure and more scalable since they would have dedicated instance tuned for their data (based on SLAs, etc). I am certain not everyone chooses SEPERATE_DATABASE to leverage multiple schemas; so in those cases the two are interchangeable.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Fri Mar 30 21:49:30 CEST 2012</date>
    <body>Good -- thanks, this is helpful feedback. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Mon Apr 02 15:14:22 CEST 2012</date>
    <body>Linda, Overall the proposal looks fine. However I was expecting an update to JPA from the SaaS standpoint as well ("Application managed SaaS" in your terminology :)), providing more flexibility to be able to work with the prevailing database partitioning/sharding approaches. (comments inline) Most secure &amp;amp; ease of management. I see couple of variants of PaaS providers in the evolving PaaS: A) PaaS provider who provisions database instances B) PaaS provider who integrates with a third-party DBaaS Type A PaaS provider could provide options to create database instance "per application" or "per tenant". Per application gives the best possible isolation. Type B PaaS provider who integrates with existing cloud DBaaS providers like Amazon RDS or Google SQL Cloud ( https://developers.google.com/cloud-sql/ ). Some of these services already provide multiple database instances per user/account which could be easily mapped to "per application" or "per tenant" strategies. The biggest challenge for a Java EE PaaS provider, is to be able to configure/deploy an existing Java EE application without much of re-architecting code. There are so many applications already out there, which have hardcoded schema names and database procedures for which the next two approaches fail to get them onto the cloud. I'm not in favour of the other two approaches. In Summary, I would only vote for the following: (1) Separate database per application (2) Separate database per tenant Too risky, shouldn't be taking this route. Rather, we should completely leave it out to the application developer to manage multi-tenancy by providing better support through JPA to address database multi-tenancy approaches. I'm not sure if we can standardize these various approaches through a single API, but can definitely make some progress to be able to catch up with the future. "Application managed SaaS" provides the best degree of control over multi-tenancy and not be able to support that would definitely be a minus. I guess some of the JPA vendors Hibernate, EclipseLink et al. have already introduced support for database multi-tenancy features and we have experts on this group who backs a lot of experience in this area to help in building or standardizing the support for mulit-tenancy. updates separately (For Master/Slave replication) - Support for multi-tenancy at the EMF (mechanism to choose the most appropriate EMF based on tenant/session/other criteria) Provided there's enough time frame and if the group is inclined towards this, we can definitely brainstorm the possibilities (plus or minus) -Deepak</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Mon Apr 02 16:39:48 CEST 2012</date>
    <body>The SaaS approach certainly adds more complexity.  While I certainly agree with Deepak here and think this is very widely useful, I guess as a group we need to decide if the extra complexity is "worth it".  From my experience I can say that its actually not as complex as it looks at first glance, if that helps.  Really it came down to 2 things that would affect stock JPA: 1) Getting Connections.  For the SHARED_TABLE approach, this is not any different.  But for the other 2, the provider will need access to tenant-specific Connections.  And to date, JPA has not standardized the contract for how providers obtain Connections which makes this a little tricky. 2) Segmenting shared cache.  Caching of data in the process-scoped, shared cache needs to be segmented by each tenant since we are talking about the same process.  Actually this is a concern anyway in implementing PaaS style multi-tenancy depending on how the cache provider is deployed, so not sure this is that big of a deal. Of course, as pointed out before, even if this is deemed outside the scope of JPA 2.1, nothing stops the individual providers from implementing this support.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Mon Apr 02 23:22:16 CEST 2012</date>
    <body>Hi Deepak, Thanks for the feedback.  More below..... Not sure I understand what you mean by "per application here":  per  application instance? or per application where multiple application instances are using the  database? Yes, applications that were not matched to the facilities of the cloud  platform provider would fail to deploy if they were expecting resources that could not be made  available.  That's one of the reasons I think we need to provide metadata as to an application's  expectations as to what it needs in its environment.   Providing no such metadata could  potentially default to the "I need a separate database" assumption, but that wouldn't be great in  terms of scalability. I don't understand.  Can you explain further? Just to avoid misunderstanding, by "application-managed multitenancy", I mean  that the application itself is managing multitenancy, not necessarily with any  additional support from the persistence provider or platform.  I.e., the app will need to manage tenant  identity, on-boarding of additional tenants, tenant-specific configuration information,  intermediation on access to tenant-specific data, etc., etc. This wouldn't be application-managed SaaS in my terminology, but rather some  point intermediate between container-/provider-managed SaaS and application-managed  SaaS</body>
  </mail>
  <mail>
    <header>[jpa-spec users] @OrderColumn questions along with @ManyToMany + @JoinTable mappings</header>
    <date>Tue Apr 03 05:20:53 CEST 2012</date>
    <body>Dear JPA experts, I am trying to model a tree structure with ordered lists using @OrderColumn on a @ManyToMany + @JoinTable into both directions. I have put up a detailed question including an image here: http://stackoverflow.com/questions/9957247/is-manytomanymappedby-ordercolumn-supported-by-the-jpa I was wondering about three things (as taken from there): 1. Are @OrderColumn's supported for @ManyToMany(mappedBy = ...) + @JoinTable (inverse bi-dir relationship)? 2. Are @OrderColumn's also being part of a foreign key supposed to be supported, even though the JPA spec remains silent about this? I see no reason why not requiring all JPA implementations to support this. 3. Because Hibernate throws a MappingException on 2., does this probably qualify as a bug? I'd very much appreciate your input on this. Thanks Karsten Wutzke    Ihr WEB.DE Postfach immer dabei: die kostenlose WEB.DE Mail App für iPhone und Android.    https://produkte.web.de/freemail_mobile_startseite/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Tue Apr 03 08:10:09 CEST 2012</date>
    <body>By "per application" I meant dedicating a separate database instance for each tenant application. The reason for this is one of the tenant application's could be a beast taking up a lot of database resources. Provisioning a separate process/instance provides isolation not just for the data but also for resource consumption as well. Moreover its not too weird to think of two different tenant applications using the same schema names, in which case this option also does the job. I agree with the metadata being specified to provide the database mapping strategy, but what I do not agree is the usefulness of "separate schema per application" and "separate table" approaches. They not only add complexity to the thinking process, but also influence the application design in defining schema and data. Java EE PaaS should be targeting at a wide range of applications, which could be easily ported to the cloud. Support for "shared table approach" could be error prone, if the vendor doesn't carefully implement it and the chances are high in messing up with application data. Application deployer wouldn't appreciate provider gaining control of the application schema. Why would an application deployer choose the "shared table" approach? May be he/she doesn't have too much of data right now (less "isolation" less price to be paid), but as the application grows bigger he/she may want to move to another approach which provides better isolation (probably at a higher price). Vendors are bound to not only support these approaches, but also implement various migration strategies. I deploy application specifying the database mapping strategy as "multitenancy = SEPARATE_TABLE" in my persistence.xml and later I want to re-deploy the same application with "multitenancy = SEPARATE_SCHEMA". Can we support this and what effort does go into it? Yes exactly, see there is so much of effort an application developer has to put in order to achieve this. Why can't we standardize/productize these essential things through JPA? In order to get the database multi-tenancy support, developers are bound to drop JPA and build things from scratch going by the JDBC route. We really do not want the developers doing that, do we? -Deepak</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Tue Apr 03 11:15:19 CEST 2012</date>
    <body>Steve I agree with you. We need to understand what does it take to roll out such a support. This is always useful when you have a database partitioning scheme or a topology where you have to deal with reads &amp;amp; updates to multiple databases within an application. Completely agree. Web scale applications are architected to use systems like Memcached to keep the cached data partitioned/segmented. In this context, JPA vendors could think of integrating their shared caches with systems like Memcached or JCache based implementations with a few partitioning strategies. I'm sure this is off-topic, probably we can start another thread and discuss possible outcomes. -Deepak</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Wed Apr 04 10:47:28 CEST 2012</date>
    <body>We already have a solution to both of those problems, it's called an EntityManagerFactory :-) Really, though, an EMF is what isolates tenants from each other. The problem is that people want to be able to define a single configuration unit in their persistence.xml file and apply it to multiple tenants, i.e vary it by tenant/connection information and get a new EMF for it. People ask for this all the time. Support for a persistence template is what I think would get us most of the way there. For example, we could define a "javax.persistence.template" property that could be passed to createEMF. One could create a new EMF from the template simply by passing in the template and connection information: ... EntityManagerFactory emf = This would look for the persistence unit named "SomePU" and dynamically create a new persistence unit/EMF named "MyPU", using all the information from "SomePU" but overriding connection params with the props passed in the map. Container support is a little more involved and would require some additional integration than what we are planning to add to EE 7. -Mike</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Wed Apr 04 15:16:36 CEST 2012</date>
    <body>I would think most providers already have the capability to integrate with third-party cache/clustering providers for shared cache. Personally I don't think there is anything in such integrations that needs to be defined as part of the JPA spec.  Really I was just trying to point out the real, concrete things I needed to solve to support this SaaS approach.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Wed Apr 04 15:28:32 CEST 2012</date>
    <body>Sorry Mike, but you are wrong.  The feature discussed as "SaaS" here would use the same EMF for multiple tenants.  That's the whole point/problem.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Wed Apr 04 19:51:22 CEST 2012</date>
    <body> (1) Separate database approach  (2) Shared database / separate schema  approach  (3) Shared table approach The third approach seems to be fundamentally different than the former two w.r.t JPA.  At runtime, there is a separate application instance (or set of instances, e.g., in a clustered environment) per tenant. This scenario could be supported currently if we consider a persistence unit (a.k.a EntityManagerFactory) has an 1:1 affinity to a tenant, except in the case of SHARED_TABLE approach. Of course, to support the first two approaches, the configuration via persistence.xml could be extended with some sort of variable substitution mechanics that allows the deployment process to generate individual persistence.xml for each tenant from a template by substituting tenant-specific database, schema, credentials etc. Essentially, the specification should strive to keep EntityManagerFactory least aware of multi-tenancy at runtime and push the complexity towards configuration phase as much as possible. The SHARED_TABLE or "striped" use case is fundamentally different because it would be more intrusive to the current runtime behavior. Every database operation would now has to be scoped by the tenant identifier. All tenants data in the same table does not appear to be a recommended approach for multi-tenant environment -- and it may be prudent to wait-and-watch how the data storage strategy emerges in cloud environment, before trying to accommodate this use case in JPA 2.1 timeframe.  JPA 2.1 can be extended to encompass a more general approach to SaaS in the future in which a single application instance serves multiple tenants This requirement turns the affinity of persistence unit and tenant to 1:n. One possibility to address the issue could be to consider a wider scope that encloses EntityManagerFactory itself, something like a PersitenceUnitFactory that gets a tenant-specific EntityManagerFactory. Such an abstraction will retain the current EntityManagerFactory scoped per tenant and hence least intrusive to accommodate multi-tenancy aspects. Regards -- Pinaki Poddar Chair, Apache OpenJPA Project            http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware To:     jsr338-experts@... Date:   03/26/2012 04:54 PM Subject:        [jsr338-experts] support for multitenancy One of the main items on the agenda for the JPA 2.1 release is support for multitenancy in Java EE 7 cloud environments. In Java EE 7, an application can be submitted into a cloud environment for use by multiple tenants in what can be viewed as a basic form of software as a service (SaaS).  The application is customized and deployed on a per-tenant basis.  At runtime, there is a separate application instance (or set of instances, e.g., in a clustered environment) per tenant.  The instances used by different tenants are isolated from one another.  The resources used by a tenant's application may also be isolated from one another, or may be shared. In general, however, it is assumed that a tenant's data is isolated from that other tenants. There are three well-known approaches to support for multitenancy at the database level:    (1)  separate database approach    (2)  shared database / separate schema approach    (3)  shared schema / shared table approach To get the discussion started, this is a high-level strawman sketch of how the 3 approaches might be used with JPA in keeping with the Java EE 7 approach.  At the same time, however, we also want to be sure that what we specify in JPA 2.1 can be extended to encompass a more general approach to SaaS in the future in which a single application instance serves multiple tenants and in which multitenancy is managed by the Java EE environment. For further information on how Java EE 7 is approaching PaaS/SaaS, you might find the documents on the javaee-spec.java.net project useful, particularly http://java.net/projects/javaee-spec/downloads/download/PaaS.pdf and the latest draft of the Java EE 7 Platform spec, http://java.net/projects/javaee-spec/downloads/download/JavaEE_Platform_Spec.pdf . Note that the identifier for the tenant will be made available to the application in JNDI as java:comp/tenantId.  The tenantId will be a string, whose max length should allow it to be portably stored in a single database column. APPROACHES: (1) Separate database approach In this approach, each tenant's persistence unit is mapped to a separate database.  This approach provides the greatest isolation between tenants and does not impose any additional constraints over the object/relational mapping of the persistence unit or over the operations that can be performed.  In particular, the use of multiple database schemas or catalogs are supported as are native queries. In some cloud environments, use of this approach might not be available, as a tenant might be allocated storage within a database rather than a separate database. (2) Shared database / separate schema  approach In this approach, each tenant's data is stored in database tables that are isolated from those of any other tenant.  In databases that support schemas, this will typically be achieved by allocating a separate schema per tenant.  The database's permissions facility is used to confine a tenant's access to the designated schema, thus providing isolation between tenants at the schema level. Support for this approach is straightforward if the persistence unit uses only the default schema or catalog (i.e., if it does not specify schema names or catalogs in the object/relational mapping metadata). A native query that attempts to access data in a schema other than that assigned to the tenant by the platform provider will be trapped by the database authorization mechanisms and will result in an exception. [While the case where the persistence unit metadata explicitly specifies one or more schemas could potentially be handled by the persistence provider by remapping schema names and native queries that embed schema names, I would not propose that we specify or require support for this case, although a more sophisticated persistence provider might choose to support it.] (3) Shared table approach In this approach, database tables are shared ("striped") across tenants. It is the reponsibility of the persistence provider to provide per-tenant isolation in accessing data.  This will typically be done by mapping and maintaining a tenant ID column in the respective tables, and augmenting data retrieval and query operations, updates, and inserts with tenant IDs.  The use of native queries would need to be trapped by the persistence provider and not allowed unless the persistence provider were able to modify them to provide isolation of tenant data. Ideally, the management of the tenant id should be transparent to the application, although we should revisit this in Java EE 8 as we move further into support for SaaS. I believe that the main use case for the shared table approach is in SaaS environments in which a single application instance is servicing multiple tenants.  This is outside the scope of Java EE 7, so I don't think that we need to standardize on support for this approach now, although we should not lose sight of it as we standardize on other aspects. DETERMINING THE MULTITENANCY STORAGE MAPPING STRATEGY: We see two general approaches to determining the multitenancy storage mapping strategy that should be used for a persistence unit.  In some cases, these approaches might be combined. Again, note that a cloud platform provider might use a single strategy for all tenants in allocating database storage.  For example, each tenant might be allocated a separate database, or each tenant might only be allocated a schema within a database. (A)  The Application Specifies Its Requirements In this approach, the application specifies its functional requirements (in terms of need for named, multiple schemas and/or use of native queries) in the persistence.xml descriptor, and the deployer and/or cloud platform provider determine the storage strategy that is used for the tenant.  This metadata serves as input to the deployer for the tenant or as input into the automated provisioning of the application by the cloud platform provider (if automated provisioning is supported by the platform instance). For example, an application might specify that it requires support for multiple schemas and native queries.  In general, such requirements would mean that a separate database would need to be provisioned for the tenant.  If this is not possible, then unless the platform provider supported a persistence provider that could perform schema remapping and/or modification of native queries, the application might fail to deploy or fail to initialize.  On the other hand, if an application specifies that it uses only the default schema and native queries, then either the separate database or separate schema approach could be used. (B)  The Application Specifies the Multitenancy Storage Mapping Strategy An alternative approach is that the application specifies the required (or preferred) multitenancy storage mapping strategy in the persistence.xml. For example, a multitenant application that is designed with the intention that separate databases be used might indicate this in the persistence.xml as multitenancy = SEPARATE_DATABASE. An application that is designed with the intention that databases may be shared by partitioning at the database schema level might indicate this in the persistence.xml as multitenancy = SHARED_DATABASE.  [A portable application that specifies this strategy should not specify schema or catalog names, as it might otherwise fail to deploy or fail to initialize.] An application that is designed with the intention that tables be shared might indicate this in the persistence.xml as multitenancy = SHARED_SCHEMA.  An app that uses explicit multitenant mapping metadata would be expected to specify this. [Open Issue: Is it useful to specify requirements along the lines of those used in approach (A) with this approach?  If so, is the platform provider allowed to choose a different mapping strategy as long as that approach is more isolated?  If no functional requirements are specified as in approach (A) and if a mapping strategy is specified in the persistence.xml that is provided by the application submitter, then if this information is not observed, the risk is that the app will fail.  For example, observation of the specified mapping strategy might be required for the case where explicit multitenant mapping metadata is supplied for the striped mapping approach.] With both the approaches (A) and (B), different storage mapping strategies may be used for different tenants of the same application if the cloud platform provider supports a range of storage mapping choices. REQUIREMENTS FOR PORTABLE APPLICATIONS Applications that are intended to be portable in cloud environments should not specify schema or catalog names. DEPLOYMENT When an application instance is deployed for a tenant, the container needs to make the tenant identifier and tenant-related configuration information available to the persistence provider.  The container needs to pass to the persistence provider a data source that is configured with appropriate credentials for the tenant, and which will provide isolation between that tenant and other tenants of the application.  We should probably also define an interface to capture the tenant identifier and tenant-related metadata and configuration information that the container needs to pass to the persistence provider, e.g., a TenantContext. OTHER OPEN ISSUES 1. Additional metadata to support schema generation. 2. Do we need metadata to indicate whether an application supports     multitenant use -- i.e., whether it is "multitenant enabled"?     Do we need this information specifically for JPA? 3. Specification of resources that are shared across tenants--e.g.,     a persistence unit for reference data that can be accessed by     multiple tenants.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: support for multitenancy</header>
    <date>Wed Apr 04 23:13:19 CEST 2012</date>
    <body>No, the problem is the ability to do application-managed SaaS tenant isolation conveniently. Your preconceived implementation approach is apparently to share an EMF, but that is by no means the problem, just one possible solution. I am proposing what I think is a much better one.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: support for multitenancy</header>
    <date>Wed Apr 04 23:23:50 CEST 2012</date>
    <body> an EMF is what isolates tenants from each other. Agreed. Keeping this EMF:Tenant affinity to 1:1 would be the clean way to proceed.  Support for a persistence template is what I think would get us most of the way there. Agreed. Regards -- Pinaki Poddar To:     jsr338-experts@... Cc:     Steve Ebersole &amp;lt;steve.ebersole@...&amp;gt;, Deepak Anupalli Date:   04/04/2012 02:13 PM Subject:        [jsr338-experts] Re: [jpa-spec users] Re: support for             multitenancy No, the problem is the ability to do application-managed SaaS tenant isolation conveniently. Your preconceived implementation approach is apparently to share an EMF, but that is by no means the problem, just one possible solution. I am proposing what I think is a much better one.  Sorry Mike, but you are wrong.  The feature discussed as "SaaS" here  would use the same EMF for multiple tenants.  That's the whole  point/problem.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Wed Apr 04 23:24:54 CEST 2012</date>
    <body>Right, I agree, in these cases the JPA the provider does not need to be tenant-aware at all. Exactly. Since an EMF is already a perfect unit of isolation it makes perfect sense to provide tenant isolation using that. We would just need to provide some flexibility to enable it.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: support for multitenancy</header>
    <date>Wed Apr 04 23:33:41 CEST 2012</date>
    <body>Well we'll agree to disagree about which is better :)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Tue Apr 10 18:26:42 CEST 2012</date>
    <body>Will the tenant identifier be available in ENC/JNDI regardless of strategy being used? Will we at least have a standardized annotation for mapping the "tenant identifier" column?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Tue Apr 10 18:33:20 CEST 2012</date>
    <body>Yes Certainly when we standardize on this approach.  I'm not seeing why it is  essential in purely application-managed SaaS however.  Is it?  If so, why?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Tue Apr 10 18:48:29 CEST 2012</date>
    <body>In this SHARED_TABLE approach the table *will* have a column that identifies to which tenant a particular row's data belongs.  This column has to be named.  How else where you propsing that the JPA provider be made aware of this column name? Obviously if JPA 2.1 is not going to be supporting this SHARED_TABLE approach at all, this is "not needed".  I am just assuming that other providers will be supporting this as well (Hibernate will) and it seems to me that standardizing this annotation up front saves end-user migration later.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Tue Apr 10 18:52:30 CEST 2012</date>
    <body>In the expected case, the JPA provider knows that it is managing a shared  table approach, so it can generate the column.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Tue Apr 10 19:14:17 CEST 2012</date>
    <body>So your assumption is that in a SHARED_TABLE approach, all tables: (a) would be shared (no mixing multitenant and non-) and (b) would have the same "tenant column" name? Specifically, you are assuming that 2 tables could not have different "tenant column" names?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Tue Apr 10 19:16:50 CEST 2012</date>
    <body>And will it be available from JNDI when the EMF is being built?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Tue Apr 10 19:19:16 CEST 2012</date>
    <body>I wasn't assuming that.  I was assuming that the container would pass in the requisite information to the createEMF call, as we do today.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Tue Apr 10 19:47:23 CEST 2012</date>
    <body>Sounds reasonable, thanks.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Tue Apr 10 23:25:11 CEST 2012</date>
    <body>Er.. just to clarify... We started out talking about a per-thread tenant id for SaaS (to be offered in the future) and am I right that somehow the impression was given that that tenant id will be in JNDI or passed in EE 7? As it stands right now (in EE 7), containers are not going to pass in (or post in JNDI) the tenant id of a per-thread (SaaS) tenant. They will only be expected to maintain a single tenant id for the life of an application instance, so this won't help for SaaS.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Tue Apr 10 23:27:02 CEST 2012</date>
    <body>Right.  This is not expected to be on a per-thread basis in Java EE 7.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Wed Apr 11 02:18:28 CEST 2012</date>
    <body>Sorry if you got that impression.  That was not what I meant. Sophisticated multi-tenancy implementations supporting this (what y'all like to call SaaS) approach would already have means to contextually obtain this information.  And its already been determined (and repeated) this approach wont be a standardized part of JPA 2.1 ;) I was just trying to figure out how providers would found out the tenant when "starting up".  Linda, you said the values would get passed in to the "createEMF".  Does that cover both createEntityManagerFactory and createContainerEntityManagerFactory?  I assume this value be part of the "integration map" under a known key?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Wed Apr 11 04:20:43 CEST 2012</date>
    <body>Good. Just wanted to make sure the thread was not misconstrued by anyone, given that     "Right. For shared application instances we will need this." was followed by     "You had said this would be available via ENC/JNDI before I think?"</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: proposal : @Entity on interfaces</header>
    <date>Mon Apr 16 18:46:39 CEST 2012</date>
    <body> I fear we get into a philosophical discussion here. Objects have state   *and* behavior, which is what especially proponents of Domain Driven Design   emphasize. Why should JPA get in the way of using objects this way? +1 for this. I also consider JPA entities as rich domain objects and not just  structs mapped to tables.   I always considered JPA an API to allow mapping objects onto a relational   store, not creating Java representations of data tables.    Cheers,  Ollie    Am 12.03.2012 um 17:44 schrieb Pinaki Poddar:   http://openjpa.apache.org/    --   /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */  </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Fwd: Re: proposal : @Entity on interfaces</header>
    <date>Tue Apr 17 19:52:26 CEST 2012</date>
    <body>[forwarding a message from users] Proxying is probably too much, but I really would appreciate the ability of  getting a reference to the EntityManager :-). I'm back from the JAX. There  were ~10 requests in my workshop about how to get a reference to the  EntityManager... An @OnAttachment annotation on a method with the EntityManager as parameter  would solve the problem. Begin forwarded message:  Subject: Re: [jpa-spec users] [jsr338-experts] Re: proposal : @Entity on   interfaces  Date: 17. April 2012 07:10:46 MESZ  Cc: Adam Bien &amp;lt;abien@...&amp;gt;, jsr338-users@..., michael.keith@..., Oliver     [Replying on -users list because of lack of post permission on the main   list]      They can't be full participants in the EE environment, though, as they're   not subject to injection, interceptors, the alternatives mechanism, etc.   Perhaps most importantly, if a transaction fails you have to toss them all   out and replace them, and if a merge succeeds you have to start using new   versions of the objects that were created by the JPA implementation. Those   limitations make it frustrating and often impractical - IMO - to use them   for much more than data storage and access.    What I'd like to see in this case, instead of support for @Entity on   interfaces, is a proxying system that works on entities and isn't insanely   verbose. Rather than enhancing entities themselves, allow the creation of   proxies/wrappers that can delegate all calls to an entity but can wrap all   operations, can exchange wrapped entities (ie: after a merge, replace old   with new), can implement additional interfaces, are subject to injection &amp;amp;   interception, etc.    As far as I can tell, right now there isn't any suitable proxying support.   JavaAssist or CGLib can't be used by the end-developer for this because   they require you to control the object's instantiation in order to proxy it   and they can't exchange proxied objects. java.util.Proxy *can* change   proxied objects, bu5t can only proxy interfaces, so it won't work with   JPA's concrete-class driven approach.    If only we could effectively proxy entities, and do so without having to   hand-write or code-generate wrappers for *every* *single* *method* of   *every* entity class, then I think that'd solve a fair few issues. Folks:   How would you feel about being able to have *proxies* for your entities   that could implement the interfaces you wanted and otherwise be extended?    --  Craig Ringer    POST Newspapers  276 Onslow Rd, Shenton Park  Ph: 08 9381 3088     Fax: 08 9388 2258  ABN: 50 008 917 717   http://www.postnewspapers.com.au/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Wed Apr 18 15:05:34 CEST 2012</date>
    <body>We can easily support this today without a new property as the connection information is not required to be defined within the persistence.xml file.  Today a user can provide new connection information for a create EMF call and have a reasonable expectation that this EMF will be different from or isolated from another EMF with different connection information.  As we move forward with multi-tenancy the specification should clarify the requirements with respect to providing alternate connection information. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: support for multitenancy</header>
    <date>Wed Apr 18 15:15:35 CEST 2012</date>
    <body>Also, if we add a javax.persistence.manditory-catalog and javax.persistence.manditory-schema to the connection information that should result in isolation/an alternate EMF and we have support for the PAAS requirements.  Allow the connection information to be specified within a multi-tenant context and we have a clear path towards more complex multi-tenancy functionality.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] clarification about use of "immediately" word in describing 3.6.1 Automatic Validation Upon Lifecycle Events</header>
    <date>Fri Apr 20 17:39:47 CEST 2012</date>
    <body>I'm exactly sure how the word "immediately" applies in section 3.6.1, where we describe when the validation will happen after lifecycle events. If the application calls EntityManager.persist() and then calls EntityManager.flush().  Then sometime later the jta transaction commits. What if the pre-persist is triggered as part of em.persist() but the validation happens during em.flush or commit (before completion)? How does the term "immediately" apply in this context and why is that a requirement? Current text from 2.1: " 3.6.1 Automatic Validation Upon Lifecycle Events This specification supports the use of bean validation for the automatic validation of entities upon the pre-persist, pre-update, and pre-remove lifecycle validation events. These lifecycle validation events occur immediately after the point at which all the PrePersist, PreUpdate, and PreRemove lifecycle callback method invocations respectively have been completed, or immediately after the point at which such lifecycle callback methods would have been completed (in the event that such callback methods are not present). In the case where an entity is persisted and subsequently modified in a single transaction or when an entity is modified and subsequently removed in a single transaction, it is implementation dependent as to whether the pre-update validation event occurs. Portable applications should not rely on this behavior. " Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: clarification about use of "immediately" word in describing 3.6.1 Automatic Validation Upon Lifecycle Events</header>
    <date>Fri Apr 20 20:31:34 CEST 2012</date>
    <body>PrePersist validation is supposed to occur at the same time as the PrePersist  callback methods after all of the callbacks methods have been invoked--i.e., before persist().    PostPersist validation may happen at the end of the transaction, but I don't understand your question about  PrePersist validation happening then. Well, for one thing, assuming the application has invoked persist(), it gives  it a deterministic point at which to catch any validation errors.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Introduce javax.persistence.LazyLoadException</header>
    <date>Sat Apr 21 22:49:41 CEST 2012</date>
    <body>JPA 2.0 does not specify the behaviour of a persistence provider when the application accesses an unloaded member of a detached entity. In this case, Hibernate throws a LazyInitializationException, whereas OpenJPA just leaves the member uninitialized which leads to a rather non-descriptive NullPointerException. It would help JPA users to write portable code if there were a standard exception for this standard scenario. Proposal for JPA 2.1: The persistence provider MUST throw a javax.persistence.LazyLoadException (derived from javax.persistence.PersistenceException) when the application accesses a member of a detached entity for which PersistenceUtil.isLoaded() returns false. Best regards, Harald</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: clarification about use of "immediately" word in describing 3.6.1 Automatic Validation Upon Lifecycle Events</header>
    <date>Mon Apr 23 16:05:57 CEST 2012</date>
    <body>I'm trying to understand if its up to persistence providers to define what "immediately" means to them or if a stricter definition should be used.  My read of the current wording, is that it is not 100% clear what the intention of the use of "immediately" is. In the above example that I gave, if the validation is performed after pre-persist and before TX completion, then the validation is performed "immediately enough" to function correctly (I think).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Location of named queries</header>
    <date>Sat Apr 28 23:47:05 CEST 2012</date>
    <body>Hello JPA experts, I have a question regarding named queries. AFAIK they can only be placed onto entities via annotations. For people using code generators for their entities, this isn't an option. Working around it by using XML as described here http://jdevelopment.nl/put-named-queries-jpa/ is useful but suboptimal. Overall, putting named queries onto entities and falling back to XML in an annotation-only world is pretty clumsy IMHO. Q: Aren't there any plans to improve the situation in the upcoming JPA spec? I'd really like to put them into my service layer. I would really like to see some improvement here. Can it be done? Comments appreciated. Karsten Wutzke    Ihr WEB.DE Postfach immer dabei: die kostenlose WEB.DE Mail App für iPhone und Android.    https://produkte.web.de/freemail_mobile_startseite/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] converter errors</header>
    <date>Thu May 03 19:32:21 CEST 2012</date>
    <body>Our TCK engineer has pointed out to me that the spec doesn't define what happens if a converter throws an exception. I would propose that the persistence provider wrap the exception in a PersistenceException and mark the transaction for rollback. Other opinions? -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converter errors</header>
    <date>Thu May 03 20:59:03 CEST 2012</date>
    <body>Yes (if transaction exists).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Duplicate key exceptions (RE: Re: converter errors)</header>
    <date>Thu May 03 23:29:22 CEST 2012</date>
    <body>Folks, The discussion on converter exceptions had me reconsidering duplicate key exceptions. Has anyone found it inconvenient that duplicate key exceptions are not recoverable, in the sense that TX is marked for rollback?  -----Original Message-----  From: michael keith [ mailto:michael.keith@...]   Sent: Friday, 4 May 2012 6:59 a.m.  To: jsr338-experts@...  Subject: [jsr338-experts] Re: converter errors    Yes (if transaction exists).    &amp;gt; Our TCK engineer has pointed out to me that the spec doesn't  &amp;gt; define what happens if a converter throws an exception.  &amp;gt; I would propose that the persistence provider wrap the   exception in a  &amp;gt; PersistenceException and mark the transaction for rollback.  &amp;gt; Other opinions?  &amp;gt; -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: converter errors</header>
    <date>Fri May 04 20:45:11 CEST 2012</date>
    <body>Yes, and if the persistence context is joined to it.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] JPA schema generation</header>
    <date>Wed May 09 20:36:51 CEST 2012</date>
    <body>In conjunction with support for use of JPA in PaaS environments, we will need to address the issue of schema generation.  After we discuss this area, I'd like to return to a broader discussion of multitenancy support with this in view. Here's what I think we'll need to address: 1.  How/when schema generation happens:     a. at a deployment time phase (or predeployment time phase)     b. at EMF creation time 2.  Providing an option whereby the application can supply DDL for     schema generation as an alternative to the persistence provider     doing schema generation based on the ORM metadata 3.  How, once schema generation has happened, tables can get loaded with     data.  If we enable the application to provide DDL scripts for     schema generation, then supporting the use of SQL bulk insert scripts     would go hand-in-hand with this, but perhaps there are also other     approaches, e.g., bulk insert via JPQL(?). 4.  The need for additional ORM metadata:  e.g. metadata for index     generation; for foreign key constraints; other(?). 5.  More precise definition of existing metadata that can be used     for schema generation:  what it comprises, and how it can/must     be used. 6.  Additional persistence.xml level metadata and additional metadata     the platform provider passes to the persistence provider to control     or configure the operations outlined above. I am assuming for that Java EE 7 we need to support both the separate database and shared database (aka separate schema) approaches. I am also assuming that it is the platform provider that does the provisioning of either the database or the schema respectively (i.e., in accordance with whether the separate database or shared database approach is taken).   I.e., in the separate database approach, I'm assuming that it is the persistence provider that creates the schemas directly in the database or creates the DDL to generate the schemas needed for the persistence unit.  [I'll use the term "schema generation" generically below to mean either table generation or schema+table generation with this assumption in view.] Here's a proposal to get the discussion started. SCHEMA GENERATION I think we should allow schema generation to be done either as a deployment (or pre-deployment) phase or at createEMF time. Deployment-time (or predeployment-time) schema generation could be done either by the platform provider calling into the PersistenceProvider implementation to generate the database schema or by the platform provider running SQL DDL scripts. SQL DDL scripts might be provided with the application (bundled as part of the persistence unit) or the platform provider might invoke the PersistenceProvider implementation to generate them from the persistence unit metadata.  If scripts are provided or generated, it should be the platform provider's responsibility to run them against the database.  If the application has provided DDL scripts, the application should also have the option to provide SQL scripts to do a bulk insert of data.  It would be the responsibility of the platform provider to run these as well. An alternative to the use of scripts is for the platform provider to call into the PersistenceProvider to directly generate the tables in the database.  In the case that a database has been provisioned rather than only a schema, the persistence provider would generate the schemas and tables. For the sake of argument, let's assume that we add a generateSchema() method to the PersistenceProvider interface, which the platform provider invokes during the deployment phase if SQL scripts have not been provided by the application. Information that needs to be passed in would include the following: (1) Information that specifies how schema generation is to proceed.     This information could take the form of a SchemaGenerationInfo     interface or it could be in the form of additional standard properties. (2) Connection to the database or schema.  [Is this a Connection or a     DataSource ?] (3) PersistenceUnitInfo The SchemaGenerationInfo should include the following:   ddl-generation-mode:  GENERATE_AND_EXECUTE | GENERATE_ONLY | EXECUTE_ONLY       GENERATE_AND_EXECUTE =&amp;gt; generate ddl scripts and generate schema (if         applicable) and tables directly in the database       GENERATE_ONLY =&amp;gt; generate ddl scripts       EXECUTE_ONLY =&amp;gt; generate schema (if applicable) and tables directly in          the database   create-ddl-target:  a Writer configured for the persistence provider       for outputting of the DDL scripts for the creation of schema/tables.       This should be null if EXECUTE_ONLY ddl-generation-mode is specified.   drop-ddl-target: a Writer configured for the persistence provider       for outputting of the DDL scripts for the dropping of schema/tables.       The should be null if EXECUTE_ONLY ddl-generation-mode is specified.   ddl-execution-mode: Used when executing directly against the database,       rather than generating scripts.  Options should include:        CREATE_TABLES | DROP_TABLES | DROP_AND_CREATE_TABLES      Do we also need to distinguish CREATE_SCHEMAS_AND_TABLES ? DROP_SCHEMA ?        Or can these be implicit?      Do we also need an ALTER_TABLES capability here?   properties:  These could include any vendor-specific properties ADDITIONAL METADATA (persistence.xml) If scripts are provided with the application (i.e., bundled as part of the persistence unit), we should add entries in the persistence.xml to identify them:       create-ddl-script       drop-ddl-script       data-load-script(s)  [Do we need to support more than one load script?          If so, the ordering may need to be specified as well.] Open issue: do we need metadata in the persistence.xml for   ddl-execution-mode as well?  For example, this might be used in   standalone scenarios (?) ORM METADATA (annotations and orm.xml) (1) Indexes I think we need to support indexes. Here's a strawman annotation:   The columnList syntax could follow that of the OrderBy annotation:     columnList::= indexColumn [, indexColumn]*     indexColumn::= columnName [ASC | DESC]     If ASC or DESC is not specified, ASC is assumed We'd also need to add to Table, Secondary Table, CollectionTable, JoinTable, and TableGenerator (2) Foreign Key Constraints I see two possible approaches: (a) Add a string-valued foreignKeyDefinition element to JoinColumn, JoinColumns, MapKeyJoinColumn, etc. to specify a SQL fragment (intended along the lines of columnDefinition) for defining a foreign key constraint and/or for overriding the persistence provider's default foreign key definition.  It might also be helpful to allow a name for the foreign key constraint to be specified for the case where the provider is using its default foreign key generation strategy. or (b) Add a ForeignKey annotation to specify the foreign key constraint -- for example, I have some misgivings about approach (b), and, given a lack of database portability here, we might wind up needing a foreignKeyDefinition fragment anyway. (3) Other What about the ability to distinguish a CHAR rather than VARCHAR mapping for strings?  Or should we just leave this as belonging in a SQL fragment (with the understanding that we would be intentionally discouraging the use of CHAR strings)? (4) Anything else I'm missing? APIs I assumed the addition of a PersistenceProvider.generateSchema() method above. If we also support schema generation at EMF creation time, do we want another createContainerEntityManagerFactory() method that takes a SchemaGenerationInfo as well as PersistenceUnitInfo argument ? If generation is done directly in the database, at some point the container (or application) may need to have the schema/tables dropped. I'm not sure of the best way to support this.   A dropAndClose() method on the EMF??</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Wed May 09 22:39:16 CEST 2012</date>
    <body>Linda, My five cents worth (no 2 cents here, due to inflation!) 1(a) is typically of interest to DBAs, who don't want to let any DML be run in "their" database if they haven't seen/checked the DML. 1(b) is of great use to developers, but also is valuable when you have a deployment of an application that the DBA doesn't know about the schema for (e.g. they weren't involved in defining it, as it came from a third party), so the DBA isn't so likely to be able to usefully "vet" it anyway. 4 (index meta-data in particular) is absolutely essential. If we only do one thing (no doubt we will do more than one thing), then index meta-data (e.g. via annotations) in my mind is the most critical.  -----Original Message-----  From: Linda DeMichiel [ mailto:linda.demichiel@...]   Sent: Thursday, 10 May 2012 6:37 a.m.  To: jsr338-experts@...  Subject: [jsr338-experts] JPA schema generation      In conjunction with support for use of JPA in PaaS environments, we  will need to address the issue of schema generation.  After we discuss  this area, I'd like to return to a broader discussion of multitenancy  support with this in view.      Here's what I think we'll need to address:    1.  How/when schema generation happens:       a. at a deployment time phase (or predeployment time phase)       b. at EMF creation time    2.  Providing an option whereby the application can supply DDL for       schema generation as an alternative to the persistence provider       doing schema generation based on the ORM metadata    3.  How, once schema generation has happened, tables can get   loaded with       data.  If we enable the application to provide DDL scripts for       schema generation, then supporting the use of SQL bulk   insert scripts       would go hand-in-hand with this, but perhaps there are also other       approaches, e.g., bulk insert via JPQL(?).    4.  The need for additional ORM metadata:  e.g. metadata for index       generation; for foreign key constraints; other(?).    5.  More precise definition of existing metadata that can be used       for schema generation:  what it comprises, and how it can/must       be used.    6.  Additional persistence.xml level metadata and additional metadata       the platform provider passes to the persistence provider   to control       or configure the operations outlined above.      I am assuming for that Java EE 7 we need to support both the  separate database and shared database (aka separate schema)   approaches.    I am also assuming that it is the platform provider that does the  provisioning of either the database or the schema respectively (i.e.,  in accordance with whether the separate database or shared database  approach is taken).   I.e., in the separate database approach, I'm  assuming that it is the persistence provider that creates the schemas  directly in the database or creates the DDL to generate the schemas  needed for the persistence unit.  [I'll use the term "schema  generation" generically below to mean either table generation or  schema+table generation with this assumption in view.]      Here's a proposal to get the discussion started.    SCHEMA GENERATION    I think we should allow schema generation to be done either as a  deployment (or pre-deployment) phase or at createEMF time.    Deployment-time (or predeployment-time) schema generation could be  done either by the platform provider calling into the  PersistenceProvider implementation to generate the database schema or  by the platform provider running SQL DDL scripts.    SQL DDL scripts might be provided with the application (bundled as  part of the persistence unit) or the platform provider might invoke  the PersistenceProvider implementation to generate them from the  persistence unit metadata.  If scripts are provided or generated, it  should be the platform provider's responsibility to run them against  the database.  If the application has provided DDL scripts, the  application should also have the option to provide SQL scripts to do  a bulk insert of data.  It would be the responsibility of the  platform provider to run these as well.    An alternative to the use of scripts is for the platform provider to  call into the PersistenceProvider to directly generate the tables in  the database.  In the case that a database has been provisioned rather  than only a schema, the persistence provider would generate the  schemas and tables.    For the sake of argument, let's assume that we add a generateSchema()  method to the PersistenceProvider interface, which the platform  provider invokes during the deployment phase if SQL scripts have not  been provided by the application.    Information that needs to be passed in would include the following:    (1) Information that specifies how schema generation is to proceed.       This information could take the form of a SchemaGenerationInfo       interface or it could be in the form of additional   standard properties.  (2) Connection to the database or schema.  [Is this a Connection or a       DataSource ?]  (3) PersistenceUnitInfo      The SchemaGenerationInfo should include the following:       ddl-generation-mode:  GENERATE_AND_EXECUTE | GENERATE_ONLY   | EXECUTE_ONLY           GENERATE_AND_EXECUTE =&amp;gt; generate ddl scripts and   generate schema (if           applicable) and tables directly in the database           GENERATE_ONLY =&amp;gt; generate ddl scripts           EXECUTE_ONLY =&amp;gt; generate schema (if applicable) and   tables directly in            the database       create-ddl-target:  a Writer configured for the   persistence provider         for outputting of the DDL scripts for the creation of   schema/tables.         This should be null if EXECUTE_ONLY   ddl-generation-mode is specified.       drop-ddl-target: a Writer configured for the persistence provider         for outputting of the DDL scripts for the dropping of   schema/tables.         The should be null if EXECUTE_ONLY ddl-generation-mode   is specified.       ddl-execution-mode: Used when executing directly against   the database,         rather than generating scripts.  Options should include:          CREATE_TABLES | DROP_TABLES | DROP_AND_CREATE_TABLES        Do we also need to distinguish   CREATE_SCHEMAS_AND_TABLES ? DROP_SCHEMA ?          Or can these be implicit?        Do we also need an ALTER_TABLES capability here?       properties:  These could include any vendor-specific properties      ADDITIONAL METADATA (persistence.xml)    If scripts are provided with the application (i.e., bundled as part  of the persistence unit), we should add entries in the persistence.xml  to identify them:         create-ddl-script         drop-ddl-script         data-load-script(s)  [Do we need to support more than   one load script?            If so, the ordering may need to be specified as well.]    Open issue: do we need metadata in the persistence.xml for     ddl-execution-mode as well?  For example, this might be used in     standalone scenarios (?)      ORM METADATA (annotations and orm.xml)    (1) Indexes    I think we need to support indexes.    Here's a strawman annotation:         The columnList syntax could follow that of the OrderBy annotation:         columnList::= indexColumn [, indexColumn]*       indexColumn::= columnName [ASC | DESC]         If ASC or DESC is not specified, ASC is assumed    We'd also need to add  to Table, Secondary Table, CollectionTable, JoinTable, and   TableGenerator      (2) Foreign Key Constraints    I see two possible approaches:    (a) Add a string-valued foreignKeyDefinition element to JoinColumn,  JoinColumns, MapKeyJoinColumn, etc. to specify a SQL fragment  (intended along the lines of columnDefinition) for defining a foreign  key constraint and/or for overriding the persistence provider's  default foreign key definition.  It might also be helpful to allow a  name for the foreign key constraint to be specified for the case where  the provider is using its default foreign key generation strategy.    or    (b) Add a ForeignKey annotation to specify the foreign key   constraint --  for example,        I have some misgivings about approach (b), and, given a lack of  database portability here, we might wind up needing a   foreignKeyDefinition  fragment anyway.      (3) Other    What about the ability to distinguish a CHAR rather than VARCHAR  mapping for strings?  Or should we just leave this as belonging in a  SQL fragment (with the understanding that we would be intentionally  discouraging the use of CHAR strings)?    (4) Anything else I'm missing?      APIs    I assumed the addition of a   PersistenceProvider.generateSchema() method  above.    If we also support schema generation at EMF creation time, do we want  another createContainerEntityManagerFactory() method that takes a  SchemaGenerationInfo as well as PersistenceUnitInfo argument ?    If generation is done directly in the database, at some point the  container (or application) may need to have the schema/tables dropped.  I'm not sure of the best way to support this.   A   dropAndClose() method  on the EMF??        </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Thu May 10 08:30:14 CEST 2012</date>
    <body>See below. -----Original Message----- From: Linda DeMichiel [ mailto:linda.demichiel@...]  Sent: Wednesday, May 09, 2012 8:37 PM To: jsr338-experts@... Subject: [jsr338-experts] JPA schema generation In conjunction with support for use of JPA in PaaS environments, we will need  to address the issue of schema generation.  After we discuss this area, I'd  like to return to a broader discussion of multitenancy support with this in  view. Here's what I think we'll need to address: 1.  How/when schema generation happens:      a. at a deployment time phase (or predeployment time phase) [lmcnise] Useful for testing and or quick deployment, so it should be in.      b. at EMF creation time [lmcnise] That is an interesting idea, as you could then have entities not  generating a schema/table for as long as the entity manager is inactive. 2.  Providing an option whereby the application can supply DDL for      schema generation as an alternative to the persistence provider      doing schema generation based on the ORM metadata [lmcnise] Only valid option for a production system. It is also how you can  upgrade an in production system by applying small changes to the DDL. 3.  How, once schema generation has happened, tables can get loaded with      data.  If we enable the application to provide DDL scripts for      schema generation, then supporting the use of SQL bulk insert scripts      would go hand-in-hand with this, but perhaps there are also other      approaches, e.g., bulk insert via JPQL(?). [lmcnise] Don’t understand. 4.  The need for additional ORM metadata:  e.g. metadata for index      generation; for foreign key constraints; other(?). [lmcnise] Yes.  Hibernate has interesting annotations to cover this.  Much  more convenient than to have a separate file containing the different  indexes, etc. Ex:     @Basic  // Std JPA     @Index(name = "EDB_NODE_PARENT_ID_IDX")  // hibernate @java.lang.annotation.Retention(java.lang.annotation.RetentionPolicy.RUNTIME)           5.  More precise definition of existing metadata that can be used      for schema generation:  what it comprises, and how it can/must      be used. [lmcnise] Goes hand in hand with 4.   6.  Additional persistence.xml level metadata and additional metadata      the platform provider passes to the persistence provider to control      or configure the operations outlined above. [lmcnise] Any ideas what sort of control/configuration?  Is it something like  being able to control the schema generation based on the DB engine? For  example, MySQL InnoDB, NDB are different schemas as NDB does not support  Foreign key support. I am assuming for that Java EE 7 we need to support both the separate  database and shared database (aka separate schema) approaches. I am also assuming that it is the platform provider that does the  provisioning of either the database or the schema respectively (i.e., in  accordance with whether the separate database or shared database approach is taken).   I.e., in the separate database approach, I'm assuming that it is the persistence provider that creates the schemas  directly in the database or creates the DDL to generate the schemas needed  for the persistence unit.  [I'll use the term "schema generation" generically  below to mean either table generation or schema+table generation with this assumption in view.] Here's a proposal to get the discussion started. SCHEMA GENERATION I think we should allow schema generation to be done either as a deployment  (or pre-deployment) phase or at createEMF time. Deployment-time (or predeployment-time) schema generation could be done  either by the platform provider calling into the PersistenceProvider  implementation to generate the database schema or by the platform provider  running SQL DDL scripts. SQL DDL scripts might be provided with the application (bundled as part of  the persistence unit) or the platform provider might invoke the  PersistenceProvider implementation to generate them from the persistence unit  metadata.  If scripts are provided or generated, it should be the platform  provider's responsibility to run them against the database.  If the  application has provided DDL scripts, the application should also have the  option to provide SQL scripts to do a bulk insert of data.  It would be the  responsibility of the platform provider to run these as well. An alternative to the use of scripts is for the platform provider to call  into the PersistenceProvider to directly generate the tables in the database.   In the case that a database has been provisioned rather than only a schema,  the persistence provider would generate the schemas and tables. For the sake of argument, let's assume that we add a generateSchema() method  to the PersistenceProvider interface, which the platform provider invokes  during the deployment phase if SQL scripts have not been provided by the  application. Information that needs to be passed in would include the following: (1) Information that specifies how schema generation is to proceed.      This information could take the form of a SchemaGenerationInfo      interface or it could be in the form of additional standard properties. (2) Connection to the database or schema.  [Is this a Connection or a      DataSource ?] (3) PersistenceUnitInfo The SchemaGenerationInfo should include the following:    ddl-generation-mode:  GENERATE_AND_EXECUTE | GENERATE_ONLY | EXECUTE_ONLY        GENERATE_AND_EXECUTE =&amp;gt; generate ddl scripts and generate schema (if          applicable) and tables directly in the database        GENERATE_ONLY =&amp;gt; generate ddl scripts        EXECUTE_ONLY =&amp;gt; generate schema (if applicable) and tables directly in           the database    create-ddl-target:  a Writer configured for the persistence provider        for outputting of the DDL scripts for the creation of schema/tables.        This should be null if EXECUTE_ONLY ddl-generation-mode is specified.    drop-ddl-target: a Writer configured for the persistence provider        for outputting of the DDL scripts for the dropping of schema/tables.        The should be null if EXECUTE_ONLY ddl-generation-mode is specified.    ddl-execution-mode: Used when executing directly against the database,        rather than generating scripts.  Options should include:         CREATE_TABLES | DROP_TABLES | DROP_AND_CREATE_TABLES       Do we also need to distinguish CREATE_SCHEMAS_AND_TABLES ? DROP_SCHEMA ?         Or can these be implicit?       Do we also need an ALTER_TABLES capability here?    properties:  These could include any vendor-specific properties ADDITIONAL METADATA (persistence.xml) If scripts are provided with the application (i.e., bundled as part of the  persistence unit), we should add entries in the persistence.xml to identify  them:        create-ddl-script        drop-ddl-script        data-load-script(s)  [Do we need to support more than one load script?           If so, the ordering may need to be specified as well.] Open issue: do we need metadata in the persistence.xml for    ddl-execution-mode as well?  For example, this might be used in    standalone scenarios (?) ORM METADATA (annotations and orm.xml) (1) Indexes I think we need to support indexes. Here's a strawman annotation:    The columnList syntax could follow that of the OrderBy annotation:      columnList::= indexColumn [, indexColumn]*      indexColumn::= columnName [ASC | DESC]      If ASC or DESC is not specified, ASC is assumed We'd also need to add to Table, Secondary Table, CollectionTable, JoinTable, and TableGenerator (2) Foreign Key Constraints I see two possible approaches: (a) Add a string-valued foreignKeyDefinition element to JoinColumn,  JoinColumns, MapKeyJoinColumn, etc. to specify a SQL fragment (intended along  the lines of columnDefinition) for defining a foreign key constraint and/or  for overriding the persistence provider's default foreign key definition.  It  might also be helpful to allow a name for the foreign key constraint to be  specified for the case where the provider is using its default foreign key  generation strategy. or (b) Add a ForeignKey annotation to specify the foreign key constraint -- for  example, I have some misgivings about approach (b), and, given a lack of database  portability here, we might wind up needing a foreignKeyDefinition fragment  anyway. (3) Other What about the ability to distinguish a CHAR rather than VARCHAR mapping for  strings?  Or should we just leave this as belonging in a SQL fragment (with  the understanding that we would be intentionally discouraging the use of CHAR  strings)? (4) Anything else I'm missing? APIs I assumed the addition of a PersistenceProvider.generateSchema() method above. If we also support schema generation at EMF creation time, do we want another  createContainerEntityManagerFactory() method that takes a  SchemaGenerationInfo as well as PersistenceUnitInfo argument ? If generation is done directly in the database, at some point the container  (or application) may need to have the schema/tables dropped. I'm not sure of the best way to support this.   A dropAndClose() method on the EMF??</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] question regarding jta-data-source</header>
    <date>Thu May 10 13:50:37 CEST 2012</date>
    <body>Hello, It is not explicitly mentioned in the spec, but I guess I can safely assume, that the data source denoted by jta-data-source is of type "javax.sql.XADataSource"? After the JPA implementation has obtained an XAConnection from this XADataSource, is it expected to to do any calls (and if so, in any specific order) on this XAConnection? E.g. is it necessary, to call XAConnection.getConnection() to trigger that the app server calls XAConnection.getXAResource() on the very same XAConnection? Or is it sufficient to just call "XADataSource.getXAConnection()" to trigger this? (Some background: we don't use JDBC connections (being not an ORM) and probably we would need to provide our own XADataSource implementation, so that the app server picks up our own XAResource implementation) Thank you! Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] clarification needed regarding EntityManager pooling in J2EE environment</header>
    <date>Thu May 10 15:58:34 CEST 2012</date>
    <body>Dear group, The spec (2.0) says, p.304, section 7.9.1, footnote 79: "The container may choose to pool EntityManagers: it instead of creating and closing in each case, it may acquire one from its pool and call clear() on it." What should happen with any property settings that were changed by the user? E.g. setFlushMode or any vendor-specific properties? Should clear() also reset them to the values configured for the EntityManagerFactory? I think this would make sense to get a clean, reusable EntityManager. And if this is the common understanding of how clear() should work, I suggest that we add that in the spec. Thank you! Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding jta-data-source</header>
    <date>Fri May 11 15:14:37 CEST 2012</date>
    <body>Hi Christian, The data source referenced by the jta-data-source name is a DataSource provided by the container in JNDI. It is the same one that any application could look up and use, hence it is a javax.sql.DataSource. XADataSource is an internal type used by the driver and container to coordinate XA, but users of the data source are not expected to participate at that level. The JPA provider acts like a client of the data source, using its connections to read and write, so it does not need to be aware of the XA protocol being implemented underneath. Hope this makes things clearer. Regards, -Mike Hello, It is not explicitly mentioned in the spec, but I guess I can safely assume, that the data source denoted by jta-data-source is of type "javax.sql.XADataSource"? After the JPA implementation has obtained an XAConnection from this XADataSource, is it expected to to do any calls (and if so, in any specific order) on this XAConnection? E.g. is it necessary, to call XAConnection.getConnection() to trigger that the app server calls XAConnection.getXAResource() on the very same XAConnection? Or is it sufficient to just call "XADataSource.getXAConnection()" to trigger this? (Some background: we don't use JDBC connections (being not an ORM) and probably we would need to provide our own XADataSource implementation, so that the app server picks up our own XAResource implementation) Thank you! Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: clarification needed regarding EntityManager pooling in J2EE environment</header>
    <date>Fri May 11 15:22:53 CEST 2012</date>
    <body>Hi Christian, The clear() method was not intended to reinitialize the EM, but is just a method that causes all of the entities in the persistence context to be detached. You can actually see an example on page 300 that highlights this fact. The footnote you reference was just making reference to the fact that a clear() call would need to be made on the EM in order for it to be reused. It was not intended to be a complete description of how to reuse an EM, or of other issues that might also need to be handled. I admit that it could be a bit misleading, though... Regards, -Mike Dear group, The spec (2.0) says, p.304, section 7.9.1, footnote 79: "The container may choose to pool EntityManagers: it instead of creating and closing in each case, it may acquire one from its pool and call clear() on it." What should happen with any property settings that were changed by the user? E.g. setFlushMode or any vendor-specific properties? Should clear() also reset them to the values configured for the EntityManagerFactory? I think this would make sense to get a clean, reusable EntityManager. And if this is the common understanding of how clear() should work, I suggest that we add that in the spec. Thank you! Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: clarification needed regarding EntityManager pooling in J2EE environment</header>
    <date>Fri May 11 15:35:38 CEST 2012</date>
    <body>Hi Mike, How should the app server reinitialize the entity manager instance then? Regards, Christian On Fri, May 11, 2012 at 3:22 PM, michael keith michael.keith@... Hi Christian, The clear() method was not intended to reinitialize the EM, but is just a method that causes all of the entities in the persistence context to be detached. You can actually see an example on page 300 that highlights this fact. The footnote you reference was just making reference to the fact that a clear() call would need to be made on the EM in order for it to be reused. It was not intended to be a complete description of how to reuse an EM, or of other issues that might also need to be handled. I admit that it could be a bit misleading, though... Regards, -Mike Dear group, The spec (2.0) says, p.304, section 7.9.1, footnote 79: "The container may choose to pool EntityManagers: it instead of creating and closing in each case, it may acquire one from its pool and call clear() on it." What should happen with any property settings that were changed by the user? E.g. setFlushMode or any vendor-specific properties? Should clear() also reset them to the values configured for the EntityManagerFactory? I think this would make sense to get a clean, reusable EntityManager. And if this is the common understanding of how clear() should work, I suggest that we add that in the spec. Thank you! Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding jta-data-source</header>
    <date>Fri May 11 15:45:45 CEST 2012</date>
    <body>Hi Mike, Unlike ORMs, we need to provide the XAResource instances to the app server (which in turn are associated with our internal connections to the database server) But I guess, if the user just sees a DataSource, then that will be an app server proxy which eventually delegates to the XADataSource of our "JDBC" driver (provided I configure a DataSource that returns enlisted connection in the app server using our JDBC driver) So my pseudo-code assumption of the app servers DataSource proxy is like this:         XADataSource xaDs = ...;   //XADataSource of the installed JDBC driver for this pool     ... So we could simply call getConnection() (and not using the returned value at all) to trigger the app servers enlisting our XAResource. Regards, Christian On Fri, May 11, 2012 at 3:14 PM, michael keith michael.keith@... Hi Christian, The data source referenced by the jta-data-source name is a DataSource provided by the container in JNDI. It is the same one that any application could look up and use, hence it is a javax.sql.DataSource. XADataSource is an internal type used by the driver and container to coordinate XA, but users of the data source are not expected to participate at that level. The JPA provider acts like a client of the data source, using its connections to read and write, so it does not need to be aware of the XA protocol being implemented underneath. Hope this makes things clearer. Regards, -Mike Hello, It is not explicitly mentioned in the spec, but I guess I can safely assume, that the data source denoted by jta-data-source is of type "javax.sql.XADataSource"? After the JPA implementation has obtained an XAConnection from this XADataSource, is it expected to to do any calls (and if so, in any specific order) on this XAConnection? E.g. is it necessary, to call XAConnection.getConnection() to trigger that the app server calls XAConnection.getXAResource() on the very same XAConnection? Or is it sufficient to just call "XADataSource.getXAConnection()" to trigger this? (Some background: we don't use JDBC connections (being not an ORM) and probably we would need to provide our own XADataSource implementation, so that the app server picks up our own XAResource implementation) Thank you! Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding jta-data-source</header>
    <date>Fri May 11 20:29:53 CEST 2012</date>
    <body>Hi Christian,   A persistence unit in JPA does not expose whether it is using a XAResouce for underlying database. In a managed container enviroment, a persistence context synchronizes its transaction with the containers transaction  via JTA transaction synchrization interface i.e. before/afterCompletion(). . Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Christian Romberg ---05/11/2012 06:46:00 AM---Hi Mike, Unlike ORMs, we need to provide the XAResource instances to the app server From: To: Cc: jsr338-experts@... Date: 05/11/2012 06:46 AM Subject: [jsr338-experts] Re: question regarding jta-data-source Hi Mike, Unlike ORMs, we need to provide the XAResource instances to the app server (which in turn are associated with our internal connections to the database server) But I guess, if the user just sees a DataSource, then that will be an app server proxy which eventually delegates to the XADataSource of our "JDBC" driver (provided I configure a DataSource that returns enlisted connection in the app server using our JDBC driver) So my pseudo-code assumption of the app servers DataSource proxy is like this:         XADataSource xaDs = ...;   //XADataSource of the installed JDBC driver for this pool     ... So we could simply call getConnection() (and not using the returned value at all) to trigger the app servers enlisting our XAResource. Regards, Christian michael.keith@... Hi Christian, The data source referenced by the jta-data-source name is a DataSource provided by the container in JNDI. It is the same one that any application could look up and use, hence it is a javax.sql.DataSource. XADataSource is an internal type used by the driver and container to coordinate XA, but users of the data source are not expected to participate at that level. The JPA provider acts like a client of the data source, using its connections to read and write, so it does not need to be aware of the XA protocol being implemented underneath. Hope this makes things clearer. Regards, -Mike Hello, It is not explicitly mentioned in the spec, but I guess I can safely assume, that the data source denoted by jta-data-source is of type "javax.sql.XADataSource"? After the JPA implementation has obtained an XAConnection from this XADataSource, is it expected to to do any calls (and if so, in any specific order) on this XAConnection? E.g. is it necessary, to call XAConnection.getConnection() to trigger that the app server calls XAConnection.getXAResource() on the very same XAConnection? Or is it sufficient to just call "XADataSource.getXAConnection()" to trigger this? (Some background: we don't use JDBC connections (being not an ORM) and probably we would need to provide our own XADataSource implementation, so that the app server picks up our own XAResource implementation) Thank you! Christian -- Christian Romberg Chief Engineer  | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com  | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message. -- Christian Romberg Chief Engineer  | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com  | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding jta-data-source</header>
    <date>Fri May 11 22:45:05 CEST 2012</date>
    <body>Hi Christian, Yes, that's pretty much how it works. -Mike Hi Mike, Unlike ORMs, we need to provide the XAResource instances to the app server (which in turn are associated with our internal connections to the database server) But I guess, if the user just sees a DataSource, then that will be an app server proxy which eventually delegates to the XADataSource of our "JDBC" driver (provided I configure a DataSource that returns enlisted connection in the app server using our JDBC driver) So my pseudo-code assumption of the app servers DataSource proxy is like this:         XADataSource xaDs = ...;   //XADataSource of the installed JDBC driver for this pool     ... So we could simply call getConnection() (and not using the returned value at all) to trigger the app servers enlisting our XAResource. Regards, Christian On Fri, May 11, 2012 at 3:14 PM, michael keith michael.keith@... &lt;blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"&gt; Hi Christian, The data source referenced by the jta-data-source name is a DataSource provided by the container in JNDI. It is the same one that any application could look up and use, hence it is a javax.sql.DataSource. XADataSource is an internal type used by the driver and container to coordinate XA, but users of the data source are not expected to participate at that level. The JPA provider acts like a client of the data source, using its connections to read and write, so it does not need to be aware of the XA protocol being implemented underneath. Hope this makes things clearer. Regards, -Mike Hello, It is not explicitly mentioned in the spec, but I guess I can safely assume, that the data source denoted by jta-data-source is of type "javax.sql.XADataSource"? After the JPA implementation has obtained an XAConnection from this XADataSource, is it expected to to do any calls (and if so, in any specific order) on this XAConnection? E.g. is it necessary, to call XAConnection.getConnection() to trigger that the app server calls XAConnection.getXAResource() on the very same XAConnection? Or is it sufficient to just call "XADataSource.getXAConnection()" to trigger this? (Some background: we don't use JDBC connections (being not an ORM) and probably we would need to provide our own XADataSource implementation, so that the app server picks up our own XAResource implementation) Thank you! Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: clarification needed regarding EntityManager pooling in J2EE environment</header>
    <date>Fri May 11 22:48:34 CEST 2012</date>
    <body>Hi Christian, Well, that's up to the container, but I suppose it would have to query the properties and save the initial values, then reset back to the default ones when reassigning. I wouldn't actually recommend doing it, since EM's are supposed to be lightweight. -Mike Hi Mike, How should the app server reinitialize the entity manager instance then? Regards, Christian On Fri, May 11, 2012 at 3:22 PM, michael keith michael.keith@... &lt;blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"&gt; Hi Christian, The clear() method was not intended to reinitialize the EM, but is just a method that causes all of the entities in the persistence context to be detached. You can actually see an example on page 300 that highlights this fact. The footnote you reference was just making reference to the fact that a clear() call would need to be made on the EM in order for it to be reused. It was not intended to be a complete description of how to reuse an EM, or of other issues that might also need to be handled. I admit that it could be a bit misleading, though... Regards, -Mike Dear group, The spec (2.0) says, p.304, section 7.9.1, footnote 79: "The container may choose to pool EntityManagers: it instead of creating and closing in each case, it may acquire one from its pool and call clear() on it." What should happen with any property settings that were changed by the user? E.g. setFlushMode or any vendor-specific properties? Should clear() also reset them to the values configured for the EntityManagerFactory? I think this would make sense to get a clean, reusable EntityManager. And if this is the common understanding of how clear() should work, I suggest that we add that in the spec. Thank you! Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding jta-data-source</header>
    <date>Mon May 14 10:17:09 CEST 2012</date>
    <body>Hi Michael and Pinaki, Yes, I understood how this is supposed to work, however this does not and can not work for us this way out-of-the box. I think I forgot to emphasize, that we are not an ORM and are not using JDBC connections. Of course we can (and would) register a Synchronization object with the global transaction. But where should the container get the XAResource from? Because we are not using JDBC connections, which implies, we are not using any DataSource. So there is simply no way for the app server to drive the XA protocol. Unless of course, we would provide a "fake" XADataSource in a "fake" JDBC driver, and given the pseudo-code assumption from my previous mail, we would then just call "getConnection()" on the jta-data-source (which points to an app server pool configured with our "fake" JDBC driver, configured to return enlisted connections) and don't actually use the result of the "getConnection()" invocation (which is a dummy implementation anyways) I'm pretty sure, that my pseudo-code assumption is correct, mostly because I can not think of any other straight-forward (or even weird) way how to implement that in the app server. So this would be the way to go for us. Regards, Christian On Fri, May 11, 2012 at 10:45 PM, michael keith michael.keith@... Hi Christian, Yes, that's pretty much how it works. -Mike Hi Mike, Unlike ORMs, we need to provide the XAResource instances to the app server (which in turn are associated with our internal connections to the database server) But I guess, if the user just sees a DataSource, then that will be an app server proxy which eventually delegates to the XADataSource of our "JDBC" driver (provided I configure a DataSource that returns enlisted connection in the app server using our JDBC driver) So my pseudo-code assumption of the app servers DataSource proxy is like this:         XADataSource xaDs = ...;   //XADataSource of the installed JDBC driver for this pool     ... So we could simply call getConnection() (and not using the returned value at all) to trigger the app servers enlisting our XAResource. Regards, Christian On Fri, May 11, 2012 at 3:14 PM, michael keith michael.keith@... Hi Christian, The data source referenced by the jta-data-source name is a DataSource provided by the container in JNDI. It is the same one that any application could look up and use, hence it is a javax.sql.DataSource. XADataSource is an internal type used by the driver and container to coordinate XA, but users of the data source are not expected to participate at that level. The JPA provider acts like a client of the data source, using its connections to read and write, so it does not need to be aware of the XA protocol being implemented underneath. Hope this makes things clearer. Regards, -Mike Hello, It is not explicitly mentioned in the spec, but I guess I can safely assume, that the data source denoted by jta-data-source is of type "javax.sql.XADataSource"? After the JPA implementation has obtained an XAConnection from this XADataSource, is it expected to to do any calls (and if so, in any specific order) on this XAConnection? E.g. is it necessary, to call XAConnection.getConnection() to trigger that the app server calls XAConnection.getXAResource() on the very same XAConnection? Or is it sufficient to just call "XADataSource.getXAConnection()" to trigger this? (Some background: we don't use JDBC connections (being not an ORM) and probably we would need to provide our own XADataSource implementation, so that the app server picks up our own XAResource implementation) Thank you! Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: clarification needed regarding EntityManager pooling in J2EE environment</header>
    <date>Mon May 14 10:21:30 CEST 2012</date>
    <body>Hi Mike, I agree, if I were up to implement that part in an app server, I also would not pool EMs. However given the fact, that I can't influence the behavior of the app server, and if we find out, that the app server does not restore the properties, then we propably need to introduce a workaround-switch in our implementation like "versant.resetPropertiesOnClear". Regards, Christian On Fri, May 11, 2012 at 10:48 PM, michael keith michael.keith@... Hi Christian, Well, that's up to the container, but I suppose it would have to query the properties and save the initial values, then reset back to the default ones when reassigning. I wouldn't actually recommend doing it, since EM's are supposed to be lightweight. -Mike Hi Mike, How should the app server reinitialize the entity manager instance then? Regards, Christian On Fri, May 11, 2012 at 3:22 PM, michael keith michael.keith@... Hi Christian, The clear() method was not intended to reinitialize the EM, but is just a method that causes all of the entities in the persistence context to be detached. You can actually see an example on page 300 that highlights this fact. The footnote you reference was just making reference to the fact that a clear() call would need to be made on the EM in order for it to be reused. It was not intended to be a complete description of how to reuse an EM, or of other issues that might also need to be handled. I admit that it could be a bit misleading, though... Regards, -Mike Dear group, The spec (2.0) says, p.304, section 7.9.1, footnote 79: "The container may choose to pool EntityManagers: it instead of creating and closing in each case, it may acquire one from its pool and call clear() on it." What should happen with any property settings that were changed by the user? E.g. setFlushMode or any vendor-specific properties? Should clear() also reset them to the values configured for the EntityManagerFactory? I think this would make sense to get a clean, reusable EntityManager. And if this is the common understanding of how clear() should work, I suggest that we add that in the spec. Thank you! Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding jta-data-source</header>
    <date>Mon May 14 14:42:43 CEST 2012</date>
    <body>Christian, Non-JDBC data sources are typically deployed in the form of resource adapters, using the connector SPI. JPA was designed for JDBC data sources and drivers, not other kinds of back end data.  The spec can't help you, there. -Mike Hi Michael and Pinaki, Yes, I understood how this is supposed to work, however this does not and can not work for us this way out-of-the box. I think I forgot to emphasize, that we are not an ORM and are not using JDBC connections. Of course we can (and would) register a Synchronization object with the global transaction. But where should the container get the XAResource from? Because we are not using JDBC connections, which implies, we are not using any DataSource. So there is simply no way for the app server to drive the XA protocol. Unless of course, we would provide a "fake" XADataSource in a "fake" JDBC driver, and given the pseudo-code assumption from my previous mail, we would then just call "getConnection()" on the jta-data-source (which points to an app server pool configured with our "fake" JDBC driver, configured to return enlisted connections) and don't actually use the result of the "getConnection()" invocation (which is a dummy implementation anyways) I'm pretty sure, that my pseudo-code assumption is correct, mostly because I can not think of any other straight-forward (or even weird) way how to implement that in the app server. So this would be the way to go for us. Regards, Christian On Fri, May 11, 2012 at 10:45 PM, michael keith michael.keith@... &lt;blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"&gt; Hi Christian, Yes, that's pretty much how it works. -Mike Hi Mike, Unlike ORMs, we need to provide the XAResource instances to the app server (which in turn are associated with our internal connections to the database server) But I guess, if the user just sees a DataSource, then that will be an app server proxy which eventually delegates to the XADataSource of our "JDBC" driver (provided I configure a DataSource that returns enlisted connection in the app server using our JDBC driver) So my pseudo-code assumption of the app servers DataSource proxy is like this: public class ContainerDataSourceProxy implements         XADataSource xaDs = ...;   //XADataSource of the installed JDBC driver for this pool         public Connection getConnection() throws ... So we could simply call getConnection() (and not using the returned value at all) to trigger the app servers enlisting our XAResource. Regards, Christian On Fri, May 11, 2012 at 3:14 PM, michael keith michael.keith@... &lt;blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"&gt; Hi Christian, The data source referenced by the jta-data-source name is a DataSource provided by the container in JNDI. It is the same one that any application could look up and use, hence it is a javax.sql.DataSource. XADataSource is an internal type used by the driver and container to coordinate XA, but users of the data source are not expected to participate at that level. The JPA provider acts like a client of the data source, using its connections to read and write, so it does not need to be aware of the XA protocol being implemented underneath. Hope this makes things clearer. Regards, -Mike On 10/05/2012 7:50 AM, Christian Romberg Hello, It is not explicitly mentioned in the spec, but I guess I can safely assume, that the data source denoted by jta-data-source is of type "javax.sql.XADataSource"? After the JPA implementation has obtained an XAConnection from this XADataSource, is it expected to to do any calls (and if so, in any specific order) on this XAConnection? E.g. is it necessary, to call XAConnection.getConnection() to trigger that the app server calls XAConnection.getXAResource() on the very same XAConnection? Or is it sufficient to just call "XADataSource.getXAConnection()" to trigger this? (Some background: we don't use JDBC connections (being not an ORM) and probably we would need to provide our own XADataSource implementation, so that the app server picks up our own XAResource implementation) Thank you! Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding jta-data-source</header>
    <date>Mon May 14 14:54:46 CEST 2012</date>
    <body>Mike, Adding the indirection of a resource adapter only adds an indirection, but the control flow stays the same. Also we had our huge share of experiences with JCA in the past, cough, cough, then better a fake JDBC directly, at least JDBC connection pools should be properly tested and scalable in every app server. Alternatively I have to research the JTA spec, whether there is a compliant way to register an XADataSource directly with the transaction manager, which could also work. Christian On Mon, May 14, 2012 at 2:42 PM, michael keith michael.keith@... Christian, Non-JDBC data sources are typically deployed in the form of resource adapters, using the connector SPI. JPA was designed for JDBC data sources and drivers, not other kinds of back end data.  The spec can't help you, there. -Mike Hi Michael and Pinaki, Yes, I understood how this is supposed to work, however this does not and can not work for us this way out-of-the box. I think I forgot to emphasize, that we are not an ORM and are not using JDBC connections. Of course we can (and would) register a Synchronization object with the global transaction. But where should the container get the XAResource from? Because we are not using JDBC connections, which implies, we are not using any DataSource. So there is simply no way for the app server to drive the XA protocol. Unless of course, we would provide a "fake" XADataSource in a "fake" JDBC driver, and given the pseudo-code assumption from my previous mail, we would then just call "getConnection()" on the jta-data-source (which points to an app server pool configured with our "fake" JDBC driver, configured to return enlisted connections) and don't actually use the result of the "getConnection()" invocation (which is a dummy implementation anyways) I'm pretty sure, that my pseudo-code assumption is correct, mostly because I can not think of any other straight-forward (or even weird) way how to implement that in the app server. So this would be the way to go for us. Regards, Christian On Fri, May 11, 2012 at 10:45 PM, michael keith michael.keith@... Hi Christian, Yes, that's pretty much how it works. -Mike Hi Mike, Unlike ORMs, we need to provide the XAResource instances to the app server (which in turn are associated with our internal connections to the database server) But I guess, if the user just sees a DataSource, then that will be an app server proxy which eventually delegates to the XADataSource of our "JDBC" driver (provided I configure a DataSource that returns enlisted connection in the app server using our JDBC driver) So my pseudo-code assumption of the app servers DataSource proxy is like this: public class ContainerDataSourceProxy implements         XADataSource xaDs = ...;   //XADataSource of the installed JDBC driver for this pool         public Connection getConnection() throws ... So we could simply call getConnection() (and not using the returned value at all) to trigger the app servers enlisting our XAResource. Regards, Christian On Fri, May 11, 2012 at 3:14 PM, michael keith michael.keith@... Hi Christian, The data source referenced by the jta-data-source name is a DataSource provided by the container in JNDI. It is the same one that any application could look up and use, hence it is a javax.sql.DataSource. XADataSource is an internal type used by the driver and container to coordinate XA, but users of the data source are not expected to participate at that level. The JPA provider acts like a client of the data source, using its connections to read and write, so it does not need to be aware of the XA protocol being implemented underneath. Hope this makes things clearer. Regards, -Mike On 10/05/2012 7:50 AM, Christian Romberg Hello, It is not explicitly mentioned in the spec, but I guess I can safely assume, that the data source denoted by jta-data-source is of type "javax.sql.XADataSource"? After the JPA implementation has obtained an XAConnection from this XADataSource, is it expected to to do any calls (and if so, in any specific order) on this XAConnection? E.g. is it necessary, to call XAConnection.getConnection() to trigger that the app server calls XAConnection.getXAResource() on the very same XAConnection? Or is it sufficient to just call "XADataSource.getXAConnection()" to trigger this? (Some background: we don't use JDBC connections (being not an ORM) and probably we would need to provide our own XADataSource implementation, so that the app server picks up our own XAResource implementation) Thank you! Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding jta-data-source</header>
    <date>Mon May 14 15:17:45 CEST 2012</date>
    <body>Looks like that besides a Synchronization object, we can also register our XADataSource directly via the javax.transaction.Transaction interface. If the JTA implementation is allowed to lookup the transaction manager for the purpose of registering a Synchronization instance, then I expect it permission-wise not to be a problem, to also register an XADataSource. This would be an alternative to write a "fake" JDBC driver. On Mon, May 14, 2012 at 2:54 PM, Christian Romberg cromberg@... Mike, Adding the indirection of a resource adapter only adds an indirection, but the control flow stays the same. Also we had our huge share of experiences with JCA in the past, cough, cough, then better a fake JDBC directly, at least JDBC connection pools should be properly tested and scalable in every app server. Alternatively I have to research the JTA spec, whether there is a compliant way to register an XADataSource directly with the transaction manager, which could also work. Christian On Mon, May 14, 2012 at 2:42 PM, michael keith michael.keith@... Christian, Non-JDBC data sources are typically deployed in the form of resource adapters, using the connector SPI. JPA was designed for JDBC data sources and drivers, not other kinds of back end data.  The spec can't help you, there. -Mike Hi Michael and Pinaki, Yes, I understood how this is supposed to work, however this does not and can not work for us this way out-of-the box. I think I forgot to emphasize, that we are not an ORM and are not using JDBC connections. Of course we can (and would) register a Synchronization object with the global transaction. But where should the container get the XAResource from? Because we are not using JDBC connections, which implies, we are not using any DataSource. So there is simply no way for the app server to drive the XA protocol. Unless of course, we would provide a "fake" XADataSource in a "fake" JDBC driver, and given the pseudo-code assumption from my previous mail, we would then just call "getConnection()" on the jta-data-source (which points to an app server pool configured with our "fake" JDBC driver, configured to return enlisted connections) and don't actually use the result of the "getConnection()" invocation (which is a dummy implementation anyways) I'm pretty sure, that my pseudo-code assumption is correct, mostly because I can not think of any other straight-forward (or even weird) way how to implement that in the app server. So this would be the way to go for us. Regards, Christian On Fri, May 11, 2012 at 10:45 PM, michael keith michael.keith@... Hi Christian, Yes, that's pretty much how it works. -Mike Hi Mike, Unlike ORMs, we need to provide the XAResource instances to the app server (which in turn are associated with our internal connections to the database server) But I guess, if the user just sees a DataSource, then that will be an app server proxy which eventually delegates to the XADataSource of our "JDBC" driver (provided I configure a DataSource that returns enlisted connection in the app server using our JDBC driver) So my pseudo-code assumption of the app servers DataSource proxy is like this: public class ContainerDataSourceProxy implements         XADataSource xaDs = ...;   //XADataSource of the installed JDBC driver for this pool         public Connection getConnection() throws ... So we could simply call getConnection() (and not using the returned value at all) to trigger the app servers enlisting our XAResource. Regards, Christian On Fri, May 11, 2012 at 3:14 PM, michael keith michael.keith@... Hi Christian, The data source referenced by the jta-data-source name is a DataSource provided by the container in JNDI. It is the same one that any application could look up and use, hence it is a javax.sql.DataSource. XADataSource is an internal type used by the driver and container to coordinate XA, but users of the data source are not expected to participate at that level. The JPA provider acts like a client of the data source, using its connections to read and write, so it does not need to be aware of the XA protocol being implemented underneath. Hope this makes things clearer. Regards, -Mike On 10/05/2012 7:50 AM, Christian Romberg Hello, It is not explicitly mentioned in the spec, but I guess I can safely assume, that the data source denoted by jta-data-source is of type "javax.sql.XADataSource"? After the JPA implementation has obtained an XAConnection from this XADataSource, is it expected to to do any calls (and if so, in any specific order) on this XAConnection? E.g. is it necessary, to call XAConnection.getConnection() to trigger that the app server calls XAConnection.getXAResource() on the very same XAConnection? Or is it sufficient to just call "XADataSource.getXAConnection()" to trigger this? (Some background: we don't use JDBC connections (being not an ORM) and probably we would need to provide our own XADataSource implementation, so that the app server picks up our own XAResource implementation) Thank you! Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding jta-data-source</header>
    <date>Mon May 14 15:27:35 CEST 2012</date>
    <body>Oops, typo, should have read "XAResource" instead of "XADataSource" in my last two emails. On Mon, May 14, 2012 at 3:17 PM, Christian Romberg cromberg@... Looks like that besides a Synchronization object, we can also register our XADataSource directly via the javax.transaction.Transaction interface. If the JTA implementation is allowed to lookup the transaction manager for the purpose of registering a Synchronization instance, then I expect it permission-wise not to be a problem, to also register an XADataSource. This would be an alternative to write a "fake" JDBC driver. On Mon, May 14, 2012 at 2:54 PM, Christian Romberg cromberg@... Mike, Adding the indirection of a resource adapter only adds an indirection, but the control flow stays the same. Also we had our huge share of experiences with JCA in the past, cough, cough, then better a fake JDBC directly, at least JDBC connection pools should be properly tested and scalable in every app server. Alternatively I have to research the JTA spec, whether there is a compliant way to register an XADataSource directly with the transaction manager, which could also work. Christian On Mon, May 14, 2012 at 2:42 PM, michael keith michael.keith@... Christian, Non-JDBC data sources are typically deployed in the form of resource adapters, using the connector SPI. JPA was designed for JDBC data sources and drivers, not other kinds of back end data.  The spec can't help you, there. -Mike Hi Michael and Pinaki, Yes, I understood how this is supposed to work, however this does not and can not work for us this way out-of-the box. I think I forgot to emphasize, that we are not an ORM and are not using JDBC connections. Of course we can (and would) register a Synchronization object with the global transaction. But where should the container get the XAResource from? Because we are not using JDBC connections, which implies, we are not using any DataSource. So there is simply no way for the app server to drive the XA protocol. Unless of course, we would provide a "fake" XADataSource in a "fake" JDBC driver, and given the pseudo-code assumption from my previous mail, we would then just call "getConnection()" on the jta-data-source (which points to an app server pool configured with our "fake" JDBC driver, configured to return enlisted connections) and don't actually use the result of the "getConnection()" invocation (which is a dummy implementation anyways) I'm pretty sure, that my pseudo-code assumption is correct, mostly because I can not think of any other straight-forward (or even weird) way how to implement that in the app server. So this would be the way to go for us. Regards, Christian On Fri, May 11, 2012 at 10:45 PM, michael keith michael.keith@... Hi Christian, Yes, that's pretty much how it works. -Mike Hi Mike, Unlike ORMs, we need to provide the XAResource instances to the app server (which in turn are associated with our internal connections to the database server) But I guess, if the user just sees a DataSource, then that will be an app server proxy which eventually delegates to the XADataSource of our "JDBC" driver (provided I configure a DataSource that returns enlisted connection in the app server using our JDBC driver) So my pseudo-code assumption of the app servers DataSource proxy is like this: public class ContainerDataSourceProxy implements         XADataSource xaDs = ...;   //XADataSource of the installed JDBC driver for this pool         public Connection getConnection() throws ... So we could simply call getConnection() (and not using the returned value at all) to trigger the app servers enlisting our XAResource. Regards, Christian On Fri, May 11, 2012 at 3:14 PM, michael keith michael.keith@... Hi Christian, The data source referenced by the jta-data-source name is a DataSource provided by the container in JNDI. It is the same one that any application could look up and use, hence it is a javax.sql.DataSource. XADataSource is an internal type used by the driver and container to coordinate XA, but users of the data source are not expected to participate at that level. The JPA provider acts like a client of the data source, using its connections to read and write, so it does not need to be aware of the XA protocol being implemented underneath. Hope this makes things clearer. Regards, -Mike On 10/05/2012 7:50 AM, Christian Romberg Hello, It is not explicitly mentioned in the spec, but I guess I can safely assume, that the data source denoted by jta-data-source is of type "javax.sql.XADataSource"? After the JPA implementation has obtained an XAConnection from this XADataSource, is it expected to to do any calls (and if so, in any specific order) on this XAConnection? E.g. is it necessary, to call XAConnection.getConnection() to trigger that the app server calls XAConnection.getXAResource() on the very same XAConnection? Or is it sufficient to just call "XADataSource.getXAConnection()" to trigger this? (Some background: we don't use JDBC connections (being not an ORM) and probably we would need to provide our own XADataSource implementation, so that the app server picks up our own XAResource implementation) Thank you! Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding jta-data-source</header>
    <date>Mon May 14 19:11:44 CEST 2012</date>
    <body>Christian, In a JPA environment, a JPA Persistence Unit registers as a transactional resource to the application server's global transaction management coordination system via JTA standard i.e. by  caling javax.transation.registerSynchronization(x) where x is something that a provider provides which implements javax.transaction.Synchronization. In a non-JDBC JPA provider like yours, you will possibly pass an instance of persistence context i.e EntityManager  as x. For an application server transaction management sevices, all it knows is x whether x is using an underlying JDBC or non-JDBC database is not relevant. So essentially the provider  will receive a beforeCompletion() call from application server when time comes to commit the global transaction via x because x is a registered Synchronization object.  At beforeCompletion(), the provider is responsible to commit on the underlying database be it JDBC or somethiing else. If a provider's persistence unit is also XA-enabled that capability is communicated by a) implementing javax.transaction.xa.XAResource and b) enlisting that resource to the application server. A resource, in this case, is the persistence unit, not the jta-data-source as declared in persistence.xml. In effect, I do not see why a non-JDBC based JPA provider has to appear as a JDBC connection to a application server to participate in a JTA transaction. I did notice that :) Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Christian Romberg ---05/14/2012 01:17:26 AM---Hi Michael and Pinaki, Yes, I understood how this is supposed to work, however this does not and From: To: Cc: jsr338-experts@... Date: 05/14/2012 01:17 AM Subject: [jsr338-experts] Re: question regarding jta-data-source Hi Michael and Pinaki, Yes, I understood how this is supposed to work, however this does not and can not work for us this way out-of-the box. I think I forgot to emphasize, that we are not an ORM and are not using JDBC connections. Of course we can (and would) register a Synchronization object with the global transaction. But where should the container get the XAResource from? Because we are not using JDBC connections, which implies, we are not using any DataSource. So there is simply no way for the app server to drive the XA protocol. Unless of course, we would provide a "fake" XADataSource in a "fake" JDBC driver, and given the pseudo-code assumption from my previous mail, we would then just call "getConnection()" on the jta-data-source (which points to an app server pool configured with our "fake" JDBC driver, configured to return enlisted connections) and don't actually use the result of the "getConnection()" invocation (which is a dummy implementation anyways) I'm pretty sure, that my pseudo-code assumption is correct, mostly because I can not think of any other straight-forward (or even weird) way how to implement that in the app server. So this would be the way to go for us. Regards, Christian michael.keith@... Hi Christian, Yes, that's pretty much how it works. -Mike Hi Mike, Unlike ORMs, we need to provide the XAResource instances to the app server (which in turn are associated with our internal connections to the database server) But I guess, if the user just sees a DataSource, then that will be an app server proxy which eventually delegates to the XADataSource of our "JDBC" driver (provided I configure a DataSource that returns enlisted connection in the app server using our JDBC driver) So my pseudo-code assumption of the app servers DataSource proxy is like this:         XADataSource xaDs = ...;   //XADataSource of the installed JDBC driver for this pool     ... So we could simply call getConnection() (and not using the returned value at all) to trigger the app servers enlisting our XAResource. Regards, Christian michael.keith@... Hi Christian, The data source referenced by the jta-data-source name is a DataSource provided by the container in JNDI. It is the same one that any application could look up and use, hence it is a javax.sql.DataSource. XADataSource is an internal type used by the driver and container to coordinate XA, but users of the data source are not expected to participate at that level. The JPA provider acts like a client of the data source, using its connections to read and write, so it does not need to be aware of the XA protocol being implemented underneath. Hope this makes things clearer. Regards, -Mike Hello, It is not explicitly mentioned in the spec, but I guess I can safely assume, that the data source denoted by jta-data-source is of type "javax.sql.XADataSource"? After the JPA implementation has obtained an XAConnection from this XADataSource, is it expected to to do any calls (and if so, in any specific order) on this XAConnection? E.g. is it necessary, to call XAConnection.getConnection() to trigger that the app server calls XAConnection.getXAResource() on the very same XAConnection? Or is it sufficient to just call "XADataSource.getXAConnection()" to trigger this? (Some background: we don't use JDBC connections (being not an ORM) and probably we would need to provide our own XADataSource implementation, so that the app server picks up our own XAResource implementation) Thank you! Christian -- Christian Romberg Chief Engineer  | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com  | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message. -- Christian Romberg Chief Engineer  | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com  | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message. -- Christian Romberg Chief Engineer  | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com  | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding jta-data-source</header>
    <date>Tue May 15 09:26:25 CEST 2012</date>
    <body>Hi Pinaki, As I pointed out, it would only be a "fake" JDBC driver,  I know, that the TM does not care, what type of resource is behind the XAResource, it's just that we need to enlist our XAResource with the current transactions. Either the app server picks it up itself (pull-approach, via a fake JDBC driver, or -in theory- a JCA adapter) or we enlist it directly (push-approach) In effect, I do not see why a non-JDBC based JPA provider has to appear as a JDBC connection to a application server to participate in a JTA transaction. Two simple reasons: -app servers do support JDBC connection pools and JCA is not a viable approach -there is no standard way to get access to the TransactionManager (however there is a standard way to get the TransactionSynchronizationRegistry, which is sufficient for other JPA implementations) However we will probably still use the alternative, to enlist our XAResource directly with the TransactionManager. So we will do some probing in the code, to find out which app server we are in and/or try out a list of well-known JNDI-names to look-up the TransactionManager. Regards, Christian On Mon, May 14, 2012 at 7:11 PM, Pinaki Poddar ppoddar@... Christian, In a JPA environment, a JPA Persistence Unit registers as a transactional resource to the application server's global transaction management coordination system via JTA standard i.e. by  caling javax.transation.registerSynchronization(x) where x is something that a provider provides which implements javax.transaction.Synchronization. In a non-JDBC JPA provider like yours, you will possibly pass an instance of persistence context i.e EntityManager  as x. For an application server transaction management sevices, all it knows is x whether x is using an underlying JDBC or non-JDBC database is not relevant. So essentially the provider  will receive a beforeCompletion() call from application server when time comes to commit the global transaction via x because x is a registered Synchronization object.  At beforeCompletion(), the provider is responsible to commit on the underlying database be it JDBC or somethiing else. If a provider's persistence unit is also XA-enabled that capability is communicated by a) implementing javax.transaction.xa.XAResource and b) enlisting that resource to the application server. A resource, in this case, is the persistence unit, not the jta-data-source as declared in persistence.xml. In effect, I do not see why a non-JDBC based JPA provider has to appear as a JDBC connection to a application server to participate in a JTA transaction. I did notice that :) Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Christian Romberg ---05/14/2012 01:17:26 AM---Hi Michael and Pinaki, Yes, I understood how this is supposed to work, however this does not and From: cromberg@... To: michael.keith@... Cc: jsr338-experts@... Date: 05/14/2012 01:17 AM Subject: [jsr338-experts] Re: question regarding jta-data-source Hi Michael and Pinaki, Yes, I understood how this is supposed to work, however this does not and can not work for us this way out-of-the box. I think I forgot to emphasize, that we are not an ORM and are not using JDBC connections. Of course we can (and would) register a Synchronization object with the global transaction. But where should the container get the XAResource from? Because we are not using JDBC connections, which implies, we are not using any DataSource. So there is simply no way for the app server to drive the XA protocol. Unless of course, we would provide a "fake" XADataSource in a "fake" JDBC driver, and given the pseudo-code assumption from my previous mail, we would then just call "getConnection()" on the jta-data-source (which points to an app server pool configured with our "fake" JDBC driver, configured to return enlisted connections) and don't actually use the result of the "getConnection()" invocation (which is a dummy implementation anyways) I'm pretty sure, that my pseudo-code assumption is correct, mostly because I can not think of any other straight-forward (or even weird) way how to implement that in the app server. So this would be the way to go for us. Regards, Christian michael.keith@... Hi Christian, Yes, that's pretty much how it works. -Mike Hi Mike, Unlike ORMs, we need to provide the XAResource instances to the app server (which in turn are associated with our internal connections to the database server) But I guess, if the user just sees a DataSource, then that will be an app server proxy which eventually delegates to the XADataSource of our "JDBC" driver (provided I configure a DataSource that returns enlisted connection in the app server using our JDBC driver) So my pseudo-code assumption of the app servers DataSource proxy is like this:         XADataSource xaDs = ...;   //XADataSource of the installed JDBC driver for this pool     ... So we could simply call getConnection() (and not using the returned value at all) to trigger the app servers enlisting our XAResource. Regards, Christian michael.keith@... Hi Christian, The data source referenced by the jta-data-source name is a DataSource provided by the container in JNDI. It is the same one that any application could look up and use, hence it is a javax.sql.DataSource. XADataSource is an internal type used by the driver and container to coordinate XA, but users of the data source are not expected to participate at that level. The JPA provider acts like a client of the data source, using its connections to read and write, so it does not need to be aware of the XA protocol being implemented underneath. Hope this makes things clearer. Regards, -Mike Hello, It is not explicitly mentioned in the spec, but I guess I can safely assume, that the data source denoted by jta-data-source is of type "javax.sql.XADataSource"? After the JPA implementation has obtained an XAConnection from this XADataSource, is it expected to to do any calls (and if so, in any specific order) on this XAConnection? E.g. is it necessary, to call XAConnection.getConnection() to trigger that the app server calls XAConnection.getXAResource() on the very same XAConnection? Or is it sufficient to just call "XADataSource.getXAConnection()" to trigger this? (Some background: we don't use JDBC connections (being not an ORM) and probably we would need to provide our own XADataSource implementation, so that the app server picks up our own XAResource implementation) Thank you! Christian -- Christian Romberg Chief Engineer  | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com  | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message. -- Christian Romberg Chief Engineer  | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com  | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message. -- Christian Romberg Chief Engineer  | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com  | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding jta-data-source</header>
    <date>Tue May 15 14:37:12 CEST 2012</date>
    <body>Hi, Now that I'm busy with implementing our own transaction manager (and eventually application server) for our test environment, I stumbled across a new issue, that is an argument pro-JDBC pool and against direct TM interaction: There seems to be no standard way, how to restore XAResources after a system failure, which is a short-coming of the JTA spec. The described way there suggests, that the TM logs JNDI names in it's transaction log records, which only works, if the pull-approach was used via a JDBC pool (or JCA based pool). However e.g. JBoss handles that properly, if the XAResource is Serializable, it will serialize it (so we will make our XAResource serializable) But for other app servers this might not work, and a fake JDBC pool would solve it for those. We will still go for direct TM interaction, especially since JBoss supports XAResource serialization. Regards, Christian On Tue, May 15, 2012 at 9:26 AM, Christian Romberg cromberg@... Hi Pinaki, As I pointed out, it would only be a "fake" JDBC driver,  I know, that the TM does not care, what type of resource is behind the XAResource, it's just that we need to enlist our XAResource with the current transactions. Either the app server picks it up itself (pull-approach, via a fake JDBC driver, or -in theory- a JCA adapter) or we enlist it directly (push-approach) In effect, I do not see why a non-JDBC based JPA provider has to appear as a JDBC connection to a application server to participate in a JTA transaction. Two simple reasons: -app servers do support JDBC connection pools and JCA is not a viable approach -there is no standard way to get access to the TransactionManager (however there is a standard way to get the TransactionSynchronizationRegistry, which is sufficient for other JPA implementations) However we will probably still use the alternative, to enlist our XAResource directly with the TransactionManager. So we will do some probing in the code, to find out which app server we are in and/or try out a list of well-known JNDI-names to look-up the TransactionManager. Regards, Christian On Mon, May 14, 2012 at 7:11 PM, Pinaki Poddar ppoddar@... Christian, In a JPA environment, a JPA Persistence Unit registers as a transactional resource to the application server's global transaction management coordination system via JTA standard i.e. by  caling javax.transation.registerSynchronization(x) where x is something that a provider provides which implements javax.transaction.Synchronization. In a non-JDBC JPA provider like yours, you will possibly pass an instance of persistence context i.e EntityManager  as x. For an application server transaction management sevices, all it knows is x whether x is using an underlying JDBC or non-JDBC database is not relevant. So essentially the provider  will receive a beforeCompletion() call from application server when time comes to commit the global transaction via x because x is a registered Synchronization object.  At beforeCompletion(), the provider is responsible to commit on the underlying database be it JDBC or somethiing else. If a provider's persistence unit is also XA-enabled that capability is communicated by a) implementing javax.transaction.xa.XAResource and b) enlisting that resource to the application server. A resource, in this case, is the persistence unit, not the jta-data-source as declared in persistence.xml. In effect, I do not see why a non-JDBC based JPA provider has to appear as a JDBC connection to a application server to participate in a JTA transaction. I did notice that :) Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Christian Romberg ---05/14/2012 01:17:26 AM---Hi Michael and Pinaki, Yes, I understood how this is supposed to work, however this does not and From: cromberg@... To: michael.keith@... Cc: jsr338-experts@... Date: 05/14/2012 01:17 AM Subject: [jsr338-experts] Re: question regarding jta-data-source Hi Michael and Pinaki, Yes, I understood how this is supposed to work, however this does not and can not work for us this way out-of-the box. I think I forgot to emphasize, that we are not an ORM and are not using JDBC connections. Of course we can (and would) register a Synchronization object with the global transaction. But where should the container get the XAResource from? Because we are not using JDBC connections, which implies, we are not using any DataSource. So there is simply no way for the app server to drive the XA protocol. Unless of course, we would provide a "fake" XADataSource in a "fake" JDBC driver, and given the pseudo-code assumption from my previous mail, we would then just call "getConnection()" on the jta-data-source (which points to an app server pool configured with our "fake" JDBC driver, configured to return enlisted connections) and don't actually use the result of the "getConnection()" invocation (which is a dummy implementation anyways) I'm pretty sure, that my pseudo-code assumption is correct, mostly because I can not think of any other straight-forward (or even weird) way how to implement that in the app server. So this would be the way to go for us. Regards, Christian michael.keith@... Hi Christian, Yes, that's pretty much how it works. -Mike Hi Mike, Unlike ORMs, we need to provide the XAResource instances to the app server (which in turn are associated with our internal connections to the database server) But I guess, if the user just sees a DataSource, then that will be an app server proxy which eventually delegates to the XADataSource of our "JDBC" driver (provided I configure a DataSource that returns enlisted connection in the app server using our JDBC driver) So my pseudo-code assumption of the app servers DataSource proxy is like this:         XADataSource xaDs = ...;   //XADataSource of the installed JDBC driver for this pool     ... So we could simply call getConnection() (and not using the returned value at all) to trigger the app servers enlisting our XAResource. Regards, Christian michael.keith@... Hi Christian, The data source referenced by the jta-data-source name is a DataSource provided by the container in JNDI. It is the same one that any application could look up and use, hence it is a javax.sql.DataSource. XADataSource is an internal type used by the driver and container to coordinate XA, but users of the data source are not expected to participate at that level. The JPA provider acts like a client of the data source, using its connections to read and write, so it does not need to be aware of the XA protocol being implemented underneath. Hope this makes things clearer. Regards, -Mike Hello, It is not explicitly mentioned in the spec, but I guess I can safely assume, that the data source denoted by jta-data-source is of type "javax.sql.XADataSource"? After the JPA implementation has obtained an XAConnection from this XADataSource, is it expected to to do any calls (and if so, in any specific order) on this XAConnection? E.g. is it necessary, to call XAConnection.getConnection() to trigger that the app server calls XAConnection.getXAResource() on the very same XAConnection? Or is it sufficient to just call "XADataSource.getXAConnection()" to trigger this? (Some background: we don't use JDBC connections (being not an ORM) and probably we would need to provide our own XADataSource implementation, so that the app server picks up our own XAResource implementation) Thank you! Christian -- Christian Romberg Chief Engineer  | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com  | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message. -- Christian Romberg Chief Engineer  | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com  | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message. -- Christian Romberg Chief Engineer  | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com  | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Tue May 15 19:57:15 CEST 2012</date>
    <body>Folks, I'd like to get more feedback on this. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Tue May 15 20:09:47 CEST 2012</date>
    <body>Is the hope to have schema generation supported with multi-tenancy?  I see no why for the JPA provider to know whether the schema needs to be generated because of this segmentation where an EMF only serves a single tenant.  So is the plan there to put the onus to keep track of that on the environment?  You seem to prefer the environment to manage the schema generation (the JPA provider simply generates a generation script as I read your proposal), so maybe this is not so bad. As for know if dropping is necessary, heck put all the onus on the environment ;)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Tue May 15 20:17:19 CEST 2012</date>
    <body>Yes -- all the pieces need to fit together. I see no why for the JPA provider to know whether I've also proposed that when the JPA provider "owns" the database (i.e., the  database is a dedicated rather than a shared database) that the JPA provider creates  the schemas in the database.  The process as a whole, however, is driven from the  platform provider side. I would really like to get feedback from you and the other experts in the  group on the details here.  The hard part is always in the details :-) thanks again, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Tue May 15 20:35:32 CEST 2012</date>
    <body>Well part of the problem is that JPA decided to go a totally different implementation direction for multi-tenancy support than what I did for Hibernate.  It takes a fair amount of extrapolation based on my work and experiences there to bear relevant discussion on this JPA approach. But I will read through your proposal some more this evening and respond back.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Tue May 15 20:43:24 CEST 2012</date>
    <body>We will need to make sure that we will be able to accommodate both  container-managed SaaS multitenancy (with stiping) as well as application-managed SaaS in the  future, so your experience here is very valuable.  If you could flag anything that might  stand in the way of that, it would be very helpful. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Tue May 15 20:56:56 CEST 2012</date>
    <body>I tried to point some of them out on the other thread(s), but they were not well received.  I think we are missing consolidated annotations for (1) naming/defining the "tenant discriminator column" and (2) an annotation to identify entities that are "shared data" (non tenant aware).  As is, every vendor is going to have to create custom versions of these annotations (or not be flexible in terms of user needs).  I think it would be better done up front to avoid fragmentation out of the gate.  And neither of them are necessarily even PaaS versus SaaS specific. Anyway, I will certainly look over the proposal more this evening and respond.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Tue May 15 21:22:05 CEST 2012</date>
    <body>Hello Linda,    Please find some feedback below. --Gordon Both would be a good.  We currently support ddl generation at createEMF time, expanding the new functionality to this point in time seems like a good idea as well Or a combination of both.  Should the deployer require specific structures not supported by the persistence provider, the provider could have the bulk of the generation completed by the persistence provider then tweak the db structures with scripts. Do you mean it is the platform provider's responsibility to invoke the deployment time interfaces of the persistence provider to have the scripts run?  If we are going to have the functionality available in the persistence provider we should keep the roles distinct. It may be problematic to have persistence providers generate schema.  In many databases user security permissions would need to be updated to allow access to the new schema and there is currently no metadata to define users and their security permissions.  Defining that metadata may be more than we want the spec to take on.  We should require the platform provider to provision users along with the database. We should use the current createEMF APIs where connection info or a datasource can be provided.  We may want to put these new APIs on the EMF We should allow for SE type deployments as well where the persistence provider discovers the PUInfo.  The platform provider may not be a EE platform and should not be required to implement that role in the SPI. Schema generation should be implicit. Perhaps optionally. Properties should be sufficient for this metadata. We also have the user supplied scripts to fall back on.  If the persistence provider's functionality is insufficient scripts can be deployed as well.  FK definition is likely to devolve into a definition string. The fragment should be sufficient. This generateSchema() method should be placed on the EMF.  The there is no need to have a separate API on PersistenceProvider. Having the GenerationInfo contents supplied through properties means we do not need new APIs.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Tue May 15 22:37:43 CEST 2012</date>
    <body>Hi Gordon, all, Thanks for the feedback.  More in-line below.... -Linda What I meant was that the platform provider would run the scripts.  The  scripts might have been packaged with the app or the platform provider might have  called into the persistence provider to have generated them. I was proposing that the persistence provider only create the schemas if the persistence provider "owned" the database -- i.e., had privileges to create schemas in the database.  This would correspond to the SEPARATE_DATABASE approach and to the SaaS approach where tables were striped across tenants and multiple schemas were used.   That said, we could take the alternative approach where the platform provider always created the schemas, but then the platform provider would need to be supplied with the metadata as to which schemas to create.  (I.e., I want to avoid having the platform provider analyze the mapping annotations and orm.xml). In the case that the database is a shared database and a tenant corresponds to a schema, I was proposing that the platform provider create the database user (aka authid) for the tenant -- which would thus result in the creation of the default schema for the tenant. Just to be sure -- did you mean for just ddl-execution-mode or for the scripts as well?  I was proposing separate elements for the scripts.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Tue May 15 22:52:23 CEST 2012</date>
    <body>Thanks Linda,    Some clarifications below. It would be better if the Persistence provider was the only script executor.   I suspect the persistence provider will need to have this functionality anyway. So I was not clear.  I was talking about login credentials specifically, not tenants.  We should not expect the persistence provider to generate login credentials from some new metadata.  The platform provider would be responsible for ensuring the login credentials would have access to the persistence provider generated schema. both, I was suggesting new properties instead of elements.  Properties will be required to allow a predefined PU to perform schema generation for a particular deployment.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Thu May 17 15:56:39 CEST 2012</date>
    <body>My thoughts inline... Well if the creation is driven by the platform provider, its 6-in-one IMO.  But in standalone cases we obviously do not have the benefit of a platform provider, so there the answer has to be "at EMF creation". Is this per deployment?  Or per entity? basically allowing create/drop definitions for database objects other than those explicitly named in mappings.  Functions, procedures, view, etc, etc. +1  If you are exporting schemas, you will generally need this behavior. I think SQL is much better than JPAQL here because of re-usability.  I get the desire to insulate from object (table/column) naming, but still think the re-usability is more important. +1  I assume most vendors have this already in vendor-specific annotations. Not following this.  Could you elaborate? Again, not following.  What would some specifics be? Why not discriminator-based as well? Personally, I think the ability to provide SQL init scripts for initial data insertion should be available regardless of whether the application provided the DDL or the persistence provider generated the DDL. Schema evolution is infinitely more tricky than schema export and infinitely more tricky than it first appears.  -1 to schema evolution capability IMO. Can you give an example of what you are thinking with 'foreignKeyDefinition'?  Personally I was leaning towards (b) until I read this last bit.  Do you just mean different capabilities (i.e., some dbs do not support deferred constraints, etc)? There is an implication in terms of how a persistence provider needs to handle the case of cascading foreign keys in terms of mapped cascading. Take the case of remove operation, with ForeignKey.deleteAction==CASCADE all of a sudden the database has now physically deleted corresponding rows "underneath" the provider which has ramifications on the persistence provider in terms of cleaning up cache entries and possibly in terms of additional cascading.  Yes, we have this mismatch today as is, but today we also don't have the needed visibility into that situation because we do not know the foreign key has been defined that way.  Here that changes.  Just want to make sure we then account for that in the other "entity operation"-related sections. Either way, I think the ability to name the foreign key is definitely warranted.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: question regarding jta-data-source</header>
    <date>Thu May 17 20:46:55 CEST 2012</date>
    <body>It might be worth your while to introduce a Versant specific persistence unit property for the TransactionManager to be passed in to you.  That might be useful in non-EE environments and some EE environments that might be able to pass that in.  Some other persistence providers contain similar TransactionManager lookup code but that is a fragile solution that can break over time (e.g. https://bugs.eclipse.org/bugs/show_bug.cgi?id=365704 ). Persistence providers also aren't supposed to use the TransactionManager in EE (since EE5).  Otherwise, I think we could have a standard PU property for passing the TransactionManager in.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Fri May 18 02:14:19 CEST 2012</date>
    <body>Thanks for the input!  A few remarks below..... Per deployment Right.  This would be doable via the scripts Right now, the spec contains a lot of "physical" annotations and XML elements that would be used in schema generation, in addition to logical elements that have corresponding "physical" defaults.  However, their semantics aren't explicitly defined in the spec with schema generation in view. What I describe further below under "Additional Metadata" Since Java EE 7 isn't supporting full SaaS, spec'ing out JPA provider-managed SaaS seemed premature.  However, after having thought more about this, I do agree with you that it would be good if we could at least standardize on the annotations to support application-managed single-table SaaS. OK, but if the provider generates the DDL, we do need to be very precise as to exactly what would be generated to guarantee that this would work. Even then, I'd expect developers to have the provider generate the DDL and  then package that DDL together with the data insertion scripts. Analogous to the columnDefinition, it would allow capturing database-specific  syntax/semantics. E.g., different variants of foreign key actions, what might be deferrable /  deferred, etc. If the application bundles the DDL, that problem remains, doesn't it?  I  didn't understand what you meant by the last sentence above though.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Fri May 18 17:26:42 CEST 2012</date>
    <body>You mean the "init scripts"?  If so, I'd caution against that plan.  For these auxiliary objects you need both create and drop capability, whereas typically the init script will only contain "create" information (INSERTS, etc). Sure, same as with native-sql queries... Maybe there is an option to supply either SQL or JPQL-like statements, since I am guessing your reservation is around column/table names matching up.  That way the user can decide which format to write the init script in. Correct.  However, if the application is providing the DDL or if the schema already exists there is in essence a limited culpability because persistence providers could simply claim "well we did not know about these FK-level cascades".  Really this is true even in the case of the 'foreignKeyDefinition' solution. However, with @ForeignKey.deleteAction the provider unequivocally knows about the FK-level cascades.  Really, @ForeignKey.deleteAction is specifying runtime behavior in addition to FK-creation metadata.  It implies REMOVE cascading.  So I was just saying that it would be good to note this in the sections that deal with REMOVE cascading and the remove operation in general.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Fri May 18 20:51:41 CEST 2012</date>
    <body>No, I had assumed that these would be in the DDL scripts. I'd like this to work like the raw JDBC/SQL case, where the platform provider  would be running the scripts.  In that case, JPQL presumably wouldn't come into play. Yes -- I agree.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: question regarding jta-data-source</header>
    <date>Mon May 21 09:24:51 CEST 2012</date>
    <body>Hi Scott, This is indeed a good idea, so I think we will introduce a versant specific property which (if specified) takes precedence over our probing algorithms to find the TM. Christian On Thu, May 17, 2012 at 8:46 PM, Scott Marlow smarlow@... Hi Pinaki, As I pointed out, it would only be a "fake" JDBC driver,  I know, that the TM does not care, what type of resource is behind the XAResource, it's just that we need to enlist our XAResource with the current transactions. Either the app server picks it up itself (pull-approach, via a fake JDBC driver, or -in theory- a JCA adapter) or we enlist it directly (push-approach)  &amp;gt; In effect, I do not see why a non-JDBC based JPA provider has to appear as a JDBC connection to a application server to participate in a JTA transaction. Two simple reasons: -app servers do support JDBC connection pools and JCA is not a viable approach -there is no standard way to get access to the TransactionManager (however there is a standard way to get the TransactionSynchronizationRegi stry, which is sufficient for other JPA implementations) However we will probably still use the alternative, to enlist our XAResource directly with the TransactionManager. So we will do some probing in the code, to find out which app server we are in and/or try out a list of well-known JNDI-names to look-up the TransactionManager.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Mon May 21 20:44:25 CEST 2012</date>
    <body>Do any of the rest of you have any feedback on this?  If so, please send it now. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Tue May 22 01:32:20 CEST 2012</date>
    <body>Perfect.  But, are you planning on allowing both user-provided DDL and persistence provider generated DDL scripts to be used simultaneously? If not, users would have no way to define "auxiliary database objects" using persistence provider generated DDL scripts.  Unless, of course, there is an annotation for them to define "auxiliary database objects" Is the plan: 1) only allow "auxiliary database objects" with custom DDL scripts 2) allow mixing custom DDL script and persistence provider generated DDL scripts 3) provide annotations (or other means) to tell persistence provider about "auxiliary database objects" to be generated into the DDL script</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Tue May 22 14:54:04 CEST 2012</date>
    <body>Hi Linda, all, Linda DeMichiel, am 21 May 2012 hast Du um 11:44 zum Thema "[jsr338- experts] Re: JPA schema gen"  geschrieben :  Do any of the rest of you have any feedback on this?  If so, please  send it now.    thanks,    -Linda     [...]  &amp;gt;&amp;gt; ddl-execution-mode: Used when executing directly against the database,  &amp;gt;&amp;gt; rather than generating scripts. Options should include:  &amp;gt;&amp;gt; CREATE_TABLES | DROP_TABLES | DROP_AND_CREATE_TABLES  &amp;gt;&amp;gt; Do we also need to distinguish CREATE_SCHEMAS_AND_TABLES ? DROP_SCHEMA ?  &amp;gt;&amp;gt; Or can these be implicit?  &amp;gt;&amp;gt; Do we also need an ALTER_TABLES capability here? Actually, I think we do. Once applications have delivered their first  release, they soon face the issue of having to extend the database  scheme they are using. Then they realise that drop and create is not  really what they want to. An alter table mode preserving existing data  would therefore be considered extremely helpful.  This however raises the question of a table life cycle and its  management. A comparatively elementary solution could be based on  comparison with either a previously provided DDL script or the database  catalogue. In productive environments a chain Automatically generate DDL scripts -&amp;gt; manually revise them -&amp;gt; have them  executed would be desirable. Best regards Rainer --- Rainer Schweigkoffer               SAP AG Walldorf Java Persistence                   TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, I.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...       fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Bill  McDermott (Co-CEO), Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP  Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse  oder sonstige vertrauliche Informationen enthalten.  Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist  Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung  oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte  benachrichtigen Sie uns und vernichten Sie die empfangene  E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged,  undisclosed, or otherwise confidential information. If you  have received this e-mail in error, you are hereby  notified that any review, copying, or distribution of it  is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your  cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA schema generation</header>
    <date>Tue May 22 19:33:04 CEST 2012</date>
    <body>What I had been assuming was that there would be one set of scripts, which  might have been produced iteratively -- e.g., running the persistence provider to generate DDL  scripts, then customizing / augmenting them with additional auxiliary database objects and/or perhaps  further tuning them. I hadn't been contemplating (3).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Fwd: Re: Re: JPA schema generation</header>
    <date>Tue May 22 20:06:22 CEST 2012</date>
    <body>Forwarding to the group as Mark's request..... -------- Original Message -------- Subject: Re: [jpa-spec users] [jsr338-experts] Re: JPA schema generation Date: Mon, 21 May 2012 19:18:26 +0000 (GMT) To: linda.demichiel@... Hi lords and ladies! I originally replied to this thread on 9th already, but it seems it didn't get through. Trying again via the webmail. --------------- I basically share the same sentiment regarding automated schema creation. Maybe I'm understanding your approach the wrong way, so I better sync up: * You like to have the EE container create the schema at deploy time or runtime if you add a new tenant? * For production as well? Or only for tests? I hope I got this wrong, because I've never seen a big project where you do _not_ need one of the following * additional tables for non-jpa use (e.g. accessed via nativeQuery or even JDBC) * additional indices for performance tuning * sometimes even non-portable indices (Oracle Text Search) * doing manual statistic runs or enabling automatically scheduled statistic runs * setting up tons of other stuff: triggers, stored procedures, db replication to other nodes, etc * where do you get the required db privileges from? In the projects *I* know, you most times cannot even use the auto-generated stuff for your testing db. This feature is imo really only good for small samples which use in-memory derby or hsqldb. Of course, the generated schema is the base (and really good), but most times you need many manual steps in addition. So, did I get the intention wrong? Regarding the Indexes. I'd rather have a typesafe @Index nnotation which allows for own typesafe index groups. Just a very first idea:   int value() default Integer.MAX_VALUE; // for ordering the index if no group annotation is used   // add asc, desc etc and other stuff as you like @Entity   // @Index meta-information to show that this is an index group     int value() default Integer.MAX_VALUE; // mandatory for @Index groups: for ordering inside the grp     // if no int value() is specified in an index, the order is undefined   @CarLookup   @CarLookup The @Index an groups should also be respected when sorting the query criterias. For older DBs you still sometimes need unnecessary nativeQueries just to get the order right to match the data (first query criteria should let you get rid of most data). Not sure if this is still true nowadays with the latest DBs but I think even Oracle10 had problems with queries which didnt fit the order of the index. Maybe for logicalKeys we could do something similar to EmbeddedId for @NaturalId. Automatically generating a unique index for those fields and making them usable in em.find(MyEntity, mylogicalKey) ...  More about it in JPA_SPEC-22 txs and LieGrue, strub PS: hello everybody and thanks for the hard work on JPA! I'm some random old jerk who started writing DB frameworks some decades ago on the mainframe (in C on OS/360), and use/write db stuff in Java since late 90s or so. PPS: plz excuse me if I miss something obvious...</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Fwd: Re: Re: JPA schema generation</header>
    <date>Tue May 22 20:07:11 CEST 2012</date>
    <body>My reply on the thread.... -------- Original Message -------- Subject: Re: [jpa-spec users] [jsr338-experts] Re: JPA schema generation Date: Mon, 21 May 2012 13:01:43 -0700 Organization: Oracle Corporation To: struberg@... Hi Mark, Thanks for your perseverence -- this is helpful. Your email went only to me though.  Are you a member of the jpa-spec project?  If you are, you should be able to post to the users@... email list. It depends on the app.  Please see below. I'm not going to disagree with you on any of the above There are 2 distinct cases here in my view: (1) the application (aka JPA provider) "owns" the database -- this corresponds to the separate-database provisioning case, where the tenant is allocated a database rather than just a schema in a database.  In this  case, the platform provider hands the JPA provider a datasource that has been  properly configured for the provider to create schemas and tables in the database. (2) the application (JPA provider) owns only a schema in the database, and receives a datasource that restricts access to that schema.  In this case, the provider will be restricted to creating tables within that schema. Agree I think so.   At least we seem to be in agreement AFAICT. In more complex scenarios and for production apps, I'm expecting schema generation to be a multi-phase process.  Perhaps there in an initial schema generated by the JPA provider (and output as DDL) which is then tuned by a database expert (eg DBA) to the needs of the application. That resulting schema might then be submitted as DDL packaged with the application. Assuming the target database type is known (which I am assuming it is in such cases), that DDL might be used to produce database-specific artifacts such as you suggest above. Not sure I'm following.  I was proposing that if the index included multiple columns that the @Indexes annotation would need to be applied to the class. Please explain further.  A good query optimizer should consider the indexes. (But that doesn't *guarantee* that the result would be in that order) Beyond a certain point I start worrying about too many indexes being added - and that being counterproductive. OS/360?  Wow. I don't think you have.  Thanks again for taking the trouble to write this up. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Nationalized character data</header>
    <date>Tue May 22 23:07:48 CEST 2012</date>
    <body>I apologize if I missed reference to this in previous 2.1 discussions. Has there been discussion about support for nationalized character data (nvarchar, nchar, nclob) mappings in 2.1?  These are new types standardized in JDBC 4.  Handling them differently is important in both: (1) read and write operations (2) DDL exporting I'd love to see a @Nationalized annotation that can be applied to properties that correspond to character data.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Fwd: Re: Re: JPA schema generation</header>
    <date>Tue May 22 23:16:45 CEST 2012</date>
    <body>Did I miss JPA adding @NaturalId ?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Fwd: Re: Re: JPA schema generation</header>
    <date>Tue May 22 23:23:05 CEST 2012</date>
    <body>No, JPA doesn't have a @NaturalId annotation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Latest specification file</header>
    <date>Thu May 24 19:45:03 CEST 2012</date>
    <body>Could someone point me to the latest spec draft available for us to look at?  Is it the one available at http://download.oracle.com/otndocs/jcp/persistence-2_1-edr-spec/index.html ?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Latest specification file</header>
    <date>Thu May 24 20:35:41 CEST 2012</date>
    <body>Nevermind, I found  http://java.net/projects/jpa-spec/downloads</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] JPQL joins and ON keyword</header>
    <date>Thu May 24 21:09:12 CEST 2012</date>
    <body>I was not a member on the list when this was originally discussed, so I apologize for dragging up a potentially old discussion.  But I wanted to caution against the use of 'ON' as a keyword in the way it is currently proposed in the specification. The problem is ambiguity in cases where the provider supports 'ON' as a more SQL-like ad-hoc joining capability between unassociated entities. In such cases the keyword 'ON' is often the only SYNTACTIC disambiguation between the 2 cases. Consider: select s.name, count(p.id) from javax.persistence.ex.Supplier s     inner join javax.persistence.ex.Product p         on s.id = p.supplierId So here we have Supplier and Product as unrelated classes (no mapped association).  The problem is that structurally (syntactically) the query is completely ambiguous with the proposed form: select s.name, count(p.id) from javax.persistence.ex.Supplier s     inner join s.product         on p.status = 'inStock' where the join is an association join. When parsing queries its always better to disambiguate based on syntax whenever possible.  Here we instead have to fall back to semantic disambiguation, which essentially means that we now have to hold parsing and interpret the meaning of the 2 sides of the join in oder to know what type of join it is. Not to mention that it is odd in my opinion for developers versed in SQL to see ON used here.  The first thought is whether that adds to the SQL ON clause defined by the association mapping or whether that replaces it.  So we lose a little intuitiveness. I'd really rather see a different keyword here.  In Hibernate we chose WITH as the keyword for this for just these reasons: select s.name, count(p.id) from javax.persistence.ex.Supplier s     inner join s.product         with p.status = 'inStock' there I think it is very obvious that the condition is added to the SQL ON clause.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL joins and ON keyword</header>
    <date>Thu May 24 21:31:14 CEST 2012</date>
    <body>Hi Steve, Please see the thread that started March 11, 2011.  This should be available  in the archives. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL joins and ON keyword</header>
    <date>Thu May 24 21:51:21 CEST 2012</date>
    <body>Cool, thanks for the pointer. So the answer to the question asked there about why a different keyword is "needed", is that its not "needed".  But it makes things much cleaner to specify in EBNF and much more efficient to parse.  Again the issue is not really evident today because JPQL only allows association joins.  If and when it allows ad-hoc (non-association) joins thats when this becomes an issue. Like I mentioned in my initial email its a difference between syntactic analysis versus semantic analysis of the query.  Syntactic differences are much easier to describe in an EBNF and much more efficient to parse.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] How to get the JPQL/SQL String From a CriteriaQuery in JPA ?</header>
    <date>Fri May 25 11:08:42 CEST 2012</date>
    <body>Hi all, Recently I had to debug a complex CriteriaQuery and found it very useful to be able to have the SQL representation. Because this is not specified, I wrote a blog about "how to get the query String with Hibernate, OpenJPA ans EclipseLink" : http://agoncal.wordpress.com/2012/05/24/how-to-get-the-jpqlsql-string-f rom-a-criteriaquery-in-jpa/ This feature is very useful for developers and I think it would be good to specify it.  Christian Beikov has created a JIRA : http://java.net/jira/browse/JPA_SPEC-25. Hoping to read your comments on that. Thank you, Antonio</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: Re: Fwd: Re: Re: JPA schema generation</header>
    <date>Sun May 27 13:21:41 CEST 2012</date>
    <body>Imo the 'Natural Key' logic pretty close to a @Index(unique=true). But with a  very special logical meaning.  JPA doesn't know a logical key yet, but it might be nice to think about such  a fundamental thing. LieGrue, strub ----- Original Message -----  To: jsr338-experts@...  Sent: Tuesday, May 22, 2012 11:23 PM  Subject: [jpa-spec users] [jsr338-experts] Re: Re: Fwd: Re: Re: JPA schema   generation          No, JPA doesn't have a @NaturalId annotation.    generation  better sync  time or  where  or  this  tenant  database.  really  most  schema  database-specific  nnotation  had  order)  fields  random  late  write  </body>
  </mail>
  <mail>
    <header>[jpa-spec users] Full text search</header>
    <date>Sun May 27 21:31:48 CEST 2012</date>
    <body>Before adding another issue to JIRA I wanted to discuss the following. Mark Struberg has added the issue for indices, http://java.net/jira/browse/JPA_SPEC-22 Depending on this issue I would like to see something like full text search support/integration in JPA. Hibernate has the possibility, via hibernate search, to create full text queries. The entity manager interface would probably have to be extended if a similar approach would be offered in JPA. I would really like to see a standaradized way of integrating full text search in JPA via search providers or so. What do you think? -- Mit freundlichen Grüßen, Christian Beikov</body>
  </mail>
  <mail>
    <header>[jpa-spec users] EE7 PaaS 'tenent' term question</header>
    <date>Tue May 29 08:16:11 CEST 2012</date>
    <body>Hi Linda et al! I've now read through quite a few of Oracles EE7  PaaS/SaaS documents and I think I now have at least an idea about what  could have been my problem understanding it. My problem with  all those discussions is mostly about the terminus 'tenant' in this  respect. This is actually a pretty well coined term in my surrounding,  but the meaning seams not to fit the meaning which it is used for in the  EE7 specification. I try to lay out what I understood and please correct me if I got things  wrong - txs. EE7  aims to use the Java EE platform for PaaS providers. My personal  opinion is that this is almost like buzzword-bingo and it's not that  important for 98% of all Java EE users, but let's take this apart for a  moment as this is on the manager/salesman roadmap it seems. It's at  least a complete contrary step to EE6 which aimed for 'simplification'  (and thus was very successful imo).  Nontheless, according to the EE7  PaaS documents, the classic scenario is a company A which offers a PaaS  to a 'customer' B which buys the Platform services to serve _his_  customers C which in turn could have customers D themselfs as well. The  EE7 PaaS documents now only call B as 'tenant'. And this is where the  tenant term of EE7 doesn't fit most classical definitions.  In  my original understanding a 'tenant' is a 'role + branding layer' of an  _application_. E.g. back in 2005 we programmed (and for some operators also  hosted)  Napster.mobile for 80++ cellphone companies. Each cellphone company was a  'tenant' for us - with it's own branding and data view. And sometimes a  single cellphone provider even had multiple sub-tenants (MVNO virtual  operators, prepaid business, etc). The classical 'tenant' term is imo  much more business/application oriented. A classical 'tenant' is also  most times used to define 'exceptional behaviour' with a well defined  default-behaviour as fallback. I would not have liked it to import 50  million tracks and 35 TB songs for EACH of our 'tenants'. This stuff is  simply shared if no special overriding rule applies to the specific tenant ... Back to the EE7 platform as a whole (all  is still true for JPA as well). Most PaaS providers atm deliberately  provide the isolation at the operating system level. E.g. via virtual  machines, Xen, kvm, etc. Or they just start another tomcat/EE-server for  their customers. Most times simply because doing proper security  hardening is MUCH easier with such a strict isolation. Or they provide a  deliberately cut down subset of a few APIs and nothing else (like GAE). Running  an EE server as PaaS and giving someone free access to deploy any  application he wants to a _shared_ EE server might become either a.) a  huge security problem, or b.) a huge performance problem (caused by  excessive security) and c.) a huge testing problem. Up to now  _tons_ of projects/tools are pretty edgy and use lots of 'hardcore'  tricks to get their work done. E.g. taking a JAR and doing bytecode  analyzing and dynamic Class modifications instead of going via java.lang  ClassLoader reflection (mostly just to prevent blasting up the PermGen  space). And tons of other stuff you see when digging deeply into some  containers. There is more of that stuff around than most people think...  All this stuff would most probably not work anymore in a tightly  secured environment! The EE server would basically need to be completely  paranoid when it comes to customer code. Until now we are pretty  relaxed - because we know _exactly_ what runs on our servers! (Or we  don't care because the JVM/OS is strictly isolated) Back to the database. I'm not worried about dynamically creating  EntityManagerFactories - I'm more worried about securing them. What are the concepts we have in this regard? LieGrue, strub PS: sorry that it took me so long to answer, but I'm pretty loaded at  $$dayjob atm PPS: will answer the mail regarding @Index meta-annotation after a few  meetings.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Full text search</header>
    <date>Tue May 29 09:08:47 CEST 2012</date>
    <body>Hi Christian, Normal indexes in standard ACID databases (regardless whether an RDBMS or an ODBMS like ours) are transactionally consistent. With full text indexes this becomes a problem, and I think this is the point, which would need to be discussed before discussing any integration in JPA. Regards, Christian On Sun, May 27, 2012 at 9:31 PM, Christian Beikov christian.beikov@... Before adding another issue to JIRA I wanted to discuss the following. Mark Struberg has added the issue for indices, http://java.net/jira/browse/JPA_SPEC-22 Depending on this issue I would like to see something like full text search support/integration in JPA. Hibernate has the possibility, via hibernate search, to create full text queries. The entity manager interface would probably have to be extended if a similar approach would be offered in JPA. I would really like to see a standaradized way of integrating full text search in JPA via search providers or so. What do you think? -- Mit freundlichen Grüßen, Christian Beikov</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Full text search</header>
    <date>Tue May 29 14:10:20 CEST 2012</date>
    <body>Hello! I have just read some lines of the documentation of hibernate search and found out that the index is written transactional if a transaction exists. In other words, the implementation would have to participate in a JDBC or JTA transaction. Here a little excerpt of the documentation(also see http://docs.jboss.org/hibernate/search/3.2/reference/en/html_single/#d0e488 ): To be more efficient, Hibernate Search batches the write interactions with the Lucene index. There is currently two types of batching depending on the expected scope. Outside a transaction, the index update operation is executed right after the actual database operation. This scope is really a no scoping setup and no batching is performed. However, it is recommended - for both your database and Hibernate Search - to execute your operation in a transaction be it JDBC or JTA. When in a transaction, the index update operation is scheduled for the transaction commit phase and discarded in case of transaction rollback. The batching scope is the transaction. There are two immediate benefits: Performance: Lucene indexing works better when operation are executed in batch. ACIDity: The work executed has the same scoping as the one executed by the database transaction and is executed if and only if the transaction is committed. This is not ACID in the strict sense of it, but ACID behavior is rarely useful for full text search indexes since they can be rebuilt from the source at any time. You can think of those two scopes (no scope vs transactional) as the equivalent of the (infamous) autocommit vs transactional behavior. From a performance perspective, the in transaction mode is recommended. The scoping choice is made transparently. Hibernate Search detects the presence of a transaction and adjust the scoping.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Full text search</header>
    <date>Tue May 29 14:22:59 CEST 2012</date>
    <body>Hi Christian, No, it's not that simple. Updating full-text indexes takes an enourmous amount of time compared to the time needed for all other things that happen in a database commit operation. Thus, for most users it's hardly usable to do transactional full text index updates. Batching such updates would be the way to go, however this imposes quite some limitations. It's totally fine for any JPA vendor to provide special extensions for special use cases, or use cases with serious restrictions. However, and this is just my personal opinion, the scope of any specifications should not include such, they don't make a specification sound but brittle instead. Regards, Christian On Tue, May 29, 2012 at 2:10 PM, Christian Beikov christian.beikov@... Hello! I have just read some lines of the documentation of hibernate search and found out that the index is written transactional if a transaction exists. In other words, the implementation would have to participate in a JDBC or JTA transaction. Here a little excerpt of the documentation(also see http://docs.jboss.org/hibernate/search/3.2/reference/en/html_single/#d0e488 ): To be more efficient, Hibernate Search batches the write interactions with the Lucene index. There is currently two types of batching depending on the expected scope. Outside a transaction, the index update operation is executed right after the actual database operation. This scope is really a no scoping setup and no batching is performed. However, it is recommended - for both your database and Hibernate Search - to execute your operation in a transaction be it JDBC or JTA. When in a transaction, the index update operation is scheduled for the transaction commit phase and discarded in case of transaction rollback. The batching scope is the transaction. There are two immediate benefits: Performance: Lucene indexing works better when operation are executed in batch. ACIDity: The work executed has the same scoping as the one executed by the database transaction and is executed if and only if the transaction is committed. This is not ACID in the strict sense of it, but ACID behavior is rarely useful for full text search indexes since they can be rebuilt from the source at any time. You can think of those two scopes (no scope vs transactional) as the equivalent of the (infamous) autocommit vs transactional behavior. From a performance perspective, the in transaction mode is recommended. The scoping choice is made transparently. Hibernate Search detects the presence of a transaction and adjust the scoping.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: Re: Fwd: Re: Re: JPA schema generation</header>
    <date>Tue May 29 15:31:47 CEST 2012</date>
    <body>+1 For what its worth, we have this in Hibernate and users have been extremely positive abouts its inclusion.  Specifically, we allow them to annotate the natural id attribute(s) and then use them to perform entity loading (through an alternate API). http://docs.jboss.org/hibernate/orm/4.1/devguide/en-US/html_single/#d5e850 Not sure whether there is time to slip it into 2.1, but I think it is a great feature.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Full text search</header>
    <date>Tue May 29 16:00:29 CEST 2012</date>
    <body>Hibernate search supports batching within transactions, generally the behavior described in the docs could be reused. What kind of limitations are you thinking of? If the index for the full text search is managed by the DBMS, there shouldn't be any problems? IMO JPA should provide the annotations for the full text indexing and extend the EntityManager, Query, etc. to allow a nice way to search for data. The implementation of such a search provider should be pluggable, you could for example configure your JPA provider(in a standardized way) to use the DBMS, Lucene or any other full text search engine as the provider. Mit freundlichen Grüßen, Christian Beikov Am 29.05.2012 14:22, schrieb Christian Romberg: Hi Christian, No, it's not that simple. Updating full-text indexes takes an enourmous amount of time compared to the time needed for all other things that happen in a database commit operation. Thus, for most users it's hardly usable to do transactional full text index updates. Batching such updates would be the way to go, however this imposes quite some limitations. It's totally fine for any JPA vendor to provide special extensions for special use cases, or use cases with serious restrictions. However, and this is just my personal opinion, the scope of any specifications should not include such, they don't make a specification sound but brittle instead. Regards, Christian On Tue, May 29, 2012 at 2:10 PM, Christian Beikov christian.beikov@... &lt;blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"&gt; Hello! I have just read some lines of the documentation of hibernate search and found out that the index is written transactional if a transaction exists. In other words, the implementation would have to participate in a JDBC or JTA transaction. Here a little excerpt of the documentation(also see http://docs.jboss.org/hibernate/search/3.2/reference/en/html_single/#d0e488 ): To be more efficient, Hibernate Search batches the write interactions with the Lucene index. There is currently two types of batching depending on the expected scope. Outside a transaction, the index update operation is executed right after the actual database operation. This scope is really a no scoping setup and no batching is performed. However, it is recommended - for both your database and Hibernate Search - to execute your operation in a transaction be it JDBC or JTA. When in a transaction, the index update operation is scheduled for the transaction commit phase and discarded in case of transaction rollback. The batching scope is the transaction. There are two immediate benefits: Performance: Lucene indexing works better when operation are executed in batch. ACIDity: The work executed has the same scoping as the one executed by the database transaction and is executed if and only if the transaction is committed. This is not ACID in the strict sense of it, but ACID behavior is rarely useful for full text search indexes since they can be rebuilt from the source at any time. You can think of those two scopes (no scope vs transactional) as the equivalent of the (infamous) autocommit vs transactional behavior. From a performance perspective, the in transaction mode is recommended. The scoping choice is made transparently. Hibernate Search detects the presence of a transaction and adjust the scoping.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Full text search</header>
    <date>Tue May 29 16:42:56 CEST 2012</date>
    <body>Hi Christian, Batching per transaction works only nicely, if there is a low transaction rate, and each transaction contains lot's of changes. E.g. a batch-import scenario. It does not work for high-load, small change-set scenarios. The point is, IMO not everything should be standardized and any spec should have a clear scope. Any vendor is free to offer a fulltext syntax extension for JPQL for full-text search. Any vendor can offer pluggability for any search providers or things like that. I don't see, how this could be included in the spec in a way, that it is still sound. For this it would need to work (almost) orthogonally with all other features, i.e. scenario independent. All this are indicators to me, that this exceeds the scope of what should be standardized. Regards, Christian On Tue, May 29, 2012 at 4:00 PM, Christian Beikov christian.beikov@... Hibernate search supports batching within transactions, generally the behavior described in the docs could be reused. What kind of limitations are you thinking of? If the index for the full text search is managed by the DBMS, there shouldn't be any problems? IMO JPA should provide the annotations for the full text indexing and extend the EntityManager, Query, etc. to allow a nice way to search for data. The implementation of such a search provider should be pluggable, you could for example configure your JPA provider(in a standardized way) to use the DBMS, Lucene or any other full text search engine as the provider. Mit freundlichen Grüßen, Christian Beikov Am 29.05.2012 14:22, schrieb Christian Romberg: Hi Christian, No, it's not that simple. Updating full-text indexes takes an enourmous amount of time compared to the time needed for all other things that happen in a database commit operation. Thus, for most users it's hardly usable to do transactional full text index updates. Batching such updates would be the way to go, however this imposes quite some limitations. It's totally fine for any JPA vendor to provide special extensions for special use cases, or use cases with serious restrictions. However, and this is just my personal opinion, the scope of any specifications should not include such, they don't make a specification sound but brittle instead. Regards, Christian On Tue, May 29, 2012 at 2:10 PM, Christian Beikov christian.beikov@... Hello! I have just read some lines of the documentation of hibernate search and found out that the index is written transactional if a transaction exists. In other words, the implementation would have to participate in a JDBC or JTA transaction. Here a little excerpt of the documentation(also see http://docs.jboss.org/hibernate/search/3.2/reference/en/html_single/#d0e488 ): To be more efficient, Hibernate Search batches the write interactions with the Lucene index. There is currently two types of batching depending on the expected scope. Outside a transaction, the index update operation is executed right after the actual database operation. This scope is really a no scoping setup and no batching is performed. However, it is recommended - for both your database and Hibernate Search - to execute your operation in a transaction be it JDBC or JTA. When in a transaction, the index update operation is scheduled for the transaction commit phase and discarded in case of transaction rollback. The batching scope is the transaction. There are two immediate benefits: Performance: Lucene indexing works better when operation are executed in batch. ACIDity: The work executed has the same scoping as the one executed by the database transaction and is executed if and only if the transaction is committed. This is not ACID in the strict sense of it, but ACID behavior is rarely useful for full text search indexes since they can be rebuilt from the source at any time. You can think of those two scopes (no scope vs transactional) as the equivalent of the (infamous) autocommit vs transactional behavior. From a performance perspective, the in transaction mode is recommended. The scoping choice is made transparently. Hibernate Search detects the presence of a transaction and adjust the scoping.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Full text search</header>
    <date>Tue May 29 17:46:44 CEST 2012</date>
    <body>The problem that the index update will have a bad performance will always exist. I don't really get your point, you always can make something bad and where is the difference between making it bad because of defining DBMS specific indices that will be updated after updating a table or declaring inidces(maybe full text) in the entity model(of course those indices will probably only be used in schema generation and for query hints). My point is, that you would be able to generate the db specific statements for creating indices if needed. Configuring for example lucene as search engine provider might disable the generation of indices in the schema generation but could use the declared metadata to build it's own indices. Maybe you are right that this would exceed the purpose of JPA, anyway the index annotations proposed by Mark Struberg should IMO provide at least a way to use "extensions". Regards, Christian cromberg@... Hi Christian, Batching per transaction works only nicely, if there is a low transaction rate, and each transaction contains lot's of changes. E.g. a batch-import scenario. It does not work for high-load, small change-set scenarios. The point is, IMO not everything should be standardized and any spec should have a clear scope. Any vendor is free to offer a fulltext syntax extension for JPQL for full-text search. Any vendor can offer pluggability for any search providers or things like that. I don't see, how this could be included in the spec in a way, that it is still sound. For this it would need to work (almost) orthogonally with all other features, i.e. scenario independent. All this are indicators to me, that this exceeds the scope of what should be standardized. Regards, Christian On Tue, May 29, 2012 at 4:00 PM, Christian Beikov christian.beikov@... Hibernate search supports batching within transactions, generally the behavior described in the docs could be reused. What kind of limitations are you thinking of? If the index for the full text search is managed by the DBMS, there shouldn't be any problems? IMO JPA should provide the annotations for the full text indexing and extend the EntityManager, Query, etc. to allow a nice way to search for data. The implementation of such a search provider should be pluggable, you could for example configure your JPA provider(in a standardized way) to use the DBMS, Lucene or any other full text search engine as the provider. Mit freundlichen Grüßen, Christian Beikov Am 29.05.2012 14:22, schrieb Christian Romberg: Hi Christian, No, it's not that simple. Updating full-text indexes takes an enourmous amount of time compared to the time needed for all other things that happen in a database commit operation. Thus, for most users it's hardly usable to do transactional full text index updates. Batching such updates would be the way to go, however this imposes quite some limitations. It's totally fine for any JPA vendor to provide special extensions for special use cases, or use cases with serious restrictions. However, and this is just my personal opinion, the scope of any specifications should not include such, they don't make a specification sound but brittle instead. Regards, Christian On Tue, May 29, 2012 at 2:10 PM, Christian Beikov christian.beikov@... Hello! I have just read some lines of the documentation of hibernate search and found out that the index is written transactional if a transaction exists. In other words, the implementation would have to participate in a JDBC or JTA transaction. Here a little excerpt of the documentation(also see http://docs.jboss.org/hibernate/search/3.2/reference/en/html_single/#d0e488 ): To be more efficient, Hibernate Search batches the write interactions with the Lucene index. There is currently two types of batching depending on the expected scope. Outside a transaction, the index update operation is executed right after the actual database operation. This scope is really a no scoping setup and no batching is performed. However, it is recommended - for both your database and Hibernate Search - to execute your operation in a transaction be it JDBC or JTA. When in a transaction, the index update operation is scheduled for the transaction commit phase and discarded in case of transaction rollback. The batching scope is the transaction. There are two immediate benefits: Performance: Lucene indexing works better when operation are executed in batch. ACIDity: The work executed has the same scoping as the one executed by the database transaction and is executed if and only if the transaction is committed. This is not ACID in the strict sense of it, but ACID behavior is rarely useful for full text search indexes since they can be rebuilt from the source at any time. You can think of those two scopes (no scope vs transactional) as the equivalent of the (infamous) autocommit vs transactional behavior. From a performance perspective, the in transaction mode is recommended. The scoping choice is made transparently. Hibernate Search detects the presence of a transaction and adjust the scoping.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: EE7 PaaS 'tenent' term question</header>
    <date>Tue May 29 22:59:51 CEST 2012</date>
    <body>Hi Mark, all, Well, I have to disagree, especially given that a number of the facilities we are adding to support use in cloud environments also apply to non-cloud environments as well and contribute to ease-of-use -- e.g., the resource definition metadata.  We are also trying to better align ease-of-use functionality offered by some components more broadly across the platform.  While the focus of the EE 7 JSR itself needs to be on the platform as a whole, many of the component JSRs are also adding features that aim to enhance and/or simplify application development. There are too many layers here.   A customer (B) of a PaaS environment may deploy an application for its own use and/or B may offer such an application for the use of other customers.  We refer to such customers of the application as "tenants".  [In the trivial case, B could be regarded as its own tenant.]  An application instance that is deployed for a tenant may be accessed by the end users of that tenant's organization. And we are certainly not requiring this (for the reasons you cite). Um, can you be more specific?  In Java EE 7, our model is still single-application-instance-per-tenant, so it is not assumed that entity manager factories would be shared across tenants, unless the application itself were managing multitenancy ("application-managed Saas") in my earlier terminology.   Some JPA vendors support this today, and there has been some discussion in this group as to whether we should build in any support in JPA 2.1 to standardize on any of this.  That is still a TBD. LieGrue, and thanks again for posting, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: EE7 PaaS 'tenent' term question</header>
    <date>Tue May 29 23:46:49 CEST 2012</date>
    <body>Thanks Linda, answers below. LieGrue, strub ----- Original Message -----  Sent: Tuesday, May 29, 2012 10:59 PM  Subject: Re: [jpa-spec users] EE7 PaaS 'tenent' term question    Hi Mark, all,    wrong - txs.  'simplification'    Well, I have to disagree, especially given that a number of the  facilities we are adding to support use in cloud environments also  apply to non-cloud environments as well and contribute to ease-of-use  -- e.g., the resource definition metadata.  We are also trying to  better align ease-of-use functionality offered by some components more  broadly across the platform.  While the focus of the EE 7 JSR itself  needs to be on the platform as a whole, many of the component JSRs are  also adding features that aim to enhance and/or simplify application  development. [strub] Thanks for clarifying. I'm sure there will be plenty of well worthy additions  to the JPA and other specs. My opinion above was purely targeting the PaaS core feature itself. Reading through a few Oracle PaaS documents and the EE7 spec preview gave me  the impression that the goal is to create an EE7 server which is capable to  serve as PaaS out of the box. Not via starting new JVMs for each 'tenant' but  directly in the EE contaier. Similar to dynamically adding a webapp you could  then dynamically add a new tenant and his apps to the EE container.  Obviously/fortunately I got this wrong. I will try to further dig through  documents to find out how this should work instead.    the    There are too many layers here.   A customer (B) of a PaaS environment  may deploy an application for its own use and/or B may offer such an  application for the use of other customers.  We refer to such  customers of the application as "tenants".  [In the trivial case,  B could be regarded as its own tenant.]  An application instance  that is deployed for a tenant may be accessed by the end users of that  tenant's organization. [strub]  That's the point. As far as I've read it, the PaaS documentation only calls B  a 'tenant', not C, not D. Only the one who installs his application to the  PaaS server - regardless if he is using it for himself or for serving other  people. So I agree that it's not about 'application managed tenancy' as you  call it, but purely about the one who buys a platform service.      layer' of an  also hosted)  sometimes a  imo  also  defined  stuff is  ...    And we are certainly not requiring this (for the reasons you cite).    EntityManagerFactories - I'm more worried about securing them.    Um, can you be more specific?  In Java EE 7, our model is still  single-application-instance-per-tenant, so it is not assumed that  entity manager factories would be shared across tenants, unless  the application itself were managing multitenancy ("application-managed  Saas") in my earlier terminology.   Some JPA vendors support this  today, and there has been some discussion in this group as to  whether we should build in any support in JPA 2.1 to standardize on  any of this.  That is still a TBD. [strub] if there is no way to access the PU of another tenant, then all is fine.      LieGrue, and thanks again for posting,    -Linda    $$dayjob atm  meetings.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: Re: Fwd: Re: Re: JPA schema generation</header>
    <date>Wed May 30 01:35:16 CEST 2012</date>
    <body>On Tue, May 29, 2012 at 9:31 AM, Steve Ebersole steve.ebersole@... +1</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Full text search</header>
    <date>Wed May 30 10:28:57 CEST 2012</date>
    <body>Hi! My original @Index proposal was mainly targeted at  a.) single- and multi-column unique indexes b.) single- and multi-column unique ACID standard SQL search indexes We might also think about supporting asynchronous and other non-standard  indices, but this is really far more work. Most time it's not only about a  special create index notation, but often also requires even a nativeQuery to  use them (eg 'CONTAINS' for the Oracle Text Search Index). Lucene also has  other specialties: it cannot update a search-key, but must delete and  subsequently re-import it. Also a lot of those special search indices have  tons of additional (implementation specific) attributes, like the special  character sorting behaviour, stop word lists, soundEx nearby values, etc Next anomaly: geo and triangular indices which are often used for location  based services. Might become hard to define a 1-fits-it-all. All things that are not well defined in any specification (like e.g. SQL_2008  ISO/IEC 9075:2008) or a widely adopted industry standard are really hard to  grasp. LieGrue, strub ________________________________ To: users@...  Sent: Tuesday, May 29, 2012 5:46 PM Subject: [jpa-spec users] Re: Full text search   The problem that the index update will have a bad performance will always  exist. I don't really get your point, you always can make something bad and  where is the difference between making it bad because of defining DBMS  specific indices that will be updated after updating a table or declaring  inidces(maybe full text) in the entity model(of course those indices will  probably only be used in schema generation and for query hints). My point  is, that you would be able to generate the db specific statements for  creating indices if needed. Configuring for example lucene as search engine  provider might disable the generation of indices in the schema generation  but could use the declared metadata to build it's own indices. Maybe you are right that this would exceed the purpose of JPA, anyway the  index annotations proposed by Mark Struberg should IMO provide at least a  way to use "extensions". Regards, Christian Am 29.05.2012 16:43 schrieb "Christian Romberg" &amp;lt;cromberg@...&amp;gt;: Hi Christian,     and extend the EntityManager, Query, etc. to allow a nice way to     search for data. The implementation of such a search provider should     be pluggable, you could for example configure your JPA provider(in a     standardized way) to use the DBMS, Lucene or any other full text     search engine as the provider. Am 29.05.2012 14:22, schrieb Christian Romberg:        happen in a database commit operation.       text index updates.       quite some limitations.       for special use cases, or use cases with serious restrictions.       specifications should not include such, they don't make             hibernate search and found out that the index is written             transactional if a transaction exists. In other words, the             implementation would have to participate in a JDBC or JTA             transaction. http://docs.jboss.org/hibernate/search/3.2/reference/en/html_single/#d0e488 ): Adapting this would fulfill your requirement, wouldn't it?  Am 29.05.2012 09:08, schrieb Christian Romberg:                    whether an RDBMS or an ODBMS like ours) are                   transactionally consistent.                   think this is the point, which would need to be                   discussed before discussing any  http://java.net/jira/browse/JPA_SPEC-22                         something like full text search                         support/integration in JPA.                         search, to create full text queries. The entity                         manager interface would probably have to be                         extended if a similar approach would be offered                         in JPA. I would really like to see a                         standaradized way of integrating full text                         search in JPA via search providers or so.                     registration number: HRB                      Halenreie 42, 22359                      Volker John                     attachments, is for the sole                     confidential or                      use, disclosure or                      intended recipient,                      destroy all copies of         HRB          22359          the sole         disclosure or          recipient,          copies of             </body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Full text search</header>
    <date>Wed May 30 10:31:54 CEST 2012</date>
    <body> b.) single- and multi-column unique ACID standard SQL search indexes whoops -unique. should read: b.) single- and multi-column ACID standard SQL search indexes txs! ----- Original Message -----  Cc:   Sent: Wednesday, May 30, 2012 10:28 AM  Subject: [jpa-spec users] Re: Full text search    Hi!    My original @Index proposal was mainly targeted at     a.) single- and multi-column unique indexes    b.) single- and multi-column unique ACID standard SQL search indexes      We might also think about supporting asynchronous and other non-standard   indices, but this is really far more work. Most time it's not only about a   special create index notation, but often also requires even a nativeQuery   to use   them (eg 'CONTAINS' for the Oracle Text Search Index). Lucene also has   other specialties: it cannot update a search-key, but must delete and   subsequently re-import it. Also a lot of those special search indices have   tons   of additional (implementation specific) attributes, like the special   character   sorting behaviour, stop word lists, soundEx nearby values, etc    Next anomaly: geo and triangular indices which are often used for location   based   services. Might become hard to define a 1-fits-it-all.      All things that are not well defined in any specification (like e.g.   SQL_2008   ISO/IEC 9075:2008) or a widely adopted industry standard are really hard to   grasp.    LieGrue,  strub    exist. I don't really get your point, you always can make something bad and   where is the difference between making it bad because of defining DBMS   specific   indices that will be updated after updating a table or declaring   inidces(maybe   full text) in the entity model(of course those indices will probably only   be   used in schema generation and for query hints). My point is, that you would   be   able to generate the db specific statements for creating indices if needed.   Configuring for example lucene as search engine provider might disable the   generation of indices in the schema generation but could use the declared   metadata to build it's own indices.  index annotations proposed by Mark Struberg should IMO provide at least a   way to   use "extensions".  &amp;lt;cromberg@...&amp;gt;:  transaction rate, and each transaction contains lot's of changes.  should have a clear scope.  full-text search.  like that.  it is still sound.  features, i.e. scenario independent.  should be standardized.  behavior described in the docs could be reused. What kind of limitations   are you   thinking of? If the index for the full text search is managed by the DBMS,   there   shouldn't be any problems?      and extend the EntityManager, Query, etc. to allow a nice way to      search for data. The implementation of such a search provider should      be pluggable, you could for example configure your JPA provider(in a      standardized way) to use the DBMS, Lucene or any other full text      search engine as the provider.  Am 29.05.2012 14:22, schrieb Christian Romberg:         happen in a database commit operation.  full        text index updates.  imposes        quite some limitations.  extensions        for special use cases, or use cases with serious restrictions.        specifications should not include such, they don't make              hibernate search and found out that the index is written              transactional if a transaction exists. In other words, the              implementation would have to participate in a JDBC or JTA              transaction.   http://docs.jboss.org/hibernate/search/3.2/reference/en/html_single/#d0e488 ):  interactions with the Lucene index. There is currently two types of   batching   depending on the expected scope. Outside a transaction, the index update   operation is executed right after the actual database operation. This scope   is   really a no scoping setup and no batching is performed. However, it is   recommended - for both your database and Hibernate Search - to execute your   operation in a transaction be it JDBC or JTA. When in a transaction, the   index   update operation is scheduled for the transaction commit phase and   discarded in   case of transaction rollback. The batching scope is the transaction. There   are   two immediate benefits:  operation are executed in batch.  the one executed by the database transaction and is executed if and only if   the   transaction is committed. This is not ACID in the strict sense of it, but   ACID   behavior is rarely useful for full text search indexes since they can be   rebuilt   from the source at any time.  transactional) as the equivalent of the (infamous) autocommit vs   transactional   behavior. From a performance perspective, the in transaction mode is   recommended. The scoping choice is made transparently. Hibernate Search   detects   the presence of a transaction and adjust the scoping.  Adapting this would fulfill your requirement, wouldn't it?   Am 29.05.2012 09:08, schrieb Christian Romberg:                     whether an RDBMS or an ODBMS like ours) are                    transactionally consistent.                    think this is the point, which would need to be                    discussed before discussing any   the following.   http://java.net/jira/browse/JPA_SPEC-22                          something like full text search                          support/integration in JPA.                          search, to create full text queries. The entity                          manager interface would probably have to be                          extended if a similar approach would be offered                          in JPA. I would really like to see a                          standaradized way of integrating full text                          search in JPA via search providers or so.                      registration number: HRB                       Halenreie 42, 22359                       Volker John                      attachments, is for the sole                      confidential or                       use, disclosure or                       intended recipient,                       destroy all copies of          HRB           22359           the sole            disclosure or           recipient,           copies of</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: JPA schema generation</header>
    <date>Thu May 31 00:41:48 CEST 2012</date>
    <body>Hi Linda! I finally found some time (and free brain) for answering the 2nd part of the  mail (the index story). Answers inline. txs and LieGrue, strub ________________________________ To: struberg@...  Sent: Monday, May 21, 2012 10:01 PM Subject: Re: [jpa-spec users] [jsr338-experts] Re: JPA schema generation   Not sure I'm following.  I was proposing that if the index included multiple columns that the @Indexes annotation would need to be applied to the class. [strub] I'm not exactly a fan of any @Indexes annotation on the class itself, because  we have such a thingy already (@UniqueContraints). The reason why I don't  really like it that it's a.) not easy to maintain, b.) not typesafe (they are  just random strings) c.) hard to refactor and d.) hard to detect unused  indices. We all learned a few tricks while doing this annotation stuff all the years  (especially while working on CDI) and meta-annoations would allow us to do  all this in a type-safe way. I've wrote up my ideas in a  http://java.net/jira/browse/JPA_SPEC-22 including a sample of how this could  probably look like.  The basic idea is to create an inner-class Annotation for your multi-column  Index and annotate your columns with it.  I'm not sure if the sample in the issue is self-explaining. And it certainly  takes some time to get familiar with this kind of style. Any feedback is  highly appreciated! I'm not saying this is THE way, but rather like to showcase other options we  have and trigger some brainstorming. So keep your ideas rolling, folks ;) Please explain further.  A good query optimizer should consider the indexes. (But that doesn't *guarantee* that the result would be in that order) [strub] The order in which you write the columns in your CREATE INDEX statement did  matter a lot in the past. The first one should be the one which has the best  'spread'. This gets better with more and more DB systems switching to dynamic  statistics and cost based optimizers. But there are still use cases where the  column 'ordinal' inside the Index does matter a lot. Thus I'd still like to  have a way to explicitly state the number which denotes this ordinal. In some  DBs that was so bad that the index was ignored at all if the order of the  columns in the query statement didn't fit the order of the columns in the  index itself (leading to a full table space scan). Not sure if this is a  problem nowadays anymore... Beyond a certain point I start worrying about too many indexes being added - and that being counterproductive. [strub] I agree that we need to take this into consideration. But finally I don't  share this sentiment. A logical or 'natural' key is always a unique Index.  Otherwise it would not be a natural KEY.  (Btw, I think there is another thread already where Steve pointed out that  this might even be used to do an em.find(naturalkey) kind of thingy.) Let's face it: currently you need to create most of your Indices via an  externally maintained create_index.sql file. And this file is for sure much  harder to maintain than having this information right in the entity, isn't? So yes, we need to take care about people adding too many indices - but  overall we would improve the situation imo, wdyt? OS/360?  Wow. [strub] That was in the early 90s. This specific stock exchange program switched to  OS/390 later and still running z/OS nowadays afaik. And dumps and ABENDs basically still look the same on a Host, even today ;) I don't think you have.  Thanks again for taking the trouble to write this  up. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: JPA schema generation</header>
    <date>Thu May 31 21:34:57 CEST 2012</date>
    <body>Hi Mark, More comments below.... thanks again and LieGrue, -Linda I guess we have different views on this.  I see indexes as strictly a schema-generation item, used largely for performance.  Some databases force you to specify unique indexes for uniqueness constraints, whereas others recommend that you specify uniqueness constraints (which they then turn into unique indexes), so I guess we just can't win on that one  :-) I find it much more straightforward to just define a multi-column index in one place--that takes care of the column ordering without the need for an integer ordering field.  Further, such an index might also involve columns that map to attributes of embeddables, which would require additional metadata with the syntax you've proposed, and gets messy.  My expectation is that in production applications, schemas will be defined very carefully -- for example, we don't define anything with regard to the ordering of columns in tables, and yet that too can affect performance -- and indexes will probably be defined at a late stage at which point column names are certainly well-known. An open issue in my earlier proposal is whether we should add a "boolean unique() default false" element to the index annotation. This would be redundant with what we have already with unique constraints, and we'd have to define what happens if both were specified and not in sync, but we should consider it. Yes, I'm very aware of this issue (and some of the old history here). BTW, in the database systems I am familiar with, the order of the columns is definitely significant.  The syntax I proposed earlier covers this as well, although of course it too is not the only way.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: JPA schema generation</header>
    <date>Thu May 31 23:09:01 CEST 2012</date>
    <body>Hi Linda! I do agree about the argument with the @Embedded field which might be in the  index. Thinking a bit about it, the same would apply to table inheritances. What I do not like is having Column names in any annotation! This is a really  unpleasant thing in the @UniqueContraint annotation already! The reason I  don't like this is that the name of the column can depend on lots of other  things, like e.g. the database dictionary used. I know from my old DB/2 days  that we only had 8 characters per table. Some Databases make no distinction  between upper/lowercase, others do, etc. All this leads to getting different  names when generating the schema for different databases. If you don't fixate  the name via @Column then your @UniqueContraint column name strings are  basically almost ever broken. IF we use Strings, then we should denote the entity field names and not the  effective column names. This can then easily get mapped to the database  column name in the JPA provider. For embedded fields this could be done in an  ExpressionLanguage like fashion (separated via dots). Regarding the unique index vs unique constraint argument: A unique Index  always implies a unique constraint, but not the other way around, isn't? In  reality afaik most JPA providers already generate a unique index even if you  only use @UniqueConstraint. Btw, I don't see adding a @Index(unique=true)  making things worse. People will find out pretty soon if they defined too  many indices. What's more important is that there must not be 2 indices with the same name.  This also must be checked for InheritanceType.SINGLE_TABLE and JOINED. LieGrue, strub ----- Original Message -----  Cc:   Sent: Thursday, May 31, 2012 9:34 PM  Subject: Re: [jpa-spec users] Re: [jsr338-experts] Re: JPA schema generation    Hi Mark,    More comments below....    thanks again and LieGrue,    -Linda        the mail (the index story).  generation  nnotation  index if  multiple  class.  because we have such a thingy already (@UniqueContraints). The reason why I   don't really like it that it's a.) not easy to maintain, b.) not   typesafe (they are just random strings) c.) hard to refactor and d.) hard   to   detect unused indices.    I guess we have different views on this.  I see indexes as strictly  a schema-generation item, used largely for performance.  Some  databases force you to specify unique indexes for uniqueness constraints,  whereas others recommend that you specify uniqueness constraints (which  they then turn into unique indexes), so I guess we just can't win on that   one :-)    I find it much more straightforward to just define a multi-column index  in one place--that takes care of the column ordering without the  need for an integer ordering field.  Further, such an index might also  involve columns that map to attributes of embeddables, which would  require additional metadata with the syntax you've proposed, and gets  messy.  My expectation is that in production applications, schemas  will be defined very carefully -- for example, we don't define anything  with regard to the ordering of columns in tables, and yet that too  can affect performance -- and indexes will probably be defined at a late   stage  at which point column names are certainly well-known.    An open issue in my earlier proposal is whether we should add a  "boolean unique() default false" element to the index annotation.  This would be redundant with what we have already with unique  constraints, and we'd have to define what happens if both were  specified and not in sync, but we should consider it.      (especially while working on CDI) and meta-annoations would allow us to do   all   this in a type-safe way. I've wrote up my ideas in a    http://java.net/jira/browse/JPA_SPEC-22 including a sample of how this   could   probably look like.  Index and annotate your columns with it.  certainly takes some time to get familiar with this kind of style. Any   feedback   is highly appreciated!  options we have and trigger some brainstorming. So keep your ideas rolling,   folks ;)  @Index  had  indexes.  order)  matter a lot in the past. The first one should be the one which has the   best   'spread'. This gets better with more and more DB systems switching to   dynamic statistics and cost based optimizers. But there are still use cases   where the column 'ordinal' inside the Index does matter a lot. Thus   I'd still like to have a way to explicitly state the number which denotes   this ordinal. In some DBs that was so bad that the index was ignored at all   if   the order of the columns in the query statement didn't fit the order of the   columns in the index itself (leading to a full table space scan). Not sure   if   this is a problem nowadays anymore...    Yes, I'm very aware of this issue (and some of the old history here).  BTW, in the database systems I am familiar with, the order of the columns  is definitely significant.  The syntax I proposed earlier covers this as   well,  although of course it too is not the only way.    fields  don't share this sentiment. A logical or 'natural' key is always a   unique Index. Otherwise it would not be a natural KEY.  this might even be used to do an em.find(naturalkey) kind of thingy.)  externally maintained create_index.sql file. And this file is for sure much   harder to maintain than having this information right in the entity, isn't?  overall we would improve the situation imo, wdyt?  random  late  OS/390 later and still running z/OS nowadays afaik.  write this up.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: JPA schema generation</header>
    <date>Fri Jun 01 02:42:16 CEST 2012</date>
    <body>Hi Mark, I think the 8 character limit is long gone (I hope!).  In my old DB2 days it  was 18 bytes. Also, we addressed the case issue in JPA 2.0 (see section 2.13 of spec). Yes, I was thinking along the same lines, but again, I think the devil is in  the details, and my sense is that the resulting complexity would not be worth it. For example, in additional to embeddeds, we also need to consider cases where  there is no attribute that corresponds to the column (e.g., discriminator columns, columns that  correspond to FKs in unidirectional one-many FK mappings, columns in join tables, ...).  To be general, it gets  messy.  Also, IMO, to get indexes "right", one really needs to think of the tables in question, rather than the  entities. FWIW, both EclipseLink and Hibernate have gone with the column name approach  in their index metadata. I don't think it makes things worse, but I don't see that it contributes much  either.  I'm ok with going either way on this, but I'd like to get more input from others on this point though. Yes, agree. LieGrue and thanks again, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users]  Re: [jsr338-experts] Re: JPA schema generation</header>
    <date>Fri Jun 01 09:59:45 CEST 2012</date>
    <body>Hi Linda, Mark,  What I do not like is having Column names in any annotation!  This is a really unpleasant thing in the @UniqueContraint annotation  already! The reason I don't like this is that the name of the column  can depend on lots of other things, like e.g. the database dictionary  used. I know from my old DB/2 days that we only had 8 characters per  table. Some Databases make no distinction between upper/lowercase,  others do, etc. All this leads to getting different names when  generating the schema for different databases. If you don't fixate  the name via @Column then your @UniqueContraint column name strings  ar basically almost ever broken. actually I would expect the database indexes to very often depend on the underlying database platform anyway, since the behaviour of database optimizers pretty much varies from brand to brand. So, by the way, I'd even more see them being expressed in the xml descriptors than in annotations. Unless, of course, we were to distinguish between logic index definitions indicating frequent access to certain entity attributes/attribute combinations, which then could be based on entity attribute names and would be translated into database indexes by the JPA provider, and database index definitions for tuning the application on the underlying database platform, which would be based on column names. However, I'd consider that a bit over-engineered. Viele Grüße :-) Rainer --- Rainer Schweigkoffer               SAP AG Walldorf Java Persistence                   TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, I.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...       fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Bill McDermott (Co-CEO), Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP Supervisory Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse oder sonstige vertrauliche Informationen enthalten. Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte benachrichtigen Sie uns und vernichten Sie die empfangene E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged, undisclosed, or otherwise confidential information. If you have received this e-mail in error, you are hereby notified that any review, copying, or distribution of it is strictly prohibited. Please inform us immediately and destroy the original transmittal. Thank you for your cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: JPA schema generation</header>
    <date>Fri Jun 01 10:17:44 CEST 2012</date>
    <body>Hi Linda, Mark,  &amp;gt; Regarding the unique index vs unique constraint argument: A unique Index   &amp;gt; always implies a unique constraint, but not the other way around, isn't?   &amp;gt; In reality afaik most JPA providers already generate a unique index even   &amp;gt; if you only use @UniqueConstraint. Btw, I don't see adding a   &amp;gt; @Index(unique=true) making things worse. People will find out pretty soon   &amp;gt; if they defined too many indices.    I don't think it makes things worse, but I don't see that it contributes   much either.  I'm ok with going either way  on this, but I'd like to get more input from others on this point though. One might want to allow (and require) dropping the columnNames if the  name of a UniqueConstraint coincides with that of an Index and thus go  without a unique attribute for Index. This would avoid duplication and  not put too much burden on the developer. Best regards Rainer --- Rainer Schweigkoffer               SAP AG Walldorf Java Persistence                   TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, I.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...       fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Bill  McDermott (Co-CEO), Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP  Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse  oder sonstige vertrauliche Informationen enthalten.  Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist  Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung  oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte  benachrichtigen Sie uns und vernichten Sie die empfangene  E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged,  undisclosed, or otherwise confidential information. If you  have received this e-mail in error, you are hereby  notified that any review, copying, or distribution of it  is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your  cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] question regarding EntityManager.close()</header>
    <date>Fri Jun 01 10:47:55 CEST 2012</date>
    <body>Dear group, The javadoc of EntityManager.close() states: "... If this method is called when the entity manager is associated with an active transaction, ..." Is my understanding right, that "active transaction" could be a RESOURCE_LOCAL transaction or a JTA transaction? (At first glance, I thought, that only JTA transactions are meant, but then it would not make sense, that getTransaction() remains functional on a closed EntityManager, because for JTA transactions getTransaction() would throw an exception, so there is obviously a reason, that "getTransaction" (along with "isOpen" and "getProperties") is allowed on a closed EntityManager.) Thank you! Regards, Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Inheritance of Extended Persistence Context</header>
    <date>Fri Jun 01 17:50:11 CEST 2012</date>
    <body>In the interest of ensuring EE application portability, I'd like to change or add additional wording to the JPA 2.1 "7.6.3.1 Inheritance of Extended Persistence Context" section. In the following example, instances of beans TestSFSB, DAO1, DAO2 will Is that EJB container implementation specific? @Stateful @Remote(TestService.class) @Stateful     @PersistenceContext(type=PersistenceContextType.EXTENDED) @Stateful     @PersistenceContext(type=PersistenceContextType.EXTENDED) Text from the JPA 2.0 specification: " 7.6.2.1 Inheritance of Extended Persistence Context If a stateful session bean instantiates a stateful session bean (executing in the same EJB container instance) which also has such an extended persistence context, the extended persistence context of the first stateful session bean is inherited by the second stateful session bean and bound to it, and this rule recursively applies—independently of whether transactions are active or not at the point of the creation of the stateful session beans. " Text from the JPA 2.1 specification: " 7.6.3.1 Inheritance of Extended Persistence Context If a stateful session bean instantiates a stateful session bean (executing in the same EJB container instance) which also has such an extended persistence context with the same synchronization type, the extended persistence context of the first stateful session bean is inherited by the second stateful session bean and bound to it, and this rule recursively applies—independently of whether transactions are active or not at the point of the creation of the stateful session beans. If the stateful session beans differ in declared synchronization type, the EJBException is thrown by the container. " Depending on the answer to my first question, I'd like to add clarifying text (to the JPA 2.1 7.6.3.1 section) that makes it more obvious whether DAO1 + DAO2 (which could execute in the same EJB container) will inherit the same extended persistence context.  I'll try to make some suggestions after we have answered these questions as to whether DAO1 + DAO2 will always execute in the same EJB container. Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Inheritance of Extended Persistence Context</header>
    <date>Fri Jun 01 18:25:53 CEST 2012</date>
    <body>Assuming the DAO interface is local and does not have a @Remote on it, and also assuming that there is just a forgotten injection annotation of @PersistenceContext(type=PersistenceContextType.EXTENDED) on the EM in TestService, then the PC should be inherited by both SFSBs DAO1 and DAO2. If the session beans are local then you should not leave the EJB container instance. (Does that answer the question?)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Inheritance of Extended Persistence Context</header>
    <date>Fri Jun 01 18:47:04 CEST 2012</date>
    <body>Yes, agree. Scott, was TestService intended to have a transaction-scoped or an extended  persistence context?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Inheritance of Extended Persistence Context</header>
    <date>Fri Jun 01 19:00:56 CEST 2012</date>
    <body>I think that you have answered my question about whether the beans would be in the same EJB container instance.  I don't think there will be disagreement about that. Regarding whether applications can portably expect the same behaviour for this case is my other concern.  We purposely are not specifying @PersistenceContext(type=PersistenceContextType.EXTENDED) on the EM in TestService to drive this discussion. I think the current 7.6.3.1 wording is not clear enough about whether the extended persistence context inheritance will occur between the beans executing in the same EJB container instance. I think that different vendors are handling this differently, which impacts EE portability (IMO).  I think that some vendors will inherit the extended persistence context only if the beans parent (recursively up to the top level bean) has the same persistence context.  Other vendors will inherit the extended persistence context only if another bean with the same extended persistence context is executing in the same EJB container instance. I would like to clarify the intended behaviour that applications should expect, or that it is implementation defined for this case (so application writers can make a choice based on facts.)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Inheritance of Extended Persistence Context</header>
    <date>Fri Jun 01 19:15:59 CEST 2012</date>
    <body>For the purpose of this question, lets remove the entity manager from TestService.  I should of removed that before. @Stateful @Remote(TestService.class) @Stateful     @PersistenceContext(type=PersistenceContextType.EXTENDED) @Stateful     @PersistenceContext(type=PersistenceContextType.EXTENDED)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Inheritance of Extended Persistence Context</header>
    <date>Fri Jun 01 20:09:23 CEST 2012</date>
    <body>There should be two distinct persistence contexts.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Inheritance of Extended Persistence Context</header>
    <date>Fri Jun 01 20:40:30 CEST 2012</date>
    <body>Right, once the persistence context is removed from the parent SFSB then there is nothing to inherit :-)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Inheritance of Extended Persistence Context</header>
    <date>Fri Jun 01 21:01:21 CEST 2012</date>
    <body>I understand that is one possible implementation and point of view.  Is there something that I missed in my reading of the current 7.6.3.1 wording that requires that? Since both bean instances are in the same EJB container instance, does that also qualify for inheritance (from the current wording). I would of expected that portable applications, that want a separate extended persistence context would instead reference different persistence unit names: @PersistenceContext(unitName="OrderEM") public class MySessionBean implements M @Stateful @PersistenceContext(unitName="PU_ONE",type=PersistenceContextType.EXTENDED)    ... @Stateful @PersistenceContext(unitName="PU_TWO",type=PersistenceContextType.EXTENDED)     ...</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Inheritance of Extended Persistence Context</header>
    <date>Fri Jun 01 21:09:53 CEST 2012</date>
    <body>Oops, correction to example of proposed workaround: public class MySessionBean implements M @Stateful @PersistenceContext(unitName="PU_ONE",type=PersistenceContextType.EXTENDED)    ... @Stateful @PersistenceContext(unitName="PU_TWO",type=PersistenceContextType.EXTENDED)     ... Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Inheritance of Extended Persistence Context</header>
    <date>Fri Jun 01 21:53:43 CEST 2012</date>
    <body>The extended persistence context is created when the SFSB is instantiated  (JPA spec 7.6.3). The SFSB is created when it is injected (EJB spec 4.6). Inheritance only applies when one SFSB instantiates another (JPA spec 7.6.3.1) no That would definitely be the case as well.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] updated spec draft, etc.</header>
    <date>Sat Jun 02 00:01:27 CEST 2012</date>
    <body>Hi all, I've uploaded an updated spec draft to the project area   http://java.net/projects/jpa-spec/downloads . Changes are listed in the appendix. I've also uploaded my current working draft of the orm.xsd. Please review / sanity check both of these. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Inheritance of Extended Persistence Context</header>
    <date>Mon Jun 04 11:58:07 CEST 2012</date>
    <body>Hey all, I am of the opinion that both DAOs should share the same persistence context  as they are both wrapped in the same Stateful SessionBean. Is there any  reason why we should purposely not propagate the same persistence context? While I understand the propagation and the reasoning, I do think that it  "makes sense" for implementors and is sound but nevertheless miss the ease if  use and practical test especially for beginners. It would be much easier for people to understand that SFSB sharing the same  life cycle owner (the top SFSB in practice) share the same persistence  context. PC propagation and inheritance is quite complex to grasp despite the fact  that we designed it to improve ease-of-use. I wonder if we can make it easier  for people. Sure, the "fix" is one line of code, namely add the @PC to the root SFSB but  the behavior is surprising and more than one beginner falls into the trap of  not adding it and end up using 2 PC with at best performance issues and worse  case, hard to understand identity breaks. Christian has expressed my opinion with stronger words but fairly aligned  with my ideas  http://4thline.org/articles/Stateful%20persistence%20context%20propagation%20in%20JPA.html Emmanuel  Right, once the persistence context is removed from the parent SFSB then   there is nothing to inherit :-)  </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Inheritance of Extended Persistence Context</header>
    <date>Thu Jun 07 03:48:09 CEST 2012</date>
    <body>There are reasons why users would want to inherit the extended persistence context between sibling beans, as well as crawling multiple levels up the current bean call stack to inherit an existing extended persistence context. There are also reasons why users wouldn't want to inherit the extended persistence context between sibling beans (e.g. they want isolation) as well as inheriting only from the parent bean. Both possible inheritance approaches have their advantages and disadvantages.  After thinking about this for a few days, I am of the opinion that we should allow for both inheritance approaches in some fashion.  That is if we can come up with a way for the developer to express which inheritance method is to be used. Perhaps something like: @PersistenceContext(type=PersistenceContextType.EXTENDED, inheritance=PersistenceContextInheritance.DEEP) @PersistenceContext(type=PersistenceContextType.EXTENDED, inheritance=PersistenceContextInheritance.SHALLOW) If there isn't enough interest in supporting both (useful) inheritance mechanisms, perhaps vendor extensions could be used instead. Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Inheritance of Extended Persistence Context</header>
    <date>Thu Jun 07 15:07:06 CEST 2012</date>
    <body>Does the XPC inheritance also apply to a SFSB doing a JNDI lookup of another local SFSB?  Is there a distinction on when the SFSB does the JNDI lookup?  If the JNDI lookup occurs before the first SFSB PostConstruct, that is probably easier to handle than (very late) during a business method invocation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL joins and ON keyword</header>
    <date>Fri Jun 08 20:23:07 CEST 2012</date>
    <body>Also, in using ON to both (1) define the join conditions in an ad-hoc join (not supported in JPA) and (2) supply extra join conditions to an association join, we have a situation where keywords are used for 2 different purposes.  I'd really like this to be changed.  I think it will be completely confusing to users if and when JPA decides to allow ad-hoc joins.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Nationalized character data</header>
    <date>Fri Jun 08 20:25:59 CEST 2012</date>
    <body>No one else is interested in unifying this?  As-is this is yet another mapping annotation that will need to be provider specific.  Or do other providers not support nationalized datatypes?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Inheritance of Extended Persistence Context</header>
    <date>Mon Jun 11 15:36:04 CEST 2012</date>
    <body>Werner, Thanks for the feedback! Are there specific challenges that the DEEP extended persistence context inheritance (which can reduce memory footprint and has other scalability advantages) would help JSR 350/351 with?  Or is it more that the improvements will give more flexibility that will likely be useful to JSR 350/351? I think the DEEP extended persistence context inheritance will be a useful addition for many situations where you want to share the same persistence context between multiple beans (in the same EJB container instance). Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] "Proper" Java EE 6 location for storing compiled queries?</header>
    <date>Mon Jun 11 17:27:02 CEST 2012</date>
    <body>Hello; our application features lots and lots of named queries.  We use them for a number of reasons which include: * Single compilation and verification/validation at startup * Ability for XML overrides However, there are some queries that have to be dynamic (namely those that feature ORDER BY clauses that might be logically passed in by the caller, since the latter cannot be parameterized in a named query).  (Think about the use case of someone clicking on a column header in a table to sort on that column header, but where you want the sorting to be accomplished on the server side.) Although we'd greatly prefer a named query syntax that would permit such (extremely common!) parameterization, failing that we need to know where the best place is to build these using the criteria query facilities and store them in a Java EE 6 application that makes use of JPA.  I can think of several candidates ranging from static Maps to @Singleton EJBs and so forth.   However, I am not sure what state a criteria-query-built Query is required to possess or is prohibited from possessing.  Are these objects tied in some manner to the thread that created them, or the currently open EntityManager, or other things that would prevent them from being built once and cached somewhere?  Is this behavior undefined (in which case I can make no assumption whatsoever about the associated state, and therefore for portability and safety must reconstruct these queries again and again)? I am happy to provide more details as requested.  This strikes me as a really common use case that isn't fully addressed by the JPA specification which is another reason I brought it up here; my apologies if I'm in the wrong place. Best, Laird -- http://about.me/lairdnelson</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: BNF question: how to parse the example containing KEY, VALUE (and LIKE)</header>
    <date>Thu Jun 14 14:23:46 CEST 2012</date>
    <body>Hi Linda, Any update on the issue below? Thank you! Christian On Wed, Mar 28, 2012 at 9:30 AM, Christian Romberg cromberg@... Hi Linda, Yes, a bit more: All terms derived from qualified_identification_variable or composable_qualified_identification_variable start with KEY, VALUE or ENTRY, respectively. However, section 4.4.4, footnote 56 and the referred section 4.4.2 state "the use of VALUE is optional" E.g. consider this query (modfied example from the spec, VALUE instead of KEY) SELECT i.name , p FROM Item i JOIN i.photos p WHERE VALUE(p) LIKE ‘%egret’ which would now be parsable after your suggested BNF modifications, is equivalent to: SELECT i.name , p FROM Item i JOIN i.photos p WHERE p LIKE ‘%egret’ which might still not be parsable, because we do only have a 'identification_variable' here, and not a 'qualified_identification_variable'. Thank you! Christian On Tue, Mar 27, 2012 at 8:08 PM, Linda DeMichiel linda.demichiel@... Hi Christian, I had intended that we would treat KEY and VALUE the same in the below. Did you mean more than that? regards, -Linda Hi Linda and Michael, We do also need to cover the special short-cut case, that "VALUE(identification_ variable)" is equivalent to just "identification_variable" Cheers, Christian linda.demichiel@... linda.demichiel@ oracle.com    Hi Michael,        Hi Linda,        more below ...            Hi Michael,            Thanks for following up on this.  More below....                Hi Linda,                    Hi Michael,                        Hi Linda,                            Hi Christian,                                Dear group,                                While investigating how to parse queries like this (all page/chapter references refer to                                EDR 2.1, Dec 19th, 2011):                                SELECT p                                FROM Person p, in(pc.firstnames) n                                WHERE n = 'Joe'                                (firstnames is a List&amp;lt;String&amp;gt;)                                I came across the following example provided in the spec, which is related in structure:                                section 4.4.4, p. 153:                                SELECT i.name http://i.name http://i.name                                FROM Item i JOIN i.photos p                                WHERE KEY(p) LIKE ‘%egret’                                I looked at the BNF how this is parsed, and did not find any derivation for the FROM clause:                                where_clause ::= WHERE conditional_expression                                conditional_expression ::= conditional_term                                conditional_term ::= conditional_factor                                conditional_factor ::= conditional_primary                                conditional_primary ::= simple_cond_expression                                simple_cond_expression ::= like_expression                                like_expression ::= string_expression LIKE pattern_value                                So eventually I end up with 'string_expression', from which I can not derive a                                'qualified_identification___ variable'                                string_expression ::=                                state_field_path_expression |                                string_literal |                                input_parameter |                                functions_returning_strings |                                aggregate_expression |                                case_expression |                                function_invocation |                                (subquery)                                Did I miss something, or is there actually a rule missing in the grammar for this?                            Yes, you are right.  The problem is that composable_qualified___ identification_variable                            is currently assumed to not designate a state field, which it should.                            Not sure yet what the best fix for this is in the grammar yet.                        would it work if we add composable_qualified___ identification_variable as an additional                        alternative in string_expression                        (and datetime_expressio etc.):                    I believe so, but I think we have to go further.  I.e., I think we need to support this also                    in in_expression, function_arg, case_operand, aggregate_expression, and entity_or_value_expression.                    Do you agree?                yes agreed. The same holds true for orderby_item, correct?            Yes, orderBy_item needs to be covered as well.            I am hesitating about adding composable_qualified___ identification_variable            to state_field_path_expression though, because a key may often not be a            state field of the object that is the map value.  I'm also concerned            about the order-by rules.  E.g.,            @Entity              ...              @OneToMany  // map key is division name            The following query shouldn't work, because the key is            not part of the result.        I agree, the above query should not work. But how about returning the key that is used in the order by:        SELECT key(vp)        FROM Company c JOIN c.organization vp        WHERE vp.reports &amp;gt; 100        ORDER BY key(vp)    Yes.  I agree that this should be legal.        I think this query should be legal. This means the orderby_item needs to be covered the same as all the other        places we        discussed where composable_qualified___ identification_variable needs to be added. But maybe we should extend the        rules for        the orderby_item as specified in chapter 4.9 on pages 183/184. We could add        composable_qualified___ identification_variable        to the second rule that covers state_field_path_expression already.    Yes, only I would cover this as a separate rule (to make it very clear).            SELECT vp            FROM Company c JOIN c.organization vp            WHERE vp.reports &amp;gt; 100            ORDER BY key(vp)            So, I was thinking of just either adding composable_qualified___ identification_variable to            the needed places (ugly) or defining another non-terminal, e.g.,            generalized_state_expression:: __= state_field_path_expression | composable_qualified___ identification_variable.            What do you think?        I like the idea of having another non-terminal because a composable_qualified___ identification_variable like        key(vp) is        not really a state_field_path_expression.        That would mean the new non-terminal generalized_state_expression is used in all the places we discussed before,        correct?    Yes.    Thanks again!  It looks like we're on the same page here.    regards,    -Linda        Regards Michael            regards,            -Linda                So maybe we extend state_field_path_expression and add composable_qualified___ identification_variable as                an alternative:                state_field_path_expression ::=                            general_subpath.state_field |                            composable_qualified___ identification_variable                Regards Michael                    regards,                    -Linda                        string_expression ::=                           state_field_path_expression |                        composable_qualified___ identification_variable |                           string_literal |                           ...                        Regards Michael                            Thanks for pointing this out.                            -Linda                            \                                Thank you!                                Christian                                --                                Christian Romberg                                Chief Engineer| Versant GmbH                                (T) +49 40 60990-0                                (F) +49 40 60990-113                                (E) cromberg@... cromberg@... cromberg@...                                &amp;lt;mailto: cromberg@...                                 www.versant.com http://www.versant.com http://www.google.com/url?q=_ _http%3A%2F%2Fwww.versant.com% __2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=__ AFrqEzeeEBc_gN___ 8mxtt8xDB0tjXDXQVlw http://www.google.com/url?q= http%3A%2F%2Fwww.versant.com% 2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg= AFrqEzeeEBc_gN_ 8mxtt8xDB0tjXDXQVlw                                 www.db4o.com http://www.db4o.com http://www.google.com/url?q=_ __sa=D&amp;amp;sntz=1&amp;amp;usg=__ AFrqEzdo3Q40RwKQPBtnPIuBYQd1di __FxJQ http://www.google.com/url?q= sa=D&amp;amp;sntz=1&amp;amp;usg= AFrqEzdo3Q40RwKQPBtnPIuBYQd1di FxJQ                                --                                Versant                                GmbH is incorporated in Germany. Company registration number: HRB                                54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359                                Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John                                CONFIDENTIALITY                                NOTICE: This e-mail message, including any attachments, is for the sole                                use of the intended recipient(s) and may contain confidential or                                proprietary information. Any unauthorized review, use, disclosure or                                distribution is prohibited. If you are not the intended recipient,                                immediately contact the sender by reply e-mail and destroy all copies of                                the original message.                        --                        *Michael Bouschen*                        *Prokurist*                        akquinet tech@spree GmbH                        Bülowstr. 66, D-10783 Berlin                        Fon: +49 30 235 520-33                        Fax: +49 30 217 520-12                        Email: michael.bouschen@... michael.bouschen@ akquinet.de                        Web: www.akquinet.de http://www.akquinet.de http://www.akquinet.de                        akquinet tech@spree GmbH, Berlin                        Geschäftsführung: Martin Weber, Dr. Torsten Fink                        Amtsgericht Berlin-Charlottenburg HRB 86780 B                        USt.-Id. Nr.: DE 225 964 680                --                *Michael Bouschen*                *Prokurist*                akquinet tech@spree GmbH                Bülowstr. 66, D-10783 Berlin                Fon: +49 30 235 520-33                Fax: +49 30 217 520-12                Email: michael.bouschen@... michael.bouschen@ akquinet.de                Web: www.akquinet.de http://www.akquinet.de http://www.akquinet.de                akquinet tech@spree GmbH, Berlin                Geschäftsführung: Martin Weber, Dr. Torsten Fink                Amtsgericht Berlin-Charlottenburg HRB 86780 B                USt.-Id. Nr.: DE 225 964 680        --        *Michael Bouschen*        *Prokurist*        akquinet tech@spree GmbH        Bülowstr. 66, D-10783 Berlin        Fon: +49 30 235 520-33        Fax: +49 30 217 520-12        Email: michael.bouschen@... michael.bouschen@ akquinet.de        Web: www.akquinet.de http://www.akquinet.de http://www.akquinet.de        akquinet tech@spree GmbH, Berlin        Geschäftsführung: Martin Weber, Dr. Torsten Fink        Amtsgericht Berlin-Charlottenburg HRB 86780 B        USt.-Id. Nr.: DE 225 964 680 -- Christian Romberg Chief Engineer| Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... cromberg@... www.versant.com http://www.google.com/url?q= http%3A%2F%2Fwww.versant.com% 2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg= AFrqEzeeEBc_gN_ 8mxtt8xDB0tjXDXQVlw www.db4o.com http://www.google.com/url?q= sa=D&amp;amp;sntz=1&amp;amp;usg= AFrqEzdo3Q40RwKQPBtnPIuBYQd1di FxJQ -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] question regarding JPA-JTA interaction (again)</header>
    <date>Thu Jun 14 17:38:07 CEST 2012</date>
    <body>Dear group, While investigating and implementing our JTA integration, I came across the following problem for the standard implementation approach: 1. container starts a JTA transaction 2. createEntityManager is called, and registers a Synchronization instance 3. as soon as the EntityManager acquires a JDBC connection, the container gets the XAConnection from the connection pool, passes the result of XAConnection.getConnection() to the EntityManager and registers the result of XAConnection.getXAResource() with the current JTA transaction 5. all work done by the EntityManager with that connection happens in the context of the transaction (branch) denoted by xid now 6. when the method invocation (of e.g. a session bean) ends, the container will call Transaction.delistResource() which in turn will call XAResource.end(xid, TMSUCCESS) [mandated by JTA spec, JTA 1.1 sequence diagram on page 29] 7. Transaction.commit() will drive the 2-phase-commit-protocol, before invoking prepare and commit, all "beforeCompletion()" calls of registered Synchronization instances are invoked 7a. the beforeCompletion() implementation of the Synchronization instance registered in step 2 is supposed to flush all changes to the database 7b. XAResource.prepare(xid) is invoked 7c. XAResource.commit(xid) is invoked The problem ist step 7a: The connection is not associated with xid, so this simply does not work. Even if the connection is lazily acquired and released, there is no portable way to trigger the necessary start() and end() invocations by the JPA implementations. So how is this supposed to work? (Btw. we have direct control over our XAResource implementation, so we can trigger flushing by having a callback from our XAResource to our EntityManager during XAResource.end(), but this naturally is not an option, if a 3rd-party XAResource implementation is used.) Regards, Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding JPA-JTA interaction (again)</header>
    <date>Thu Jun 14 22:31:28 CEST 2012</date>
    <body>Christian, In your step 6:   "when the method invocation (of e.g. a session bean) ends, the container will call Transaction.delistResource()" I wonder if that is the root of the difficulty. If the TM keeps JDBC connections (and/or XA resources) enlisted (in case they are reused later in the same TX) until just before step 7b, then the issue you are concerned about will not occur.  -----Original Message-----  From: Christian Romberg [ mailto:cromberg@...]   Sent: Friday, 15 June 2012 3:38 a.m.  To: jsr338-experts@...  Subject: [jsr338-experts] question regarding JPA-JTA   interaction (again)    Dear group,    While investigating and implementing our JTA integration, I   came across the following problem for the standard   implementation approach:    1. container starts a JTA transaction  2. createEntityManager is called, and registers a   Synchronization instance  3. as soon as the EntityManager acquires a JDBC connection,   the container gets the XAConnection from the connection pool,  passes the result of XAConnection.getConnection() to the   EntityManager and registers the result of XAConnection.getXAResource()  with the current JTA transaction  5. all work done by the EntityManager with that connection   happens in the context of the transaction (branch) denoted by xid now  6. when the method invocation (of e.g. a session bean) ends,   the container will call Transaction.delistResource() which in   turn will  call XAResource.end(xid, TMSUCCESS) [mandated by JTA spec,   JTA 1.1 sequence diagram on page 29]  7. Transaction.commit() will drive the   2-phase-commit-protocol, before invoking prepare and commit,   all "beforeCompletion()" calls  of registered Synchronization instances are invoked  7a. the beforeCompletion() implementation of the   Synchronization instance registered in step 2 is supposed to   flush all changes to the database  7b. XAResource.prepare(xid) is invoked  7c. XAResource.commit(xid) is invoked    The problem ist step 7a: The connection is not associated   with xid, so this simply does not work.  Even if the connection is lazily acquired and released, there   is no portable way to trigger the necessary  start() and end() invocations by the JPA implementations.    So how is this supposed to work?    (Btw. we have direct control over our XAResource   implementation, so we can trigger flushing by having a   callback from our XAResource to  our EntityManager during XAResource.end(), but this naturally   is not an option, if a 3rd-party XAResource implementation is used.)    Regards,    Christian    --   Christian Romberg  Chief Engineer | Versant GmbH  (T) +49 40 60990-0  (F) +49 40 60990-113  (E) cromberg@...  www.versant.com  http://www.google.com/url?q=http%3A%2F%2Fwww.versant.com%2F&amp;amp;s  a=D&amp;amp;sntz=1&amp;amp;usg=AFrqEzeeEBc_gN_8mxtt8xDB0tjXDXQVlw&amp;gt;  |   www.db4o.com  http://www.google.com/url?q=http%3A%2F%2Fwww.db4o.com%2F&amp;amp;sa=D  &amp;amp;sntz=1&amp;amp;usg=AFrqEzdo3Q40RwKQPBtnPIuBYQd1diFxJQ&amp;gt;     --   Versant  GmbH is incorporated in Germany. Company registration number: HRB   54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359   Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John    CONFIDENTIALITY  NOTICE: This e-mail message, including any attachments, is   for the sole  use of the intended recipient(s) and may contain confidential or   proprietary information. Any unauthorized review, use, disclosure or   distribution is prohibited. If you are not the intended recipient,   immediately contact the sender by reply e-mail and destroy   all copies of  the original message.          </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding JPA-JTA interaction (again)</header>
    <date>Fri Jun 15 14:46:53 CEST 2012</date>
    <body>Evan, You are right, the issue would not occur then. However so far I have been unsuccessful to find out, whether it is portable behavior, for the container to call Transaction.commit() while there are any XAResource enlisted with this Transaction, where start() had been invoked, but end() has not. Also I have not found out yet, whether there are scenarios, where the container must not delist the XAResource. (Although not directly applicable, the control flow diagram p.28/29 in the JTA spec suggests otherwise.) If both is defined, i.e. container does not delist XAResource it keeps in it's invocation context before calling Transaction.commit() and Transaction.commit() handles still enlisted resources fine (by calling beforeCompletion() first, calling end() after that, and then prepare+commit) then we (i.e. the JPA spec) would not have a problem. If anyone knows, that that is defined somewhere, please let me know, I will also continue researching the related specs. If that is not explicitly defined, than we (i.e. the JPA spec) do have a problem, because we rely on undefined behavior. Regards, Christian On Thu, Jun 14, 2012 at 10:31 PM, Evan Ireland eireland@... Christian, In your step 6:  "when the method invocation (of e.g. a session bean) ends, the container will call Transaction.delistResource()" I wonder if that is the root of the difficulty. If the TM keeps JDBC connections (and/or XA resources) enlisted (in case they are reused later in the same TX) until just before step 7b, then the issue you are concerned about will not occur. cromberg@... ] jsr338-experts@... +49 40 60990-0 +49 40 60990-113 cromberg@... www.versant.com http://www.google.com/url?q=http%3A%2F%2Fwww.versant.com%2F&amp;amp;s www.db4o.com http://www.google.com/url?q=http%3A%2F%2Fwww.db4o.com%2F&amp;amp;sa=D</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding JPA-JTA interaction (again)</header>
    <date>Fri Jun 15 15:56:55 CEST 2012</date>
    <body>Finished my research of the ejb-core (3.0), jta (1.1) and jpa (2.0) specs and found nothing, that guarantees these two behaviors. So for me it looks like, that we rely on undefined behavior. I might have overseen something, and if anyone knows more, I would appreciate it, if you let me know. Christian On Fri, Jun 15, 2012 at 2:46 PM, Christian Romberg cromberg@... Evan, You are right, the issue would not occur then. However so far I have been unsuccessful to find out, whether it is portable behavior, for the container to call Transaction.commit() while there are any XAResource enlisted with this Transaction, where start() had been invoked, but end() has not. Also I have not found out yet, whether there are scenarios, where the container must not delist the XAResource. (Although not directly applicable, the control flow diagram p.28/29 in the JTA spec suggests otherwise.) If both is defined, i.e. container does not delist XAResource it keeps in it's invocation context before calling Transaction.commit() and Transaction.commit() handles still enlisted resources fine (by calling beforeCompletion() first, calling end() after that, and then prepare+commit) then we (i.e. the JPA spec) would not have a problem. If anyone knows, that that is defined somewhere, please let me know, I will also continue researching the related specs. If that is not explicitly defined, than we (i.e. the JPA spec) do have a problem, because we rely on undefined behavior. Regards, Christian On Thu, Jun 14, 2012 at 10:31 PM, Evan Ireland eireland@... Christian, In your step 6:  "when the method invocation (of e.g. a session bean) ends, the container will call Transaction.delistResource()" I wonder if that is the root of the difficulty. If the TM keeps JDBC connections (and/or XA resources) enlisted (in case they are reused later in the same TX) until just before step 7b, then the issue you are concerned about will not occur. cromberg@... ] jsr338-experts@... +49 40 60990-0 +49 40 60990-113 cromberg@... www.versant.com http://www.google.com/url?q=http%3A%2F%2Fwww.versant.com%2F&amp;amp;s www.db4o.com http://www.google.com/url?q=http%3A%2F%2Fwww.db4o.com%2F&amp;amp;sa=D</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding JPA-JTA interaction (again)</header>
    <date>Mon Jun 18 22:11:39 CEST 2012</date>
    <body>Hi Christian, A few comments. The process described in the JTA spec on page 28 is illustrative, not prescriptive, so there is no requirement for the app server to delist the resource. That was just an example walkthrough of what *could* happen when a client closed a connection. I think you probably already knew that, though, because I think your point was just that if that *could* happen then it could break a JPA EM that wanted to write to the connection later on? Although, container data sources all pool the connections so in practice the connections never get closed, anyway (so the above case does not ever really occur). Perhaps the strongest case is that the TransactionSynchronizationRegistry.registerInterprosedSynchronization() method in the JTA spec actually does state that beforeCompletion callback methods can access the resources, but that no transactional work can be performed on resources in afterCompletion. I would say that since the app server must provide this semantic we might be able to say that we are on reasonably firm ground. Would you agree? -Mike Finished my research of the ejb-core (3.0), jta (1.1) and jpa (2.0) specs and found nothing, that guarantees these two behaviors. So for me it looks like, that we rely on undefined behavior. I might have overseen something, and if anyone knows more, I would appreciate it, if you let me know. Christian On Fri, Jun 15, 2012 at 2:46 PM, Christian Romberg cromberg@... &lt;blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"&gt; Evan, You are right, the issue would not occur then. However so far I have been unsuccessful to find out, whether it is portable behavior, for the container to call Transaction.commit() while there are any XAResource enlisted with this Transaction, where start() had been invoked, but end() has not. Also I have not found out yet, whether there are scenarios, where the container must not delist the XAResource. (Although not directly applicable, the control flow diagram p.28/29 in the JTA spec suggests otherwise.) If both is defined, i.e. container does not delist XAResource it keeps in it's invocation context before calling Transaction.commit() and Transaction.commit() handles still enlisted resources fine (by calling beforeCompletion() first, calling end() after that, and then prepare+commit) then we (i.e. the JPA spec) would not have a problem. If anyone knows, that that is defined somewhere, please let me know, I will also continue researching the related specs. If that is not explicitly defined, than we (i.e. the JPA spec) do have a problem, because we rely on undefined behavior. Regards, Christian On Thu, Jun 14, 2012 at 10:31 PM, Evan Ireland eireland@... &lt;blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"&gt; Christian, In your step 6:  "when the method invocation (of e.g. a session bean) ends, the container will call Transaction.delistResource()" I wonder if that is the root of the difficulty. If the TM keeps JDBC connections (and/or XA resources) enlisted (in case they are reused later in the same TX) until just before step 7b, then the issue you are concerned about will not occur. cromberg@... ] jsr338-experts@... JPA-JTA integration, I standard registers a JDBC connection, connection pool, XAConnection.getConnection() to the XAConnection.getXAResource() that connection (branch) denoted by xid now session bean) ends, Transaction.delistResource() which in by JTA spec, prepare and commit, invoked the is supposed to not associated released, there implementations. XAResource having a but this naturally implementation is used.) +49 40 60990-0 +49 40 60990-113 cromberg@... www.versant.com http://www.google.com/url?q=http%3A%2F%2Fwww.versant.com%2F&amp;amp;s  | www.db4o.com http://www.google.com/url?q=http%3A%2F%2Fwww.db4o.com%2F&amp;amp;sa=D registration number: HRB Office: Halenreie 42, 22359 Wöbker, Volker John attachments, is contain confidential or review, use, disclosure or the intended recipient, e-mail and destroy</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding EntityManager.close()</header>
    <date>Mon Jun 18 23:30:06 CEST 2012</date>
    <body>Hi Christian, Yes, your understanding is correct. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Inheritance of Extended Persistence Context</header>
    <date>Mon Jun 18 23:34:58 CEST 2012</date>
    <body>Hi Emmanuel, all, OK, but this would be an incompatible change, which I don't think we should  contemplate.   Depending on further input from developers on this issue, we could consider whether addressing the  use case by means of additional metadata would be beneficial. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Inheritance of Extended Persistence Context</header>
    <date>Mon Jun 18 23:36:24 CEST 2012</date>
    <body>Yes Is there a distinction on when No If the JNDI lookup occurs before the first SFSB PostConstruct, that is  probably easier to</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Nationalized character data</header>
    <date>Mon Jun 18 23:37:29 CEST 2012</date>
    <body>I'd like to get more input on this from others in the group.  Should we do  anything (now) along these lines, or is the columnDefinition element of @Column sufficient?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding JPA-JTA interaction (again)</header>
    <date>Mon Jun 18 23:42:35 CEST 2012</date>
    <body>Hi Christian, all, The Connector spec also weighs in on the container / resource manager  interaction, but AFAICT does not address this either. The JTA spec is undergoing a Maintenance Release as part of Java EE 7.   I  would encourage you to join the project (which is just getting started) and to file an issue on this  topic. See  http://java.net/projects/jta-spec/.  Paul Parkinson, cc 'd here, is  leading this work. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL joins and ON keyword</header>
    <date>Tue Jun 19 00:15:18 CEST 2012</date>
    <body>I'm afraid I don't agree.  If someone understands the semantics of SQL ON  conditions, I don't see that they wouldn't also be able to understand the difference between its  effect on our relationship joins vs on ad hoc joins, were we to add them.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: "Proper" Java EE 6 location for storing compiled queries?</header>
    <date>Tue Jun 19 02:32:12 CEST 2012</date>
    <body>Hi Laird, The CriteriaQuery objects themselves can be cached (see spec section 6.9).   Any executable TypedQuery or Query objects that you create from them are only guaranteed to  be valid when the entity manager that you used to create them is open (see spec 3.1.1). regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding JPA-JTA interaction (again)</header>
    <date>Tue Jun 19 09:04:56 CEST 2012</date>
    <body>Hi Linda, Thank you! I will do that. Regards, Christian On Mon, Jun 18, 2012 at 11:42 PM, Linda DeMichiel linda.demichiel@... Hi Christian, all, Finished my research of the ejb-core (3.0), jta (1.1) and jpa (2.0) specs and found nothing, that guarantees these two behaviors.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Issue with JTA and JPA interaction</header>
    <date>Tue Jun 19 09:40:23 CEST 2012</date>
    <body>Hi Paul, all, Recently we came across an issue regarding the interaction of JTA and JPA. The user visible artifact of JPA is an instance of EntityManager. JPA standardizes ORMs, so without loss of generality we can assume, that the EntityManager uses a JDBC connection to access the database, and with respect to this issue, we can also assume, that the JDBC driver used supports XADataSource. (Note, that the association of JDBC Connection instance to EntityManager instance is generally not mandated by the JPA spec (although for certain points in time it is implied). This means, it is permitted for the JPA vendor (unless database locks are used) to use a different JDBC connections for every database read operation, and return them to the pool after use. If database locks are used for operations and/or if flushing occurs, then of course the JDBC connection needs to be pinned to the EntityManager instance to guarantee transactional semantics.) The intended JPA-JTA interaction is as follows: The JPA implementation does _not_ interact with XADataSources, XAConnections or XAResources, it _does_ use a JDBC connection pool to acquire it's JDBC connections. This pool is configured to return enlisted connections, and when the JPA implementation asks for a connection, the container in the background acquires an XAConnection, enlists the result of XAConnection.getXAResource() with the current transaction and returns the (wrapped) result of XAConnection.getConnection() to the JPA implementation. The JPA implementation is only supposed to register a Synchronization instance during EntityManagerFactory.createEntityManager() if a JTA transaction is active at that time or during EntityManager.joinTransaction(). The JPA implementation is supposed to flush all changes, which have not been flushed to the database yet in Synchronization.beforeCompletion(). For this to work properly, a few conditions must be met: case A: the JPA implementation has acquired and pinned a JDBC connection, the associated XAResource is enlisted condition A1. the container must not have delisted the resource when the control flow leaves the container and the container is about to commit (or rollback) the transaction. If the resource was delisted before Transaction.commit(), the JDBC connection is not currently associated to the JTA transaction and everything flushed in beforeCompletion() is not part of the global transaction. condition A2. it must be supported to call Transaction.commit() while there are still XAResources "active", i.e. start has been called, while end has not been called, those should implicitly be "end()ed", before the prepare and commit calls occurr. condition A3. Transaction.commit() must execute the following in this order: -fire beforeCompletion -implicit ending of XAResources, that have not been ended yet -do all the prepare calls -do all the commit calls I assume, that most implementations of containers and transaction managers already meet these conditions, either by chance, by trial-and-error or by common-sense requirement extrapolation. However I have not found out yet, that any of these conditions is mandated by any of the related specs, and this would be needed, otherwise the JPA-JTA interaction relies on undefined behavior. case B: the JPA implementation does not currently have a pinned JDBC connection, when the container calls Transaction.commit() condition B1: enlisting resources must be allowed if triggered from a beforeCompletion() callback (because the JPA implementation will acquire a connection right then) condition B2: same as A2 condition B3: same as A3 I suggest, that these 4 conditions (B1 is debatable, JPA could be modified to explicitly disallow that) should be addressed in the upcoming JTA specification and in particular, an example control-flow diagram like on p.28/29 of JTA 1.1 should be added Thank you! (Sorry, that the mail became a bit long) Regards, Christian Disclaimer: our own JPA implemention is not an ORM, but uses our XA-capable, object-oriented database (Versant Object Database) directly, so our implementation of the JTA-JPA might look slightly different because of that, however I would like to see this problem solved for the general ORM implementations of JPA as well -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] portable way to restore XAResources required</header>
    <date>Tue Jun 19 09:49:13 CEST 2012</date>
    <body>Hi Paul, all, JTA 1.1 does not mandate, how an XAResource is restored after a system failure for the purpose of transaction recovery conducted by the transaction manager. Common suggestions and implementations (e.g. JBoss) use the following strategies: 1. If the XAResource was acquired by a JNDI-named pool, the JNDI name can be stored in the transaction log file of the transaction manager 2. If the XAResource is Serializable, then the XAResource can be serialized into the transaction log file of the transaction manager. Nothing of this is mandated by the JTA spec. Any transaction manager implementation needs a portable way to restore XAResources. If 1. alone would be mandatory, this would not work for us, because we don't use a JNDI-based pool but register our XAResource instances directly with the Transaction. I suggest to mandate the following behavior for the transaction manager when writing it's log file: "If the XAResource was acquired via a JNDI named pool or the XAResource is serializable, the transaction manager might store the JNDI name, or the serialized XAResource in it's transaction log file. If the XAResource was not acquired via a JNDI named pool and is not  serializable, an exception will be thrown". Thank you! Regards, Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: question regarding JPA-JTA interaction (again)</header>
    <date>Tue Jun 19 09:54:03 CEST 2012</date>
    <body>Hi Mike, Yep, I knew that, the problem really is, that it does not guarantee a non-delistment in our scenario. The "TransactionSynchronizationRegistry.registerInterprosedSynchronization()" is a good indication, however it only addresses one of the many requirements, which must be standardized. I explained the problem to the JTA 1.2 group, and also suggested, that after addressing these issues, that they include a control-flow diagram for the JPA-JTA interaction as well. Regards, Christian On Mon, Jun 18, 2012 at 10:11 PM, michael keith michael.keith@... Hi Christian, A few comments. The process described in the JTA spec on page 28 is illustrative, not prescriptive, so there is no requirement for the app server to delist the resource. That was just an example walkthrough of what *could* happen when a client closed a connection. I think you probably already knew that, though, because I think your point was just that if that *could* happen then it could break a JPA EM that wanted to write to the connection later on? Although, container data sources all pool the connections so in practice the connections never get closed, anyway (so the above case does not ever really occur). Perhaps the strongest case is that the TransactionSynchronizationRegistry.registerInterprosedSynchronization() method in the JTA spec actually does state that beforeCompletion callback methods can access the resources, but that no transactional work can be performed on resources in afterCompletion. I would say that since the app server must provide this semantic we might be able to say that we are on reasonably firm ground. Would you agree? -Mike Finished my research of the ejb-core (3.0), jta (1.1) and jpa (2.0) specs and found nothing, that guarantees these two behaviors. So for me it looks like, that we rely on undefined behavior. I might have overseen something, and if anyone knows more, I would appreciate it, if you let me know. Christian On Fri, Jun 15, 2012 at 2:46 PM, Christian Romberg cromberg@... Evan, You are right, the issue would not occur then. However so far I have been unsuccessful to find out, whether it is portable behavior, for the container to call Transaction.commit() while there are any XAResource enlisted with this Transaction, where start() had been invoked, but end() has not. Also I have not found out yet, whether there are scenarios, where the container must not delist the XAResource. (Although not directly applicable, the control flow diagram p.28/29 in the JTA spec suggests otherwise.) If both is defined, i.e. container does not delist XAResource it keeps in it's invocation context before calling Transaction.commit() and Transaction.commit() handles still enlisted resources fine (by calling beforeCompletion() first, calling end() after that, and then prepare+commit) then we (i.e. the JPA spec) would not have a problem. If anyone knows, that that is defined somewhere, please let me know, I will also continue researching the related specs. If that is not explicitly defined, than we (i.e. the JPA spec) do have a problem, because we rely on undefined behavior. Regards, Christian On Thu, Jun 14, 2012 at 10:31 PM, Evan Ireland eireland@... Christian, In your step 6:  "when the method invocation (of e.g. a session bean) ends, the container will call Transaction.delistResource()" I wonder if that is the root of the difficulty. If the TM keeps JDBC connections (and/or XA resources) enlisted (in case they are reused later in the same TX) until just before step 7b, then the issue you are concerned about will not occur. cromberg@... ] jsr338-experts@... JPA-JTA integration, I standard registers a JDBC connection, connection pool, XAConnection.getConnection() to the XAConnection.getXAResource() that connection (branch) denoted by xid now session bean) ends, Transaction.delistResource() which in by JTA spec, prepare and commit, invoked the is supposed to not associated released, there implementations. XAResource having a but this naturally implementation is used.) +49 40 60990-0 +49 40 60990-113 cromberg@... www.versant.com http://www.google.com/url?q=http%3A%2F%2Fwww.versant.com%2F&amp;amp;s  | www.db4o.com http://www.google.com/url?q=http%3A%2F%2Fwww.db4o.com%2F&amp;amp;sa=D registration number: HRB Office: Halenreie 42, 22359 Wöbker, Volker John attachments, is contain confidential or review, use, disclosure or the intended recipient, e-mail and destroy</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: Inheritance of Extended Persistence Context</header>
    <date>Tue Jun 19 09:58:44 CEST 2012</date>
    <body>IMHO the XPC must be shared top-level. @Stateful     @EJB     @EJB     @PostConstruct         // Must not fail!         // Fail!     @Remove To extend Christian's example a bit, put a @PostConstruct on doSomething(). This is a perfectly legal construct by EJB standards, yet it would fail because of "broken" ;-) JPA XPC inheritance implementations. Carlo</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: BNF question: how to parse the example containing KEY, VALUE (and LIKE)</header>
    <date>Tue Jun 19 18:54:23 CEST 2012</date>
    <body>Hi Christian, I thought I had fixed this in the spec draft I had uploaded a couple of weeks  ago. On closer inspection, it looks like I had garbled the editing.  Please have a  look at the revised version which I just uploaded. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL joins and ON keyword</header>
    <date>Tue Jun 19 19:50:39 CEST 2012</date>
    <body>We'll have to agree to disagree I guess (unfortunately for me).  I guess we are too far down the current path. Anyway, a related concern I would like to raise is the on(...) method definitions on Join and Fetch.  Join and Fetch are unrelated interface hierarchies.  In Hibernate at least I decided to combine those interface hierarchies so I have an interface JoinImplementor that extends both Joing and Fetch (a Fetch really is just a specialization of a Join).  Now in JPA 2.1 with the addition of these on(...) methods, we now hit a bug in javac in every Java 6 JDK (Oracle on all platforms, OpenJDK, Mac JDK).  The original bug report (6294779) is unfortunately no longer accessible on the Oracle Java bug tracker, although google searches find tons of references to it..  The bug was fixed in Java 7's JDK. Any possible way to get Fetch to extend Join?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Nationalized character data</header>
    <date>Tue Jun 19 19:54:21 CEST 2012</date>
    <body>Tied how?  JDBC 4 has been the JDBC level since SE 6.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Issue with JTA and JPA interaction</header>
    <date>Wed Jun 20 12:00:32 CEST 2012</date>
    <body>sorry, sent it to the wrong list by mistake On Tue, Jun 19, 2012 at 9:11 PM, Paul Parkinson paul.parkinson@... Hi Christian, Yes, I understand (and actually I think B1 is implied already if nothing else actually).  Could I ask you to post this on  users@...  for comment from the community there as it relateds to JTA? Thanks, Paul Hi Paul, all, Recently we came across an issue regarding the interaction of JTA and JPA. The user visible artifact of JPA is an instance of EntityManager. JPA standardizes ORMs, so without loss of generality we can assume, that the EntityManager uses a JDBC connection to access the database, and with respect to this issue, we can also assume, that the JDBC driver used supports XADataSource. (Note, that the association of JDBC Connection instance to EntityManager instance is generally not mandated by the JPA spec (although for certain points in time it is implied). This means, it is permitted for the JPA vendor (unless database locks are used) to use a different JDBC connections for every database read operation, and return them to the pool after use. If database locks are used for operations and/or if flushing occurs, then of course the JDBC connection needs to be pinned to the EntityManager instance to guarantee transactional semantics.) The intended JPA-JTA interaction is as follows: The JPA implementation does _not_ interact with XADataSources, XAConnections or XAResources, it _does_ use a JDBC connection pool to acquire it's JDBC connections. This pool is configured to return enlisted connections, and when the JPA implementation asks for a connection, the container in the background acquires an XAConnection, enlists the result of XAConnection.getXAResource() with the current transaction and returns the (wrapped) result of XAConnection.getConnection() to the JPA implementation. The JPA implementation is only supposed to register a Synchronization instance during EntityManagerFactory.createEntityManager() if a JTA transaction is active at that time or during EntityManager.joinTransaction(). The JPA implementation is supposed to flush all changes, which have not been flushed to the database yet in Synchronization.beforeCompletion(). For this to work properly, a few conditions must be met: case A: the JPA implementation has acquired and pinned a JDBC connection, the associated XAResource is enlisted condition A1. the container must not have delisted the resource when the control flow leaves the container and the container is about to commit (or rollback) the transaction. If the resource was delisted before Transaction.commit(), the JDBC connection is not currently associated to the JTA transaction and everything flushed in beforeCompletion() is not part of the global transaction. condition A2. it must be supported to call Transaction.commit() while there are still XAResources "active", i.e. start has been called, while end has not been called, those should implicitly be "end()ed", before the prepare and commit calls occurr. condition A3. Transaction.commit() must execute the following in this order: -fire beforeCompletion -implicit ending of XAResources, that have not been ended yet -do all the prepare calls -do all the commit calls I assume, that most implementations of containers and transaction managers already meet these conditions, either by chance, by trial-and-error or by common-sense requirement extrapolation. However I have not found out yet, that any of these conditions is mandated by any of the related specs, and this would be needed, otherwise the JPA-JTA interaction relies on undefined behavior. case B: the JPA implementation does not currently have a pinned JDBC connection, when the container calls Transaction.commit() condition B1: enlisting resources must be allowed if triggered from a beforeCompletion() callback (because the JPA implementation will acquire a connection right then) condition B2: same as A2 condition B3: same as A3 I suggest, that these 4 conditions (B1 is debatable, JPA could be modified to explicitly disallow that) should be addressed in the upcoming JTA specification and in particular, an example control-flow diagram like on p.28/29 of JTA 1.1 should be added Thank you! (Sorry, that the mail became a bit long) Regards, Christian Disclaimer: our own JPA implemention is not an ORM, but uses our XA-capable, object-oriented database (Versant Object Database) directly, so our implementation of the JTA-JPA might look slightly different because of that, however I would like to see this problem solved for the general ORM implementations of JPA as well -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: portable way to restore XAResources required</header>
    <date>Wed Jun 20 12:02:24 CEST 2012</date>
    <body>Hi Paul, I made the same mistake here, I sent it to the wrong list, sorry for that, I will send it to the correct one. Christian On Tue, Jun 19, 2012 at 9:13 PM, Paul Parkinson paul.parkinson@... Hi Christian, Yes, the lack of certain recovery specification is a long standing known omission and has come up many times in the past.  I expect this may take some time to negotiate in the community as most application servers do this in different ways if only subtly in some cases.  Could I ask you to post this on  users@...  for comment from the community there as it relateds to JTA? Thanks, Paul Hi Paul, all, JTA 1.1 does not mandate, how an XAResource is restored after a system failure for the purpose of transaction recovery conducted by the transaction manager. Common suggestions and implementations (e.g. JBoss) use the following strategies: 1. If the XAResource was acquired by a JNDI-named pool, the JNDI name can be stored in the transaction log file of the transaction manager 2. If the XAResource is Serializable, then the XAResource can be serialized into the transaction log file of the transaction manager. Nothing of this is mandated by the JTA spec. Any transaction manager implementation needs a portable way to restore XAResources. If 1. alone would be mandatory, this would not work for us, because we don't use a JNDI-based pool but register our XAResource instances directly with the Transaction. I suggest to mandate the following behavior for the transaction manager when writing it's log file: "If the XAResource was acquired via a JNDI named pool or the XAResource is serializable, the transaction manager might store the JNDI name, or the serialized XAResource in it's transaction log file. If the XAResource was not acquired via a JNDI named pool and is not  serializable, an exception will be thrown". Thank you! Regards, Christian -- Christian Romberg Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Inheritance of Extended Persistence Context</header>
    <date>Wed Jun 20 15:52:08 CEST 2012</date>
    <body>Vendors could use an extension to switch between the two extended persistence inheritance techniques.  That could give some developers, a concrete way to experiment with both inheritance techniques (single level parent only or recursing up/across multiple levels). My preference is to add the metadata for JPA 2.1, rather than vendors using an extension. Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL joins and ON keyword</header>
    <date>Thu Jun 21 00:00:57 CEST 2012</date>
    <body>Um, what about the conflicting getParent() methods?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL joins and ON keyword</header>
    <date>Thu Jun 21 14:44:45 CEST 2012</date>
    <body>Hard to say exactly without access to the bug we cannot see ;) But my understanding is that the bug is with compiling convariant returns for methods on interfaces.  getParent() return is non-dynamic.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Annotating NamedQueries</header>
    <date>Thu Jun 21 15:23:54 CEST 2012</date>
    <body>From the spec: "The NamedQuery and NamedQueries annotations can be applied to an entity or mapped superclass." Why was that limitation chosen? Why not on any class you like? Is there a workaround available to put annotations on POJOs? For example on a "Service" Java POJO which contains logic/queries that stretch across several Entities? Regards,</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL joins and ON keyword</header>
    <date>Thu Jun 21 18:58:57 CEST 2012</date>
    <body>My point was that independent of the earlier bug, having Fetch extend Join  resulted in conflicting getParent() methods. Did I misunderstand what you were proposing?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL joins and ON keyword</header>
    <date>Thu Jun 21 20:54:48 CEST 2012</date>
    <body>Sorry, but I am not understanding you.  How would getParent() conflict? They both return From.  There is no covariant returns there.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL joins and ON keyword</header>
    <date>Thu Jun 21 22:52:03 CEST 2012</date>
    <body>Fetch.getParent() returns  FetchParent, not From.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft, etc.</header>
    <date>Fri Jun 22 01:42:47 CEST 2012</date>
    <body>I've uploaded the corresponding updated javadocs, also to the project  downloads area</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: BNF question: how to parse the example containing KEY, VALUE (and LIKE)</header>
    <date>Fri Jun 22 12:02:00 CEST 2012</date>
    <body>Hi Linda, I think this issue is still open. I think, while it might now be possible to parse the following two queries: (1) SELECT i.name , p FROM Item i JOIN i.photos p WHERE KEY(p) LIKE ‘%egret’ (2) SELECT i.name , p FROM Item i JOIN i.photos p WHERE VALUE(p) LIKE ‘%egret’ the following query still can't be parsed: (3) SELECT i.name , p FROM Item i JOIN i.photos p WHERE p LIKE ‘%egret’ But (3) is defined to be semantically identical to (2), e.g. see footnote 57 on page 153 of the draft dated 06/19/2012 Regards, Christian On Tue, Jun 19, 2012 at 6:54 PM, Linda DeMichiel linda.demichiel@... Hi Christian, I thought I had fixed this in the spec draft I had uploaded a couple of weeks ago. On closer inspection, it looks like I had garbled the editing.  Please have a look at the revised version which I just uploaded. thanks, -Linda Hi Linda, Any update on the issue below? Thank you! Christian cromberg@... cromberg@...    Hi Linda,     &amp;gt; Did you mean more than that?    Yes, a bit more:    All terms derived from qualified_identification_ variable or composable_qualified_ identification_variable start with    KEY, VALUE or ENTRY, respectively.    However, section 4.4.4, footnote 56 and the referred section 4.4.2 state "the use of VALUE is optional"    E.g. consider this query (modfied example from the spec, VALUE instead of KEY)    SELECT i.name http://i.name    FROM Item i JOIN i.photos p    WHERE VALUE(p) LIKE ‘%egret’    which would now be parsable after your suggested BNF modifications, is equivalent to:    SELECT i.name http://i.name    FROM Item i JOIN i.photos p    WHERE p LIKE ‘%egret’    which might still not be parsable, because we do only have a 'identification_variable' here, and not a    'qualified_identification_ variable'.    Thank you!    Christian linda.demichiel@... linda.demichiel@ oracle.com        Hi Christian,        I had intended that we would treat KEY and VALUE the same in the below.        Did you mean more than that?        regards,        -Linda            Hi Linda and Michael,            We do also need to cover the special short-cut case, that            "VALUE(identification___ variable)" is equivalent to just "identification_variable"            Cheers,            Christian linda.demichiel@...            &amp;lt;mailto: linda.demichiel@ oracle.com linda.demichiel@ __ orac le.com            &amp;lt;mailto: linda.demichiel@ oracle.com                Hi Michael,                    Hi Linda,                    more below ...                        Hi Michael,                        Thanks for following up on this.  More below....                            Hi Linda,                                Hi Michael,                                    Hi Linda,                                        Hi Christian,                                            Dear group,                                            While investigating how to parse queries like this (all page/chapter            references refer to                                            EDR 2.1, Dec 19th, 2011):                                            SELECT p                                            FROM Person p, in(pc.firstnames) n                                            WHERE n = 'Joe'                                            (firstnames is a List&amp;lt;String&amp;gt;)                                            I came across the following example provided in the spec, which is related            in structure:                                            section 4.4.4, p. 153:                                            SELECT i.name http://i.name http://i.name http://i.name                                            FROM Item i JOIN i.photos p                                            WHERE KEY(p) LIKE ‘%egret’                                            I looked at the BNF how this is parsed, and did not find any derivation for            the FROM clause:                                            where_clause ::= WHERE conditional_expression                                            conditional_expression ::= conditional_term                                            conditional_term ::= conditional_factor                                            conditional_factor ::= conditional_primary                                            conditional_primary ::= simple_cond_expression                                            simple_cond_expression ::= like_expression                                            like_expression ::= string_expression LIKE pattern_value                                            So eventually I end up with 'string_expression', from which I can not derive a            'qualified_identification_____ variable'                                            string_expression ::=                                            state_field_path_expression |                                            string_literal |                                            input_parameter |                                            functions_returning_strings |                                            aggregate_expression |                                            case_expression |                                            function_invocation |                                            (subquery)                                            Did I miss something, or is there actually a rule missing in the grammar for            this?                                        Yes, you are right.  The problem is that            composable_qualified_____ identification_variable                                        is currently assumed to not designate a state field, which it should.                                        Not sure yet what the best fix for this is in the grammar yet.                                    would it work if we add composable_qualified_____ identification_variable as an            additional                                    alternative in string_expression                                    (and datetime_expressio etc.):                                I believe so, but I think we have to go further.  I.e., I think we need to support this also                                in in_expression, function_arg, case_operand, aggregate_expression, and            entity_or_value_expression.                                Do you agree?                            yes agreed. The same holds true for orderby_item, correct?                        Yes, orderBy_item needs to be covered as well.                        I am hesitating about adding composable_qualified_____ identification_variable                        to state_field_path_expression though, because a key may often not be a                        state field of the object that is the map value.  I'm also concerned                        about the order-by rules.  E.g.,                        @Entity                          ...                          @OneToMany  // map key is division name                        The following query shouldn't work, because the key is                        not part of the result.                    I agree, the above query should not work. But how about returning the key that is used in the order by:                    SELECT key(vp)                    FROM Company c JOIN c.organization vp                    WHERE vp.reports &amp;gt; 100                    ORDER BY key(vp)                Yes.  I agree that this should be legal.                    I think this query should be legal. This means the orderby_item needs to be covered the same as all            the other                    places we                    discussed where composable_qualified_____ identification_variable needs to be added. But maybe we            should extend the                    rules for                    the orderby_item as specified in chapter 4.9 on pages 183/184. We could add                    composable_qualified_____ identification_variable                    to the second rule that covers state_field_path_expression already.                Yes, only I would cover this as a separate rule (to make it very clear).                        SELECT vp                        FROM Company c JOIN c.organization vp                        WHERE vp.reports &amp;gt; 100                        ORDER BY key(vp)                        So, I was thinking of just either adding composable_qualified_____ identification_variable to                        the needed places (ugly) or defining another non-terminal, e.g.,                        generalized_state_expression:: ____= state_field_path_expression |            composable_qualified_____ identification_variable.                        What do you think?                    I like the idea of having another non-terminal because a            composable_qualified_____ identification_variable like                    key(vp) is                    not really a state_field_path_expression.                    That would mean the new non-terminal generalized_state_expression is used in all the places we            discussed before,                    correct?                Yes.                Thanks again!  It looks like we're on the same page here.                regards,                -Linda                    Regards Michael                        regards,                        -Linda                            So maybe we extend state_field_path_expression and add            composable_qualified_____ identification_variable as                            an alternative:                            state_field_path_expression ::=                                        general_subpath.state_field |                                        composable_qualified_____ identification_variable                            Regards Michael                                regards,                                -Linda                                    string_expression ::=                                       state_field_path_expression |                                    composable_qualified_____ identification_variable |                                       string_literal |                                       ...                                    Regards Michael                                        Thanks for pointing this out.                                        -Linda                                        \                                            Thank you!                                            Christian                                            --                                            Christian Romberg                                            Chief Engineer| Versant GmbH                                            (T) +49 40 60990-0                                            (F) +49 40 60990-113                                            (E) cromberg@... cromberg@...            &amp;lt;mailto: cromberg@... cromberg@... cromberg@...            &amp;lt;mailto: cromberg@...            &amp;lt;mailto: cromberg@... cromberg@...             www.versant.com http://www.versant.com http://www.versant.com http://www.google.com/url?q=_ ___http%3A%2F%2Fwww.versant. com%____2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=___ _AFrqEzeeEBc_gN_____ 8mxtt8xDB0tjXDXQVlw http://www.google.com/url?q=_ _http%3A%2F%2Fwww.versant.com% __2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=__ AFrqEzeeEBc_gN___ 8mxtt8xDB0tjXDXQVlw http://www.google.com/url?q=_ _http%3A%2F%2Fwww.versant.com% __2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=__ AFrqEzeeEBc_gN___ 8mxtt8xDB0tjXDXQVlw http://www.google.com/url?q= http%3A%2F%2Fwww.versant.com% 2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg= AFrqEzeeEBc_gN_ 8mxtt8xDB0tjXDXQVlw             www.db4o.com http://www.db4o.com http://www.db4o.com http://www.google.com/url?q=_ ___http%3A%2F%2Fwww.db4o.com% 2F&amp;amp;____sa=D&amp;amp;sntz=1&amp;amp;usg=____ AFrqEzdo3Q40RwKQPBtnPIuBYQd1di ____FxJQ http://www.google.com/url?q=_ __sa=D&amp;amp;sntz=1&amp;amp;usg=__ AFrqEzdo3Q40RwKQPBtnPIuBYQd1di __FxJQ http://www.google.com/url?q=_ __sa=D&amp;amp;sntz=1&amp;amp;usg=__ AFrqEzdo3Q40RwKQPBtnPIuBYQd1di __FxJQ http://www.google.com/url?q= sa=D&amp;amp;sntz=1&amp;amp;usg= AFrqEzdo3Q40RwKQPBtnPIuBYQd1di FxJQ                                            --                                            Versant                                            GmbH is incorporated in Germany. Company registration number: HRB                                            54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359                                            Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John                                            CONFIDENTIALITY                                            NOTICE: This e-mail message, including any attachments, is for the sole                                            use of the intended recipient(s) and may contain confidential or                                            proprietary information. Any unauthorized review, use, disclosure or                                            distribution is prohibited. If you are not the intended recipient,                                            immediately contact the sender by reply e-mail and destroy all copies of                                            the original message.                                    --                                    *Michael Bouschen*                                    *Prokurist*                                    akquinet tech@spree GmbH                                    Bülowstr. 66, D-10783 Berlin                                    Fon: +49 30 235 520-33 33&amp;gt; &amp;lt;tel:%2B49%2030%20235%20520-__                                    Fax: +49 30 217 520-12 12&amp;gt; &amp;lt;tel:%2B49%2030%20217%20520-__                                    Email: michael.bouschen@... michael.bouschen@ akquinet.de            &amp;lt;mailto: michael.bouschen@ __ akq uinet.de michael.bouschen@ akquinet.de                                    Web: www.akquinet.de http://www.akquinet.de http://www.akquinet.de http://www.akquinet.de                                    akquinet tech@spree GmbH, Berlin                                    Geschäftsführung: Martin Weber, Dr. Torsten Fink                                    Amtsgericht Berlin-Charlottenburg HRB 86780 B                                    USt.-Id. Nr.: DE 225 964 680                            --                            *Michael Bouschen*                            *Prokurist*                            akquinet tech@spree GmbH                            Bülowstr. 66, D-10783 Berlin                            Fon: +49 30 235 520-33 33&amp;gt; &amp;lt;tel:%2B49%2030%20235%20520-__                            Fax: +49 30 217 520-12 12&amp;gt; &amp;lt;tel:%2B49%2030%20217%20520-__                            Email: michael.bouschen@... michael.bouschen@ akquinet.de            &amp;lt;mailto: michael.bouschen@ __ akq uinet.de michael.bouschen@ akquinet.de                            Web: www.akquinet.de http://www.akquinet.de http://www.akquinet.de http://www.akquinet.de                            akquinet tech@spree GmbH, Berlin                            Geschäftsführung: Martin Weber, Dr. Torsten Fink                            Amtsgericht Berlin-Charlottenburg HRB 86780 B                            USt.-Id. Nr.: DE 225 964 680                    --                    *Michael Bouschen*                    *Prokurist*                    akquinet tech@spree GmbH                    Bülowstr. 66, D-10783 Berlin                    Fon: +49 30 235 520-33 33&amp;gt; &amp;lt;tel:%2B49%2030%20235%20520-__                    Fax: +49 30 217 520-12 12&amp;gt; &amp;lt;tel:%2B49%2030%20217%20520-__                    Email: michael.bouschen@... michael.bouschen@ akquinet.de            &amp;lt;mailto: michael.bouschen@ __ akq uinet.de michael.bouschen@ akquinet.de                    Web: www.akquinet.de http://www.akquinet.de http://www.akquinet.de http://www.akquinet.de                    akquinet tech@spree GmbH, Berlin                    Geschäftsführung: Martin Weber, Dr. Torsten Fink                    Amtsgericht Berlin-Charlottenburg HRB 86780 B                    USt.-Id. Nr.: DE 225 964 680            --            Christian Romberg            Chief Engineer| Versant GmbH            (T) +49 40 60990-0            (F) +49 40 60990-113            (E) cromberg@... cromberg@... cromberg@...            &amp;lt;mailto: cromberg@...             www.versant.com http://www.versant.com http://www.google.com/url?q=_ _http%3A%2F%2Fwww.versant.com% __2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=__ AFrqEzeeEBc_gN___ 8mxtt8xDB0tjXDXQVlw http://www.google.com/url?q= http%3A%2F%2Fwww.versant.com% 2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg= AFrqEzeeEBc_gN_ 8mxtt8xDB0tjXDXQVlw             www.db4o.com http://www.db4o.com http://www.google.com/url?q=_ __sa=D&amp;amp;sntz=1&amp;amp;usg=__ AFrqEzdo3Q40RwKQPBtnPIuBYQd1di __FxJQ http://www.google.com/url?q= sa=D&amp;amp;sntz=1&amp;amp;usg= AFrqEzdo3Q40RwKQPBtnPIuBYQd1di FxJQ            --            Versant            GmbH is incorporated in Germany. Company registration number: HRB            54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359            Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John            CONFIDENTIALITY            NOTICE: This e-mail message, including any attachments, is for the sole            use of the intended recipient(s) and may contain confidential or            proprietary information. Any unauthorized review, use, disclosure or            distribution is prohibited. If you are not the intended recipient,            immediately contact the sender by reply e-mail and destroy all copies of            the original message.    --    Christian Romberg    Chief Engineer| Versant GmbH    (T) +49 40 60990-0    (F) +49 40 60990-113    (E) cromberg@... cromberg@...     www.versant.com http://www.google.com/url?q= http%3A%2F%2Fwww.versant.com% 2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg= AFrqEzeeEBc_gN_ 8mxtt8xDB0tjXDXQVlw     www.db4o.com http://www.google.com/url?q= sa=D&amp;amp;sntz=1&amp;amp;usg= AFrqEzdo3Q40RwKQPBtnPIuBYQd1di FxJQ    --    Versant    GmbH is incorporated in Germany. Company registration number: HRB    54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359    Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John    CONFIDENTIALITY    NOTICE: This e-mail message, including any attachments, is for the sole    use of the intended recipient(s) and may contain confidential or    proprietary information. Any unauthorized review, use, disclosure or    distribution is prohibited. If you are not the intended recipient,    immediately contact the sender by reply e-mail and destroy all copies of    the original message. -- Christian Romberg Chief Engineer| Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cromberg@... cromberg@... www.versant.com http://www.google.com/url?q= http%3A%2F%2Fwww.versant.com% 2F&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg= AFrqEzeeEBc_gN_ 8mxtt8xDB0tjXDXQVlw www.db4o.com http://www.google.com/url?q= sa=D&amp;amp;sntz=1&amp;amp;usg= AFrqEzdo3Q40RwKQPBtnPIuBYQd1di FxJQ -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL joins and ON keyword</header>
    <date>Fri Jun 22 14:04:36 CEST 2012</date>
    <body>FetchParent extends From.  Its not a conflict.  Well except to this Java 6 compiler bug, which maybe is your point.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL joins and ON keyword</header>
    <date>Fri Jun 22 18:31:52 CEST 2012</date>
    <body>It is the other way around -- From extends FetchParent</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: BNF question: how to parse the example containing KEY, VALUE (and LIKE)</header>
    <date>Fri Jun 22 20:17:27 CEST 2012</date>
    <body>Hi Christian, Yes, you are correct.  This would also arise with other types of expressions  as well. Replacing map_field_identification_variable with  general_identification_variable should do it, right? thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] A question on FETCH JOIN and NEW</header>
    <date>Fri Jun 22 23:36:49 CEST 2012</date>
    <body>A colleague of mine has the following (apparently invalid) JPQL query: SELECT NEW com.foobar.jpa.DonationAllocationDTOEntity( a.id , a.campaign, a.campAppeal, a.campDivision, a.divisionFund) FROM DonationAllocation a JOIN a.donation d JOIN a.allocationType t JOIN FETCH a.campaign WHERE d.id = :donationId AND (t.code = 'Pledge' OR t.code = 'MatchingPledge')</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: BNF question: how to parse the example containing KEY, VALUE (and LIKE)</header>
    <date>Sat Jun 23 00:33:14 CEST 2012</date>
    <body>Hi Linda, hi Christian, Hi Christian, Hi Linda, I think this issue is still open.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] JPA 2.1: Enhance per-query and per-property control over fetch eagerness, fetch mode, fetch groups</header>
    <date>Sat Jun 23 15:30:15 CEST 2012</date>
    <body>To the JPA spec team, the users list, and possibly interested others I've BCC'd: I've been seeing increasing evidence on user-facing forums and mailing lists that control over fetching via JPA is a real challenge for developers. It's certainly been a huge one for me. I'm interested in whether this can be improved for JPA 2.1 and Java EE 7, as in my view the fetch issues are a big pain point. I'm writing to raise this with the JPA 2.1 spec team, as I don't see any enhancements regarding fetch strategies and modes in the latest draft and didn't spot discussion of it on the list. I'd like to strike up a discussion about what, if anything, can/should be done about this for Java EE 7. What do apps need to do? In JPA 2.0 there's solid control over lazy vs eager fetching on an entity/property/relationship basis using the usual @...ToOne / @...ToMany (fetch=FetchType.[LAZY|EAGER])  annotations and the orm.xml equivalents. This works well, but is too simple for many projects. From my admittedly rather limited experience, and from discussions I've seen, it seems common to have widely referenced entities that you don't want to eagerly load the relationships of most of the time, but *need* them loaded in some situations. Commonly this is because you'll be using them detached from an entity manager context and know you'll need access to normally lazily loaded properties. Sometimes it's a performance issue where you can't afford the expense of lots of little database hits as proxied lazily loaded properties are loaded. What's currently possible? Right now, my understanding - and I don't claim it's a great one - is exactly one option to override normally lazy fetching with standard JPA: use a left join fetch, either in JPQL or via Criteria API. That's OK much of the time. What's wrong with the current situation? Being limited to a "left join fetch" can also be really problematic: There's no way to ask the provider to use a different fetching strategy, like a follow-up batched SELECT, or use subselect fetching. A left join fetch is fine when you're eagerly fetching one or two lazily fetched entity relationships. It scales extremely poorly if you have several things to fetch and/or more than one level, eg "a.b.c". Apps sometimes need to do extra JPQL / criteria queries and repeat work in order to load required entities into the persistence context without expensive multiple joins. The key problem in my view is that the JPA API doesn't give the user any way to ask for normally-lazy relationships to be eagerly fetched without also forcing them to be fetched in a single SQL query . That can be really sub-optimal, and it conflates joins (a matter of query logic) with fetching (a matter of what's retrieved). You can't say "fetch x.y in whatever way is optimal". I've seen numerous recommendations, especially on the Vaadin lists and around Swing apps, to use EclipseLink and allow it to lazily load properties of detached entities using proxies. This is a nasty thing for people to be relying on, as (a) each load is a query, so it's the ultimate in n+1 or worse with nested properties; and (b) those later loads are generally in new transactions, breaking the DB's consistency guarantees in ways optimistic locking often can't help with. That people are having to rely on this is IMO of concern. It doesn't help that the Root&amp;lt;T&amp;gt;.fetch(...) API is difficult to use correctly and has been acknowledged to be poorly specified. It's easy to land up doing a second unnecessary join, or to get a " query specified join fetching, but the owner of the fetched association was not present in the select list" error. This article used to talk about it:   http://blogs.sun.com/ldemichiel/entry/jpa_next_thinking_about_the#comment-1291653518000 but has since been devoured by the Oracle transition. What can be done via implementation-specific extensions? Some JPA implementations offer fetch controls via extensions, but there's nothing consistently available. EclipseLink gives quite good fetching control via JPA query hints, allowing default fetch modes to be overridden on a per-property basis and allowing the specification of alternative fetch strategies. It also supports lazy loading of properties in detached entities, which has several problems as mentioned above. Hibernate, as far as I've been able to determine, doesn't expose anything equivalent at the JPA level. It has setFetchMode(...) in its own Criteria API, but as far as I've been able to find out it doesn't expose that to JPA via hints or other mechanisms. I'm frequently told that Hibernate is best suited for short-transaction stateless applications because it doesn't lazy load on detached entities - presumably because it's too hard to specify what you want eagerly loaded. I'm not sufficiently familiar with other implementations to say what they offer. What's needed? In my view, the key thing is that JPA needs to do is provide join mode and strategy controls at a per-query, per-relationship level without requiring a left join fetch. I'd be interested in what your thoughts are. Per-query, per-property overrides for eager vs lazy fetching Clients need to be able to specify to the ORM that a given property should be eagerly or lazily fetched in a particular query. An API that avoids the need for providers to have to parse free-form properties (and is thus more checkable) would be good, so adding something like:   CriteriaQuery.setFetchMode(String propertyName, FetchType fetchType) would seem ideal to me, where "propertyName" can be a dot-path to sub-properties, or of course a metamodel object/path. Different fetch strategies are supported by different implementations, and I don't think the JPA spec can really specify a complete set of possible strategies, so the fetch mode type should probably be a simple EAGER | LAZY enum, handily already provided by javax.persistence.FetchType . The implementer should be free to choose the most appropriate fetch method, so long as properties marked EAGER are in fact attached to the persistence context when the query completes. Per-query, per-property control over fetch strategies IMO if explicit specification of fetch strategy is provided though the JPA API (which would be nice) it should be by string names for strategies, or at least allow them. There's no predicting what fetch strategies will be possible. For example, with PostgreSQL's new JSON data type support it's possible to do an eager fetch of a relationship using a join or subquery with query_to_json, using array_agg and array_to_json, or using record_to_json. The ORM no longer needs to de-duplicate a cross product. Standardizing this would be nuts, but a way to ask an ORM that's aware of it to use it makes sense. I'd like to see something like:     CriteriaQuery.setFetchStrategy(String propertyName, String strategy) ... and maybe ...     CriteriaQuery.setFetchStategy(String propertyName, FetchStrategy strategy) the implementation choose (default for FetchType.EAGER ). Fetch groups? It may also be worth thinking about another often-sought-after facility, fetch groups, but IMO control over fetch mode and strategy on a per-query, per-relationship level is much more important. BTW, I wrote a bit about this earlier here:    http://blog.ringerc.id.au/2012/06/jpa2-is-very-inflexible-with-eagerlazy.html -- Craig Ringer POST Newspapers 276 Onslow Rd, Shenton Park Ph: 08 9381 3088     Fax: 08 9388 2258 ABN: 50 008 917 717 http://www.postnewspapers.com.au/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] CriteriaUpdate/CriteriaDelete questions</header>
    <date>Sun Jun 24 17:31:48 CEST 2012</date>
    <body>In implementing theses, I ran into 2 questions: 1) In terms of a Subquery associated with a CriteriaUpdate/CriteriaDelete, what should be the return for the Subquery#getParent which is defined to return an AbstractQuery which neither CriteriaUpdate/CriteriaDelete implement. 2) Seems to me we duplicate the effort to define the Root here; 2 calls when one would suffice.  First users pass the entity class to be updated/deleted to the CriteriaBuilder method, then they have to call one of the from() methods on CriteriaUpdate/CriteriaDelete.  But unless I miss something the actual "entity type" of the from argument has to be the same as the one passed to CriteriaBuilder.  Long story short, seems to me that we could simply do away with the from() methods and remove the need for the user to call the second method.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: JPA 2.1: Enhance per-query and per-property control over fetch eagerness, fetch mode, fetch groups</header>
    <date>Mon Jun 25 08:06:05 CEST 2012</date>
    <body>BTW, a post outlining some of the issues can be found here: http://jdevelopment.nl/fetching-arbitrary-object-graphs-jpa-2/ -- Craig Ringer POST Newspapers 276 Onslow Rd, Shenton Park Ph: 08 9381 3088     Fax: 08 9388 2258 ABN: 50 008 917 717 http://www.postnewspapers.com.au/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: JPA 2.1: Enhance per-query and per-property control over fetch eagerness, fetch mode, fetch groups</header>
    <date>Mon Jun 25 10:23:26 CEST 2012</date>
    <body>There already exist two similar issues on jira but nobody responded to them yet. One suggestion is based on criterias and the other on plain jpql query strings. The target of both is the same. IMO this is a very common need and i dont understand why nobody wants to specify that. I'd really like to hear opinions from the EG about this! Regards, Christian Beikov craig@... BTW, a post outlining some of the issues can be found here: http://jdevelopment.nl/fetching-arbitrary-object-graphs-jpa-2/ -- Craig Ringer POST Newspapers 276 Onslow Rd, Shenton Park Ph: 08 9381 3088     Fax: 08 9388 2258 ABN: 50 008 917 717 http://www.postnewspapers.com.au/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: BNF question: how to parse the example containing KEY, VALUE (and LIKE)</header>
    <date>Mon Jun 25 13:58:00 CEST 2012</date>
    <body>Hi Linda and Michael, Yes, that should do the trick. However it also increases the number of ambiguities that we already have in the grammar, because this is likely to introduce multiple alternatives that resolve to general_identification_variable, and requires implementors to rewrite the grammar and/or do "tricks" when parsing with specific libraries like ANTLR. For the sole purpose of defining the query language it is fine of course. Thank you! Christian On Sat, Jun 23, 2012 at 12:33 AM, Michael Bouschen michael.bouschen@... Hi Linda, hi Christian, Hi Christian, Hi Linda, I think this issue is still open.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: CriteriaUpdate/CriteriaDelete questions</header>
    <date>Mon Jun 25 21:09:13 CEST 2012</date>
    <body>Hi Steve, all, Steve, thanks for pointing this problem out -- good catch. If Subquery#getParent is going to work (which I think it needs to), then I  think we are back to CriteriaUpdate/CriteriaDelete having to extend AbstractQuery. If CriteriaUpdate/CriteriaDelete need to extend AbstractQuery, then I think  this becomes a moot point, as from() is defined there. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: CriteriaUpdate/CriteriaDelete questions</header>
    <date>Mon Jun 25 21:34:09 CEST 2012</date>
    <body>I also think Subquery#getParent should work if the parent is a CriteriaUpdate/CriteriaDelete.  Couldn't Subquery#getParent just return Query? Sure, but AbstractQuery also defines `public Set&amp;lt;Root&amp;lt;?&amp;gt;&amp;gt; getRoots()` which is a total misnomer in the case of CriteriaUpdate/CriteriaDelete. I think a new base contract would be better, though not totally backwards compatible.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: CriteriaUpdate/CriteriaDelete questions</header>
    <date>Mon Jun 25 21:41:55 CEST 2012</date>
    <body>Sorry, that second sentence should have asked whether Subquery#getParent could just return the new "base contract" referenced in the last sentence....</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: CriteriaUpdate/CriteriaDelete questions</header>
    <date>Mon Jun 25 21:51:12 CEST 2012</date>
    <body>Something like:    // rest of stuff currently on AbstractQuery,    // or just redefine AbstractQuery to extend Criteria    // not completely backwards compatible, but most correct imo:    // rest same    ...    ...    ...</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: CriteriaUpdate/CriteriaDelete questions</header>
    <date>Mon Jun 25 22:29:28 CEST 2012</date>
    <body>The problem is the backwards-compatibility. getRoots would just return a single root, which is ok (although not optimal).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] StoredProcedureQuery</header>
    <date>Thu Jun 28 22:06:09 CEST 2012</date>
    <body>Some more questions/commenst from implementation, this time in regards to javax.persistence.StoredProcedureQuery: 1) StoredProcedureQuery extends Query.  Query defines a few methods that are questionable being applied to a stored procedure call of any sort, namely the paging values for firstResults (getFirstResult/setFirstResult) and maxResults (getMaxResults/setMaxResults).  What is the expectation for those calls (mainly the setters I guess) when applied to StoredProcedureQuery? Personally I'd like to throw an exception, but the javadocs on those Query methods allow only for "@throws IllegalArgumentException if the argument is negative" 2) In regards to named versus positional parameters, I had a few questions: a) Could we possibly add a restriction that developers should use only one form or the other?  I see zero benefit to allowing developers to mix named and positional parameters in a single query, and in fact see only confusion about how those parameters ultimately get merged together. b) Given the javadoc statement that named parameters need to be registered "in order", I am assuming that their names have no relation to the notion of named parameters added to java.sql.CallableStatement in Java 7? Thanks, Steve</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: StoredProcedureQuery</header>
    <date>Thu Jun 28 22:59:37 CEST 2012</date>
    <body>3) WRT ParameterMode.REF_CURSOR, are those values supposed to be accessible by calling StoredProcedureQuery#getOutputParameterValue?  Or by calling StoredProcedureQuery#getResultList/getSingleResult?  If the former, do the Classes/ResultSetMappings used to create the StoredProcedureQuery apply to those ResultSets as well?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: CriteriaUpdate/CriteriaDelete questions</header>
    <date>Fri Jun 29 21:31:50 CEST 2012</date>
    <body>Hi Steve, all, My concern here is that this is a backwards incompatible change with regard to Subquery#getParent(). I'm not sure what the best thing to do here is.  I've attached revisions of CriteriaUpdate and CriteriaDelete that try to work around the problem, but I'm really not happy about how these abuse the typing. Suggestions appreciated. thanks, -Linda Attachment: CriteriaUpdate.java Description: Text document Attachment: CriteriaDelete.java Description: Text document</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: CriteriaUpdate/CriteriaDelete questions</header>
    <date>Mon Jul 02 15:36:57 CEST 2012</date>
    <body>The original API was never designed with these use-case in mind.  So you are stuck, unfortunately, between the rock (backwards compatibility) and hard place (changing the API to accurately reflect the situation). Personally, I tend toward correcting bad API designs.  One argument is that the design of the APIs here is a bug, if that helps justify it. Guess it all comes down to how strongly you feel about the API modeling an inaccurate view.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: CriteriaUpdate/CriteriaDelete questions</header>
    <date>Tue Jul 03 17:07:19 CEST 2012</date>
    <body>Hi Linda, all, Linda DeMichiel, am 29 Jun 2012 hast Du um 12:31 zum Thema "[jsr338- experts] Re: CriteriaUpdate/CriteriaDelet"  geschrieben :  Hi Steve, all,    My concern here is that this is a backwards incompatible change with  regard to Subquery#getParent().    I'm not sure what the best thing to do here is.  I've attached revisions  of CriteriaUpdate and CriteriaDelete that try to work around the problem,  but I'm really not happy about how these abuse the typing.    Suggestions appreciated. another - also not very nice - possibility is to o factor out all methods from Subquery into AbstractSubquery apart from  the getParent method o have Subquery extend AbstractSubquery, additionally providing its  getParent method o create a ModificationSubquery interface extending AbstractSubquery  and providing AbstractModification&amp;lt;?&amp;gt; getParent() where AbstractModification is an interface extended by both  CriteriaUpdate and CriteriaDelete (as proposed in my mail message of  April 1st, 2011) o have CriteriaUpdate/CriteriaDelete's subquery() method return  ModificationSubquery.  It's a patch, of course. Best regards Rainer --- Rainer Schweigkoffer               SAP AG Walldorf Java Persistence                   TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, I.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...       fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Bill  McDermott (Co-CEO), Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP  Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse  oder sonstige vertrauliche Informationen enthalten.  Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist  Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung  oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte  benachrichtigen Sie uns und vernichten Sie die empfangene  E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged,  undisclosed, or otherwise confidential information. If you  have received this e-mail in error, you are hereby  notified that any review, copying, or distribution of it  is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your  cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: CriteriaUpdate/CriteriaDelete questions</header>
    <date>Tue Jul 03 17:13:15 CEST 2012</date>
    <body>Rainer/all, Thanks a lot for the update. I try to reply here to see, if the new ML setup is working. I noticed, the "reply" address isn't the mailing list, but maybe that's the way you sent this message. Cheers, --  Werner Keil  |  JCP Executive Committee Member | Eclipse  UOMo Lead Twitter @wernerkeil  |  #Java_Social |  #EclipseUOMo | #OpenDDR Skype  werner.keil | Google+  gplus.to/wernerkeil * Chip-to-Cloud Security Forum: September 19 2012, Nice, French Riviera. Werner Keil, JCP Executive Committee, JSR-321 EG  Member will present "Trusted Computing API for Java™ " On Tue, Jul 3, 2012 at 5:07 PM, Rainer Kwesi Schweigkoffer kwesi@... Hi Linda, all, Linda DeMichiel, am 29 Jun 2012 hast Du um 12:31 zum Thema "[jsr338- experts] Re: CriteriaUpdate/CriteriaDelet"  geschrieben : another - also not very nice - possibility is to o factor out all methods from Subquery into AbstractSubquery apart from the getParent method o have Subquery extend AbstractSubquery, additionally providing its getParent method o create a ModificationSubquery interface extending AbstractSubquery and providing AbstractModification&amp;lt;?&amp;gt; getParent() where AbstractModification is an interface extended by both CriteriaUpdate and CriteriaDelete (as proposed in my mail message of April 1st, 2011) o have CriteriaUpdate/CriteriaDelete's subquery() method return ModificationSubquery. It's a patch, of course. Best regards Rainer --- Rainer Schweigkoffer               SAP AG Walldorf Java Persistence                   TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, I.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...       fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Bill McDermott (Co-CEO), Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP Supervisory Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse oder sonstige vertrauliche Informationen enthalten. Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte benachrichtigen Sie uns und vernichten Sie die empfangene E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged, undisclosed, or otherwise confidential information. If you have received this e-mail in error, you are hereby notified that any review, copying, or distribution of it is strictly prohibited. Please inform us immediately and destroy the original transmittal. Thank you for your cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Thu Jul 05 17:25:46 CEST 2012</date>
    <body>I did not see this covered specifically in the spec draft nor this discussion.  I apologize if I missed it. Say we encounter @Convert(converter=MyConverter.class) Does MyConverter *need to be known already*?  Obviously if scanning is enabled we will know about it (since by spec it needs to be annotated with @Converter...).  But what I mean is if scanning is disabled, does MyConverter have to be listed in XML for an @Convert reference to it to work?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Thu Jul 05 19:41:28 CEST 2012</date>
    <body>Hi Steve, Please see section 8.2.1.6.  It should be treated as the other managed  classes. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft (converters)</header>
    <date>Thu Jul 05 19:43:39 CEST 2012</date>
    <body>Still working from Draft 6 which does not call out converters specially in 8.2.1.6.  Sorry if that has been addressed in a later spec draft. Guess I'll find out next week when I move past draft 6 implementation :D</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: CriteriaUpdate/CriteriaDelete questions</header>
    <date>Thu Jul 05 20:41:17 CEST 2012</date>
    <body>Hi Rainer, all Thanks for the suggestions! When you say 'factor out' did you mean duplicate or move the methods that currently return Subquery?  If that latter, that is a backwards-incompatible change, which we can't make. Yes. Another approach is that we effectively "deprecate" Subquery#getParent and add a Subquery#getContainingQuery method, which would return a CommonAbstractQuery from which AbstractQuery, CriteriaUpdate, and CriteriaDelete would all inherit (i.e., back to the CommonAbstractQuery idea). More opinions?? best regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: CriteriaUpdate/CriteriaDelete questions</header>
    <date>Thu Jul 05 20:56:04 CEST 2012</date>
    <body>I like the deprecation idea. Just a suggestion though that CommonAbstractQuery  be CommonAbstractCriteria instead, since you already use "Query" (as in CriteriaQuery) for selection queries specifically.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: CriteriaUpdate/CriteriaDelete questions</header>
    <date>Thu Jul 05 23:56:13 CEST 2012</date>
    <body>Refactored APIs look like the attached.  Please let me know if you see any issues. -Linda Attachment: CommonAbstractCriteria.java Description: Text document Attachment: AbstractQuery.java Description: Text document Attachment: CriteriaUpdate.java Description: Text document Attachment: CriteriaDelete.java Description: Text document Attachment: Subquery.java Description: Text document</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: StoredProcedureQuery</header>
    <date>Fri Jul 06 00:03:48 CEST 2012</date>
    <body>I think these calls should work. The spec covers this in 3.9.12. The named parameters correspond to the parameter names of the stored procedure.  I  think the "in order" is in error and will remove.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: StoredProcedureQuery</header>
    <date>Fri Jul 06 00:08:49 CEST 2012</date>
    <body>From a JPA point of view, by calling getResultList/getSingleResult.  Under  the covers, the persistence provider would obtain the result from the stored procedure's  output parameter. Lance kindly passed on to me what JDBC 4.2 is doing for REF CURSOR support  --- see below. -Linda ------------------------ 13.3.3.4  REF CURSOR Support The  REF CURSOR data type is supported by several databases.  To return a REF CURSOR from a stored procedure,  the CallableStatement method registerOutParameter may be used specifying Types.REF_CURSOR as the data type to be returned.   The CallableStatement method getObject, specifying  ResultSet as the type to convert the returned object to, would be called to retrieve the ResultSet representing the REF CURSOR.  The returned result set is a forward, read-only result set. if registerOutParameter is called specifying Types.REF_CURSOR and the JDBC driver does not support this data type, a SQLFeatureNotSupportedException will be thrown. code example 13-28 Executing a callable statement that returns a ResultSet  using a REF CURSOR To determine if a JDBC Driver supports REF CURSOR, an application may call  DatabaseMetaData.supportsRefCursors.     /**      * Retrieves whether this database supports REF CURSOR.      *      * @exception SQLException if a database access error occurs      * @since 1.8      */ The entry in Types.java     /**      * The constant in the Java programming language, sometimes referred to      *      * @since 1.8      */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: CriteriaUpdate/CriteriaDelete questions</header>
    <date>Fri Jul 06 00:11:49 CEST 2012</date>
    <body>Subquery#getParent is not deprecated in these.  Is that just an oversight? Other than that I think these look great! So back to calling from(...) on CriteriaDelete and CriteriaUpdate, what is the expectation of multiple calls to those methods.  Do we just replace the old root?  Or is that an error?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: CriteriaUpdate/CriteriaDelete questions</header>
    <date>Fri Jul 06 00:16:39 CEST 2012</date>
    <body>No.  We don't mark things as @Deprecated as users find this noisy and  annoying. The getParent methods still works and except for CriteriaUpdate/Delete is  fine to use. I tried to be clear in the javadocs, but if you think these should be beefed up, feel free to suggest how. It should be an error.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: StoredProcedureQuery</header>
    <date>Fri Jul 06 00:17:19 CEST 2012</date>
    <body>Perfect, that is exactly what I figured, just making sure.  It is actually the right thing to do as well in my opinion as it now makes handling resultsets from functions/procs much more portable across databases when using JPA. Will there be a corollary supportsProcedureNamedParameters() added to DatabaseMetaData for 1.8 as well?  Or is the assumption that 1.8 compliant drivers *will* support it?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: StoredProcedureQuery</header>
    <date>Fri Jul 06 00:28:41 CEST 2012</date>
    <body>There is already a supportsNamedParameters() which has been there since J2SE 1.4 so we should be covered. Best Lance Perfect, that is exactly what I figured, just making sure.  It is actually the right thing to do as well in my opinion as it now makes handling resultsets from functions/procs much more portable across databases when using JPA. Will there be a corollary supportsProcedureNamedParameters() added to DatabaseMetaData for 1.8 as well?  Or is the assumption that 1.8 compliant drivers *will* support it?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: StoredProcedureQuery</header>
    <date>Fri Jul 06 00:35:33 CEST 2012</date>
    <body>Thanks Lance</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] difficulty in ensuring PersistenceUnitInfo.addTransformer(ClassTransformer transformer) happens before application classes are loaded...</header>
    <date>Fri Jul 06 16:30:25 CEST 2012</date>
    <body>Does anyone else want to see improvements in how PersistenceProvider.createContainerEntityManagerFactory() is expected to allow PersistenceUnitInfo.addTransformer(ClassTransformer transformer) to register class transformers before application classes are loaded? As more EE application server implementations look to deploy applications in parallel (making full use of all CPU cores), ensuring that the PersistenceProvider.createContainerEntityManagerFactory() is called before any application classes are loaded, having a separate way to invoke the PersistenceUnitInfo.addTransformer becomes more important. Then it doesn't matter if PersistenceProvider.createContainerEntityManagerFactory() is invoked after application classes have been loaded (as long as the addTransformer() happened early enough). For example, if I want to also use datasources during createContainerEntityManagerFactory(), I will need @DataSourceDefinitions to already be scanned and deployed. I have some ideas in mind but maybe you have something better.  So, my two questions are: 1.  Do you agree that this should be addressed in the JPA 2.1 specification? 2.  How can we separate the registration of the class transformers from the provider.createContainerEntityManagerFactory()? Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: StoredProcedureQuery</header>
    <date>Thu Jul 12 22:08:35 CEST 2012</date>
    <body>Is explicitly passing in ResultSet to getObject important?  Or could this: be similarly accomplished by:</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: StoredProcedureQuery</header>
    <date>Thu Jul 12 22:22:54 CEST 2012</date>
    <body>Is explicitly passing in ResultSet to getObject important?  Or could this: be similarly accomplished by:</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: StoredProcedureQuery</header>
    <date>Thu Jul 12 22:25:07 CEST 2012</date>
    <body>Yes, but if I need the (int,Class) signature I would need to make the invocation reflectively so avoiding the cast is irrelevant (for now).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Ambiguities in exceptions for EntityManager.createQuery(…)</header>
    <date>Tue Jul 17 11:05:16 CEST 2012</date>
    <body>Hi all, there are multiple (contradicting) sections in the spec defining the error  cases of EntityManager.createQuery(…). The JavaDoc in 3.1.1 states:  /**   * …   * @throws IllegalArgumentException if the query string is   *   found to be invalid   */ Below the code sample we find the following section:  If the argument to the createQuery method is not a valid Java Persistence   query string or a valid CriteriaQuery object, the IllegalArgumentException   may be thrown or the query execution will fail and a PersistenceException   will be thrown. What does "may be thrown" exactly mean? Why is query execution discussed in  this context? Does that mean that createQuery(…) might return a Query object  that will only fail eventually when being executed? If so, why is this  allowed actually? If so how can one reliably check a query string for  syntactical correctness? I currently have code that tries to validate a query string by calling  em.createQuery(…) and catching IllegalArgumentException as defined in the  JavaDoc. Unfortunately I've just discovered Hibernate (3.x and 4.x) throwing  IllegalStateException in case the query tree seems valid in general but fails  being parsed later on. em.createQuery("somethingAbsurd") -&amp;gt; IllegalArgumentException em.createQuery("select disinct(u) from User u") -&amp;gt; IllegalStateException I'd like to get a clear picture on what is actually required before filing a  ticket against Hibernate. Is it worth to straighten that out in the upcoming  version of the spec? Cheers, Ollie --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Improving the TCK</header>
    <date>Tue Jul 17 11:22:25 CEST 2012</date>
    <body>Hi all, during my development on Spring Data JPA I've stumbled over a few issues in  various persistence providers [0], [1]. I wondered why they were not  discovered by the TCK actually as the ones I listed clearly could have. So I  wonder whether it was possible to tighten the TCK in this regard which leads  to the following questions: 1. The original JSR for JPA 2.0 [2] stated in section 2.18:  The Java Persistence API, version 2.0 Technology Compatibility Kit (TCK)   will be available both as a standalone TCK and included as part of the Java   EE 6 Compatibility Test Suite (CTS). The Java Persistence API, version 2.0   Reference Implementation (RI) will be available both separately and as part   of the Java EE 6 RI. Where can one get access to the standalone version of the JPA 2.0 TCK? 2. How can we actually help out to improve the TCK? I wouldn't mind investing  some time to actually work on the code (assuming the TCK consists of JUnit  tests e.g.) Cheers, Ollie [0]  https://bugs.eclipse.org/bugs/show_bug.cgi?id=322579 [1]  https://hibernate.onjira.com/browse/HHH-6896 [2]  http://jcp.org/en/jsr/detail?id=317 --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Improving the TCK</header>
    <date>Tue Jul 17 20:36:24 CEST 2012</date>
    <body>Am 17.07.2012 11:22, schrieb Oliver Gierke: The TCK is still closed source, AFAIK. Over the past two years, I've found numerous spec-related bugs in all 3 certified JPA 2.0 implementations (Eclipselink, Hibernate, OpenJPA) that should have been caught by the TCK. Some 18 months ago, when there was not even a public mailing list, I sent a request to the write-only list jsr-317-feedback@... to release the TCK to the public. There is also the sad story of a fourth persistence provider who has given up on certification after trying in vain to get access to the TCK: http://datanucleus.blogspot.de/2011/01/jpa-tck-request-and-jpa21.html It is good to see that the current JCP is more open than former versions, but of course a community process that really deserves the name would open source all TCKs for all JSRs, no exceptions. Well, we can't as long as the TCK isn't open. That's the main point why it doesn't make sense to keep it closed. The CDI TCK has been Open Source from the very beginning, part of the same Java EE 6 umbrella release and governed by the same JCP. I really don't see why this shouldn't work for JPA just as well. Best regards, Harald</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Ambiguities in exceptions for EntityManager.createQuery(…)</header>
    <date>Tue Jul 17 23:14:32 CEST 2012</date>
    <body>Yes.  The provider may defer this check until the point where the query is to  be executed. If so, why is this allowed actually? If so how can one reliably check a query  string for syntactical correctness? This point was discussed back in the JPA 1.0 days when this decision was made. I'd like to hear the points of view of the JPA implementations represented  here as to whether it would be desirable to strengthen the requirements here or not.  For  example, we could consider adding a query property to indicate that the query string should be  validated early. -LInda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Ambiguities in exceptions for EntityManager.createQuery(…)</header>
    <date>Wed Jul 18 04:14:19 CEST 2012</date>
    <body>Hi, The current spec allows certain flexibility for the implementations to parse a query during execution as opposed to construction. In OpenJPA, we prefer such flexibility. Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Linda DeMichiel ---07/17/2012 02:15:29 PM---On 7/17/2012 2:05 AM, Oliver Gierke wrote: &amp;gt; Hi all, From: To: jsr338-experts@... Cc: Date: 07/17/2012 02:15 PM Subject: [jsr338-experts] Re: Ambiguities in exceptions for EntityManager.createQuery(…) Yes.  The provider may defer this check until the point where the query is to be executed. If so, why is this allowed actually? If so how can one reliably check a query string for syntactical correctness? This point was discussed back in the JPA 1.0 days when this decision was made. I'd like to hear the points of view of the JPA implementations represented here as to whether it would be desirable to strengthen the requirements here or not.  For example, we could consider adding a query property to indicate that the query string should be validated early. -LInda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Ambiguities in exceptions for EntityManager.createQuery(…)</header>
    <date>Wed Jul 18 11:20:53 CEST 2012</date>
    <body>I just did a brief check against OpenJpa 2.2.0. It's currently rejecting both  createQuery(String foo) and createNamedQuery(String name) if the query is  invalid or the named query with the name does not even exist. What benefits does artificially delaying query validation have? It's the  opposite of the fail fast principle. Right now you'd actually be the odd one  out if you returned a Query object not being executable which severely  impacts portability of applications and thus subverts the purpose of having  an API like this in the first place. How shall a client actually deal with  the information that the method could throw an exception but may be not? In every case I think the JavaDoc needs to be updated in case we really  decide to go for that option (which I'd like to vote against and which is  currently no one using and I don't see any benefit) as it's highly misleading  currently. Cheers, Ollie Am 18.07.2012 um 04:14 schrieb Pinaki Poddar:  Hi,  &amp;gt; I'd like to hear the points of view of the JPA implementations   &amp;gt; represented here as to whether  &amp;gt; it would be desirable to strengthen the requirements here or not.    The current spec allows certain flexibility for the implementations to   parse a query during execution as opposed to construction. In OpenJPA, we   prefer such flexibility.     Regards --    Pinaki Poddar                            Chair, Apache OpenJPA Project            http://openjpa.apache.org/  JPA Expert Group Member  Application &amp;amp; Integration Middleware              &amp;lt;graycol.gif&amp;gt;Linda DeMichiel ---07/17/2012 02:15:29 PM---On 7/17/2012 2:05   AM, Oliver Gierke wrote: &amp;gt; Hi all,    To:   jsr338-experts@...  Date: 07/17/2012 02:15 PM  Subject:      [jsr338-experts] Re: Ambiguities in exceptions for   EntityManager.createQuery(…)            &amp;gt; Hi all,  &amp;gt; there are multiple (contradicting) sections in the spec defining the   &amp;gt; error cases of EntityManager.createQuery(…). The JavaDoc in 3.1.1 states:  &amp;gt;&amp;gt; /**  &amp;gt;&amp;gt;   * …  &amp;gt;&amp;gt;   * @throws IllegalArgumentException if the query string is  &amp;gt;&amp;gt;   *   found to be invalid  &amp;gt;&amp;gt;   */  &amp;gt; Below the code sample we find the following section:  &amp;gt;&amp;gt; If the argument to the createQuery method is not a valid Java   &amp;gt;&amp;gt; Persistence query string or a valid CriteriaQuery object, the   &amp;gt;&amp;gt; IllegalArgumentException may be thrown or the query execution will fail   &amp;gt;&amp;gt; and a PersistenceException will be thrown.  &amp;gt; What does "may be thrown" exactly mean? Why is query execution discussed   &amp;gt; in this context? Does that mean that createQuery(…) might return a Query   &amp;gt; object that will only fail eventually when being executed?    Yes.  The provider may defer this check until the point where the query is   to be executed.    If so, why is this allowed actually? If so how can one reliably check a   query string for syntactical correctness?  &amp;gt; I currently have code that tries to validate a query string by calling   &amp;gt; em.createQuery(…) and catching IllegalArgumentException as defined in the   &amp;gt; JavaDoc. Unfortunately I've just discovered Hibernate (3.x and 4.x)   &amp;gt; throwing IllegalStateException in case the query tree seems valid in   &amp;gt; general but fails being parsed later on.  &amp;gt; em.createQuery("somethingAbsurd") -&amp;gt;  IllegalArgumentException  &amp;gt; em.createQuery("select disinct(u) from User u") -&amp;gt;  IllegalStateException  &amp;gt; I'd like to get a clear picture on what is actually required before   &amp;gt; filing a ticket against Hibernate. Is it worth to straighten that out in   &amp;gt; the upcoming version of the spec?    This point was discussed back in the JPA 1.0 days when this decision was   made.    I'd like to hear the points of view of the JPA implementations represented   here as to whether  it would be desirable to strengthen the requirements here or not.  For   example, we could  consider adding a query property to indicate that the query string should   be validated early.    -LInda      &amp;gt; Cheers,  &amp;gt; Ollie     --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Improving the TCK</header>
    <date>Wed Jul 18 11:27:08 CEST 2012</date>
    <body>Is there a chance we get official feedback in this regard? I don't care as  much about the JPA 2.1 TCK as it's done. Still I'd like to help improving the  upcoming version of the spec. I think this is crucial as I have seen tickets in implementors bug trackers  with comments that bugs won't be fixed if the TCK doesn't actually bark. This  way, it's actually not the spec driving the features/bugs of the  implementation but a TCK not even members of the expert group get access to.  This effectively creates a shadow spec which is a sub-optimal thing to see I  think. Regards, Ollie Am 17.07.2012 um 20:36 schrieb Harald Wellmann:  Am 17.07.2012 11:22, schrieb Oliver Gierke:    The TCK is still closed source, AFAIK.    Over the past two years, I've found numerous spec-related bugs in all 3  certified JPA 2.0 implementations (Eclipselink, Hibernate, OpenJPA) that  should have been caught by the TCK.    Some 18 months ago, when there was not even a public mailing list, I  sent a request to the write-only list jsr-317-feedback@... to  release the TCK to the public.    There is also the sad story of a fourth persistence provider who has  given up on certification after trying in vain to get access to the TCK:     http://datanucleus.blogspot.de/2011/01/jpa-tck-request-and-jpa21.html    It is good to see that the current JCP is more open than former  versions, but of course a community process that really deserves the  name would open source all TCKs for all JSRs, no exceptions.      Well, we can't as long as the TCK isn't open. That's the main point why  it doesn't make sense to keep it closed.    The CDI TCK has been Open Source from the very beginning, part of the  same Java EE 6 umbrella release and governed by the same JCP.    I really don't see why this shouldn't work for JPA just as well.    Best regards,  Harald         https://bugs.eclipse.org/bugs/show_bug.cgi?id=322579 [1 ] https://hibernate.onjira.com/browse/HHH-6896 [2 ] http://jcp.org/en/jsr/detail?id=317     --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Improving the TCK</header>
    <date>Wed Jul 18 11:47:08 CEST 2012</date>
    <body>Ollie/all, Thanks for your input. As we heard, some crucial part Java 8 has just died, so unless EE 8 was to skip 8 and build right onto 9, we're very likely to see a "less significant" or mostly "Maintainance" release of Java EE, too. If you want to improve things that don't make it into EE7, there might be a good chance to do so in 8 Regards, --  Werner Keil  |  JCP Executive Committee Member | Eclipse  UOMo Lead Twitter @wernerkeil  |  #Java_Social |  #EclipseUOMo | #OpenDDR Skype  werner.keil | Google+  gplus.to/wernerkeil * Chip-to-Cloud Security Forum: September 19 2012, Nice, French Riviera. Werner Keil, JCP Executive Committee, JSR-321 EG  Member will present "Trusted Computing API for Java™ " * Eclipse Day Delft: September 27 2012, Delft, Netherlands. Werner Keil, Eclipse Committer, UOMo Lead, Mærsk Build Manager  will present "Triple-E class Continuous Delivery with Hudson, Maven and Mylyn " On Wed, Jul 18, 2012 at 11:27 AM, Oliver Gierke ogierke@... Is there a chance we get official feedback in this regard? I don't care as much about the JPA 2.1 TCK as it's done. Still I'd like to help improving the upcoming version of the spec. I think this is crucial as I have seen tickets in implementors bug trackers with comments that bugs won't be fixed if the TCK doesn't actually bark. This way, it's actually not the spec driving the features/bugs of the implementation but a TCK not even members of the expert group get access to. This effectively creates a shadow spec which is a sub-optimal thing to see I think. Regards, Ollie Am 17.07.2012 um 20:36 schrieb Harald Wellmann: jsr-317-feedback@... to http://datanucleus.blogspot.de/2011/01/jpa-tck-request-and-jpa21.html https://bugs.eclipse.org/bugs/show_bug.cgi?id=322579 [1] https://hibernate.onjira.com/browse/HHH-6896 [2] http://jcp.org/en/jsr/detail?id=317 -- /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Improving the TCK</header>
    <date>Wed Jul 18 13:39:03 CEST 2012</date>
    <body>Maybe it's me or I am not getting the joke :). How does Jigsaw not making  into Java 8 relate to the general availability to get access to a specs TCK? Am 18.07.2012 um 11:47 schrieb Werner Keil:  Ollie/all,    Thanks for your input. As we heard, some crucial part Java 8 has just died,   so unless EE 8 was to skip 8 and build right onto 9, we're very likely to   see a "less significant" or mostly "Maintainance" release of Java EE, too.    If you want to improve things that don't make it into EE7, there might be a     Regards,  --   Werner Keil | JCP Executive Committee Member | Eclipse UOMo Lead  Twitter @wernerkeil | #Java_Social | #EclipseUOMo | #OpenDDR  Skype werner.keil | Google+ gplus.to/wernerkeil    * Chip-to-Cloud Security Forum: September 19 2012, Nice, French Riviera.   Werner Keil, JCP Executive Committee, JSR-321 EG Member will present   "Trusted Computing API for Java™"    * Eclipse Day Delft: September 27 2012, Delft, Netherlands. Werner Keil,   Eclipse Committer, UOMo Lead, Mærsk Build Manager will present "Triple-E   class Continuous Delivery with Hudson, Maven and Mylyn"      Is there a chance we get official feedback in this regard? I don't care as   much about the JPA 2.1 TCK as it's done. Still I'd like to help improving   the upcoming version of the spec.    I think this is crucial as I have seen tickets in implementors bug trackers   with comments that bugs won't be fixed if the TCK doesn't actually bark.   This way, it's actually not the spec driving the features/bugs of the   implementation but a TCK not even members of the expert group get access   to. This effectively creates a shadow spec which is a sub-optimal thing to   see I think.    Regards,  Ollie    Am 17.07.2012 um 20:36 schrieb Harald Wellmann:    &amp;gt; Am 17.07.2012 11:22, schrieb Oliver Gierke:  &amp;gt;&amp;gt; Where can one get access to the standalone version of the JPA 2.0  &amp;gt;&amp;gt; TCK?  &amp;gt; The TCK is still closed source, AFAIK.  &amp;gt; Over the past two years, I've found numerous spec-related bugs in all 3  &amp;gt; certified JPA 2.0 implementations (Eclipselink, Hibernate, OpenJPA) that  &amp;gt; should have been caught by the TCK.  &amp;gt; Some 18 months ago, when there was not even a public mailing list, I  &amp;gt; sent a request to the write-only list jsr-317-feedback@... to  &amp;gt; release the TCK to the public.  &amp;gt; There is also the sad story of a fourth persistence provider who has  &amp;gt; given up on certification after trying in vain to get access to the TCK:  &amp;gt;  http://datanucleus.blogspot.de/2011/01/jpa-tck-request-and-jpa21.html  &amp;gt; It is good to see that the current JCP is more open than former  &amp;gt; versions, but of course a community process that really deserves the  &amp;gt; name would open source all TCKs for all JSRs, no exceptions.  &amp;gt;&amp;gt; 2. How can we actually help out to improve the TCK?  &amp;gt; Well, we can't as long as the TCK isn't open. That's the main point why  &amp;gt; it doesn't make sense to keep it closed.  &amp;gt; The CDI TCK has been Open Source from the very beginning, part of the  &amp;gt; same Java EE 6 umbrella release and governed by the same JCP.  &amp;gt; I really don't see why this shouldn't work for JPA just as well.  &amp;gt; Best regards,  &amp;gt; Harald  &amp;gt;&amp;gt; Cheers, Ollie  &amp;gt;&amp;gt; [0]  https://bugs.eclipse.org/bugs/show_bug.cgi?id=322579 [1 ]  &amp;gt;&amp;gt;  https://hibernate.onjira.com/browse/HHH-6896 [2 ]  &amp;gt;&amp;gt;  http://jcp.org/en/jsr/detail?id=317    --  /**   * @author Oliver Gierke - Senior Member Technical Staff   *   * @param email ogierke@...   * @param phone +49-351-30929001   * @param fax   +49-351-418898439   * @param skype einsdreizehn   * @see  http://www.olivergierke.de   */           --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Improved injection capabilities for EntityManager(Factory)</header>
    <date>Wed Jul 18 14:02:51 CEST 2012</date>
    <body>Hi all, currently @PersistenceContext and @PersistenceUnit can not be used with  constructor parameters which essentially prevents clients from being designed  in an immutable way. Is there a dedicated reason this is the case? I propose  to add ElementType.PARAMETER to the array of @Target for both annotations so  that it can be used as follows: @Named   @Inject   public MyComponent(@PersistenceContext EntityManager em, MyDependency      // null checks to fail fast Especially if you're used to using constructor arguments for mandatory  dependencies an EntityManager is always the odd one out, as it cannot be  assigned to a final field, not handed into a constructor etc. Essentially we could allow @PersistenceContext and @PersistenceUnit be  entirely optional as the identification can be done through the type. The  additional annotation would then only be necessary in case you actually want  to configure something special. Thoughts? Ollie --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Improving the TCK</header>
    <date>Wed Jul 18 14:28:54 CEST 2012</date>
    <body>I'm not talking about the EE7 TCK, are you? You said, JPA 2.1 TCK is done, so you're looking forward to 2.2 TCK which may (if the release train contains it) be part of EE8.  EE8 is clearly be affected most by the Jigsaw disaster, many things the EE Umbrella EG and involved companies offering containers were hoping for are now delayed until at least EE9 if dependencies should be followed as they were so far. Thus there's at least an EE version where you may contribute improvements, if that's what you were talking about?  No joke, it is actually a very sad situation On Wed, Jul 18, 2012 at 1:39 PM, Oliver Gierke ogierke@... Maybe it's me or I am not getting the joke :). How does Jigsaw not making into Java 8 relate to the general availability to get access to a specs TCK? Am 18.07.2012 um 11:47 schrieb Werner Keil: gplus.to/wernerkeil ogierke@... jsr-317-feedback@... to http://datanucleus.blogspot.de/2011/01/jpa-tck-request-and-jpa21.html https://bugs.eclipse.org/bugs/show_bug.cgi?id=322579 [1] https://hibernate.onjira.com/browse/HHH-6896 [2] http://jcp.org/en/jsr/detail?id=317 ogierke@... +49-351-30929001 +49-351-418898439 http://www.olivergierke.de -- /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Improving the TCK</header>
    <date>Wed Jul 18 16:25:30 CEST 2012</date>
    <body>Sorry, Werner. I got it wrong in the first place. I meant the TCK for 2.0 is  done, I'd like to see the 2.1 one be improved. That is where we can at least  theoretically help out improving things. Sorry for the confusion! Am 18.07.2012 um 14:28 schrieb Werner Keil:  I'm not talking about the EE7 TCK, are you? You said, JPA 2.1 TCK is done,   so you're looking forward to 2.2 TCK which may (if the release train   contains it) be part of EE8.     EE8 is clearly be affected most by the Jigsaw disaster, many things the EE   Umbrella EG and involved companies offering containers were hoping for are   now delayed until at least EE9 if dependencies should be followed as they   were so far.    Thus there's at least an EE version where you may contribute improvements,   if that's what you were talking about?       Maybe it's me or I am not getting the joke :). How does Jigsaw not making   into Java 8 relate to the general availability to get access to a specs TCK?    Am 18.07.2012 um 11:47 schrieb Werner Keil:    &amp;gt; Ollie/all,  &amp;gt; Thanks for your input. As we heard, some crucial part Java 8 has just   &amp;gt; died, so unless EE 8 was to skip 8 and build right onto 9, we're very   &amp;gt; likely to see a "less significant" or mostly "Maintainance" release of   &amp;gt; Java EE, too.  &amp;gt; If you want to improve things that don't make it into EE7, there might be   &amp;gt; Regards,  &amp;gt; --  &amp;gt; Werner Keil | JCP Executive Committee Member | Eclipse UOMo Lead  &amp;gt; Twitter @wernerkeil | #Java_Social | #EclipseUOMo | #OpenDDR  &amp;gt; Skype werner.keil | Google+ gplus.to/wernerkeil  &amp;gt; * Chip-to-Cloud Security Forum: September 19 2012, Nice, French Riviera.   &amp;gt; Werner Keil, JCP Executive Committee, JSR-321 EG Member will present   &amp;gt; "Trusted Computing API for Java™"  &amp;gt; * Eclipse Day Delft: September 27 2012, Delft, Netherlands. Werner Keil,   &amp;gt; Eclipse Committer, UOMo Lead, Mærsk Build Manager will present "Triple-E   &amp;gt; class Continuous Delivery with Hudson, Maven and Mylyn"  &amp;gt; Is there a chance we get official feedback in this regard? I don't care   &amp;gt; as much about the JPA 2.1 TCK as it's done. Still I'd like to help   &amp;gt; improving the upcoming version of the spec.  &amp;gt; I think this is crucial as I have seen tickets in implementors bug   &amp;gt; trackers with comments that bugs won't be fixed if the TCK doesn't   &amp;gt; actually bark. This way, it's actually not the spec driving the   &amp;gt; features/bugs of the implementation but a TCK not even members of the   &amp;gt; expert group get access to. This effectively creates a shadow spec which   &amp;gt; is a sub-optimal thing to see I think.  &amp;gt; Regards,  &amp;gt; Ollie  &amp;gt; Am 17.07.2012 um 20:36 schrieb Harald Wellmann:  &amp;gt; &amp;gt; Am 17.07.2012 11:22, schrieb Oliver Gierke:  &amp;gt; &amp;gt;&amp;gt; Where can one get access to the standalone version of the JPA 2.0  &amp;gt; &amp;gt;&amp;gt; TCK?  &amp;gt; &amp;gt; The TCK is still closed source, AFAIK.  &amp;gt; &amp;gt; Over the past two years, I've found numerous spec-related bugs in all 3  &amp;gt; &amp;gt; certified JPA 2.0 implementations (Eclipselink, Hibernate, OpenJPA) that  &amp;gt; &amp;gt; should have been caught by the TCK.  &amp;gt; &amp;gt; Some 18 months ago, when there was not even a public mailing list, I  &amp;gt; &amp;gt; sent a request to the write-only list jsr-317-feedback@... to  &amp;gt; &amp;gt; release the TCK to the public.  &amp;gt; &amp;gt; There is also the sad story of a fourth persistence provider who has  &amp;gt; &amp;gt; given up on certification after trying in vain to get access to the TCK:  &amp;gt; &amp;gt;  http://datanucleus.blogspot.de/2011/01/jpa-tck-request-and-jpa21.html  &amp;gt; &amp;gt; It is good to see that the current JCP is more open than former  &amp;gt; &amp;gt; versions, but of course a community process that really deserves the  &amp;gt; &amp;gt; name would open source all TCKs for all JSRs, no exceptions.  &amp;gt; &amp;gt;&amp;gt; 2. How can we actually help out to improve the TCK?  &amp;gt; &amp;gt; Well, we can't as long as the TCK isn't open. That's the main point why  &amp;gt; &amp;gt; it doesn't make sense to keep it closed.  &amp;gt; &amp;gt; The CDI TCK has been Open Source from the very beginning, part of the  &amp;gt; &amp;gt; same Java EE 6 umbrella release and governed by the same JCP.  &amp;gt; &amp;gt; I really don't see why this shouldn't work for JPA just as well.  &amp;gt; &amp;gt; Best regards,  &amp;gt; &amp;gt; Harald  &amp;gt; &amp;gt;&amp;gt; Cheers, Ollie  &amp;gt; &amp;gt;&amp;gt; [0]  https://bugs.eclipse.org/bugs/show_bug.cgi?id=322579 [1 ]  &amp;gt; &amp;gt;&amp;gt;  https://hibernate.onjira.com/browse/HHH-6896 [2 ]  &amp;gt; &amp;gt;&amp;gt;  http://jcp.org/en/jsr/detail?id=317  &amp;gt; --  &amp;gt; /**  &amp;gt;  * @author Oliver Gierke - Senior Member Technical Staff  &amp;gt;  *  &amp;gt;  * @param email ogierke@...  &amp;gt;  * @param phone +49-351-30929001  &amp;gt;  * @param fax   +49-351-418898439  &amp;gt;  * @param skype einsdreizehn  &amp;gt;  * @see  http://www.olivergierke.de  &amp;gt;  */    --  /**   * @author Oliver Gierke - Senior Member Technical Staff   *   * @param email ogierke@...   * @param phone +49-351-30929001   * @param fax   +49-351-418898439   * @param skype einsdreizehn   * @see  http://www.olivergierke.de   */     --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Improving the TCK</header>
    <date>Wed Jul 18 16:29:50 CEST 2012</date>
    <body>OK, that makes more sense. Thanks, Werner On Wed, Jul 18, 2012 at 4:25 PM, Oliver Gierke ogierke@... Sorry, Werner. I got it wrong in the first place. I meant the TCK for 2.0 is done, I'd like to see the 2.1 one be improved. That is where we can at least theoretically help out improving things. Sorry for the confusion! Am 18.07.2012 um 14:28 schrieb Werner Keil: ogierke@... gplus.to/wernerkeil ogierke@... jsr-317-feedback@... to http://datanucleus.blogspot.de/2011/01/jpa-tck-request-and-jpa21.html https://bugs.eclipse.org/bugs/show_bug.cgi?id=322579 [1] https://hibernate.onjira.com/browse/HHH-6896 [2] http://jcp.org/en/jsr/detail?id=317 ogierke@... +49-351-30929001 +49-351-418898439 http://www.olivergierke.de ogierke@... +49-351-30929001 +49-351-418898439 http://www.olivergierke.de -- /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Ambiguities in exceptions for EntityManager.createQuery(…)</header>
    <date>Wed Jul 18 19:07:15 CEST 2012</date>
    <body>OpenJPA does "compile" the query in createQuery(). So you will see fail-fast behavior i.e. an exception would be thrown if a query string is syntactically incorrect. However, JPA spec made the choice that allowed an implementation to choose when (i.e. at construction or at execution) to validate/parse a query. I see that as a choice for a JPA provider rather than a hindrance to application portability of some sort. JDBC API Connection.prepareStatement(String) works in similar fashion -- (from JavaDoc of JDBC API) If the driver supports precompilation, the method prepareStatement will send the statement to the database for precompilation. Some drivers may not support precompilation. In this case, the statement may not be sent to the database until the PreparedStatement object is executed.     What perhaps makes an explicit Query.compile() useful (other than enforcing syntactic parse in a fail-fast manner), if a compiled query can be seen *independent* of a persistence context. Something like EntityManager em1 = ... String jpql = ...// some valid JPQL EntityManager em2 = ... Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Oliver Gierke ---07/18/2012 02:21:19 AM---I just did a brief check against OpenJpa 2.2.0. It's currently rejecting both createQuery(String foo From: To: Date: 07/18/2012 02:21 AM Subject: [jsr338-experts] Re: Ambiguities in exceptions for EntityManager.createQuery(…) I just did a brief check against OpenJpa 2.2.0. It's currently rejecting both createQuery(String foo) and createNamedQuery(String name) if the query is invalid or the named query with the name does not even exist. What benefits does artificially delaying query validation have? It's the opposite of the fail fast principle. Right now you'd actually be the odd one out if you returned a Query object not being executable which severely impacts portability of applications and thus subverts the purpose of having an API like this in the first place. How shall a client actually deal with the information that the method could throw an exception but may be not? In every case I think the JavaDoc needs to be updated in case we really decide to go for that option (which I'd like to vote against and which is currently no one using and I don't see any benefit) as it's highly misleading currently. Cheers, Ollie Am 18.07.2012 um 04:14 schrieb Pinaki Poddar: http://openjpa.apache.org/ -- /** * @author Oliver Gierke - Senior Member Technical Staff * * @param email ogierke@... * @param phone +49-351-30929001 * @param fax   +49-351-418898439 * @param skype einsdreizehn * @see http://www.olivergierke.de */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Improving the TCK</header>
    <date>Wed Jul 18 21:11:55 CEST 2012</date>
    <body>Hi Ollie, all, You can obtain a standalone license for the TCK through your Oracle sales rep.  Qualified non-profits can apply for a TCK scholarship. Our TCK team advises me that what would be most helpful is to continue to describe the cases that you've had problems with in different providers, as pass that information on.  The TCK engineers all track our expert groups, so they read the feedback that is posted here. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: Improving the TCK</header>
    <date>Wed Jul 18 21:20:52 CEST 2012</date>
    <body>I hope my last response addressed this.  If not, let me know what more  detailed info you are looking for. If you could help in pointing these out, that would be very useful.  Our team  certainly wants to know about such problems. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Improved injection capabilities for EntityManager(Factory)</header>
    <date>Thu Jul 19 04:45:56 CEST 2012</date>
    <body>Cant speak to the container implementation side of this in terms of feasibility.  But I think it absolutely makes sense from a user perspective.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Improving the TCK</header>
    <date>Thu Jul 19 19:03:39 CEST 2012</date>
    <body>   If you could help in pointing these out, that would be very useful.  Our   team certainly wants to know about such  problems. There's been a couple of comment threads in implementors bug trackers which  I've stumbled above. The quickest one I could find was one inside the  Hibernate bugtracker [0]. This is by no means finger pointing at Steve, as  ironically he and all the other leads of the implementations actually can't  do much more than follow the TCK. If it leaves holes and the spec is not as  tight as it should be, they essentially have no choice. One practical example of these is that EntityManager.createNamedQuery(…)  didn't throw an exception in case of an invalid query name provided for *two  major versions* (2.0 and 2.1). This actually relates to the discussion we  have in the separate thread which I'll respond to in a bit. Yet another  example is again Hibernate [2] (sorry Steve, I owe you a beer :), which  didn't implement Metamodel.managedType(…) correctly by throwing an exception  for embeddables and mapped superclasses. Once again, I think the TCK could  have caught that, making our lives (the implementor as well as the clients) a  bit easier. Essentially this boils down to two categories: one is slight ambiguities in  the spec which need to be discussed and improved (and which we probably won't  ever get rid off entirely). The other - even worse - are things that actually  *are* defined in detail by the spec but not enforced and then potentially  show up to anger users and implementors. They are not worse because they are  more crucial. They are worse because it would have been easy to catch them  early on if the TCK actually underwent the same open discussion as the spec  does. Regards, Ollie [0]  https://hibernate.onjira.com/browse/HHH-1134 [1]  https://bugs.eclipse.org/bugs/show_bug.cgi?id=322579 [2]  https://hibernate.onjira.com/browse/HHH-6896 http://datanucleus.blogspot.de/2011/01/jpa-tck-request-and-jpa21.html https://bugs.eclipse.org/bugs/show_bug.cgi?id=322579 [1 ] https://hibernate.onjira.com/browse/HHH-6896 [2 ] http://jcp.org/en/jsr/detail?id=317 --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Ambiguities in exceptions for EntityManager.createQuery(…)</header>
    <date>Thu Jul 19 19:18:27 CEST 2012</date>
    <body>Am 18.07.2012 um 19:07 schrieb Pinaki Poddar:  &amp;gt; I just did a brief check against OpenJpa 2.2.0.  OpenJPA does "compile" the query in createQuery(). So you will see   fail-fast behavior i.e. an exception would be thrown if a query string is   syntactically incorrect.    However, JPA spec made the choice that allowed an implementation to choose   when (i.e. at construction or at execution) to validate/parse a query. I   see that as a choice for a JPA provider rather than a hindrance to   application portability of some sort. If the option stays available no one can implement a validity check of a  query in a reliable and portable way, except writing lots of boilerplate  code.    // still not sure, what next?   // query broken To summarize: 1. None of the implementors currently returns an Query proxy for an invalid  query. 2. EntityManager.createNamedQuery(…) is not specified to allow a proxy to be  returned which creates an asymmetry between methods of the same purpose. The  TCK doesn't seem to check that actually, see [0]. 3. Making use of the option not only would potentially break thousands of  clients out there, it would also open up another Pandoras box of needed  specification: how shall the Query object behave on method calls like  setParameter(…)? Does it throw an exception, because of a cause that could  have been introduced earlier on? 4. There *are* implementations that throw other exceptions than the  IllegalArgumentException specified to be thrown. So here are my questions boiled down: 1. What benefit does keeping the option provide? Straightening the spec here  would provide more consistency and not break any clients and eventually allow  queries to be validated safely without extending the API. Beyond that the  current flexibility is not used, causes ambiguities, a less discoverable API. 2. Shall implementors be allowed to throw other exceptions than  IllegalArgumentException? If so, why? Once again allowing this makes it  harder to write portable and concise client code. Regards, Ollie [0]  https://bugs.eclipse.org/bugs/show_bug.cgi?id=322579  &amp;lt;graycol.gif&amp;gt;Oliver Gierke ---07/18/2012 02:21:19 AM---I just did a brief   check against OpenJpa 2.2.0. It's currently rejecting both   createQuery(String foo    Date: 07/18/2012 02:21 AM  Subject:      [jsr338-experts] Re: Ambiguities in exceptions for   EntityManager.createQuery(…)        I just did a brief check against OpenJpa 2.2.0. It's currently rejecting   both createQuery(String foo) and createNamedQuery(String name) if the query   is invalid or the named query with the name does not even exist.    What benefits does artificially delaying query validation have? It's the   opposite of the fail fast principle. Right now you'd actually be the odd   one out if you returned a Query object not being executable which severely   impacts portability of applications and thus subverts the purpose of having   an API like this in the first place. How shall a client actually deal with   the information that the method could throw an exception but may be not?    In every case I think the JavaDoc needs to be updated in case we really   decide to go for that option (which I'd like to vote against and which is   currently no one using and I don't see any benefit) as it's highly   misleading currently.    Cheers,  Ollie    Am 18.07.2012 um 04:14 schrieb Pinaki Poddar:    &amp;gt; Hi,  &amp;gt; &amp;gt; I'd like to hear the points of view of the JPA implementations   &amp;gt; &amp;gt; represented here as to whether  &amp;gt; &amp;gt; it would be desirable to strengthen the requirements here or not.  &amp;gt;   &amp;gt; The current spec allows certain flexibility for the implementations to   &amp;gt; parse a query during execution as opposed to construction. In OpenJPA, we   &amp;gt; prefer such flexibility.   &amp;gt;   &amp;gt; Regards --  &amp;gt;   &amp;gt; Pinaki Poddar                            &amp;gt; Chair, Apache OpenJPA Project            http://openjpa.apache.org/  &amp;gt; JPA Expert Group Member  &amp;gt; Application &amp;amp; Integration Middleware  &amp;gt;   &amp;gt;   &amp;gt;   &amp;gt;   &amp;gt;   &amp;gt;   &amp;gt; &amp;lt;graycol.gif&amp;gt;Linda DeMichiel ---07/17/2012 02:15:29 PM---On 7/17/2012   &amp;gt; 2:05 AM, Oliver Gierke wrote: &amp;gt; Hi all,  &amp;gt;   &amp;gt; To:  jsr338-experts@...  &amp;gt; Date:        07/17/2012 02:15 PM  &amp;gt; Subject:     [jsr338-experts] Re: Ambiguities in exceptions for   &amp;gt; EntityManager.createQuery(…)  &amp;gt;   &amp;gt;   &amp;gt;   &amp;gt;   &amp;gt;   &amp;gt; &amp;gt; Hi all,  &amp;gt; &amp;gt; there are multiple (contradicting) sections in the spec defining the   &amp;gt; &amp;gt; error cases of EntityManager.createQuery(…). The JavaDoc in 3.1.1   &amp;gt; &amp;gt; states:  &amp;gt; &amp;gt;&amp;gt; /**  &amp;gt; &amp;gt;&amp;gt;   * …  &amp;gt; &amp;gt;&amp;gt;   * @throws IllegalArgumentException if the query string is  &amp;gt; &amp;gt;&amp;gt;   *   found to be invalid  &amp;gt; &amp;gt;&amp;gt;   */  &amp;gt; &amp;gt; Below the code sample we find the following section:  &amp;gt; &amp;gt;&amp;gt; If the argument to the createQuery method is not a valid Java   &amp;gt; &amp;gt;&amp;gt; Persistence query string or a valid CriteriaQuery object, the   &amp;gt; &amp;gt;&amp;gt; IllegalArgumentException may be thrown or the query execution will   &amp;gt; &amp;gt;&amp;gt; fail and a PersistenceException will be thrown.  &amp;gt; &amp;gt; What does "may be thrown" exactly mean? Why is query execution   &amp;gt; &amp;gt; discussed in this context? Does that mean that createQuery(…) might   &amp;gt; &amp;gt; return a Query object that will only fail eventually when being   &amp;gt; &amp;gt; executed?  &amp;gt;   &amp;gt; Yes.  The provider may defer this check until the point where the query   &amp;gt; is to be executed.  &amp;gt;   &amp;gt; If so, why is this allowed actually? If so how can one reliably check a   &amp;gt; query string for syntactical correctness?  &amp;gt; &amp;gt; I currently have code that tries to validate a query string by calling   &amp;gt; &amp;gt; em.createQuery(…) and catching IllegalArgumentException as defined in   &amp;gt; &amp;gt; the JavaDoc. Unfortunately I've just discovered Hibernate (3.x and 4.x)   &amp;gt; &amp;gt; throwing IllegalStateException in case the query tree seems valid in   &amp;gt; &amp;gt; general but fails being parsed later on.  &amp;gt; &amp;gt; em.createQuery("somethingAbsurd") -&amp;gt;  IllegalArgumentException  &amp;gt; &amp;gt; em.createQuery("select disinct(u) from User u") -&amp;gt;    &amp;gt; &amp;gt; IllegalStateException  &amp;gt; &amp;gt; I'd like to get a clear picture on what is actually required before   &amp;gt; &amp;gt; filing a ticket against Hibernate. Is it worth to straighten that out   &amp;gt; &amp;gt; in the upcoming version of the spec?  &amp;gt;   &amp;gt; This point was discussed back in the JPA 1.0 days when this decision was   &amp;gt; made.  &amp;gt;   &amp;gt; I'd like to hear the points of view of the JPA implementations   &amp;gt; represented here as to whether  &amp;gt; it would be desirable to strengthen the requirements here or not.  For   &amp;gt; example, we could  &amp;gt; consider adding a query property to indicate that the query string should   &amp;gt; be validated early.  &amp;gt;   &amp;gt; -LInda  &amp;gt;   &amp;gt;   &amp;gt; &amp;gt; Cheers,  &amp;gt; &amp;gt; Ollie  &amp;gt;   &amp;gt;     --   /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */     --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: Improving the TCK</header>
    <date>Fri Jul 20 15:10:50 CEST 2012</date>
    <body>In my experience, one of the issues is timing.  When the EE TCK becomes available, the priority for the vendors implementing EE, is to pass the TCK.  In theory, when vendors initially get the early EE TCK snapshots, that would be a good time to report to Oracle, that a certain SPEC requirement is not covered but that is probably not going to happen as much (since vendors are focused on passing the TCK instead of improving it).  I'm all for a more open/transparent TCK process but I'd like to read how we actually want to improve the process and work backwards from there (if there is interest).  For example, I would like to see more non-vendor eyeballs on the EE TCK java sources and have a way for bug reports to be opened (that include patches for the TCK).  This doesn't have to be the entire TCK but would include the test classes. I also agree that ambiguities in the spec, should be discussed and improved.  I see this as a related issue and am just guilty as others, for remaining silent on some issues.  I think that this could be improved, if we all want to put effort into it.  I think part of this, could be helped with phone call meetings (either quarterly or when requested). I would also like to see us get to a point, where we agree that the priority is delivering the best specification for users and achieving more portability between vendor implementations.  Sometimes it seems that we instead push that our (vendor) implementation is the correct way (when ambiguities come up for discussion). Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Should IdentifiableType#getId(Class) always return non-null?</header>
    <date>Mon Jul 23 15:44:53 CEST 2012</date>
    <body>Hi all, Scenario:  entity "app.domain.Profile" extends abstract mapped superclass "app.domain.AbstractEntity".  If AbstractEntity defines a single string field as its @Id, should the IdentifiableType instance corresponding to app.domain.Profile guarantee that its getId(Class) method will never return null? In other words, is it up to the client of the JPA metamodel to go up the entity's type hierarchy until it finds the SingularAttribute for the id field, or should the JPA implementation do that? Thanks, Matthew -- mailto:matthew@...   skype:matthewadams12 googletalk:matthew@... http://matthewadams.me http://www.linkedin.com/in/matthewadams</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Should IdentifiableType#getId(Class) always return non-null?</header>
    <date>Mon Jul 23 15:56:08 CEST 2012</date>
    <body>I guess until either BeanValidation is used in connection with it to prevent null values, or other future (EE8+ maybe) options like a similar annotation on top of a JSR 308 style checker, it's probably up to the implementation. As soon as either BeanValidation (mostly runtime) or JSR 308 (then you should even get a compiler warning or error) enforces it, I don't know, if we could in JPA. Regards, --  Werner Keil  |  JCP Executive Committee Member | Eclipse  UOMo Lead Twitter @wernerkeil  |  #Java_Social |  #EclipseUOMo | #OpenDDR Skype  werner.keil | Google+  gplus.to/wernerkeil * Chip-to-Cloud Security Forum: September 19 2012, Nice, French Riviera. Werner Keil, JCP Executive Committee, JSR-321 EG  Member will present "Trusted Computing API for Java™ " * Eclipse Day Delft: September 27 2012, Delft, Netherlands. Werner Keil, Eclipse Committer, UOMo Lead, Mærsk Build Manager  will present "Triple-E class Continuous Delivery with Hudson, Maven and Mylyn " * JavaOne: September 30-October 4 2012, San Francisco, USA. Werner Keil,   JCP Executive Committee will represent "Eclipse UOMo, STEM, and JSRs involved in, e.g. 331 or JCP.next " On Mon, Jul 23, 2012 at 3:44 PM, Matthew Adams matthew@... Hi all, Scenario:  entity "app.domain.Profile" extends abstract mapped superclass "app.domain.AbstractEntity".  If AbstractEntity defines a single string field as its @Id, should the IdentifiableType instance corresponding to app.domain.Profile guarantee that its getId(Class) method will never return null? In other words, is it up to the client of the JPA metamodel to go up the entity's type hierarchy until it finds the SingularAttribute for the id field, or should the JPA implementation do that? Thanks, Matthew -- mailto:matthew@...   skype:matthewadams12 googletalk:matthew@... http://matthewadams.me http://www.linkedin.com/in/matthewadams</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Should IdentifiableType#getId(Class) always return non-null?</header>
    <date>Mon Jul 23 18:05:52 CEST 2012</date>
    <body>Hi Matthew, I'm not sure I understand your question fully.  However, it is the  responsibility of the metamodel implementation to return the SingularAttribute that is the value of getId (this may involve  traversing the hierarchy if getId were not defined in the IdentifiablyType itself). -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Should IdentifiableType#getId(Class) always return non-null?</header>
    <date>Mon Jul 23 18:58:29 CEST 2012</date>
    <body>Ok, so from your answer, I take it that it is the implementation's responsibility to walk up the entity's metadata hierarchy through any mapped superclasses until the class defining the identity is found. Is that specified?  The spec currently says: /** * Return the attribute that corresponds to the id attribute of * the entity or mapped superclass. * @param type the type of the represented id attribute * @return id attribute * @throws IllegalArgumentException if id attribute of the given * type is not present in the identifiable type or if * the identifiable type has an id class */ Potential javadoc clarification: "Return the attribute that corresponds to the id attribute of the entity, mapped superclass, or the id of the entity's or mapped superclasses' superclass(es).  This method must not return null." Better?  Is there a TCK test for this? -matthew On Mon, Jul 23, 2012 at 11:05 AM, Linda DeMichiel linda.demichiel@... Hi Matthew, Hi all, Scenario:  entity "app.domain.Profile" extends abstract mapped superclass "app.domain.AbstractEntity".  If AbstractEntity defines a single string field as its @Id, should the IdentifiableType instance corresponding to app.domain.Profile guarantee that its getId(Class) method will never return null? In other words, is it up to the client of the JPA metamodel to go up the entity's type hierarchy until it finds the SingularAttribute for the id field, or should the JPA implementation do that?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Should IdentifiableType#getId(Class) always return non-null?</header>
    <date>Mon Jul 23 19:41:04 CEST 2012</date>
    <body>Not sure.  I guess I don't understand the confusion, given that there is a  getDeclaredId method. The method pairs getXXX, getDeclaredXXX mirror what java.lang.reflect does,  so I really don't think this needs a change. Under what circumstances would you imagine that it would return null rather  than throw an IAE? -Linda p.s.  I'll let our TCK engineer respond to the test question.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Should IdentifiableType#getId(Class) always return non-null?</header>
    <date>Mon Jul 23 19:54:02 CEST 2012</date>
    <body>I think Michael was under the impression that the method extracts an id  property from the Class given. If you only look at this method, you don't  necessarily consider other potentially available methods. Plus, the Java  reflection methods usually work directly on the object they are invoked on.  The passing of a parameter can lead to the impression the parameter and only  the parameter get inspected. Anyway, does it hurt to add, that the parameter actually identifies a  persistent entity or mapped superclass and the id detection will consider the  entire tree? Cheers, Ollie Am 23.07.2012 um 19:41 schrieb Linda DeMichiel:        Not sure.  I guess I don't understand the confusion, given that there is a   getDeclaredId method.    The method pairs getXXX, getDeclaredXXX mirror what java.lang.reflect does,   so I really don't  think this needs a change.    Under what circumstances would you imagine that it would return null rather   than throw an IAE?    -Linda    p.s.  I'll let our TCK engineer respond to the test question.       mailto:linda.demichiel@ mailto:matthew@...  mailto:matthew@ mailto:googletalk%3Amatthew@ mailto:googletalk%3Amatthew@__matthewadams.me  mailto:googletalk%253Amatthew@ http://matthewadams.me http://www.linkedin.com/in/__matthewadams  http://www.linkedin.com/in/matthewadams mailto:matthew@ ... mailto:googletalk%3Amatthew@ http://matthewadams.me http://www.linkedin.com/in/matthewadams --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: Should IdentifiableType#getId(Class) always return non-null?</header>
    <date>Mon Jul 23 22:44:41 CEST 2012</date>
    <body>Regarding the TCK, like the Spec and the RI the TCK is currently in the development stages. The TCK and it's contents are only available to those who have licensed it. Likewise, any questions about whether something is tested or not is also only available to licensees. Stephen</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] schema generation proposed changes</header>
    <date>Tue Jul 24 01:55:43 CEST 2012</date>
    <body>Here's an updated version of my earlier schema generation proposal, broken out as to proposed spec changes.  You should hopefully find this considerably more flexible and detailed than the earlier draft. -Linda ---------------------------- Proposed spec changes: New Section: Schema Generation [to follow section 9.3] Schema generation may either happen prior to application deployment or when the entity manager factory is created as part of the application deployment and initialization process.   *  In Java EE environments, the container may call the      PersistenceProvider generateSchema method separately from and/or      prior to the creation of the entity manager factory for the      persistence unit, or the container may pass additional      information to the createContainerEntityManagerFactory call to      cause schema generation to happen as part of the entity manager      factory creation and application initialization process.  The      information passed to these methods determines whether the      generation of schemas and/or tables occurs directly in the target      database, or whether DDL scripts for schema generation are      created, or both.  *   In Java SE environments, the application may call the Persistence      generateSchema method separately from and/or prior to the creation      of the entity manager factory or may pass information to the      createEntityManagerFactory method to cause schema generation to      occur as part of the entity manager factory creation. The application may provide DDL scripts to be used for schema generation, and package these scripts as part of the persistence unit or specify URLs corresponding to the location of such scripts.  In Java EE environments, such scripts may be executed by the container, or the container may direct the persistence provider to execute the scripts; in Java SE environments, the execution of the scripts is the responsibility of the persistence provider.  In the absence of the specification of scripts, schema generation, if requested, will be determined by the object/relational metadata of the persistence unit. The following standard properties are defined for use in schema generation.  In Java EE environments these properties are passed by the container in the Map argument to the PersistenceProvider generateSchema method or createContainerEntityManagerFactory method. In Java SE environments, they are passed in the Map argument to the Persistence generateSchema or createEntityManagerFactory method. javax.persistence.schema-generation-target:    The schema-generation-target property specifies whether the schema    is to be created in the database, whether scripts are to be    generated, or both.    values for this property: DATABASE, SCRIPTS, DATABASE_AND_SCRIPTS    [Open Issue:  introduce for these enum or use strings?  Better names??] javax.persistence.schema-generation-action:    The schema-generation-action property is used to specify the action    to be taken by the persistence provider.  If the    schema-generation-target property is not specified, no action must    be taken.    values for this property: NONE, CREATE, DROP_AND_CREATE, DROP    [Open Issue:  enum strings ("none", "create", "drop-and-create",...) javax.persistence.create-database-schemas    In Java EE environments, it is anticipated that the Java EE    platform provider may wish to control the creation of database    schemas rather than delegate this task to the persistence provider.    The create-database-schemas property specifies whether the    persistence provider is to create the database schema(s) in    addition to creating database objects such as tables, sequences,    constraints, etc.  The value of this boolean property should be set    to true if the persistence provider is to create schemas in the    database or to generate DDL which is to contain "CREATE SCHEMA"    commands.  If this property is not supplied, the provider should    not attempt to create database schemas.  This property may be    specified in Java SE environments as well. javax.persistence.ddl-create-script-target, javax.persistence.ddl-drop-script-target:    If scripts are to be generated, the target locations for the    writing of these scripts must be specified.  These targets may take    the form of either Writers or strings designating URLs.  The    persistence provider must produce both create and drop scripts if    the corresponding DDL targets are specified.  This is independent    of whether a drop action is included in the value passed for the    schema-generation-action property.  If the schema-generation-target    property specifies scripts and script targets are not specified,    the IllegalArgumentException should be thrown by the provider.    ddl-create-script-target: a Writer configured for the persistence       provider for output of the DDL script or a string specifying       the URL for the DDL script.  This property should only be       specified if scripts are to be generated.    ddl-drop-script-target: a Writer configured for the persistence       provider for output of the DDL script or a string specifying the       URL for the DDL script.  This property should only be specified       if scripts are to be generated.   [Suggestions for better names ??] javax.persistence.database-product-name, javax.persistence.database-product-version,     If scripts are to be generated by the persistence provider and a     connection to the target database is not supplied, the     javax.persistence.database-product-name property must be     specified.  The value of this property should be the value     returned for the target database by the JDBC DatabaseMetaData     method getDatabaseProductName.  If sufficient database version     information is not included in the result of this method, the     database-product-version property should also be specified, and     should contain the value returned by the JDBC     getDatabaseProductVersion method. javax.persistence.ddl-create-script-source, javax.persistence.ddl-drop-script-source:     The ddl-create-script-source and ddl-drop-script-source properties     are used for script execution.  In Java EE container environments,     it is generally expected that the container will be responsible     for executing DDL scripts, although the container is permitted to     delegate this task to the persistence provider.  If DDL scripts     are to be used in Java SE environments or if the Java EE container     delegates the execution of scripts to the persistence provider,     these properties must be specified.  The script sources may take     the form of either Readers or strings designating URLs.    ddl-create-script-source: a Reader configured for reading of the       DDL script or a string specifying the URL for the DDL script.    ddl-drop-script-source: a Reader configured for reading of the DDL       script or a string specifying the URL for the DDL script.   [Suggestions for better names ??] javax.persistence.sql-load-script-source:    A data load script may be supplied as part of the persistence    unit.  In Java EE container environments, it is generally expected    that the container will be responsible for executing data load    scripts, although the container is permitted to delegate this task    to the persistence provider.  If a load script is to be used in    Java SE environments or if the Java EE container delegates the    execution of the load script to the persistence provider, this    property must be specified.  The script source may take the form    of either a Reader or a string designating a URL.    sql-load-script-source: a Reader configured for reading of the SQL       load script for database initialization or a string specifying       the URL for the script. javax.persistence.schema-generation-connection:   JDBC connection to be used for schema generation.  This is intended   for use in Java EE environments, where the platform provider may   want to control the database privileges that are available to the   persistence provider.  This connection is provided by the container,   and should be closed by the container when the schema generation   request or entity manager factory creation completes.  The   connection provided must have credentials sufficient for the   persistence provider to carry out the requested actions.  If this   property is not specified, the persistence provider should use the   DataSource that has otherwise been provided. 9.4.1 PersistenceProvider Interface: New method: /**  * Create database schemas and/or tables and/or create DDL  * scripts as determined by the supplied properties  *  * @param info metadata for use by the persistence provider  * @param map properties for schema generation; these  *             may also contain provider-specific properties  * @throws PersistenceException if insufficient or inconsistent  *         configuration information is provided or if schema  *         generation otherwise fails.  */ public void generateSchema(PersistenceUnitInfo info, Map map) 9.6 Persistence Class New method: /**  * Create database schemas and/or tables and/or create DDL  * scripts as determined by the supplied properties  *  * @param persistenceUnitName the name of the persistence unit  * @param map properties for schema generation; these may also  *        contain provider-specific properties.  The values of  *        these properties override any values that may have been  *        configured elsewhere.  * @throws PersistenceException if insufficient or inconsistent  *         configuration information is provided or if schema  *         generation otherwise fails.  */ public void generateSchema(String persistenceUnitName, Map properties) ----------------------- Section 8.2.1 persistence.xml file To be added to section 8.2.1: Scripts for use in schema generation and bulk loading of data may be packaged as part of the persistence unit. To be added to Section 8.2.1.9 (properties): Scripts for use in schema generation are specified using the ddl-create-script and ddl-drop-script elements.  A script to specify SQL for the bulk loading of data may be specified by the sql-load-script element.  These scripts may be packaged as part of the persistence unit or designated by URL strings. [Open Issue: do we want to require a drop-script if a create-script has been provided?] ----------- Chapter 11: New annotations for use in schema generation:   boolean unique() default false;  // should this be here or just use  UniqueConstraints? columnList syntax follows that of the OrderBy annotation:    columnList::= indexColumn [, indexColumn]*    indexColumn::= columnName [ASC | DESC]    If ASC or DESC is not specified, ASC is assumed is to be added to Table, SecondaryTable, CollectionTable, JoinTable, and  TableGenerator /**  *  Provides for defining a foreign key constraint or for overriding  *  or disabling the persistence provider's default foreign key  *  definition.  */ The syntax used in the foreignKeyDefinition element should follow the SQL syntax used by the target database for foreign key constraints. E.g., this would likely be similar to the following:       [ ON UPDATE &amp;lt;referential action&amp;gt; ]       [ ON DELETE &amp;lt;referential action&amp;gt; ] If disableForeignKey is specified as true, the provider must not generate a foreign key constraint. The following is to be added to JoinColumn, JoinColumns, MapKeyJoinColumn, MapKeyJoinColumns, PrimaryKeyJoinColumn and PrimaryKeyJoinColumns:</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Should IdentifiableType#getId(Class) always return non-null?</header>
    <date>Tue Jul 24 14:42:20 CEST 2012</date>
    <body>The only reason that I discovered this is because DataNucleus returns null if the entity being examined inherits its identity from a mapped superclass.  Spring Data JPA assumes that the implementation should never return null.  I was left wondering whether the onus was on the implementation (DN) or on the metamodel client (SD JPA) to navigate the tree. I agree that the methods parallel the java.lang.reflect methods, however, a little clarity here certainly wouldn't hurt.  The only difference in the javadoc comments between the getId(Class) and getDeclaredId(Class) methods are the words "declared by", which are very subtle.  I just reread them and had to read the comments twice even when looking for the difference!  :) -matthew On Mon, Jul 23, 2012 at 12:41 PM, Linda DeMichiel linda.demichiel@... Ok, so from your answer, I take it that it is the implementation's responsibility to walk up the entity's metadata hierarchy through any mapped superclasses until the class defining the identity is found. Is that specified?  The spec currently says: /** * Return the attribute that corresponds to the id attribute of * the entity or mapped superclass. * @param type the type of the represented id attribute * @return id attribute * @throws IllegalArgumentException if id attribute of the given * type is not present in the identifiable type or if * the identifiable type has an id class */ Potential javadoc clarification: "Return the attribute that corresponds to the id attribute of the entity, mapped superclass, or the id of the entity's or mapped superclasses' superclass(es).  This method must not return null." Better?  Is there a TCK test for this?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: Improving the TCK</header>
    <date>Wed Jul 25 21:32:20 CEST 2012</date>
    <body>+10000 Early access to the TCK (in progress) is by far the best way to (1) catch holes in the TCK and (2) keep it improving.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [javaee-spec users] [jsr342-experts] Re: Re: Modularization Framework/SPI</header>
    <date>Thu Jul 26 15:54:45 CEST 2012</date>
    <body>Here are what I usually hear: 1) The comments made come along the line of the thick stack and having resources used by major components that aren't used.  Complaint is EE-bloat.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: Improving the TCK</header>
    <date>Fri Jul 27 16:24:31 CEST 2012</date>
    <body>+1 On Wed, Jul 25, 2012 at 2:32 PM, Steve Ebersole steve.ebersole@... +10000 Early access to the TCK (in progress) is by far the best way to (1) catch holes in the TCK and (2) keep it improving. I think this is crucial as I have seen tickets in implementors bug trackers with comments that bugs won't be fixed if the TCK doesn't actually bark. This way, it's actually not the spec driving the features/bugs of the implementation but a TCK not even members of the expert group get access to. This effectively creates a shadow spec which is a sub-optimal thing to see I think.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation proposed changes</header>
    <date>Sun Jul 29 23:16:06 CEST 2012</date>
    <body>I have a concern about the API here.  Specifically: public void generateSchema(PersistenceUnitInfo info, Map map) public void generateSchema(String persistenceUnitName, Map properties) The concern is that we will potentially be parsing the mapping information twice per deployment: once to comply with the "generate schema" call and then again to build the EMF.  Unless providers stash away the "parsed" information in a static context, which is rather dubious IMHO.  Yes, this works in the case when the caller only wants the schema generation to happen. I think it is more a general concern with the bootstrapping process. This actually falls into a discussion Scott Marlow and I have been having about provider bootstrapping.  The discussion started out in regards to container bootstrapping specifically, but I think it is generally applicable to SE bootstrapping as well.  The idea Scott and I decided to implement specific to JBoss/Hibernate integration is a 2-step bootstrapping process.  Not sure if that is something that we want to address in 2.1, but I think it really needs to be addressed at some point.  Scott had planned on writing up a proposal for this so I don;t want to steal his thunder, but I did want to raise this concern in regards to this proposal.  The basic premise behind the 2-step process is that the first step returns a delegate capable of building an EntityManagerFactory given the current set of inputs (Map and persistenceUnitName/persistenceUnitInfo); that first step would have certain restrictions on what it could do and what it could access.  In this scenario, we could ask this "delegate capable of building an EntityManagerFactory" to "generateSchema". This works with both cases of the schema generation happening as part of the EMF starting and in the case of the caller only wanting schema generation to happen.  And more specifically in that first case it works without double parsing the mapping information. I really do understand the desire to avoid API changes.  But I also know that sometimes they do need to change.  I think this is a good example of that.  The initial bootstrapping API was never designed with this new feature in mind (how could it have been).  In fact, in the EE case this 2-step process makes sense even without considering this new feature. Also, trying to look forward again, even though we currently only support create/drop the initial discussion discussed "schema migration" as well.  I wonder if in anticipation of that possibly being added in a later spec revision we might want to name these methods something other than "generate" (really it does not even fit in the DROP case).  Maybe "manageSchema"?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: schema generation proposed changes</header>
    <date>Mon Jul 30 01:57:55 CEST 2012</date>
    <body>On 07/30/2012 05:16 AM, Steve Ebersole I have a concern about the API here.  Specifically: public void generateSchema(PersistenceUnitInfo info, Map map) public void generateSchema(String persistenceUnitName, Map properties) The concern is that we will potentially be parsing the mapping information twice per deployment: once to comply with the "generate schema" call and then again to build the EMF.  Unless providers stash away the "parsed" information in a static context, which is rather dubious IMHO.  Yes, this works in the case when the caller only wants the schema generation to happen. [snip] I really do understand the desire to avoid API changes.  But I also know that sometimes they do need to change.  I think this is a good example of that.  The initial bootstrapping API was never designed with this new feature in mind (how could it have been).  In fact, in the EE case this 2-step process makes sense even without considering this new feature.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: schema generation proposed changes</header>
    <date>Mon Jul 30 03:19:38 CEST 2012</date>
    <body>Yep a lot of this is from the same discussion.  Like I said, I did not want to steal Scott's thunder on this point.  Scott is returning from vacation tomorrow, so I am sure he will respond on this soon. Unfortunately I leave for vacation tonight so I just wanted to raise my concern with regard to this proposal specifically.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation proposed changes</header>
    <date>Mon Jul 30 03:20:02 CEST 2012</date>
    <body>Hi Steve, Just to be clear that we're on the same page here.... The generateSchema methods are there for people who want the separate stage in the process -- perhaps to create DDL files, iterate over them offline, and  then later cause the schema generation in the database. The other option described by the proposal is that schema generation occurs as a phase in the EMF creation itself.   In this case, the mapping information would presumably only be parsed once. I'm assuming that the persistence provider could implement both paths by  delegating to some common piece of code, but I didn't see that we needed to spec that. -Linda p.s.  I'm happy to have better names suggested.  The name generateSchema like the term "schema generation" (which seems to be in common usage here) is suboptimal IMO because in many cases the database schema per se is not being created as part of the process, only tables and other artifacts that live in the schema.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation proposed changes</header>
    <date>Mon Jul 30 03:24:54 CEST 2012</date>
    <body>That is what I was thinking at first, but then I was confused by the need for PersistenceProvider.generateSchema.  Assuming the normal split audience between Persistence and PersistenceProvider, what are the cases you envision for EE containers needing to call PersistenceProvider.generateSchema in this way?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation proposed changes</header>
    <date>Mon Jul 30 03:37:36 CEST 2012</date>
    <body>It is there for flexibility - to allow for the case where the platform  provider may want to offer pre-deployment options that allow these phases to be separated.  For  example, if DDL files were packaged with the app, a platform provider might want to  handle that as a separate phase which might plausibly involve administrative intervention  in the workflow.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation proposed changes</header>
    <date>Wed Aug 08 19:17:44 CEST 2012</date>
    <body>Please let me know if there are any further comments on this, as I would otherwise like to proceed to fold it into the spec. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation proposed changes</header>
    <date>Thu Aug 09 19:50:49 CEST 2012</date>
    <body>The idea of having a separate generateSchema() that might do the same work as would of been done in createContainerEntityManagerFactory() (if certain properties are specified), brings to mind some other recent discussions (one that I raised about deployment ordering concerns). Could we add other methods that provide a two-step approach to creating the container manager entity manager factory?  EE Containers will still need to handle older JPA providers but we could offer faster/more concurrent application deployment for JPA 2.1 and greater. The reason for turning createContainerEntityManagerFactory(), into two phases, is to allow the EE container to control when different JPA deployment operations can occur.  More specifically, I would like to have control over when the persistence provider performs the following aspects of createContainerEntityManagerFactory(): - Scanning for annotations &amp;amp; adding class transformers (to ensure that this happens before other EE deployers load application classes). - DataSource is used (this will help with use of @DataSourceDefinition which might not be initialized until after the other EE deployers have at least started scanning application classes for @DataSourceDefinition). The following is from an extension that we (Steve &amp;amp; I) started to work on, to accomplish this:     // Transformers added during this call     // datasources accessible during either of these calls Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation proposed changes</header>
    <date>Sat Aug 11 01:35:15 CEST 2012</date>
    <body>Scott, Steve Thanks for the proposal. Folks, I'd like to get feedback from the group on this, particularly from  container and persistence provider implementors. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Is it legal to have two entities map to the same table?</header>
    <date>Mon Aug 27 18:03:26 CEST 2012</date>
    <body>This is a situation I wish I didn't find myself in, but I do. Does the JPA specification permit two entities to map to the same underlying table? Assume for the sake of argument that each entity maps the table in the same way, i.e. that a write involving one entity could be read by the other. Best, Laird -- http://about.me/lairdnelson</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] JPQL: Sorting on optional references</header>
    <date>Tue Aug 28 14:44:58 CEST 2012</date>
    <body>Hi all, I just came across a JPQL spec scenario that seems to be a bit weird and I  wonder whether there's something we should do about. Suppose you have a  Person with optional Addresses: @Entity @Entity Now the query scenario here is that we'd like to get all Persons sorted by  the Address' city: select p from Person p left outer join p.address order by p.address.city Surprisingly, this query will not return Persons not having an Address  associated for the following reason: JPA 2.0 spec section 4.4.4. defines path  expressions as follows:  Path expression navigability is composed using “inner join” semantics. That   is,   if the value of a non-terminal field in the path expression is null, the   path is  considered to have no value, and does not participate in the determination   of   the result. That apparently forces persistence providers into adding an additional inner  join to the query which rules out the Persons without Addresses in the first  place. I think it's rather unfortunate to have this path expression  definition applied to order by clauses as users probably don't expect adding  a sort definition would strengthen the actual query criteria. So here are my  questions: 1. Why was the path expression navigability defined as such in the first  place and not as considering the mapping metadata (nullable = true -&amp;gt; outer  join, nullable = false -&amp;gt; inner join). Not saying this is utterly wrong, just  want to understand the probably available reasons. 2. Should/can this definition be changed to require consideration of the  mapping information? The path expression definition is very much written with  the purpose of defining selection criterias which is what they are  effectively not used for when used in ORDER BY clauses. The current state  leaves JPQL in the weird state that adding a sorting criteria affects the  returned items not only in order but also in which items are returned at all,  a side-effect which is unpleasant and not easy to grasp. Cheers, Ollie --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL: Sorting on optional references</header>
    <date>Tue Aug 28 15:12:52 CEST 2012</date>
    <body>Oliver/all, Thanks for pointing that out.  While the actual query may not be changed, you can sure contradict the nullable = true by adding a @NotNull annotation from Bean Validation to it. The two are legitimate together:  http://stackoverflow.com/questions/2725111/hibernate-onetoone-notnull   Java 7 and EE7 most likely give you a Runtime Exception, maybe some IDE warning if it provides JPA support, but the compiler itself is not going to do much about it. JSR 308, so far scheduled for Java 8 aims to change that: http://types.cs.washington.edu/jsr308/ Unfortunately, both the Spec Leads of 308 deny any responsibility or ownership of checker annotations, nor have the architects at Oracle so far take precautions to avoid a big mess like this @Entity   @OneToOne(nullable = true)  @NotNull   @NonNull @Nullable(true)      BeanValidation as of now Checker Annotations based on JSR 308 as of now (from Java 8) Given a combination of Java EE 6/7 and JSR 308 this would be perfectly fine. Most likely the two checkers would cause a compiler error already, but one alone, let's say @Nullable(true) instead of a sensible integration with all existing stuff will increase the big mess around these annotations already present http://stackoverflow.com/questions/4963300/which-notnull-java-annotation-should-i-use   I probably forgot @Nonnull as of JSR 305 which at the moment polutes the JDK already, but has no effect. At the very least, instead of having people stuff in more annotations of all different kinds, either @Nonnull should be killed with Java 8 and replaced with something else, or it should be used. Most likely the conflict with BeanValidation (which is primarily used by JPA) would be unavoidable then. Sorry to hijack the thread, I hope, at least Linda keeps an eye on that or plans to do something about it, so that we can avoid annotation chaos we inherited in other areas, e.g. @Inject vs. other legacy approaches, etc.? Thanks, Werner On Tue, Aug 28, 2012 at 2:44 PM, Oliver Gierke ogierke@... Hi all, I just came across a JPQL spec scenario that seems to be a bit weird and I wonder whether there's something we should do about. Suppose you have a Person with optional Addresses: @Entity @Entity Now the query scenario here is that we'd like to get all Persons sorted by the Address' city: select p from Person p left outer join p.address order by p.address.city Surprisingly, this query will not return Persons not having an Address associated for the following reason: JPA 2.0 spec section 4.4.4. defines path expressions as follows: That apparently forces persistence providers into adding an additional inner join to the query which rules out the Persons without Addresses in the first place. I think it's rather unfortunate to have this path _expression_ definition applied to order by clauses as users probably don't expect adding a sort definition would strengthen the actual query criteria. So here are my questions: 1. Why was the path _expression_ navigability defined as such in the first place and not as considering the mapping metadata (nullable = true -&amp;gt; outer join, nullable = false -&amp;gt; inner join). Not saying this is utterly wrong, just want to understand the probably available reasons. 2. Should/can this definition be changed to require consideration of the mapping information? The path _expression_ definition is very much written with the purpose of defining selection criterias which is what they are effectively not used for when used in ORDER BY clauses. The current state leaves JPQL in the weird state that adding a sorting criteria affects the returned items not only in order but also in which items are returned at all, a side-effect which is unpleasant and not easy to grasp. Cheers, Ollie -- /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL: Sorting on optional references</header>
    <date>Tue Aug 28 16:27:34 CEST 2012</date>
    <body>Hi all, sorry for that glitch. I essentially meant the "optional" attribute. So the  sample could should have read: @Entity @Entity Note that for the …ToOne annotation the optional flag defaults to true, so  unless you're explicitly changing this to false (and thus change the  semantics of the modeling completely) you're likely to run into the situation  described. Cheers, Ollie Am 28.08.2012 um 15:12 schrieb Werner Keil &amp;lt;werner.keil@...&amp;gt;:  Oliver/all,    Thanks for pointing that out.   While the actual query may not be changed, you can sure contradict the   nullable = true by adding a @NotNull annotation from Bean Validation to it.   The two are legitimate together:     http://stackoverflow.com/questions/2725111/hibernate-onetoone-notnull     Java 7 and EE7 most likely give you a Runtime Exception, maybe some IDE   warning if it provides JPA support, but the compiler itself is not going to   do much about it.    JSR 308, so far scheduled for Java 8 aims to change that:   http://types.cs.washington.edu/jsr308/    Unfortunately, both the Spec Leads of 308 deny any responsibility or   ownership of checker annotations, nor have the architects at Oracle so far   take precautions to avoid a big mess like this    @Entity      @OneToOne(nullable = true) @NotNull @NonNull @Nullable(true) Address   address;     BeanValidation as of now  Checker Annotations based on JSR 308 as of now (from Java 8)    Given a combination of Java EE 6/7 and JSR 308 this would be perfectly   fine. Most likely the two checkers would cause a compiler error already,   but one alone, let's say @Nullable(true) instead of a sensible integration   with all existing stuff will increase the big mess around these annotations   already present   http://stackoverflow.com/questions/4963300/which-notnull-java-annotation-should-i-use       I probably forgot @Nonnull as of JSR 305 which at the moment polutes the   JDK already, but has no effect. At the very least, instead of having people   stuff in more annotations of all different kinds, either @Nonnull should be   killed with Java 8 and replaced with something else, or it should be used.   Most likely the conflict with BeanValidation (which is primarily used by   JPA) would be unavoidable then.    Sorry to hijack the thread, I hope, at least Linda keeps an eye on that or   plans to do something about it, so that we can avoid annotation chaos we   inherited in other areas, e.g. @Inject vs. other legacy approaches, etc.?    Thanks,  Werner    Hi all,    I just came across a JPQL spec scenario that seems to be a bit weird and I   wonder whether there's something we should do about. Suppose you have a   Person with optional Addresses:    @Entity      @Entity    Now the query scenario here is that we'd like to get all Persons sorted by   the Address' city:    select p from Person p left outer join p.address order by p.address.city    Surprisingly, this query will not return Persons not having an Address   associated for the following reason: JPA 2.0 spec section 4.4.4. defines   path expressions as follows:    &amp;gt; Path expression navigability is composed using “inner join” semantics.   &amp;gt; That is,  &amp;gt; if the value of a non-terminal field in the path expression is null, the   &amp;gt; path is  &amp;gt; considered to have no value, and does not participate in the   &amp;gt; determination of  &amp;gt; the result.    That apparently forces persistence providers into adding an additional   inner join to the query which rules out the Persons without Addresses in   the first place. I think it's rather unfortunate to have this path   expression definition applied to order by clauses as users probably don't   expect adding a sort definition would strengthen the actual query criteria.   So here are my questions:    1. Why was the path expression navigability defined as such in the first   place and not as considering the mapping metadata (nullable = true -&amp;gt; outer   join, nullable = false -&amp;gt; inner join). Not saying this is utterly wrong,   just want to understand the probably available reasons.  2. Should/can this definition be changed to require consideration of the   mapping information? The path expression definition is very much written   with the purpose of defining selection criterias which is what they are   effectively not used for when used in ORDER BY clauses. The current state   leaves JPQL in the weird state that adding a sorting criteria affects the   returned items not only in order but also in which items are returned at   all, a side-effect which is unpleasant and not easy to grasp.    Cheers,  Ollie    --  /**   * @author Oliver Gierke - Senior Member Technical Staff   *   * @param email ogierke@...   * @param phone +49-351-30929001   * @param fax   +49-351-418898439   * @param skype einsdreizehn   * @see  http://www.olivergierke.de   */     --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL: Sorting on optional references</header>
    <date>Tue Aug 28 16:29:55 CEST 2012</date>
    <body>Thanks for correcting that. If it wasn't for the typo, you probably wouldn't have inspired me to highlight the "nullable" issues. Maybe should go into separate thread to ensure better visibility? Werner On Tue, Aug 28, 2012 at 4:27 PM, Oliver Gierke ogierke@... Hi all, sorry for that glitch. I essentially meant the "optional" attribute. So the sample could should have read: @Entity @Entity Note that for the …ToOne annotation the optional flag defaults to true, so unless you're explicitly changing this to false (and thus change the semantics of the modeling completely) you're likely to run into the situation described. Cheers, Ollie werner.keil@... http://stackoverflow.com/questions/2725111/hibernate-onetoone-notnull http://types.cs.washington.edu/jsr308/ http://stackoverflow.com/questions/4963300/which-notnull-java-annotation-should-i-use ogierke@... ogierke@... +49-351-30929001 +49-351-418898439 http://www.olivergierke.de -- /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL: Sorting on optional references</header>
    <date>Tue Aug 28 16:52:01 CEST 2012</date>
    <body>Sounds good. To be honest, I didn't quite get the connection. And on   @OneToOne(nullable = true) @NotNull @NonNull @Nullable(true) Address  it threw me out of the corner eventually :) Cheers, Ollie Am 28.08.2012 um 16:29 schrieb Werner Keil &amp;lt;werner.keil@...&amp;gt;:  Thanks for correcting that.    If it wasn't for the typo, you probably wouldn't have inspired me to   highlight the "nullable" issues. Maybe should go into separate thread to   ensure better visibility?    Werner    Hi all,    sorry for that glitch. I essentially meant the "optional" attribute. So the   sample could should have read:    @Entity      @Entity    Note that for the …ToOne annotation the optional flag defaults to true, so   unless you're explicitly changing this to false (and thus change the   semantics of the modeling completely) you're likely to run into the   situation described.    Cheers,  Ollie    Am 28.08.2012 um 15:12 schrieb Werner Keil &amp;lt;werner.keil@...&amp;gt;:    &amp;gt; Oliver/all,  &amp;gt; Thanks for pointing that out.  &amp;gt; While the actual query may not be changed, you can sure contradict the   &amp;gt; nullable = true by adding a @NotNull annotation from Bean Validation to   &amp;gt; it. The two are legitimate together:    &amp;gt;  http://stackoverflow.com/questions/2725111/hibernate-onetoone-notnull  &amp;gt; Java 7 and EE7 most likely give you a Runtime Exception, maybe some IDE   &amp;gt; warning if it provides JPA support, but the compiler itself is not going   &amp;gt; to do much about it.  &amp;gt; JSR 308, so far scheduled for Java 8 aims to change that:  &amp;gt;  http://types.cs.washington.edu/jsr308/  &amp;gt; Unfortunately, both the Spec Leads of 308 deny any responsibility or   &amp;gt; ownership of checker annotations, nor have the architects at Oracle so   &amp;gt; far take precautions to avoid a big mess like this  &amp;gt; @Entity  &amp;gt;   @OneToOne(nullable = true) @NotNull @NonNull @Nullable(true) Address   &amp;gt; BeanValidation as of now  &amp;gt; Checker Annotations based on JSR 308 as of now (from Java 8)  &amp;gt; Given a combination of Java EE 6/7 and JSR 308 this would be perfectly   &amp;gt; fine. Most likely the two checkers would cause a compiler error already,   &amp;gt; but one alone, let's say @Nullable(true) instead of a sensible   &amp;gt; integration with all existing stuff will increase the big mess around   &amp;gt; these annotations already present  &amp;gt;  http://stackoverflow.com/questions/4963300/which-notnull-java-annotation-should-i-use  &amp;gt; I probably forgot @Nonnull as of JSR 305 which at the moment polutes the   &amp;gt; JDK already, but has no effect. At the very least, instead of having   &amp;gt; people stuff in more annotations of all different kinds, either @Nonnull   &amp;gt; should be killed with Java 8 and replaced with something else, or it   &amp;gt; should be used. Most likely the conflict with BeanValidation (which is   &amp;gt; primarily used by JPA) would be unavoidable then.  &amp;gt; Sorry to hijack the thread, I hope, at least Linda keeps an eye on that   &amp;gt; or plans to do something about it, so that we can avoid annotation chaos   &amp;gt; we inherited in other areas, e.g. @Inject vs. other legacy approaches,   &amp;gt; etc.?  &amp;gt; Thanks,  &amp;gt; Werner  &amp;gt; Hi all,  &amp;gt; I just came across a JPQL spec scenario that seems to be a bit weird and   &amp;gt; I wonder whether there's something we should do about. Suppose you have a   &amp;gt; Person with optional Addresses:  &amp;gt; @Entity  &amp;gt; @Entity  &amp;gt; Now the query scenario here is that we'd like to get all Persons sorted   &amp;gt; by the Address' city:  &amp;gt; select p from Person p left outer join p.address order by p.address.city  &amp;gt; Surprisingly, this query will not return Persons not having an Address   &amp;gt; associated for the following reason: JPA 2.0 spec section 4.4.4. defines   &amp;gt; path expressions as follows:  &amp;gt; &amp;gt; Path expression navigability is composed using “inner join” semantics.   &amp;gt; &amp;gt; That is,  &amp;gt; &amp;gt; if the value of a non-terminal field in the path expression is null,   &amp;gt; &amp;gt; the path is  &amp;gt; &amp;gt; considered to have no value, and does not participate in the   &amp;gt; &amp;gt; determination of  &amp;gt; &amp;gt; the result.  &amp;gt; That apparently forces persistence providers into adding an additional   &amp;gt; inner join to the query which rules out the Persons without Addresses in   &amp;gt; the first place. I think it's rather unfortunate to have this path   &amp;gt; expression definition applied to order by clauses as users probably don't   &amp;gt; expect adding a sort definition would strengthen the actual query   &amp;gt; criteria. So here are my questions:  &amp;gt; 1. Why was the path expression navigability defined as such in the first   &amp;gt; place and not as considering the mapping metadata (nullable = true -&amp;gt;   &amp;gt; outer join, nullable = false -&amp;gt; inner join). Not saying this is utterly   &amp;gt; wrong, just want to understand the probably available reasons.  &amp;gt; 2. Should/can this definition be changed to require consideration of the   &amp;gt; mapping information? The path expression definition is very much written   &amp;gt; with the purpose of defining selection criterias which is what they are   &amp;gt; effectively not used for when used in ORDER BY clauses. The current state   &amp;gt; leaves JPQL in the weird state that adding a sorting criteria affects the   &amp;gt; returned items not only in order but also in which items are returned at   &amp;gt; all, a side-effect which is unpleasant and not easy to grasp.  &amp;gt; Cheers,  &amp;gt; Ollie  &amp;gt; --  &amp;gt; /**  &amp;gt;  * @author Oliver Gierke - Senior Member Technical Staff  &amp;gt;  *  &amp;gt;  * @param email ogierke@...  &amp;gt;  * @param phone +49-351-30929001  &amp;gt;  * @param fax   +49-351-418898439  &amp;gt;  * @param skype einsdreizehn  &amp;gt;  * @see  http://www.olivergierke.de  &amp;gt;  */    --  /**   * @author Oliver Gierke - Senior Member Technical Staff   *   * @param email ogierke@...   * @param phone +49-351-30929001   * @param fax   +49-351-418898439   * @param skype einsdreizehn   * @see  http://www.olivergierke.de   */     --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL: Sorting on optional references</header>
    <date>Tue Aug 28 16:55:13 CEST 2012</date>
    <body>You are not the only one, but unfortunately the uncoordinated works of some JSRs catering to upcoming Java releases are getting you exactly that, and let's say you need compile-time safety, BeanValidation (probably in a wider context with other parts like JSF) and some influence on the generated query you may just end up with 3 or more of these in your code On Tue, Aug 28, 2012 at 4:52 PM, Oliver Gierke ogierke@... Sounds good. To be honest, I didn't quite get the connection. And on it threw me out of the corner eventually :) Cheers, Ollie werner.keil@... ogierke@... werner.keil@... http://stackoverflow.com/questions/2725111/hibernate-onetoone-notnull http://types.cs.washington.edu/jsr308/ http://stackoverflow.com/questions/4963300/which-notnull-java-annotation-should-i-use ogierke@... ogierke@... +49-351-30929001 +49-351-418898439 http://www.olivergierke.de ogierke@... +49-351-30929001 +49-351-418898439 http://www.olivergierke.de -- /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Ambiguity of Annotations (was Re: JPQL: Sorting on optional references)</header>
    <date>Tue Aug 28 17:17:09 CEST 2012</date>
    <body>Dear all, Please allow me to untangle that from the point Oliver raised. Potentially relevant to the overall EE list, too, let's see, what Linda says...? Looking at the "nullable" attribute some annotations, while the actual query may not be changed, you can sure contradict the  nullable = true  by adding a  @NotNull annotation from Bean Validation to it. The two are legitimate together:   http://stackoverflow.com/questions/2725111/hibernate-onetoone-notnull   Java 7 and EE7 most likely give you a Runtime Exception, maybe some IDE warning if it provides JPA support, but the compiler itself is not going to do much about it. JSR 308, so far scheduled for Java 8 aims to change that: http://types.cs.washington.edu/jsr308/ Unfortunately, both the Spec Leads of 308 deny any responsibility or ownership of checker annotations, nor have the architects at Oracle so far take precautions to avoid a big mess like this @Entity   @OneToOne(nullable = true)  @NotNull   @NonNull @Nullable(true)  Address address;  BeanValidation as of now Checker Annotations based on JSR 308 as of now  (from Java 8) Given a combination of Java EE 6/7 and JSR 308 this would be perfectly fine. Most likely the two checkers would cause a compiler error already, but one alone, let's say @Nullable(true) instead of a sensible integration with all existing stuff will increase the big mess around these annotations already present http://stackoverflow.com/questions/4963300/which-notnull-java-annotation-should-i-use   I probably forgot @Nonnull as of JSR 305 which at the moment polutes the JDK already, but has no effect. At the very least, instead of having people stuff in more annotations of all different kinds, either @Nonnull should be killed with Java 8 and replaced with something else, or it should be used. Most likely the conflict with BeanValidation (which is primarily used by JPA) would be unavoidable then. Sorry to hijack the thread, I hope, at least Linda keeps an eye on that or plans to do something about it, so that we can avoid annotation chaos we inherited in other areas, e.g. @Inject vs. other legacy approaches, etc.? Thoughts, suggestions? Thanks, --  Werner Keil | JCP Executive Committee Member | Eclipse UOMo Lead Twitter @wernerkeil | #Java_Social | #EclipseUOMo | #OpenDDR Skype  werner.keil | Google+  gplus.to/wernerkeil * Eclipse Day Poland: September 13 2012, Krakow, Poland. Werner Keil, Eclipse Committer, UOMo Lead, Mærsk Build Manager  will present "Eclipse STEM, UOMo and Hudson " * Chip-to-Cloud Security Forum: September 19 2012, Nice, French Riviera. Werner Keil, JCP Executive Committee, JSR-321 EG  Member will present "Trusted Computing API for Java™ " * Eclipse Day Delft: September 27 2012, Delft, Netherlands. Werner Keil, Eclipse Committer, UOMo Lead, Mærsk Build Manager  will present "Triple-E class Continuous Delivery with Hudson, Maven and Mylyn " * JavaOne: September 30-October 4 2012, San Francisco, USA. Werner Keil,   JCP Executive Committee will represent "Eclipse UOMo, STEM, and JSRs involved in, e.g. 331 or JCP.next "</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL: Sorting on optional references</header>
    <date>Wed Aug 29 14:47:57 CEST 2012</date>
    <body>Not sure how this "side-effect" is "unpleasant and not easy to grasp". It is explicitly called out in the spec. -1 for changing implicit joins to result in inner or outer joins depending on the mapping.  In such a case you can no longer see what will happen just by looking at the query itself, which in my opinion is far more "unpleasant and not easy to grasp".</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL: Sorting on optional references</header>
    <date>Wed Aug 29 15:26:21 CEST 2012</date>
    <body>The problem is that adding an order by clause leaks a side effect into what  is actually returned from the query, which shouldn't be the case.  Yes, it's outlined in the spec but it's also inconsistent in regards of  mapping metadata being applied to the JPQL expression. An @Column(name =  "Foo") get's regarded, why shouldn't an @ManyToOne(optional = true) be? I think it get's more obvious if you consider a more simple case: select p from Person p  VS.  select p from Person p order by p.address.city The former includes people without addresses, the latter does not? And this  is totally inexplainable from a pure mapping metadata point of view. I wonder  why the two lines defining path expressions being forced into inner joins  have made it into the spec in the first place. Were the side effects not  considered? Why explicitly not applying the mapping metadata? Cheers, Ollie PS: I've summarized my findings in this gist  ( https://gist.github.com/3497047 ). The comments might be interesting to  consider. Am 29.08.2012 um 14:47 schrieb Steve Ebersole &amp;lt;steve.ebersole@...&amp;gt;:  Not sure how this "side-effect" is "unpleasant and not easy to grasp".   It is explicitly called out in the spec.    -1 for changing implicit joins to result in inner or outer joins   depending on the mapping.  In such a case you can no longer see what   will happen just by looking at the query itself, which in my opinion is   far more "unpleasant and not easy to grasp".     --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL: Sorting on optional references</header>
    <date>Wed Aug 29 15:33:11 CEST 2012</date>
    <body>I like simple. To me: "implicit join -&amp;gt; inner join" is simple "implicit join -&amp;gt; well maybe an inner join depending on where it is in the query and then depending on where it is in the query we might have to look at the mapping information to decide it might actually be an outer join" is not simple. And I am not talking about from a provider perspective here.  I am talking from the perspective of a user that has to decide how (non-)performant these queries will be.  I prefer to be able to look at the query and know that.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL: Sorting on optional references</header>
    <date>Wed Aug 29 15:56:36 CEST 2012</date>
    <body>Am 29.08.2012 um 15:33 schrieb Steve Ebersole &amp;lt;steve.ebersole@...&amp;gt;:  I like simple.    To me:    "implicit join -&amp;gt; inner join" is simple Agreed.  "implicit join -&amp;gt; well maybe an inner join depending on where it is in   the query and then depending on where it is in the query we might have   to look at the mapping information to decide it might actually be an   outer join" is not simple. It's rather optional = true -&amp;gt; outer join, optional = false -&amp;gt; inner join.   And I am not talking about from a provider perspective here.  I am   talking from the perspective of a user that has to decide how   (non-)performant these queries will be.  I prefer to be able to look at   the query and know that. If you want performance you probably craft your joins manually anyway. But  I'd bet that more than 90% of JPA users would *not* expect adding an order by  clause to the simplest query possible would rule out entities from the result. Cheers, Ollie https://gist.github.com/3497047 ). The comments might be interesting to  --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL: Sorting on optional references</header>
    <date>Wed Aug 29 15:59:58 CEST 2012</date>
    <body>Yes and no.  I just understand implicit joins are inner joins.  I don't even have to think about optionality...</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation proposed changes</header>
    <date>Wed Aug 29 16:30:48 CEST 2012</date>
    <body>As far as: javax.persistence.database-product-name, javax.persistence.database-product-version, I'd really rather see major/minor version used: (int) java.sql.DatabaseMetaData#getDatabaseMajorVersion (int) java.sql.DatabaseMetaData#getDatabaseMinorVersion as opposed to the proposed: (String) java.sql.DatabaseMetaData#getDatabaseProductVersion For one Hibernate already uses those 2 ;)  For another its more indicative of what is/isn't available consistently across databases.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL: Sorting on optional references</header>
    <date>Wed Aug 29 20:53:25 CEST 2012</date>
    <body>If address is an embeddable, this query is legit.  If address is an entity (as in your original example, the order-by is not). Perhaps what needs clarification is the notion of "result". What the spec says (section 4.4.4) is:    Path expression navigability is composed using "inner join"    semantics. That is, if the value of a non-terminal field in the    path expression is null, the path is considered to have no value,    and does not participate in the determination of the result. Perhaps what this should say is:    Path expression navigability is composed using "inner join"    semantics. That is, if the value of a non-terminal field in the    path expression is null, the path is considered to have no value,    and does not participate in the determination of the *contents* of    the result of the query. With order-by in play, there are two aspects to the result: What is actually retrieved from the database ("contents of the result"), and how it is ordered. In my view, the order-by clause should only affect the ordering of what is returned, not filter it.  That was the original intent, and why, for example, we disallow relationship joins in the orderby clause.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL: Sorting on optional references</header>
    <date>Wed Aug 29 20:59:14 CEST 2012</date>
    <body>This is obviously an ill-formed application :-)   However, please note that there is nothing preventing anyone from writing this kind of nonsense using bean validation constraints alone.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL: Sorting on optional references</header>
    <date>Wed Aug 29 23:34:32 CEST 2012</date>
    <body>Hi Linda, what I understood from your proposal is implicit joins in an ORDER BY clause are treated as outer joins, correct? But how about implicit joins in other clauses of the query, e.g. the SELECT clause. The following query excludes persons not having an address from the result     select p.address from Person p where this one includes them:     select a from Person p LEFT OUTER JOIN p.address a I am uncomfortable with defining different semantics of an implicit join depending on whether it is used in an ORDER BY clause versus a SELECT or WHERE clause. Is GROUP BY treated the same as ORDER BY, because it should not filter what is returned, too? But HAVING is used to filter over groups ... This sounds confusing to me. That was the original intent, and why, for example, we disallow relationship joins in the orderby clause.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL: Sorting on optional references</header>
    <date>Thu Aug 30 10:40:16 CEST 2012</date>
    <body>Am 29.08.2012 um 20:53 schrieb Linda DeMichiel &amp;lt;linda.demichiel@...&amp;gt;:      If address is an embeddable, this query is legit.  If address is  an entity (as in your original example, the order-by is not). Re-reading 4.9 this becomes clear. I guess I've just been lead into thinking  this is valid as persistence providers still allow this. They simply  construct an inner join for the path expression. Not sure how strict these  should be in terms of rejecting such expressions, especially as adding the  order by leaks into the result content being returned. Still I think that forcing a user to write: select p from Person p left outer join p.address address order by address.city where select p from Person p order by p.address.city The "it's currently defined as not allowed" aside, with the latter query and  the metadata, everything is in place to build the query without the side  effect.  What the spec says (section 4.4.4) is:       Path expression navigability is composed using "inner join"     semantics. That is, if the value of a non-terminal field in the     path expression is null, the path is considered to have no value,     and does not participate in the determination of the result.    Perhaps what this should say is:       Path expression navigability is composed using "inner join"     semantics. That is, if the value of a non-terminal field in the     path expression is null, the path is considered to have no value,     and does not participate in the determination of the *contents* of     the result of the query. What is the "value" here? Isn't it actually a value in the database? If we're  considering database values already the inner join already has been executed  and the result actually *gets* filtered, right?  With order-by in play, there are two aspects to the result:    What is actually retrieved from the database ("contents of the  result"), and how it is ordered.    In my view, the order-by clause should only affect the ordering of  what is returned, not filter it.  That was the original intent,  and why, for example, we disallow relationship joins in the orderby  clause. +100 for "not filter it". Regarding the "disallow": shouldn't the persistence  provider then reject the query instead of executing it in a filtering way?  Currently only the spec defines this as disallowed, but this is not reflected  in the actual implementations. Cheers, Ollie --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL: Sorting on optional references</header>
    <date>Thu Aug 30 15:03:18 CEST 2012</date>
    <body>I think its like so many other aspects of the spec... Providers are free to support things beyond what is explicitly called out in the spec.  Why should this different?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPQL: Sorting on optional references</header>
    <date>Thu Aug 30 17:48:18 CEST 2012</date>
    <body>Not exactly.   The intention of the spec was to constrain the order-by to  attributes that were covered by the SELECT.  That is, to not exceed what SQL permits in an  ORDER BY. So, if you retrieve an entity, you can order by the state fields of that  entity (but not by statefields of a related entity not designated by the SELECT.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation proposed changes</header>
    <date>Fri Aug 31 02:24:45 CEST 2012</date>
    <body>Hi Steve, I'd like to understand this better, as the info I have from our team here indicates that database produce name together with version (in the worst case) would be sufficient.  Do we need all four properties to accommodate the range of drivers/databases? thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation proposed changes</header>
    <date>Fri Aug 31 05:04:35 CEST 2012</date>
    <body>Well a real simple example is that Derby supports sequences after 10.6, but not before.  So there DatabaseMetaData#getDatabaseMajorVersion=10 and DatabaseMetaData#getDatabaseMinorVersion=6.  Even if the Derby driver were to report the DatabaseMetaData#getDatabaseProductVersion="10.6" or somesuch, relying on being able to parse that into ints to be able to do range checks make me nervous, especially when JDBC already offers means to know the major/minor. And thats just one example that came to me immediately.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation proposed changes</header>
    <date>Fri Aug 31 05:09:36 CEST 2012</date>
    <body>Another is PostgreSQL adds "if exists" as an option for dropping tables in 8.2 (major=8, minor=2) which did not exist in 8.1 or earlier</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation proposed changes</header>
    <date>Fri Aug 31 05:41:00 CEST 2012</date>
    <body>Great, thanks! So, should we specify all 4 pieces of information?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation proposed changes</header>
    <date>Fri Aug 31 05:56:52 CEST 2012</date>
    <body>Personally I think we get into trouble having both sets of versions. What happens if the user specifies both DatabaseProductVersion and DatabaseMajorVersion/DatabaseMinorVersion? To be completely honest, personally I'd much rather have access to the DatabaseMetaData.  But if thats not going to be an option, I'd vote for just DatabaseMajorVersion and DatabaseMinorVersion (optional).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Improving the TCK</header>
    <date>Fri Aug 31 11:00:11 CEST 2012</date>
    <body>As quite a few people of the expert group agree about that the TCK should be  more openly accessible and we didn't see an official reply yet, let me  summarize my questions up: 1. What's preventing us from changing the TCK license to e.g. Apache 2.0?  Other JavaEE specifications (e.g. CDI [0]) have been on that track for years. 2. Can the current license still be changed in the course of the 2.1  specification or do we have to wait for 2.2? 3. If there's no way to open up the TCK for 2.1, what is the process to track  discovered TCK glitches? The JSR JIRA? Cheers, Ollie Am 27.07.2012 um 16:24 schrieb Matthew Adams &amp;lt;matthew@...&amp;gt;:  +1    +10000    Early access to the TCK (in progress) is by far the best way to (1) catch   holes in the TCK and (2) keep it improving.      I think this is crucial as I have seen tickets in implementors bug  trackers with comments that bugs won't be fixed if the TCK doesn't  actually bark. This way, it's actually not the spec driving the  features/bugs of the implementation but a TCK not even members of  the expert group get access to. This effectively creates a shadow  spec which is a sub-optimal thing to see I think.    If you could help in pointing these out, that would be very useful.  Our team certainly wants to know about such  problems.    There's been a couple of comment threads in implementors bug trackers  which I've stumbled above. The quickest one I could find was one  inside the Hibernate bugtracker [0]. This is by no means finger  pointing at Steve, as ironically he and all the other leads of the  implementations actually can't do much more than follow the TCK. If  it leaves holes and the spec is not as tight as it should be, they  essentially have no choice.    One practical example of these is that  EntityManager.createNamedQuery(…) didn't throw an exception in case  of an invalid query name provided for *two major versions* (2.0 and  2.1). This actually relates to the discussion we have in the separate  thread which I'll respond to in a bit. Yet another example is again  Hibernate [2] (sorry Steve, I owe you a beer :), which didn't  implement Metamodel.managedType(…) correctly by throwing an exception  for embeddables and mapped superclasses. Once again, I think the TCK  could have caught that, making our lives (the implementor as well as  the clients) a bit easier.    Essentially this boils down to two categories: one is slight  ambiguities in the spec which need to be discussed and improved (and  which we probably won't ever get rid off entirely). The other - even  worse - are things that actually *are* defined in detail by the spec  but not enforced and then potentially show up to anger users and  implementors. They are not worse because they are more crucial. They  are worse because it would have been easy to catch them early on if  the TCK actually underwent the same open discussion as the spec does.    In my experience, one of the issues is timing.  When the EE TCK  becomes available, the priority for the vendors implementing EE, is to  pass the TCK.  In theory, when vendors initially get the early EE TCK  snapshots, that would be a good time to report to Oracle, that a  certain SPEC requirement is not covered but that is probably not going  to happen as much (since vendors are focused on passing the TCK  instead of improving it).  I'm all for a more open/transparent TCK  process but I'd like to read how we actually want to improve the  process and work backwards from there (if there is interest).  For  example, I would like to see more non-vendor eyeballs on the EE TCK  java sources and have a way for bug reports to be opened (that include  patches for the TCK).  This doesn't have to be the entire TCK but  would include the test classes.    I also agree that ambiguities in the spec, should be discussed and  improved.  I see this as a related issue and am just guilty as others,  for remaining silent on some issues.  I think that this could be  improved, if we all want to put effort into it.  I think part of this,  could be helped with phone call meetings (either quarterly or when  requested).    I would also like to see us get to a point, where we agree that the  priority is delivering the best specification for users and achieving  more portability between vendor implementations.  Sometimes it seems  that we instead push that our (vendor) implementation is the correct  way (when ambiguities come up for discussion).    Scott      Regards,  Ollie    [0]  https://hibernate.onjira.com/browse/HHH-1134  [1]  https://bugs.eclipse.org/bugs/show_bug.cgi?id=322579  [2]  https://hibernate.onjira.com/browse/HHH-6896    Am 17.07.2012 um 20:36 schrieb Harald Wellmann:    Am 17.07.2012 11:22, schrieb Oliver Gierke:    Where can one get access to the standalone version of the JPA 2.0  TCK?      The TCK is still closed source, AFAIK.    Over the past two years, I've found numerous spec-related bugs in  all 3  certified JPA 2.0 implementations (Eclipselink, Hibernate,  OpenJPA) that  should have been caught by the TCK.    Some 18 months ago, when there was not even a public mailing list, I  sent a request to the write-only list jsr-317-feedback@... to  release the TCK to the public.    There is also the sad story of a fourth persistence provider who has  given up on certification after trying in vain to get access to  the TCK:     http://datanucleus.blogspot.de/2011/01/jpa-tck-request-and-jpa21.html    It is good to see that the current JCP is more open than former  versions, but of course a community process that really deserves the  name would open source all TCKs for all JSRs, no exceptions.    2. How can we actually help out to improve the TCK?    Well, we can't as long as the TCK isn't open. That's the main  point why  it doesn't make sense to keep it closed.    The CDI TCK has been Open Source from the very beginning, part of the  same Java EE 6 umbrella release and governed by the same JCP.    I really don't see why this shouldn't work for JPA just as well.    Best regards,  Harald            Cheers, Ollie    [0]  https://bugs.eclipse.org/bugs/show_bug.cgi?id=322579 [1 ]   https://hibernate.onjira.com/browse/HHH-6896 [2 ]   http://jcp.org/en/jsr/detail?id=317                      --    mailto:matthew@...   skype:matthewadams12  googletalk:matthew@...   http://matthewadams.me   http://www.linkedin.com/in/matthewadams   --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: Improving the TCK</header>
    <date>Fri Aug 31 15:07:22 CEST 2012</date>
    <body>While "more openly accessible" is certainly desirable, as a provider what would be far more desirable is early access.  For example, I it would be amazing to be able to test that work against the TCK as it stands thus far.  That serves 2 purposes: 1) Helps me validate my work (making providers better) 2) Helps identify TCK problems earlier (making providers more consistent).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: schema generation proposed changes</header>
    <date>Fri Aug 31 16:03:18 CEST 2012</date>
    <body>a few JDBC drivers can connect to multiple databases with the same driver such as jConnect to ASE, SQL Anywhere and Open Servers.  In these cases  just having the Major and minor version numbers might not be enough to distinguish the actual database you are connecting to.  So you would probably need  DatabaseProductName as well. I agree parsing could be a bit troublesome but either way you really need all 3 pieces of info if you do not leverage DatabaseProductVersion so you could use getDatabaseProductName, getDatabaseMajorVersion, getDatabaseMinorVersion Best Lance Personally I think we get into trouble having both sets of versions.  What happens if the user specifies both DatabaseProductVersion and DatabaseMajorVersion/DatabaseMinorVersion? To be completely honest, personally I'd much rather have access to the DatabaseMetaData.  But if thats not going to be an option, I'd vote for just DatabaseMajorVersion and DatabaseMinorVersion (optional). Great, thanks!</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Improving the TCK</header>
    <date>Fri Aug 31 21:18:47 CEST 2012</date>
    <body>I have raised this issue with our TCK team and asked them to follow up. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Re: schema generation proposed changes</header>
    <date>Fri Aug 31 21:20:09 CEST 2012</date>
    <body>OK, thanks -- I will add additional properties for these.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Re: schema generation proposed changes</header>
    <date>Fri Aug 31 21:27:54 CEST 2012</date>
    <body>Just to clarify...  Do you mean : 1) DatabaseProductName 2) DatabaseMajorVersion 3) DatabaseMinorVersion or 1) DatabaseProductName 2) DatabaseMajorVersion 3) DatabaseMinorVersion 4) DatabaseProductVersion ? Sorry if I was unclear before.  I totally agree that I also think we need DatabaseProductName.  What I was saying was that I thought using DatabaseMajorVersion and DatabaseMinorVersion was better than using just DatabaseProductVersion.  Which it sounds like Lance is agreeing with.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Re: schema generation proposed changes</header>
    <date>Fri Aug 31 21:50:41 CEST 2012</date>
    <body>I mean all 4.  But I will flag it as an open issue to get further input.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: Re: Re: schema generation proposed changes</header>
    <date>Fri Aug 31 22:02:01 CEST 2012</date>
    <body>Just to clarify...  Do you mean : 1) DatabaseProductName 2) DatabaseMajorVersion 3) DatabaseMinorVersion</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: Re: Re: schema generation proposed changes</header>
    <date>Fri Aug 31 22:21:44 CEST 2012</date>
    <body>Just to clarify...  Do you mean : 1) DatabaseProductName 2) DatabaseMajorVersion 3) DatabaseMinorVersion</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Re: schema generation proposed changes</header>
    <date>Sun Sep 02 18:05:21 CEST 2012</date>
    <body>I don't think passing the product name is an issue... As far as versioning, if we are going to have both sets of version identification (major/minor-version and product-version) then we potentially have consistency problems with non-matching values and different providers interpreting that differently.  What exactly is the argument for using DatabaseProductVersion instead of DatabaseMajorVersion/DatabaseMinorVersion at all?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: add description of @PersistenceContexts to section 10.4.1 of the JPA 2.1 spec...</header>
    <date>Thu Sep 13 19:22:44 CEST 2012</date>
    <body>Adding examples for @PersistenceContexts would kind of stick out, wouldn't it? If we add for that one annotation then we should probably add examples for all of them in the chapter. There are currently no examples for any of them. Sections 10.3.1 and 10.3.2 don't do any more than just mention the plural form in a sentence that states which classes they can annotate, and then they include the pluralized annotation definition. Sections 10.3.3 and 10.4.1 did include the definition, but did not mention it in text. Seems like if we wanted to make 10.3.3 and 10.4.1 be consistent with 10.3.1 and 10.3.2 the only thing that would need to be added would be a general statement of the form used in 10.3.1 and 10.3.2: -Mike</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: add description of @PersistenceContexts to section 10.4.1 of the JPA 2.1 spec...</header>
    <date>Thu Sep 13 19:03:33 CEST 2012</date>
    <body>Hi Scott, Thanks for the suggestion.  I will add. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: add description of @PersistenceContexts to section 10.4.1 of the JPA 2.1 spec...</header>
    <date>Thu Sep 13 19:29:31 CEST 2012</date>
    <body>Well, if people are confused, it doesn't hurt.  Perhaps the javadoc page  would be a better place though.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] New spec draft, updated javadocs, and orm.xsd</header>
    <date>Thu Sep 13 04:20:23 CEST 2012</date>
    <body>Hi all, I've uploaded a new spec draft containing the additions for schema generation to the project area  http://java.net/projects/jpa-spec/downloads I've also uploaded javadocs which reflect the new annotations and methods and a new orm.xsd.  (Please be sure to grab the "draft 2" orm.xsd, as I had to rename the old one to avoid deleting it, which causes it to appear more recent than it is.) All changes are flagged by changebars. The changebars however continue to be additive from the last draft, as I do not plan to reset them until after we release a milestone draft to the JCP. Appendix A has a list of the changes in this draft. Please review, and post your feedback and/or corrections. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] add description of @PersistenceContexts to section 10.4.1 of the JPA 2.1 spec...</header>
    <date>Thu Sep 13 14:37:02 CEST 2012</date>
    <body>Hi eg, I don't see a description of @PersistenceContexts.  Could we add a few words about @PersistenceContexts to section 10.4.1? http://www.oracle.com/technetwork/middleware/ias/toplink-jpa-annotations-096251.html includes the following: " @PersistenceContexts If you need to specify more than one @PersistenceContext, you must specify all your persistence contexts using a single @PersistenceContexts annotation. " The above link also has an example that would be nice to include: @Stateless     @PersistenceContext(name="OrderEM"),     @PersistenceContext(name="ItemEM")     ...</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: add description of @PersistenceContexts to section 10.4.1 of the JPA 2.1 spec...</header>
    <date>Tue Sep 18 11:52:01 CEST 2012</date>
    <body>Could we also describe the @PersistenceUnits. " @PersistenceUnits If you need to specify more than one @PersistenceUnit, specify all the persistence units using a single @PersistenceUnits annotation. " Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Re: add description of @PersistenceContexts to section 10.4.1 of the JPA 2.1 spec...</header>
    <date>Tue Sep 18 12:34:05 CEST 2012</date>
    <body>@PersistenceUnits + @PersistenceContexts are both in JPA 2.0.  I'm not proposing that we add them to the specification, just that we document them so that more people understand that they are available. I apologize for not being clearer but greatly appreciate the discussion!  :) A similar example for @PersistenceUnits might be:   @Stateless   @PersistenceUnit(name="__OrderEMF"),   @PersistenceUnit(name="__ItemEMF")         EntityManagerFactory emf =         EntityManagerFactory emf =         ... no, PersistenceUnit (copied from JPA 2.0 spec) already has: @Target(TYPE) @Retention(RUNTIME) And PersistenceContext already has: TRANSACTION, EXTENDED</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: add description of @PersistenceContexts to section 10.4.1 of the JPA 2.1 spec...</header>
    <date>Tue Sep 18 11:57:46 CEST 2012</date>
    <body>I thought, that's done as of today with  @PersistenceContext(unitName="...")   Or is this a Spring proprietary way?  (found it on a SpringSource docu) Would each PersistenceContext need more than one PersistenceUnit? Otherwise even if this has so far been Spring-specific, maybe we could keep that an attribute of the PersistenceContext instead of yet an other array of annotations?? Werner On Tue, Sep 18, 2012 at 11:52 AM, Scott Marlow smarlow@... Could we also describe the @PersistenceUnits. " @PersistenceUnits If you need to specify more than one @PersistenceUnit, specify all the persistence units using a single @PersistenceUnits annotation. " Scott Well, if people are confused, it doesn't hurt.  Perhaps the javadoc page would be a better place though. Adding examples for @PersistenceContexts would kind of stick out, wouldn't it? If we add for that one annotation then we should probably add examples for all of them in the chapter. There are currently no examples for any of them. Sections 10.3.1 and 10.3.2 don't do any more than just mention the plural form in a sentence that states which classes they can annotate, and then they include the pluralized annotation definition. Sections 10.3.3 and 10.4.1 did include the definition, but did not mention it in text. Seems like if we wanted to make 10.3.3 and 10.4.1 be consistent with 10.3.1 and 10.3.2 the only thing that would need to be added would be a general statement of the form used in 10.3.1 and 10.3.2: -Mike Hi Scott, Thanks for the suggestion. I will add. -Linda Hi eg, I don't see a description of @PersistenceContexts. Could we add a few words about @PersistenceContexts to section 10.4.1? http://www.oracle.com/ technetwork/middleware/ias/ toplink-jpa-annotations- 096251.html includes the following: " @PersistenceContexts If you need to specify more than one @PersistenceContext, you must specify all your persistence contexts using a single @PersistenceContexts annotation. " The above link also has an example that would be nice to include: @Stateless @PersistenceContext(name=" OrderEM"), @PersistenceContext(name=" ItemEM") EntityManager em = (EntityManager)ctx.lookup(" EntityManager em = (EntityManager)ctx.lookup(" ...</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Specification clarification: 11.1.17</header>
    <date>Thu Sep 20 19:29:29 CEST 2012</date>
    <body>I had a question about the specification section 11.1.17. In that section it says: This specification does not define the exact behavior of these [identity generation] strategies.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Feature request - support adding a prefix to the mapping of all properties of an embedded object</header>
    <date>Wed Oct 10 10:32:47 CEST 2012</date>
    <body>Hi, When using embeddable objects I have often found myself having to override the default mapping to make the column names understandable or to avoid collisions by adding a prefix to every column name for the embedded object. One example if for a debit card: @Embeddable         @AttributeOverride(name="month", column = @Column(name = "startmon")),         @AttributeOverride(name="year", column = @Column(name = "startyear"))         @AttributeOverride(name="month", column = @Column(name = "expmon")),         @AttributeOverride(name="year", column = @Column(name = "expyear")), I propose that support is added for specifying a prefix for column names of embedded objects or to instruct it use the property name as the prefix.  I'm not sure what the best way to achieve this would be.  You could add an attribute to the Emdedded or AttributeOverrides annotation or possibly look at Hibernate's NamingStrategy functionality.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] EDR2 [Re: New spec draft, updated javadocs, and orm.xsd]</header>
    <date>Thu Oct 25 23:17:10 CEST 2012</date>
    <body>I would like to submit this draft to the JCP as an Early Draft 2, so that we can try to get more feedback from the broader community. Please let me know asap if for some reason you think I should not. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] AUTO: Pinaki on vacation (returning 10/29/2012)</header>
    <date>Fri Oct 26 00:04:52 CEST 2012</date>
    <body>I am out of the office until 10/29/2012. Returning on Oct 29, 2012. Note: This is an automated response to your message   "[jsr338-experts] EDR2 [Re: New spec draft, updated javadocs, and orm.xsd]"  sent on 10/25/2012 15:17:10 . This is the only notification you will receive while this person is away.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EDR2 [Re: New spec draft, updated javadocs, and orm.xsd]</header>
    <date>Fri Oct 26 17:15:15 CEST 2012</date>
    <body>+1 for getting broad community feedback. We talked about describing PersistenceContexts + PersistenceUnits in more detail (either in javadocs or spec).  Are we going to do that in the next draft?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EDR2 [Re: New spec draft, updated javadocs, and orm.xsd]</header>
    <date>Fri Oct 26 20:12:16 CEST 2012</date>
    <body>Yes.  I've added notes to the annotation sections about use of these to  declare multiple persistence contexts / units.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EDR2 [Re: New spec draft, updated javadocs, and orm.xsd]</header>
    <date>Fri Oct 26 23:50:23 CEST 2012</date>
    <body>I've just uploaded Early Draft 2 to our project's Downloads areas.  This is essentially the same as Draft 8, plus a few corrections to examples and editorial fixes. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EDR2 [Re: New spec draft, updated javadocs, and orm.xsd]</header>
    <date>Tue Oct 30 20:54:55 CET 2012</date>
    <body>Seems the EDR from the main JSR community page ( http://jcp.org/aboutJava/communityprocess/edr/jsr338/index.html ) is still the first EDR (Dec 2011). Is that intended?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EDR2 [Re: New spec draft, updated javadocs, and orm.xsd]</header>
    <date>Tue Oct 30 20:59:03 CET 2012</date>
    <body>It takes them a few days to push the update.  I would expect it to appear  late this week or early next. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] polymorphic associations</header>
    <date>Wed Oct 31 00:00:38 CET 2012</date>
    <body>I've been cleaning up some of the JIRA issues, and would like to call your attention to  http://java.net/jira/browse/JPA_SPEC-39  ;("Clarify requirements on polymorphic associations"). As noted in the issue, the spec appears to imply (but doesn't explicitly call out) support for this type of polymorphic relationships. If you don't think we should explicitly support this, could you please speak  up. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] outer fetch joins with on-conditions</header>
    <date>Wed Oct 31 20:51:29 CET 2012</date>
    <body>I've been having an interesting discussion with a user on the JPA JIRA with regard to outer fetch joins.  See the issue  http://java.net/jira/browse/JPA_SPEC-40 The issue is basically whether an outer fetch join with on-condition can result in an entity whose state/relationships have been inconsistently fetched. Since we don't currently support the use of identification variables on the right hand side, I believe that we currently do not have a problem. Please double-check whether you agree with me on this or not. However, the ability to specify identification variables here has also be  requested, and is something that we could plausibly add in future.  See the issue http://java.net/jira/browse/JPA_SPEC-28 I'd like input from the group with regard to these issue, and whether we should consider: 1) not supporting outer fetch joins with on-conditions 2) not (i.e., never) supporting outer fetch joins with the use of  identification    variables on the right hand side 3) not supporting the use of on-conditions with outer fetch joins that use    identification variables on the right hand side. 4) other?? -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: outer fetch joins with on-conditions</header>
    <date>Thu Nov 01 14:08:52 CET 2012</date>
    <body>I was not able to respond on the issue, so replying here... It can lead to *incomplete* state, yes.  And not just for outer join fetches, but even inner join fetches as well. So you are correct about the grammar not allowing to assign an identification variable (alias) to an outer join.  So by spec this should not be an issue with outer joins per-se.  Hibernate for example though does allow aliasing the outer join. But like I said, the same is true of inner joins as well.  So using his domain model, if you say: select p from Person p inner join fetch p.addresses a on a.primary = true You have just filtered out non-primary addresses from the fetched graph, so the Person graph is incomplete as it is missing its non-primary addresses. The graph is *incomplete*, it is not necessarily inconsistent.  The "inconsistency" really comes down to how the provider handles the collection at flush time. If we restrict ON-conditions [still can't convince you to rename that to WITH? ;)] at all, I think the illegality should be in conjunction with join fetches of any sort. There is even insidious little things to consider as well, like imagine that Address had a further association City which has a [boolean top100] attribute: select p from Person p inner join fetch p.addresses a where a.city.top100 = true This again leads to incomplete results.  So it is not strictly limited to ON conditions.  Its related to any form of restriction on a join fetched association.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: outer fetch joins with on-conditions</header>
    <date>Thu Nov 01 18:50:02 CET 2012</date>
    <body>Yes as Steve said, currently the use of an ON condition with either an inner or outer fetch join would result in relationship elements being filtered by the ON condition as well. I think the best way to handle this is to add a note to the specification about the behaviour and recommend cache-store-mode set to bypass.  Only users who actually want to filter the contents of the relationships would be expected to use the ON clause within a FETCH JOIN.  Being able to reduce the number of elements in a relationship during a FETCH JOIN is beneficial in some circumstances. We could also add FETCH JOIN aliasing within the specification. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: outer fetch joins with on-conditions</header>
    <date>Thu Nov 01 22:58:01 CET 2012</date>
    <body>Oh, I missed the http://java.net/jira/browse/JPA_SPEC-28 reference.  +1 for me too on that.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Question about JPA metamodel, Inheritance and PluralAttribute</header>
    <date>Sun Nov 04 18:50:28 CET 2012</date>
    <body>I tried to achieve Generation Gap Pattern with JPA entities. Here is the solution I setup ( &amp;lt;-- are inheritance) BaseEntity &amp;lt;-- EntityGenerated &amp;lt;-- Entity The EntityGenerated type is abstract and mapped with @MappedSuperclass, all field are generated with correct mapping annotation, relation point to the concrete subclass, not the Generated one. The Entity is a concrete type, generated only if the class doesn't exist, initially there is just the class declaration annotated with @Entity. Other mapping attributes such as the @Table, etc are in a generated orm.xml. Now, when we generate the jpa static metamodel (using hibernate or openjpa metamodel generator), the generated classes look like : public static volatile public static volatile If I want to use User_ in a jpa criteria query, I'll do something like : But It won't compile.... User_.groups is of type SetAttribute and the jpa path api for the get method is : "&amp;lt;E, C extends collection);" Path&amp;lt;Y&amp;gt; get(SingularAttribute&amp;lt;? super X, Y&amp;gt; attribute)" witch work better) So, now, the questions are : Why the metamodel generators generate the class for MappedSuperclass as there is no way to query it directly ?, attribute and relations for superclass should be defined in each subclass (where X is of subclass type) ? Why the jpa criteria Path api doesn't define the get method for plural attribut as "get(PluralAttribute&amp;lt;? super X, C, E&amp;gt; collection)" ? How can I achieve the Generation Gap Pattern on JPA entity without giving up criteria query ? (For now, the User_ is generated by our domain generator with something like : "public static volatile SetAttribute&amp;lt;User, Group&amp;gt; userGroups = (SetAttribute) UserGenerated_.userGroups;") Thanks for any answer.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: outer fetch joins with on-conditions</header>
    <date>Tue Nov 06 22:48:59 CET 2012</date>
    <body>Note that the spec doesn't allow the identification variable on the right  hand side here either OK.  I agree with this. Thanks for your helpful analysis. -LInda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: outer fetch joins with on-conditions</header>
    <date>Tue Nov 06 22:53:41 CET 2012</date>
    <body>The problem is that supporting this opens up a can of worms with regard to consistency, as was pointed out in  http://java.net/jira/browse/JPA_SPEC-40 I think there are two aspects to consider:  1) whether this should be supported for more sophisticated developers who  very intentionally want to use this capability for its side effect in  filtering  2) whether this will be very error prone for everybody else. We also have a fetch-plan item on our agenda (which I hope to be able to address in this release, despite the tight timelines) and that would provide a general capability to specify prefetching (and also multi-level  prefetching). At this point, I'd prefer to err on the side of caution, and not introduce identification variables on the right hand side of fetch joins. Yes, do still have the remaining issue pointed out in JIRA issue 40 with regard to allowing single-valued relationship traversal through path expressions, but I think that can be dealt with by stating in the specification that it is not supported for use with on-conditions.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: outer fetch joins with on-conditions</header>
    <date>Thu Nov 08 18:39:30 CET 2012</date>
    <body>We also have a fetch-plan item on our agenda (which I hope to be able to address in this release, despite the tight timelines) I support that we explore fetch planning as an independent construct over the approach of augmenting JPQL to control the access pattern. A fetch plan as a subsidiary to find() or query operations will allow JPA to support more sophisticated data access use cases.   Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Linda DeMichiel ---11/06/2012 01:56:42 PM---On 11/1/2012 2:58 PM, Steve Ebersole wrote: &amp;gt; Oh, I missed the http://java.net/jira/browse/JPA_SPEC- From: To: jsr338-experts@... Date: 11/06/2012 01:56 PM Subject: [jsr338-experts] Re: outer fetch joins with on-conditions http://java.net/jira/browse/JPA_SPEC-28  reference. +1 for me too on that. The problem is that supporting this opens up a can of worms with regard to consistency, as was pointed out in http://java.net/jira/browse/JPA_SPEC-40 I think there are two aspects to consider:  1) whether this should be supported for more sophisticated developers who  very intentionally want to use this capability for its side effect in filtering  2) whether this will be very error prone for everybody else. We also have a fetch-plan item on our agenda (which I hope to be able to address in this release, despite the tight timelines) and that would provide a general capability to specify prefetching (and also multi-level prefetching). At this point, I'd prefer to err on the side of caution, and not introduce identification variables on the right hand side of fetch joins. Yes, do still have the remaining issue pointed out in JIRA issue 40 with regard to allowing single-valued relationship traversal through path expressions, but I think that can be dealt with by stating in the specification that it is not supported for use with on-conditions. http://java.net/jira/browse/JPA_SPEC-40 http://java.net/jira/browse/JPA_SPEC-28</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: outer fetch joins with on-conditions</header>
    <date>Mon Nov 12 13:16:37 CET 2012</date>
    <body>Pinaki Poddar, am 8 Nov 2012 hast Du um 9:39 zum Thema "[jsr338- experts] Re: outer fetch joins with on-co"  geschrieben :    &amp;gt; We also have a fetch-plan item on our agenda (which I hope to be able to  address in this release, despite the tight timelines)    I support that we explore fetch planning as an independent construct over  the approach of augmenting JPQL to control the access pattern.  A fetch plan as a subsidiary to find() or query operations will allow JPA  to support more sophisticated data access use cases. +1. As already stated in the 2.0 discussion, I don't think fetch join has  turned out to be the right approach for controlling pre-fetch.  Therefore I am not in favour of further extending that feature. Rather  invest in a thorough design of fetch plan. Best regards Rainer --- Rainer Schweigkoffer               SAP AG Walldorf Java Persistence                   TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, I.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...       fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Bill  McDermott (Co-CEO), Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP  Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse  oder sonstige vertrauliche Informationen enthalten.  Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist  Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung  oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte  benachrichtigen Sie uns und vernichten Sie die empfangene  E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged,  undisclosed, or otherwise confidential information. If you  have received this e-mail in error, you are hereby  notified that any review, copying, or distribution of it  is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your  cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] rich domain models - injecting services into JPA beans</header>
    <date>Fri Nov 16 16:19:22 CET 2012</date>
    <body>Hi We are using a programming model that is inspired by rich domains/domain-driven design. Right now we're leveraging Spring's @Configurable concept, which combined with (a static) weave step allows us to have JPA/persistable beans that get injected with collaborating services. This works seamlessly for instance for entities that are the result of a query, or for entities we create (new) ourselves to be persisted later. Would JPA provide us with any standards based support to have the same programming experience? I've asked a similar question at the Devoxx BOF yesterday evening, but got no clear/definitive answer. Thank you, Jan.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] MEMBER OF Query with @Enumerated @ElementCollection</header>
    <date>Mon Nov 19 15:03:13 CET 2012</date>
    <body>Hi, a MEMBER (OF) Query on a property which is an @Enumerated @ElementCollection is as far as I know not yet supported. - Have tested it with Hibernate and OpenJPA - Also didn't find anything in the 2.0 spec. Will this be possible, is this part of the JPA 2.1 Spec. I have read the last Draft Review, but didn't find anything. If the Question is not clear, I can provide an example to illustrate. Thanx, Olli</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Tue Nov 27 22:20:02 CET 2012</date>
    <body>Hello all,    Please find below a proposal for EntityGraphs and their use to define entity graph fetching, loading, merging and copying.  I have been working with Mike Keith and Linda DeMichiel on this proposal and we feel it is time to present to the expert group.  Please have a look and provide feedback as the intention is to integrate this within the 2.1 spec. Thank you, Gordon ---- A common functional request has been to enable users to dynamically define how an EntityManager operation or query should apply to an entity graph.  This may more commonly be known as “merge groups” or “fetch plans”.  This functionality can be especially important when detached entities must be serialized and then merged into an active Persistence Context. An entity graph is a template defined in the form of metadata or an object created by the dynamic EntityGraph API that captures the path and boundaries for an operation or a query   For static metadata the NamedEntityGraph annotation defines the attribute nodes and boundaries of the grouping. @Retention(RUNTIME) @Retention(RUNTIME)     /**      * (Optional) The name of the sub-graph.      * Defaults to the entity name of the root entity.      */     /**      * (Required) list of attributes that are included in in this sub-graph.      */     /**      * (Optional) Lists all of the attributes of this Entity class to be listed in      * the NamedEntityGraph without the need to explicitly list them.  Included      * attributes can still be fully specified by an attribute node referencing a      * sub-graph.      */     /**      * (Optional) This is a list of sub-graphs that are included in the      * entity graph. They  are referenced by name from NamedAttributeNode      * definitions.      */     /**      * (Optional) This is a list of sub-graphs that will add additional attributes      * for subclasses of the entity represented by this NamedEntityGraph to this      * entity graph.  Specified attributes from superclasses are included in      * subclasses.      */ The NamedEntityGraph annotation is applied to an Entity class that is defined as the root of a group of attributes, relationships and related entity classes.  The NamedAttributeNode annotations are embedded within the NamedEntityGraph metadata and define the   attributes of the entity that are included in the grouping.  When a NamedAttributeNode represents an Entity or an Embedded  (including a collection-valued relationship or an element collection of embeddables), the referenced type’s attributes can be included as a NamedSubGraph entry of the entity graph and referenced by name, creating a multi-level entity graph. If the sub-graph represents an entity with inheritance multiple NamedSubGraph entries can be made with the same name to extend the sub-graph for specific subclasses.  The class type that is being extended is required in the NamedSubGraph. /**  * A NamedAttributeNode is a member attribute of a NamedEntityGraph.  *  * @see javax.persistence.NamedEntityGraph  * @see javax.persistence.NamedSubGraph */ @Retention(RUNTIME)     /**      * (Required) the name of the attribute that must be in the sub-graph.      */     /**      * (Optional) if this attribute references a managed type that has its own      * AttributeNodes then this refers to that NamedSubGraph definition.  If the      * target type has inheritance than multiple sub graphs can be specified.      * These additional sub-graphs are intended to add additional attributes from      * subclasses.  Superclass sub-graph entries will be merged into subclasses      * sub-graphs.      */    /**     * (Optional) if the attribute references a Map type this can be used to specify     * a subGraph for the Key in the case of an Entity key type.  A     * keySubGraph can not be specified without the Map attribute also being     * specified.  If the target type has inheritance than multiple sub-graphs can     * be specified.  These additional sub-graphs are intended to add additional     * attributes from subclasses. Superclass sub-graph entries will be merged     * into subclasses sub-graphs.     */ /**  * A NamedSubGraph is a member of a NamedEntityGraph that represents a managed  * type. The NamedSubGraph is only referenced from within a NamedEntityGraph and  * can not be referenced on its own. It is referenced by name from a  * NamedAttributeNode of the NamedEntityGraph.  *  * @see javax.persistence.NamedEntityGraph * @see javax.persistence.NamedAttributeNode  */ @Retention(RUNTIME)     /**      * (Required) the name of the sub-graph as referenced from a      * NamedAttributeNode.      */     /**      * (Optional) the type represented by this subgraph.  Required when this      * subgraph is extending a subclass’ definition.      */     /** (Required)      * if this NamedAttributeNode references a managed type then this list is      * the attributes of that type that must be included.      */ For example: @Entity   @Id   @GeneratedValue   @Basic   @Basic   @OneToMany()   @OneToMany()   @OneToMany()   ... @Entity @Inheritance     @Id     @GeneratedValue     @OneToOne(fetch=FetchType.EAGER)    ... @Entity     @OneToOne(fetch=FetchType.LAZY)    ... @Entity     @Id    ... @Entity     @Id     @Lob     @OneToOne(fetch=FetchType.LAZY)     protected Approval approval    ... @NamedEntityGraph(         @NamedAttributeNode(             value="projects",             subGraph=”projects”         ),         @NamedttributeNode(name = "phoneNumbers"),         @NamedSubGraph(             name=”projects”,                 @NamedAttributeNode("requirements")         ),         @NamedSubGraph(             name=”projects”,             type=LargeProject.class,                 @NamedAttributeNode("approver")         ) ) @NamedEntityGraph(     name-”Project”,         @NamedAttributeNode("requirements")         @NamedSubGraph(             type=LargeProject.class,                 @NamedAttributeNode("approver")         ) ) @NamedEntityGraph(     name="EmployeeProjectRequirements"         @NamedAttributeNode(             value="projects",             subGraph=”projects”         ),         @NamedttributeNode(name = "phoneNumbers"),         @NamedSubGraph(             name=”projects”,                 @NamedAttributeNode(                      value ="requirements",                      subGraph=”requirements”                 )         ),         @NamedSubGraph(             name=”requirements”,                 @NamedAttributeNode("description"),                 @NamedAttributeNode("approval")         ) ) The entity graph could also be created dynamically through the following APIs: /**  * This type represents the root of an entity graph that will be used as a  * template to define the attribute nodes and boundaries of a graph of entities and  * entity relationships. The root must be an Entity type.  *  *            the type of the root entity.  */     /**      * Returns the name of the static EntityGraph.  Will return null if the      * EntityGraph is not a named EntityGraph.      */     /*      * Add an AttributeNode attribute to the entity graph.      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this entity.      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     public &amp;lt;X&amp;gt; void addAttributeNodes(String ...     /*      * Add an AttributeNode attribute to the entity graph.      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     public &amp;lt;X&amp;gt; void addAttributeNodes(Attribute&amp;lt;T,     /*      * Used to add a node of the graph that corresponds to a managed type. This      * allows for construction of multi-node Entity graphs that include related      * managed types.      *      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     /**      * Used to add a node of the graph that corresponds to a managed type with      * inheritance.  This allows for multiple subclass sub-graphs to be defined      * for this node of the entity graph. Subclass sub-graphs will include the      * specified attributes of superclass sub-graphs      *      * @throws IllegalArgumentException      *             if the attribute's target type is not a managed type      * @throws IllegalStateException      *             if this EntityGraph has been statically defined      */ addSubGraph(Attribute&amp;lt;T, X&amp;gt; attribute, Class&amp;lt;? extends     /*      * Used to add a node of the graph that corresponds to a managed type. This      * allows for construction of multi-node Entity graphs that include related      * managed types.      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this entity.      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     public &amp;lt;X&amp;gt; SubGraph&amp;lt;X&amp;gt; addSubGraph(String     /**      * Used to add a node of the graph that corresponds to a managed type with      * inheritance.  This allows for multiple subclass sub-graphs to be defined      * for this node of the entity graph.  Subclass sub-graphs will include the      * specified attributes of superclass sub-graphs      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this managed type.      * @throws IllegalArgumentException      *             if the attribute's target type is not a managed type      * @throws IllegalStateException      *             if this EntityGraph has been statically defined      */     public &amp;lt;X&amp;gt; SubGraph&amp;lt;X&amp;gt; addSubGraph(String     /*      * Used to add a node of the graph that corresponds to a map key that is a      * managed type. This allows for construction of multi-node Entity graphs that      * include related managed types.      *      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type entity      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     /*      * Used to add a node of the graph that corresponds to a map key that is a      * managed type with inheritance. This allows for construction of multi-      * node Entity graphs that include related managed types.  Subclass sub-graphs      * will include the specified attributes of superclass sub-graphs      *      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type entity      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */ addKeySubGraph(Attribute&amp;lt;T, X&amp;gt; attribute, Class&amp;lt;? Extends     /*      * Used to add a node of the graph that corresponds to a map key that is a      * managed type. This allows for construction of multi-node Entity graphs that      * include related managed types.      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this entity.      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     /*      * Used to add a node of the graph that corresponds to a map key that is a      * managed type with inheritance. This allows for construction of multi-      * node Entity graphs that include related managed types.  Subclass sub-graphs      * will include the specified attributes of superclass sub-graphs      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this entity.      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     /*      * returns the attributes of this entity that are included in the entity      * graph      */ /**  * Represents an AttributeNode of an entity graph.  */     /*      * returns the Type of the AttributeNode.      */     /*      * returns the name of the referencing attribute.      */ /**  * This type represents a AttributeNode of an EntityGraph that corresponds to a  * Managed Type. Using this class an entity graph can be embedded within an  * EntityGraph.  *  *            the Class type of the AttributeNode.  */ public interface SubGraph&amp;lt;T&amp;gt; extends     /**      * Add an AttributeNode attribute to the entity graph.      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this managed type.      * @throws IllegalStateException      *             if this EntityGraph has been statically defined      */     public &amp;lt;X&amp;gt; void  addAttributeNodes(String ...     /**      * Add an AttributeNode attribute to the entity graph.      *      * @throws IllegalStateException      *             if this EntityGraph has been statically defined      */     public &amp;lt;X&amp;gt; void addAttributeNodes(Attribute&amp;lt;T,     /**      * Used to add a node of the graph that corresponds to a managed type. This      * allows for construction of multi-node Entity graphs that include related      * managed types.      *      * @throws IllegalArgumentException      *             if the attribute's target type is not a managed type      * @throws IllegalStateException      *             if this EntityGraph has been statically defined      */     /**      * Used to add a node of the graph that corresponds to a managed type with      * inheritance.  This allows for multiple subclass sub-graphs to be defined      * for this node of the entity graph. Subclass sub-graphs will include the      * specified attributes of superclass sub-graphs      *      * @throws IllegalArgumentException      *             if the attribute's target type is not a managed type      * @throws IllegalStateException      *             if this EntityGraph has been statically defined      */ addSubGraph(Attribute&amp;lt;T, X&amp;gt; attribute, Class&amp;lt;? extends     /**      * Used to add a node of the graph that corresponds to a managed type. This      * allows for construction of multi-node Entity graphs that include related      * managed types.      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this managed type.      * @throws IllegalArgumentException      *             if the attribute's target type is not a managed type      * @throws IllegalStateException      *             if this EntityGraph has been statically defined      */     public &amp;lt;X&amp;gt; SubGraph&amp;lt;X&amp;gt; addSubGraph(String     /**      * Used to add a node of the graph that corresponds to a managed type with      * inheritance.  This allows for multiple subclass sub-graphs to be defined      * for this node of the entity graph. Subclass sub-graphs will include the      * specified attributes of superclass sub-graphs      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this managed type.      * @throws IllegalArgumentException      *             if the attribute's target type is not a managed type      * @throws IllegalStateException      *             if this EntityGraph has been statically defined      */     public &amp;lt;X&amp;gt; SubGraph&amp;lt;X&amp;gt; addSubGraph(String     /*      * Used to add a node of the graph that corresponds to a map key that is a      * managed type. This allows for construction of multi-node Entity graphs that      * include related managed types.      *      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type entity      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     /*      * Used to add a node of the graph that corresponds to a map key that is a      * managed type with inheritance. This allows for construction of multi-      * node Entity graphs that include related managed types.  Subclass sub-graphs      * will include the specified attributes of superclass sub-graphs      *      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type entity      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */ addKeySubGraph(Attribute&amp;lt;T, X&amp;gt; attribute, Class&amp;lt;? Extends     /*      * Used to add a node of the graph that corresponds to a map key that is a      * managed type. This allows for construction of multi-node Entity graphs that      * include related managed types.      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this entity.      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     /*      * Used to add a node of the graph that corresponds to a map key that is a      * managed type with inheritance. This allows for construction of multi-      * node Entity graphs that include related managed types.  Subclass sub-graphs      * will include the specified attributes of superclass sub-graphs      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this entity.      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     /**      * returns the attributes of this managed type that are included in the      * sub-graph      */     /**      * returns the attribute that references this sub-graph.      /     /**      * returns the type of this sub-graph if it was used to extend a superclass’      * sub-graph definition.      */      ...     /**      * returns any named EntityGraph that has been created through static      * metadata. Returned EntityGraphs should be considered immutable.      */     /**      * add a named copy of the EntityGraph to the EntityManagerFactory      */     public void addNamedEntityGraph(String graphName,     /**      * returns a mutable EntityGraph that can be used to dynamically create an      * EntityGraph.      */     /**      * returns a mutable copy of the named EntityGraph      */     public EntityGraph&amp;lt;?&amp;gt; createEntityGraph(String Example:       EntityGroup employee =    SubGraph largeProjects =           EntityGroup employee =    SubGraph requirements = employee.addSubGraph("projects")                                        Once the EntityGraph is available or has been created it can be applied to operations on the EntityManager or used with queries to control eager loading of results. The standard property javax.persistence.fetchgraph can be used with the find operation or as a query hint to specify a EntityGraph that defines additional FetchType semantics whereby  any AttributeNode of the EntityGraph is treated as FetchType.EAGER and any attribute not specified in the EntityGraph is treated as  FetchType.LAZY .  The primary key of an entity is always retrieved even if not present in the fetch group.  The following characterizes the semantics of entity graphs that are used as fetch graphs. A fetch graph attribute node specified within an entity graph or sub-graph specifies how an attribute is to be fetched.  Attributes that are not specified are treated as FetchType.LAZY . The following rules apply, depending on attribute type. A primary key attribute never needs to be specified in an attribute node.  (This applies to composite primary keys as well, including embedded id primary keys.)  When an entity is fetched, its primary key is always fetched.  It is not incorrect, however, to specify primary key attributes. Attributes other than primary key attributes are not fetched unless the attribute is specified.  The following rules apply to the specification of attributes: The default fetch graph for an entity or embeddable consists of the transitive closure of all of its attributes that are specified as FetchType.EAGER (or defaulted as such). If the attribute is an embedded attribute, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the default fetch graph for the embeddable is fetched.  If a subgraph is specified for the attribute, the attributes of the embeddable are fetched according to their specification in the corresponding subgraph, and the rules of this section recursively apply. If the attribute is an element collection of basic type, and the attribute is specified in an attribute node, the element collection together with its basic elements is fetched. If the attribute is an element collection of embeddables, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the element collection together with the default fetch graph of its embeddable elements is fetched.  If a subgraph is specified for the attribute, the attributes of the embeddable elements will be fetched according to the corresponding subgraph specification, and the rules of this section recursively apply. If the attribute is a one-to-one or many-to-one relationship, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the default fetch graph of the target entity is fetched.  If a subgraph is specified for the attribute, the attributes of the target entity will be fetched according to the corresponding subgraph specification, and the rules of this section recursively apply. If the attribute is a one-to-many or many-to-many relationship, and the attribute is specified in an attribute node, but a subgraph is not specified, the collection will be fetched and the default fetch graphs of the referenced entities will be fetched.  If a subgraph is specified for the attribute, the entities in the collection will be fetched according to the corresponding subgraph specification, and the rules of this section recursively apply. If the key of a map which has been specified in an attribute node is a basic type, it will always be fetched and if the key of a map which has been specified in an attribute node is an embedded type otherwise, if a map key subgraph is not specified for the attribute node, the map key will be fetched according to its default fetch graph.  If a key subgraph is specified for the map key attribute, the map key attribute will be fetched according to the map key subgraph specification, and the rules of this section recursively apply.  So using the following entity graph: @NamedEntityGraph( ) used to load a Phonenumber only the id and the FKs would be retrieved.   When using the following entity graph: @NamedEntityGraph(         @NamedAttributeNode("projects"), ) loading an Employee  the PKs and FKs would be retrieved and the “projects” attribute would be eagerly loaded.  The standard property javax.persistence.loadgraph can be used to specify an entity graph that defines additional FetchType semantics with any attribute node of the entity graph becoming FetchType.EAGER and any unspecified attributes remain as defined in the mapping. The following characterizes the semantics of entity graphs that are used as load graphs. A load graph attribute node specified within an entity graph or sub-graph specifies how an attribute is to be fetched.  Attributes that are not specified are treated according to their default FetchType. The following rules apply to the specification of attributes: A primary key attribute never needs to be specified in an attribute node.  (This applies to composite primary keys as well, including embedded id primary keys.)  When an entity is fetched, its primary key is always fetched.  It is not incorrect, however, to specify primary key attributes. If the attribute is an embedded attribute, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the default fetch graph for the embeddable is fetched. If a subgraph is specified for the attribute, any attributes of the embeddable that are not further specified within the subgraph are treated according to their fetch type; attributes that are specified by the subgraph are also fetched; and the rules of this section recursively apply. If the attribute is an element collection of basic type, and the attribute is specified in an attribute node, the element collection together with its basic elements is fetched.  If the attribute is an element collection of embeddables, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the element collection together with the default fetch graph of its embeddable elements is fetched.  If a subgraph is specified for the attribute, any attributes of the embeddables that attributes that are specified by the subgraph are also fetched; and the rules of this section recursively apply. If the attribute is a one-to-one or many-to-one relationship, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the default fetch graph of the target entity is fetched.  If a subgraph is specified for the attribute, any attributes of the target entity that are not further specified are treated according to their fetch type; attributes that are specified by the subgraph are also fetched; and the rules of this section recursively apply. If the attribute is a one-to-many or many-to-many relationship, and the attribute is specified in an attribute node, but a subgraph is not specified, the collection will be fetched and the default fetch graphs of the referenced entities will be fetched.  If a subgraph is specified for the attribute, any entities in the collection will be treated according to their fetch type; attributes that are specified by the subgraph are also fetched; and the rules of this section recursively apply. If a collection-valued attribute is a map, keys that are basic or embeddable types will be fetched when the map is fetched; entity map key attributes will be fetched according to their fetch type and, if a key subgraph is specified, additional entity attributes are fetched as specified in the subgraph; and the rules of this section recursively apply. So using the following entity graph: @NamedEntityGraph( ) used to load a Phonenumber only the id and the FKs would be retrieved. The requirements attribute would not be loaded but would be lazy.   When using the following entity graph: @NamedEntityGraph(         @NamedAttributeNode("projects"), ) loading an Employee  the PKs and FKs would be retrieved and the “projects” attribute would be eagerly loaded. EntityManager API     /**      * Copy the provided entity graph  using the EntityGraph as a template to specify      * the attributes that will be copied. Attributes not included in the entity graph will not be copied.      *      * @param entity      *            entity instance      * @param EntityGraph      *            the entity graph template for applying copy.      * @throws IllegalArgumentException      *             if the instance is not an entity or the EntityGraph root is      *             not the same class as the entity.      */     public void copy(Object entity, EntityGraph     ...     /**      * Merge the state of the given entity into the current persistence context      * using the EntityGraph as a template for applying the merge operation.      *      *      * @param entity      *            entity instance      * @param EntityGraph      *            the entity graph template for applying merge.      * @return the managed instance that the state was merged to      * @throws IllegalArgumentException      *             if instance is not an entity, is a removed entity or the root      *             of the EntityGraph is not of the same type as the entity      * @throws TransactionRequiredException      *             if there is no transaction when invoked on a      *             container-managed entity manager of that is of type      *                  */     public &amp;lt;T&amp;gt; T merge(T entity, EntityGraph     ...etc Merge When using an entity graph  in combination with the “merge” operation any listed attribute node will be merged and any unlisted attribute node will not be merged, this includes @Basic mappings.  Any attribute that references a Map is considered to include both the key and the value.  If the attribute node references a sub-graph or map key sub-graph for the relationship target then the merge will operate on that target object as well with only those attributes referenced in the sub-graph being merged and this will apply recursively. For example with the merge operation applied with the following entity graph : @NamedEntityGraph(         @NamedAttributeNode(“name”),         @NamedAttributeNode(             value="projects",             subGraph=”projects”         ),         @NamedAttributeNode("phoneNumbers"),         @NamedSubGraph(             name=”projects”,                 @NamedAttributeNode("requirements")         ) ) Only the name, projects and phoneNumbers attributes of the Employee will be merged.  The merge will be  applied to the projects but only the requirements attribute of the Project will be merged and the merge will not be  applied to the Requirement target.  The PhoneNumber target will not be merged. Copy A new operation “copy” is being introduced.  This operation is intended to be used when a user wishes to disconnect a graph of entities from a larger graph.  When using an entity graph  in combination with the “copy” operation a copy of the provided entity is created and the attributes of the copied entity are populated based on the attributes listed in the entity graph.  If an attribute node for an  attribute is present then the attribute will be copied.  If the attribute node represents an attribute that has a managed type as the target, element or key type then that target instance is copied as well and the copy is set in the new tree.  If a sub-graph is defined for the value or key then any attributes corresponding to the attribute nodes within the sub-graph will be copied as above.  If no sub-graph is defined for the attribute node then no attributes of the target are copied. For example with the copy operation applied with the following entity graph : @NamedEntityGraph(         @NamedAttributeNode(“name”),         @NamedAttributeNode(             value="projects",             subGraph=”projects”         ),         @NamedAttributeNode("phoneNumbers"),         @NamedSubGraph(             name=”projects”,                 @NamedAttributeNode("requirements")         ) ) When applied to an Employee instance a new copy of the Employee will be created and the “name” , “phoneNumbers” and “projects” attributes will be copied over.  For each PhoneNumber in the phoneNumbers list a new copy of the PhoneNumber will be created and used but no attributes will be copied.  For each Project in the projects list a new copy of the Project will be created and used and only the requirements attribute will be copied.  For each Requirement in the requirements list a new copy of the Requirement will be created but no attributes will be copied.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Wed Nov 28 11:59:59 CET 2012</date>
    <body>Hi all, thanks Gordon, Linda and Michael for this really extensive proposal. It looks  really thought through. I've taken a look at it mostly from the usability  point of as you'll probably get a lot of useful feedback on the  implementation challenges from the vendor representatives. 1. I felt a bit overwhelmed seeing the annotation based declaration the first  time. Assuming I have 2 or three of these plans defined on my entity, I'll  probably end up with more annotations than actual Java code. Have you thought  about adding a String[] property (let's say "attributes") to the Graph  annotations that allow simply listing the attributes instead of wrapping them  into a nested annotation: @NamedEntityGraph(                 @NamedAttributeNode("projects"),                 @NamedAttributeNode("phoneNumbers") ) could become: The more complex example you showed below would become. @NamedEntityGraph(                 @NamedSubGraph(name = "projects", attributes =  "requirements"),                 @NamedSubGraph(name = "projects", type = LargeProject.class,  attributes = "approver") ) Essentially it's about being able to use a plain property name over the  nested @NamedAttributeNode annotation in case you only need to define the  property name anyway. A similar scenario applies to the actual API. Although you can already add  nodes just by providing a name, I think we can significantly reduce the  number of codes needed to be written by equipping methods like  ….addAttributeNode(…) with varargs so that you don't have to repeat the call  over and over again. As all this stuff is "Named", does it make sense to get rid of the prefix?  There are attribute nodes only right now, so what about @EntityGraph,  @SubGraph, and @Node? This would keep the graph/node semantics in the name.  One might argue this leads to much to a graph datastore impression so  @Attribute instead of @Node could be a choice as well. In general I think we  can shorten the names a bit to reduce the noise an annotation based approach  has to introduce. 2. Will there be an externalizable version (XML) of the annotation model? 3. Is there a chance we allow @NamedEntityGraph being used on a method  declaration as well? This can be handy in case you have infrastructure that  can apply the entity graph defined on a method to all queries executed by  that method. The CDI query extension [0] or Spring Data JPA [1] expose a  programming model like this to users. Thanks again for all the effort you put into this! Ollie [0]  https://github.com/ctpconsulting/query [1]  https://github.com/SpringSource/spring-data-jpa Am 27.11.2012 um 22:20 schrieb Gordon Yorke &amp;lt;gordon.yorke@...&amp;gt;:  Hello all,     Please find below a proposal for EntityGraphs and their use to define   entity graph fetching, loading, merging and copying.  I have been working   with Mike Keith and Linda DeMichiel on this proposal and we feel it is time   to present to the expert group.  Please have a look and provide feedback as   the intention is to integrate this within the 2.1 spec.  Thank you,  Gordon    ----  A common functional request has been to enable users to dynamically define   how an EntityManager operation or query should apply to an entity graph.    This may more commonly be known as “merge groups” or “fetch plans”.  This   functionality can be especially important when detached entities must be   serialized and then merged into an active Persistence Context.    An entity graph is a template defined in the form of metadata or an object   created by the dynamic EntityGraph API that captures the path and   boundaries for an operation or a query   For static metadata the   NamedEntityGraph annotation defines the attribute nodes and boundaries of   the grouping.    @Retention(RUNTIME)    @Retention(RUNTIME)      /**       * (Optional) The name of the sub-graph.       * Defaults to the entity name of the root entity.       */        /**       * (Required) list of attributes that are included in in this sub-graph.       */        /**       * (Optional) Lists all of the attributes of this Entity class to be   listed in        * the NamedEntityGraph without the need to explicitly list them.    Included       * attributes can still be fully specified by an attribute node   referencing a       * sub-graph.       */        /**       * (Optional) This is a list of sub-graphs that are included in the       * entity graph. They  are referenced by name from NamedAttributeNode       * definitions.       */          /**       * (Optional) This is a list of sub-graphs that will add additional   attributes       * for subclasses of the entity represented by this NamedEntityGraph to   this       * entity graph.  Specified attributes from superclasses are included in       * subclasses.       */      The NamedEntityGraph annotation is applied to an Entity class that is   defined as the root of a group of attributes, relationships and related   entity classes.  The NamedAttributeNode annotations are embedded within the   NamedEntityGraph metadata and define the   attributes of the entity that   are included in the grouping.  When a NamedAttributeNode represents an   Entity or an Embedded  (including a collection-valued relationship or an   element collection of embeddables), the referenced type’s attributes can be   included as a NamedSubGraph entry of the entity graph and referenced by   name, creating a multi-level entity graph. If the sub-graph represents an   entity with inheritance multiple NamedSubGraph entries can be made with the   same name to extend the sub-graph for specific subclasses.  The class type   that is being extended is required in the NamedSubGraph.    /**   * A NamedAttributeNode is a member attribute of a NamedEntityGraph.   *   * @see javax.persistence.NamedEntityGraph   * @see javax.persistence.NamedSubGraph    */  @Retention(RUNTIME)      /**       * (Required) the name of the attribute that must be in the sub-graph.       */        /**       * (Optional) if this attribute references a managed type that has its   own       * AttributeNodes then this refers to that NamedSubGraph definition.    If the       * target type has inheritance than multiple sub graphs can be   specified.       * These additional sub-graphs are intended to add additional   attributes from       * subclasses.  Superclass sub-graph entries will be merged into   subclasses       * sub-graphs.       */       /**      * (Optional) if the attribute references a Map type this can be used to   specify      * a subGraph for the Key in the case of an Entity key type.  A      * keySubGraph can not be specified without the Map attribute also being      * specified.  If the target type has inheritance than multiple   sub-graphs can      * be specified.  These additional sub-graphs are intended to add   additional      * attributes from subclasses. Superclass sub-graph entries will be   merged      * into subclasses sub-graphs.      */      /**   * A NamedSubGraph is a member of a NamedEntityGraph that represents a   managed   * type. The NamedSubGraph is only referenced from within a   NamedEntityGraph and   * can not be referenced on its own. It is referenced by name from a   * NamedAttributeNode of the NamedEntityGraph.   *   * @see javax.persistence.NamedEntityGraph  * @see javax.persistence.NamedAttributeNode   */  @Retention(RUNTIME)      /**       * (Required) the name of the sub-graph as referenced from a       * NamedAttributeNode.       */        /**       * (Optional) the type represented by this subgraph.  Required when this       * subgraph is extending a subclass’ definition.       */        /** (Required)       * if this NamedAttributeNode references a managed type then this list   is       * the attributes of that type that must be included.       */      For example:  @Entity      @Id    @GeneratedValue      @Basic      @Basic      @OneToMany()      @OneToMany()      @OneToMany()    ...    @Entity  @Inheritance      @Id      @GeneratedValue        @OneToOne(fetch=FetchType.EAGER)     ...    @Entity        @OneToOne(fetch=FetchType.LAZY)     ...    @Entity      @Id       ...    @Entity      @Id        @Lob        @OneToOne(fetch=FetchType.LAZY)      protected Approval approval     ...    @NamedEntityGraph(          @NamedAttributeNode(              value="projects",              subGraph=”projects”          ),          @NamedttributeNode(name = "phoneNumbers"),          @NamedSubGraph(              name=”projects”,                  @NamedAttributeNode("requirements")          ),          @NamedSubGraph(              name=”projects”,              type=LargeProject.class,                  @NamedAttributeNode("approver")          )  )    @NamedEntityGraph(      name-”Project”,          @NamedAttributeNode("requirements")          @NamedSubGraph(              type=LargeProject.class,                  @NamedAttributeNode("approver")          )  )    @NamedEntityGraph(      name="EmployeeProjectRequirements"          @NamedAttributeNode(              value="projects",              subGraph=”projects”          ),          @NamedttributeNode(name = "phoneNumbers"),          @NamedSubGraph(              name=”projects”,                  @NamedAttributeNode(                       value ="requirements",                       subGraph=”requirements”                  )          ),          @NamedSubGraph(              name=”requirements”,                  @NamedAttributeNode("description"),                  @NamedAttributeNode("approval")          )  )    The entity graph could also be created dynamically through the following   APIs:    /**   * This type represents the root of an entity graph that will be used as a   * template to define the attribute nodes and boundaries of a graph of   entities and   * entity relationships. The root must be an Entity type.   *    *            the type of the root entity.   */          /**       * Returns the name of the static EntityGraph.  Will return null if the       * EntityGraph is not a named EntityGraph.       */        /*       * Add an AttributeNode attribute to the entity graph.       *        * @throws IllegalArgumentException if the attribute is not an   attribute of       * this entity.       * @throws IllegalStateException if this EntityGraph has been statically       * defined       */        /*       * Add an AttributeNode attribute to the entity graph.       *        * @throws IllegalStateException if this EntityGraph has been statically       * defined       */        /*       * Used to add a node of the graph that corresponds to a managed type.   This       * allows for construction of multi-node Entity graphs that include   related       * managed types.       *        * @throws IllegalArgumentException if the attribute's target type is   not a       * managed type       *        * @throws IllegalStateException if this EntityGraph has been statically       * defined       */        /**       * Used to add a node of the graph that corresponds to a managed type   with        * inheritance.  This allows for multiple subclass sub-graphs to be   defined       * for this node of the entity graph. Subclass sub-graphs will include   the       * specified attributes of superclass sub-graphs       *        * @throws IllegalArgumentException       *             if the attribute's target type is not a managed type       * @throws IllegalStateException       *             if this EntityGraph has been statically defined       */      public &amp;lt;X&amp;gt; SubGraph&amp;lt;? extends X&amp;gt; addSubGraph(Attribute&amp;lt;T, X&amp;gt; attribute,         /*       * Used to add a node of the graph that corresponds to a managed type.   This       * allows for construction of multi-node Entity graphs that include   related       * managed types.       *        * @throws IllegalArgumentException if the attribute is not an   attribute of       * this entity.       * @throws IllegalArgumentException if the attribute's target type is   not a       * managed type       *        * @throws IllegalStateException if this EntityGraph has been statically       * defined       */        /**       * Used to add a node of the graph that corresponds to a managed type   with        * inheritance.  This allows for multiple subclass sub-graphs to be   defined       * for this node of the entity graph.  Subclass sub-graphs will include   the       * specified attributes of superclass sub-graphs       *        * @throws IllegalArgumentException if the attribute is not an   attribute of       * this managed type.       * @throws IllegalArgumentException       *             if the attribute's target type is not a managed type       * @throws IllegalStateException       *             if this EntityGraph has been statically defined       */        /*       * Used to add a node of the graph that corresponds to a map key that   is a       * managed type. This allows for construction of multi-node Entity   graphs that       * include related managed types.       *        * @throws IllegalArgumentException if the attribute's target type is   not a       * managed type entity       *        * @throws IllegalStateException if this EntityGraph has been statically       * defined       */        /*       * Used to add a node of the graph that corresponds to a map key that   is a       * managed type with inheritance. This allows for construction of multi-       * node Entity graphs that include related managed types.  Subclass   sub-graphs       * will include the specified attributes of superclass sub-graphs       *        * @throws IllegalArgumentException if the attribute's target type is   not a       * managed type entity         *        * @throws IllegalStateException if this EntityGraph has been statically       * defined       */      public &amp;lt;X&amp;gt; SubGraph&amp;lt;? Extends X&amp;gt; addKeySubGraph(Attribute&amp;lt;T, X&amp;gt;         /*       * Used to add a node of the graph that corresponds to a map key that   is a       * managed type. This allows for construction of multi-node Entity   graphs that       * include related managed types.       *        * @throws IllegalArgumentException if the attribute is not an   attribute of       * this entity.       * @throws IllegalArgumentException if the attribute's target type is   not a       * managed type       *        * @throws IllegalStateException if this EntityGraph has been statically       * defined       */        /*       * Used to add a node of the graph that corresponds to a map key that   is a       * managed type with inheritance. This allows for construction of multi-       * node Entity graphs that include related managed types.  Subclass   sub-graphs       * will include the specified attributes of superclass sub-graphs       *        * @throws IllegalArgumentException if the attribute is not an   attribute of       * this entity.       * @throws IllegalArgumentException if the attribute's target type is   not a       * managed type       *        * @throws IllegalStateException if this EntityGraph has been statically       * defined       */        /*       * returns the attributes of this entity that are included in the entity       * graph       */      /**   * Represents an AttributeNode of an entity graph.   */        /*       * returns the Type of the AttributeNode.       */            /*       * returns the name of the referencing attribute.       */    /**   * This type represents a AttributeNode of an EntityGraph that corresponds   to a   * Managed Type. Using this class an entity graph can be embedded within an   * EntityGraph.   *    *            the Class type of the AttributeNode.   */          /**       * Add an AttributeNode attribute to the entity graph.       *        * @throws IllegalArgumentException if the attribute is not an   attribute of       * this managed type.       * @throws IllegalStateException       *             if this EntityGraph has been statically defined       */        /**       * Add an AttributeNode attribute to the entity graph.       *        * @throws IllegalStateException       *             if this EntityGraph has been statically defined       */        /**       * Used to add a node of the graph that corresponds to a managed type.   This       * allows for construction of multi-node Entity graphs that include   related       * managed types.         *        * @throws IllegalArgumentException       *             if the attribute's target type is not a managed type       * @throws IllegalStateException       *             if this EntityGraph has been statically defined       */        /**       * Used to add a node of the graph that corresponds to a managed type   with        * inheritance.  This allows for multiple subclass sub-graphs to be   defined       * for this node of the entity graph. Subclass sub-graphs will include   the       * specified attributes of superclass sub-graphs       *        * @throws IllegalArgumentException       *             if the attribute's target type is not a managed type       * @throws IllegalStateException       *             if this EntityGraph has been statically defined       */      public &amp;lt;X&amp;gt; SubGraph&amp;lt;? extends X&amp;gt; addSubGraph(Attribute&amp;lt;T, X&amp;gt; attribute,         /**       * Used to add a node of the graph that corresponds to a managed type.   This       * allows for construction of multi-node Entity graphs that include   related       * managed types.       *        * @throws IllegalArgumentException if the attribute is not an   attribute of       * this managed type.       * @throws IllegalArgumentException       *             if the attribute's target type is not a managed type       * @throws IllegalStateException       *             if this EntityGraph has been statically defined       */        /**       * Used to add a node of the graph that corresponds to a managed type   with        * inheritance.  This allows for multiple subclass sub-graphs to be   defined       * for this node of the entity graph. Subclass sub-graphs will include   the       * specified attributes of superclass sub-graphs       *        * @throws IllegalArgumentException if the attribute is not an   attribute of       * this managed type.       * @throws IllegalArgumentException       *             if the attribute's target type is not a managed type       * @throws IllegalStateException       *             if this EntityGraph has been statically defined       */        /*       * Used to add a node of the graph that corresponds to a map key that   is a       * managed type. This allows for construction of multi-node Entity   graphs that       * include related managed types.       *        * @throws IllegalArgumentException if the attribute's target type is   not a       * managed type entity         *        * @throws IllegalStateException if this EntityGraph has been statically       * defined       */        /*       * Used to add a node of the graph that corresponds to a map key that   is a       * managed type with inheritance. This allows for construction of multi-       * node Entity graphs that include related managed types.  Subclass   sub-graphs       * will include the specified attributes of superclass sub-graphs       *        * @throws IllegalArgumentException if the attribute's target type is   not a       * managed type entity         *        * @throws IllegalStateException if this EntityGraph has been statically       * defined       */      public &amp;lt;X&amp;gt; SubGraph&amp;lt;? Extends X&amp;gt; addKeySubGraph(Attribute&amp;lt;T, X&amp;gt;         /*       * Used to add a node of the graph that corresponds to a map key that   is a       * managed type. This allows for construction of multi-node Entity   graphs that       * include related managed types.       *        * @throws IllegalArgumentException if the attribute is not an   attribute of       * this entity.       * @throws IllegalArgumentException if the attribute's target type is   not a       * managed type       *        * @throws IllegalStateException if this EntityGraph has been statically       * defined       */        /*       * Used to add a node of the graph that corresponds to a map key that   is a       * managed type with inheritance. This allows for construction of multi-       * node Entity graphs that include related managed types.  Subclass   sub-graphs       * will include the specified attributes of superclass sub-graphs       *        * @throws IllegalArgumentException if the attribute is not an   attribute of       * this entity.       * @throws IllegalArgumentException if the attribute's target type is   not a       * managed type       *        * @throws IllegalStateException if this EntityGraph has been statically       * defined       */        /**       * returns the attributes of this managed type that are included in the       * sub-graph       */        /**       * returns the attribute that references this sub-graph.       /        /**       * returns the type of this sub-graph if it was used to extend a   superclass’       * sub-graph definition.       */         ...      /**       * returns any named EntityGraph that has been created through static       * metadata. Returned EntityGraphs should be considered immutable.       */        /**       * add a named copy of the EntityGraph to the EntityManagerFactory       */      public void addNamedEntityGraph(String graphName, EntityGraph         /**       * returns a mutable EntityGraph that can be used to dynamically create   an       * EntityGraph.       */        /**       * returns a mutable copy of the named EntityGraph       */    Example:       SubGraph largeProjects = employee.addSubGraph(“projects”,               SubGraph requirements = employee.addSubGraph("projects")    Once the EntityGraph is available or has been created it can be applied to   operations on the EntityManager or used with queries to control eager   loading of results.     The standard property javax.persistence.fetchgraph can be used with the   find operation or as a query hint to specify a     EntityGraph that defines   additional FetchType semantics whereby  any AttributeNode of the   EntityGraph is treated as FetchType.EAGER and any attribute not specified   in the EntityGraph is treated as  FetchType.LAZY.  The primary key of an   entity is always retrieved even if not present in the fetch group.      The following characterizes the semantics of entity graphs that are used as   fetch graphs.    A fetch graph attribute node specified within an entity graph or sub-graph   specifies how an attribute is to be fetched.  Attributes that are not   specified are treated as FetchType.LAZY.    The following rules apply, depending on attribute type.    A primary key attribute never needs to be specified in an attribute node.    (This applies to composite primary keys as well, including embedded id   primary keys.)  When an entity is fetched, its primary key is always   fetched.  It is not incorrect, however, to specify primary key attributes.    Attributes other than primary key attributes are not fetched unless the   attribute is specified.  The following rules apply to the specification of   attributes:    The default fetch graph for an entity or embeddable consists of the   transitive closure of all of its attributes that are specified as   FetchType.EAGER (or defaulted as such).    If the attribute is an embedded attribute, and the attribute is specified   in an attribute node, but a subgraph is not specified for the attribute,   the default fetch graph for the embeddable is fetched.  If a subgraph is   specified for the attribute, the attributes of the embeddable are fetched   according to their specification in the corresponding subgraph, and the   rules of this section recursively apply.    If the attribute is an element collection of basic type, and the attribute   is specified in an attribute node, the element collection together with its   basic elements is fetched.    If the attribute is an element collection of embeddables, and the attribute   is specified in an attribute node, but a subgraph is not specified for the   attribute, the element collection together with the default fetch graph of   its embeddable elements is fetched.  If a subgraph is specified for the   attribute, the attributes of the embeddable elements will be fetched   according to the corresponding subgraph specification, and the rules of   this section recursively apply.    If the attribute is a one-to-one or many-to-one relationship, and the   attribute is specified in an attribute node, but a subgraph is not   specified for the attribute, the default fetch graph of the target entity   is fetched.  If a subgraph is specified for the attribute, the attributes   of the target entity will be fetched according to the corresponding   subgraph specification, and the rules of this section recursively apply.    If the attribute is a one-to-many or many-to-many relationship, and the   attribute is specified in an attribute node, but a subgraph is not   specified, the collection will be fetched and the default fetch graphs of   the referenced entities will be fetched.  If a subgraph is specified for   the attribute, the entities in the collection will be fetched according to   the corresponding subgraph specification, and the rules of this section   recursively apply.    If the key of a map which has been specified in an attribute node is a   basic type, it will always be fetched and if the key of a map which has   been specified in an attribute node is an embedded type the default fetch   graph will be fetched for the embeddable; otherwise, if a map key subgraph   is not specified for the attribute node, the map key will be fetched   according to its default fetch graph.  If a key subgraph is specified for   the map key attribute, the map key attribute will be fetched according to   the map key subgraph specification, and the rules of this section   recursively apply.      So using the following entity graph:    @NamedEntityGraph(  )    used to load a Phonenumber only the id and the FKs would be retrieved.       When using the following entity graph:  @NamedEntityGraph(          @NamedAttributeNode("projects"),  )    loading an Employee  the PKs and FKs would be retrieved and the “projects”   attribute would be eagerly loaded.      The standard property javax.persistence.loadgraph can be used to specify an   entity graph that defines additional FetchType semantics with any attribute   node of the entity graph becoming FetchType.EAGER and any unspecified   attributes remain as defined in the mapping.    The following characterizes the semantics of entity graphs that are used as   load graphs.    A load graph attribute node specified within an entity graph or sub-graph   specifies how an attribute is to be fetched.  Attributes that are not   specified are treated according to their default FetchType.    The following rules apply to the specification of attributes:    A primary key attribute never needs to be specified in an attribute node.    (This applies to composite primary keys as well, including embedded id   primary keys.)  When an entity is fetched, its primary key is always   fetched.  It is not incorrect, however, to specify primary key attributes.    If the attribute is an embedded attribute, and the attribute is specified   in an attribute node, but a subgraph is not specified for the attribute,   the default fetch graph for the embeddable is fetched. If a subgraph is   specified for the attribute, any attributes of the embeddable that are not   further specified within the subgraph are treated according to their fetch   type; attributes that are specified by the subgraph are also fetched; and   the rules of this section recursively apply.    If the attribute is an element collection of basic type, and the attribute   is specified in an attribute node, the element collection together with its   basic elements is fetched.  If the attribute is an element collection of   embeddables, and the attribute is specified in an attribute node, but a   subgraph is not specified for the attribute, the element collection   together with the default fetch graph of its embeddable elements is   fetched.  If a subgraph is     specified for the attribute, any attributes   of the embeddables that are not further specified are treated according to   their fetch type; attributes that are specified by the subgraph are also   fetched; and the rules of this section recursively apply.    If the attribute is a one-to-one or many-to-one relationship, and the   attribute is specified in an attribute node, but a subgraph is not   specified for the attribute, the default fetch graph of the target entity   is fetched.  If a subgraph is specified for the attribute, any attributes   of the target entity that are not further specified are treated according   to their fetch type; attributes that are specified by the subgraph are also   fetched; and the rules of this section recursively apply.    If the attribute is a one-to-many or many-to-many relationship, and the   attribute is specified in an attribute node, but a subgraph is not   specified, the collection will be fetched and the default fetch graphs of   the referenced entities will be fetched.  If a subgraph is specified for   the attribute, any entities in the collection will be treated according to   their fetch type; attributes that are specified by the subgraph are also   fetched; and the rules of this section recursively apply.    If a collection-valued attribute is a map, keys that are basic or   embeddable types will be fetched when the map is fetched; entity map key   attributes will be fetched according to their fetch type and, if a key   subgraph is specified, additional entity attributes are fetched as   specified in the subgraph; and the rules of this section recursively apply.     So using the following entity graph:    @NamedEntityGraph(  )    used to load a Phonenumber only the id and the FKs would be retrieved. The   requirements attribute would not be loaded but would be lazy.       When using the following entity graph:  @NamedEntityGraph(          @NamedAttributeNode("projects"),  )    loading an Employee  the PKs and FKs would be retrieved and the “projects”   attribute would be eagerly loaded.     EntityManager API        /**       * Copy the provided entity graph  using the EntityGraph as a template   to  specify       * the attributes that will be copied. Attributes not included in the   entity graph will not be copied.       *        * @param entity       *            entity instance       * @param EntityGraph       *            the entity graph template for applying copy.       * @throws IllegalArgumentException       *             if the instance is not an entity or the EntityGraph root   is       *             not the same class as the entity.       */      public void copy(Object entity, EntityGraph entityGraph, Map&amp;lt;String,        ...        /**       * Merge the state of the given entity into the current persistence   context       * using the EntityGraph as a template for applying the merge operation.       *        *        * @param entity       *            entity instance       * @param EntityGraph       *            the entity graph template for applying merge.       * @return the managed instance that the state was merged to       * @throws IllegalArgumentException       *             if instance is not an entity, is a removed entity or the   root       *             of the EntityGraph is not of the same type as the entity       * @throws TransactionRequiredException       *             if there is no transaction when invoked on a       *             container-managed entity manager of that is of type       */      public &amp;lt;T&amp;gt; T merge(T entity, EntityGraph entityGraph, Map&amp;lt;String,       ...etc    Merge  When using an entity graph  in combination with the “merge” operation any   listed attribute node will be merged and any unlisted attribute node will   not be merged, this includes @Basic mappings.  Any attribute that   references a Map is considered to include both the key and the value.  If   the attribute node references a sub-graph or map key sub-graph for the   relationship target then the merge will operate on that target object as   well with only those attributes referenced in the sub-graph being merged   and this will apply recursively.    For example with the merge operation applied with the following entity   graph :    @NamedEntityGraph(          @NamedAttributeNode(“name”),          @NamedAttributeNode(              value="projects",              subGraph=”projects”          ),          @NamedAttributeNode("phoneNumbers"),          @NamedSubGraph(              name=”projects”,                  @NamedAttributeNode("requirements")          )  )    Only the name, projects and phoneNumbers attributes of the Employee will be   merged.  The merge will be  applied to the projects but only the   requirements attribute of the Project will be merged and the merge will not   be  applied to the Requirement target.  The PhoneNumber target will not be   merged.    Copy  A new operation “copy” is being introduced.  This operation is intended to   be used when a user wishes to disconnect a graph of entities from a larger   graph.  When using an entity graph  in combination with the “copy”   operation a copy of the provided entity is created and the attributes of   the copied entity are populated based on the attributes listed in the   entity graph.  If an attribute node for an  attribute is present then the   attribute will be copied.  If the attribute node represents an attribute   that has a managed type as the target, element or key type then that target   instance is copied as well and the copy is set in the new tree.  If a   sub-graph is defined for the value or key then any attributes corresponding   to the attribute nodes within the sub-graph will be copied as above.  If no   sub-graph is defined for the attribute node then no attributes of the   target are copied.    For example with the copy operation applied with the following entity graph   :    @NamedEntityGraph(          @NamedAttributeNode(“name”),          @NamedAttributeNode(              value="projects",              subGraph=”projects”          ),          @NamedAttributeNode("phoneNumbers"),          @NamedSubGraph(              name=”projects”,                  @NamedAttributeNode("requirements")          )  )    When applied to an Employee instance a new copy of the Employee will be   created and the “name” , “phoneNumbers” and “projects” attributes will be   copied over.  For each PhoneNumber in the phoneNumbers list a new copy of   the PhoneNumber will be created and used but no attributes will be copied.    For each Project in the projects list a new copy of the Project will be   created and used and only the requirements attribute will be copied.  For   each Requirement in the requirements list a new copy of the Requirement   will be created but no attributes will be copied.   --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Wed Nov 28 14:29:33 CET 2012</date>
    <body>Hello All,    Based on suggestions from a colleague I would like to suggest some clarification and updates.    1 - with respect to @Version add to the proposal that version is always fetched and copied along with the PK.    2 - Add :     /**      * returns all named EntityGraphs that have been defined for the provided      * class type.      */     public &amp;lt;T&amp;gt; List&amp;lt;EntityGraph&amp;lt;? super    3 - Fix EntityManager.copy :     public &amp;lt;X&amp;gt; X copy(X entity, EntityGraph&amp;lt;? super Thank you, --Gordon    On 27/11/2012 5:20 PM, Gordon Yorke Hello all,    Please find below a proposal for EntityGraphs and their use to define entity graph fetching, loading, merging and copying.  I have been working with Mike Keith and Linda DeMichiel on this proposal and we feel it is time to present to the expert group.  Please have a look and provide feedback as the intention is to integrate this within the 2.1 spec. Thank you, Gordon ---- A common functional request has been to enable users to dynamically define how an EntityManager operation or query should apply to an entity graph.  This may more commonly be known as “merge groups” or “fetch plans”.  This functionality can be especially important when detached entities must be serialized and then merged into an active Persistence Context. An entity graph is a template defined in the form of metadata or an object created by the dynamic EntityGraph API that captures the path and boundaries for an operation or a query   For static metadata the NamedEntityGraph annotation defines the attribute nodes and boundaries of the grouping. @Retention(RUNTIME) @Retention(RUNTIME)     /**      * (Optional) The name of the sub-graph.      * Defaults to the entity name of the root entity.      */     /**      * (Required) list of attributes that are included in in this sub-graph.      */     /**      * (Optional) Lists all of the attributes of this Entity class to be listed in      * the NamedEntityGraph without the need to explicitly list them.  Included      * attributes can still be fully specified by an attribute node referencing a      * sub-graph.      */     /**      * (Optional) This is a list of sub-graphs that are included in the      * entity graph. They  are referenced by name from NamedAttributeNode      * definitions.      */     /**      * (Optional) This is a list of sub-graphs that will add additional attributes      * for subclasses of the entity represented by this NamedEntityGraph to this      * entity graph.  Specified attributes from superclasses are included in      * subclasses.      */ The NamedEntityGraph annotation is applied to an Entity class that is defined as the root of a group of attributes, relationships and related entity classes.  The NamedAttributeNode annotations are embedded within the NamedEntityGraph metadata and define the   attributes of the entity that are included in the grouping.  When a NamedAttributeNode represents an Entity or an Embedded  (including a collection-valued relationship or an element collection of embeddables), the referenced type’s attributes can be included as a NamedSubGraph entry of the entity graph and referenced by name, creating a multi-level entity graph. If the sub-graph represents an entity with inheritance multiple NamedSubGraph entries can be made with the same name to extend the sub-graph for specific subclasses.  The class type that is being extended is required in the NamedSubGraph. /**  * A NamedAttributeNode is a member attribute of a NamedEntityGraph.  *  * @see javax.persistence.NamedEntityGraph  * @see javax.persistence.NamedSubGraph */ @Retention(RUNTIME)     /**      * (Required) the name of the attribute that must be in the sub-graph.      */     /**      * (Optional) if this attribute references a managed type that has its own      * AttributeNodes then this refers to that NamedSubGraph definition.  If the      * target type has inheritance than multiple sub graphs can be specified.      * These additional sub-graphs are intended to add additional attributes from      * subclasses.  Superclass sub-graph entries will be merged into subclasses      * sub-graphs.      */    /**     * (Optional) if the attribute references a Map type this can be used to specify     * a subGraph for the Key in the case of an Entity key type.  A     * keySubGraph can not be specified without the Map attribute also being     * specified.  If the target type has inheritance than multiple sub-graphs can     * be specified.  These additional sub-graphs are intended to add additional     * attributes from subclasses. Superclass sub-graph entries will be merged     * into subclasses sub-graphs.     */ /**  * A NamedSubGraph is a member of a NamedEntityGraph that represents a managed  * type. The NamedSubGraph is only referenced from within a NamedEntityGraph and  * can not be referenced on its own. It is referenced by name from a  * NamedAttributeNode of the NamedEntityGraph.  *  * @see javax.persistence.NamedEntityGraph * @see javax.persistence.NamedAttributeNode  */ @Retention(RUNTIME)     /**      * (Required) the name of the sub-graph as referenced from a      * NamedAttributeNode.      */     /**      * (Optional) the type represented by this subgraph.  Required when this      * subgraph is extending a subclass’ definition.      */     /** (Required)      * if this NamedAttributeNode references a managed type then this list is      * the attributes of that type that must be included.      */ For example: @Entity   @Id   @GeneratedValue   @Basic   @Basic   @OneToMany()   @OneToMany()   @OneToMany()   ... @Entity @Inheritance     @Id     @GeneratedValue     @OneToOne(fetch=FetchType.EAGER)    ... @Entity     @OneToOne(fetch=FetchType.LAZY)    ... @Entity     @Id    ... @Entity     @Id     @Lob     @OneToOne(fetch=FetchType.LAZY)     protected Approval approval    ... @NamedEntityGraph(         @NamedAttributeNode(             value="projects",             subGraph=”projects”         ),         @NamedttributeNode(name = "phoneNumbers"),         @NamedSubGraph(             name=”projects”,                 @NamedAttributeNode("requirements")         ),         @NamedSubGraph(             name=”projects”,             type=LargeProject.class,                 @NamedAttributeNode("approver")         ) ) @NamedEntityGraph(     name-”Project”,         @NamedAttributeNode("requirements")         @NamedSubGraph(             type=LargeProject.class,                 @NamedAttributeNode("approver")         ) ) @NamedEntityGraph(     name="EmployeeProjectRequirements"         @NamedAttributeNode(             value="projects",             subGraph=”projects”         ),         @NamedttributeNode(name = "phoneNumbers"),         @NamedSubGraph(             name=”projects”,                 @NamedAttributeNode(                      value ="requirements",                      subGraph=”requirements”                 )         ),         @NamedSubGraph(             name=”requirements”,                 @NamedAttributeNode("description"),                 @NamedAttributeNode("approval")         ) ) The entity graph could also be created dynamically through the following APIs: /**  * This type represents the root of an entity graph that will be used as a  * template to define the attribute nodes and boundaries of a graph of entities and  * entity relationships. The root must be an Entity type.  *  *            the type of the root entity.  */     /**      * Returns the name of the static EntityGraph.  Will return null if the      * EntityGraph is not a named EntityGraph.      */     /*      * Add an AttributeNode attribute to the entity graph.      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this entity.      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     public &amp;lt;X&amp;gt; void addAttributeNodes(String ...     /*      * Add an AttributeNode attribute to the entity graph.      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     public &amp;lt;X&amp;gt; void     /*      * Used to add a node of the graph that corresponds to a managed type. This      * allows for construction of multi-node Entity graphs that include related      * managed types.      *      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     /**      * Used to add a node of the graph that corresponds to a managed type with      * inheritance.  This allows for multiple subclass sub-graphs to be defined      * for this node of the entity graph. Subclass sub-graphs will include the      * specified attributes of superclass sub-graphs      *      * @throws IllegalArgumentException      *             if the attribute's target type is not a managed type      * @throws IllegalStateException      *             if this EntityGraph has been statically defined      */ addSubGraph(Attribute&amp;lt;T, X&amp;gt; attribute, Class&amp;lt;? extends     /*      * Used to add a node of the graph that corresponds to a managed type. This      * allows for construction of multi-node Entity graphs that include related      * managed types.      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this entity.      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     public &amp;lt;X&amp;gt; SubGraph&amp;lt;X&amp;gt; addSubGraph(String     /**      * Used to add a node of the graph that corresponds to a managed type with      * inheritance.  This allows for multiple subclass sub-graphs to be defined      * for this node of the entity graph.  Subclass sub-graphs will include the      * specified attributes of superclass sub-graphs      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this managed type.      * @throws IllegalArgumentException      *             if the attribute's target type is not a managed type      * @throws IllegalStateException      *             if this EntityGraph has been statically defined      */     public &amp;lt;X&amp;gt; SubGraph&amp;lt;X&amp;gt; addSubGraph(String     /*      * Used to add a node of the graph that corresponds to a map key that is a      * managed type. This allows for construction of multi-node Entity graphs that      * include related managed types.      *      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type entity      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     /*      * Used to add a node of the graph that corresponds to a map key that is a      * managed type with inheritance. This allows for construction of multi-      * node Entity graphs that include related managed types.  Subclass sub-graphs      * will include the specified attributes of superclass sub-graphs      *      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type entity      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */ addKeySubGraph(Attribute&amp;lt;T, X&amp;gt; attribute, Class&amp;lt;?     /*      * Used to add a node of the graph that corresponds to a map key that is a      * managed type. This allows for construction of multi-node Entity graphs that      * include related managed types.      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this entity.      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     /*      * Used to add a node of the graph that corresponds to a map key that is a      * managed type with inheritance. This allows for construction of multi-      * node Entity graphs that include related managed types.  Subclass sub-graphs      * will include the specified attributes of superclass sub-graphs      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this entity.      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     /*      * returns the attributes of this entity that are included in the entity      * graph      */ /**  * Represents an AttributeNode of an entity graph.  */     /*      * returns the Type of the AttributeNode.      */     /*      * returns the name of the referencing attribute.      */ /**  * This type represents a AttributeNode of an EntityGraph that corresponds to a  * Managed Type. Using this class an entity graph can be embedded within an  * EntityGraph.  *  *            the Class type of the AttributeNode.  */ public interface SubGraph&amp;lt;T&amp;gt; extends     /**      * Add an AttributeNode attribute to the entity graph.      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this managed type.      * @throws IllegalStateException      *             if this EntityGraph has been statically defined      */     public &amp;lt;X&amp;gt; void  addAttributeNodes(String ...     /**      * Add an AttributeNode attribute to the entity graph.      *      * @throws IllegalStateException      *             if this EntityGraph has been statically defined      */     public &amp;lt;X&amp;gt; void     /**      * Used to add a node of the graph that corresponds to a managed type. This      * allows for construction of multi-node Entity graphs that include related      * managed types.      *      * @throws IllegalArgumentException      *             if the attribute's target type is not a managed type      * @throws IllegalStateException      *             if this EntityGraph has been statically defined      */     /**      * Used to add a node of the graph that corresponds to a managed type with      * inheritance.  This allows for multiple subclass sub-graphs to be defined      * for this node of the entity graph. Subclass sub-graphs will include the      * specified attributes of superclass sub-graphs      *      * @throws IllegalArgumentException      *             if the attribute's target type is not a managed type      * @throws IllegalStateException      *             if this EntityGraph has been statically defined      */ addSubGraph(Attribute&amp;lt;T, X&amp;gt; attribute, Class&amp;lt;? extends     /**      * Used to add a node of the graph that corresponds to a managed type. This      * allows for construction of multi-node Entity graphs that include related      * managed types.      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this managed type.      * @throws IllegalArgumentException      *             if the attribute's target type is not a managed type      * @throws IllegalStateException      *             if this EntityGraph has been statically defined      */     public &amp;lt;X&amp;gt; SubGraph&amp;lt;X&amp;gt; addSubGraph(String     /**      * Used to add a node of the graph that corresponds to a managed type with      * inheritance.  This allows for multiple subclass sub-graphs to be defined      * for this node of the entity graph. Subclass sub-graphs will include the      * specified attributes of superclass sub-graphs      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this managed type.      * @throws IllegalArgumentException      *             if the attribute's target type is not a managed type      * @throws IllegalStateException      *             if this EntityGraph has been statically defined      */     public &amp;lt;X&amp;gt; SubGraph&amp;lt;X&amp;gt; addSubGraph(String     /*      * Used to add a node of the graph that corresponds to a map key that is a      * managed type. This allows for construction of multi-node Entity graphs that      * include related managed types.      *      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type entity      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     /*      * Used to add a node of the graph that corresponds to a map key that is a      * managed type with inheritance. This allows for construction of multi-      * node Entity graphs that include related managed types.  Subclass sub-graphs      * will include the specified attributes of superclass sub-graphs      *      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type entity      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */ addKeySubGraph(Attribute&amp;lt;T, X&amp;gt; attribute, Class&amp;lt;?     /*      * Used to add a node of the graph that corresponds to a map key that is a      * managed type. This allows for construction of multi-node Entity graphs that      * include related managed types.      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this entity.      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     /*      * Used to add a node of the graph that corresponds to a map key that is a      * managed type with inheritance. This allows for construction of multi-      * node Entity graphs that include related managed types.  Subclass sub-graphs      * will include the specified attributes of superclass sub-graphs      *      * @throws IllegalArgumentException if the attribute is not an attribute of      * this entity.      * @throws IllegalArgumentException if the attribute's target type is not a      * managed type      *      * @throws IllegalStateException if this EntityGraph has been statically      * defined      */     /**      * returns the attributes of this managed type that are included in the      * sub-graph      */     /**      * returns the attribute that references this sub-graph.      /     /**      * returns the type of this sub-graph if it was used to extend a superclass’      * sub-graph definition.      */      ...     /**      * returns any named EntityGraph that has been created through static      * metadata. Returned EntityGraphs should be considered immutable.      */     /**      * add a named copy of the EntityGraph to the EntityManagerFactory      */     public void addNamedEntityGraph(String graphName,     /**      * returns a mutable EntityGraph that can be used to dynamically create an      * EntityGraph.      */     /**      * returns a mutable copy of the named EntityGraph      */     public EntityGraph&amp;lt;?&amp;gt; createEntityGraph(String Example:       EntityGroup employee =    SubGraph largeProjects =           EntityGroup employee =    SubGraph requirements = employee.addSubGraph("projects")                                        Once the EntityGraph is available or has been created it can be applied to operations on the EntityManager or used with queries to control eager loading of results. The standard property javax.persistence.fetchgraph can be used with the find operation or as a query hint to specify a EntityGraph that defines additional FetchType semantics whereby  any AttributeNode of the EntityGraph is treated as FetchType.EAGER and any attribute not specified in the EntityGraph is treated as  FetchType.LAZY .  The primary key of an entity is always retrieved even if not present in the fetch group.  The following characterizes the semantics of entity graphs that are used as fetch graphs. A fetch graph attribute node specified within an entity graph or sub-graph specifies how an attribute is to be fetched.  Attributes that are not specified are treated as FetchType.LAZY . The following rules apply, depending on attribute type. A primary key attribute never needs to be specified in an attribute node.  (This applies to composite primary keys as well, including embedded id primary keys.)  When an entity is fetched, its primary key is always fetched.  It is not incorrect, however, to specify primary key attributes. Attributes other than primary key attributes are not fetched unless the attribute is specified.  The following rules apply to the specification of attributes: The default fetch graph for an entity or embeddable consists of the transitive closure of all of its attributes that are specified as FetchType.EAGER (or defaulted as such). If the attribute is an embedded attribute, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the default fetch graph for the embeddable is fetched.  If a subgraph is specified for the attribute, the attributes of the embeddable are fetched according to their specification in the corresponding subgraph, and the rules of this section recursively apply. If the attribute is an element collection of basic type, and the attribute is specified in an attribute node, the element collection together with its basic elements is fetched. If the attribute is an element collection of embeddables, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the element collection together with the default fetch graph of its embeddable elements is fetched.  If a subgraph is specified for the attribute, the attributes of the embeddable elements will be fetched according to the corresponding subgraph specification, and the rules of this section recursively apply. If the attribute is a one-to-one or many-to-one relationship, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the default fetch graph of the target entity is fetched.  If a subgraph is specified for the attribute, the attributes of the target entity will be fetched according to the corresponding subgraph specification, and the rules of this section recursively apply. If the attribute is a one-to-many or many-to-many relationship, and the attribute is specified in an attribute node, but a subgraph is not specified, the collection will be fetched and the default fetch graphs of the referenced entities will be fetched.  If a subgraph is specified for the attribute, the entities in the collection will be fetched according to the corresponding subgraph specification, and the rules of this section recursively apply. If the key of a map which has been specified in an attribute node is a basic type, it will always be fetched and if the key of a map which has been specified in an attribute node is an embedded type otherwise, if a map key subgraph is not specified for the attribute node, the map key will be fetched according to its default fetch graph.  If a key subgraph is specified for the map key attribute, the map key attribute will be fetched according to the map key subgraph specification, and the rules of this section recursively apply.  So using the following entity graph: @NamedEntityGraph( ) used to load a Phonenumber only the id and the FKs would be retrieved.   When using the following entity graph: @NamedEntityGraph(         @NamedAttributeNode("projects"), ) loading an Employee  the PKs and FKs would be retrieved and the “projects” attribute would be eagerly loaded.  The standard property javax.persistence.loadgraph can be used to specify an entity graph that defines additional FetchType semantics with any attribute node of the entity graph becoming FetchType.EAGER and any unspecified attributes remain as defined in the mapping. The following characterizes the semantics of entity graphs that are used as load graphs. A load graph attribute node specified within an entity graph or sub-graph specifies how an attribute is to be fetched.  Attributes that are not specified are treated according to their default FetchType. The following rules apply to the specification of attributes: A primary key attribute never needs to be specified in an attribute node.  (This applies to composite primary keys as well, including embedded id primary keys.)  When an entity is fetched, its primary key is always fetched.  It is not incorrect, however, to specify primary key attributes. If the attribute is an embedded attribute, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the default fetch graph for the embeddable is fetched. If a subgraph is specified for the attribute, any attributes of the embeddable that are not further specified within the subgraph are treated according to their fetch type; attributes that are specified by the subgraph are also fetched; and the rules of this section recursively apply. If the attribute is an element collection of basic type, and the attribute is specified in an attribute node, the element collection together with its basic elements is fetched.  If the attribute is an element collection of embeddables, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the element collection together with the default fetch graph of its embeddable elements is fetched.  If a subgraph is specified for the attribute, any attributes of the embeddables that are not further specified are treated according to their fetch type; attributes that are specified by the subgraph are also fetched; and the rules of this section recursively apply. If the attribute is a one-to-one or many-to-one relationship, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the default fetch graph of the target entity is fetched.  If a subgraph is specified for the attribute, any attributes of the target entity that are not and the rules of this section recursively apply. If the attribute is a one-to-many or many-to-many relationship, and the attribute is specified in an attribute node, but a subgraph is not specified, the collection will be fetched and the default fetch graphs of the referenced entities will be fetched.  If a subgraph is specified for the attribute, any entities in the and the rules of this section recursively apply. If a collection-valued attribute is a map, keys that are basic or embeddable types will be fetched when the map is fetched; entity map key attributes will be fetched according to their fetch type and, if a key subgraph is specified, additional entity attributes are fetched as specified in the subgraph; and the rules of this section recursively apply. So using the following entity graph: @NamedEntityGraph( ) used to load a Phonenumber only the id and the FKs would be retrieved. The requirements attribute would not be loaded but would be lazy.   When using the following entity graph: @NamedEntityGraph(         @NamedAttributeNode("projects"), ) loading an Employee  the PKs and FKs would be retrieved and the “projects” attribute would be eagerly loaded. EntityManager API     /**      * Copy the provided entity graph  using the EntityGraph as a template to specify      * the attributes that will be copied. Attributes not included in the entity graph will not be copied.      *      * @param entity      *            entity instance      * @param EntityGraph      *            the entity graph template for applying copy.      * @throws IllegalArgumentException      *             if the instance is not an entity or the EntityGraph root is      *             not the same class as the entity.      */     public void copy(Object entity, EntityGraph     ...     /**      * Merge the state of the given entity into the current persistence context      * using the EntityGraph as a template for applying the merge operation.      *      *      * @param entity      *            entity instance      * @param EntityGraph      *            the entity graph template for applying merge.      * @return the managed instance that the state was merged to      * @throws IllegalArgumentException      *             if instance is not an entity, is a removed entity or the root      *             of the EntityGraph is not of the same type as the entity      * @throws TransactionRequiredException      *             if there is no transaction when invoked on a      *             container-managed entity manager of that is of type      *                  */     public &amp;lt;T&amp;gt; T merge(T entity, EntityGraph     ...etc Merge When using an entity graph  in combination with the “merge” operation any listed attribute node will be merged and any unlisted attribute node will not be merged, this includes @Basic mappings.  Any attribute that references a Map is considered to include both the key and the value.  If the attribute node references a sub-graph or map key sub-graph for the relationship target then the merge will operate on that target object as well with only those attributes referenced in the sub-graph being merged and this will apply recursively. For example with the merge operation applied with the following entity graph : @NamedEntityGraph(         @NamedAttributeNode(“name”),         @NamedAttributeNode(             value="projects",             subGraph=”projects”         ),         @NamedAttributeNode("phoneNumbers"),         @NamedSubGraph(             name=”projects”,                 @NamedAttributeNode("requirements")         ) ) Only the name, projects and phoneNumbers attributes of the Employee will be merged.  The merge will be  applied to the projects but only the requirements attribute of the Project will be merged and the merge will not be  applied to the Requirement target.  The PhoneNumber target will not be merged. Copy A new operation “copy” is being introduced.  This operation is intended to be used when a user wishes to disconnect a graph of entities from a larger graph.  When using an entity graph  in combination with the “copy” operation a copy of the provided entity is created and the attributes of the copied entity are populated based on the attributes listed in the entity graph.  If an attribute node for an  attribute is present then the attribute will be copied.  If the attribute node represents an attribute that has a managed type as the target, element or key type then that target instance is copied as well and the copy is set in the new tree.  If a sub-graph is defined for the value or key then any attributes corresponding to the attribute nodes within the sub-graph will be copied as above.  If no sub-graph is defined for the attribute node then no attributes of the target are copied. For example with the copy operation applied with the following entity graph : @NamedEntityGraph(         @NamedAttributeNode(“name”),         @NamedAttributeNode(             value="projects",             subGraph=”projects”         ),         @NamedAttributeNode("phoneNumbers"),         @NamedSubGraph(             name=”projects”,                 @NamedAttributeNode("requirements")         ) ) When applied to an Employee instance a new copy of the Employee will be created and the “name” , “phoneNumbers” and “projects” attributes will be copied over.  For each PhoneNumber in the phoneNumbers list a new copy of the PhoneNumber will be created and used but no attributes will be copied.  For each Project in the projects list a new copy of the Project will be created and used and only the requirements attribute will be copied.  For each Requirement in the requirements list a new copy of the Requirement will be created but no attributes will be copied.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Wed Nov 28 19:15:21 CET 2012</date>
    <body>Hello Oliver,    A couple responses to your suggestions. The "Named" prefix was added as a solution to a package naming issue.  If the interfaces and the annotations have the same names what packages will they be placed in? Yes, --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Wed Nov 28 19:48:10 CET 2012</date>
    <body>Hi Oliver, Thanks for the feedback.  I'll just comment here on your item (1) for now. There is a problem here with regard to naming (which the relatively straightforward examples in the proposal don't expose) -- namely that attributes at different levels of the entity graph (e.g., attributes of different entities) might have the same attribute names, but would have different subgraphs associated with them.  Hence we need to use a "link" name that associates the attribute with the corresponding subgraph, resulting in a more complex (and therefore nested) annotation structure.  In the examples, the attribute names and these linking names are the same, but for those cases this wouldn't work. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Wed Nov 28 21:32:59 CET 2012</date>
    <body>Thank you for bringing Fetch Plan for discussion and an extensive proposal to realize it. Few comments: 1. The naming could be FetchPlan/FetchAttribute etc as they are more commonly used than NamedEntityGraph etc. 2. The need for a SubGraph may not be required. If every FetchGroup/NamedEntityGraph defined/rooted in entity type X can include FetchGroup(s) defined in other entity type Y, then a separate definition of SubGraph would not be necessary. 3. The fetch attributes can simply be persistent property names  (i.e. Strings) to reduce verbosity of annotations/XML descriptors. The type information is already available through MetaModel and entity type definitions, so why would the graph specification require them? 4. A concept of recursion depth is required on relationship paths to control the cycles in a graph. 5. With our usage of Fetch Plans, we found it useful to control the maximum fetch depth (counted as a number of edges from the root to a leaf node being fetched) 6. A lock specification could be an optional qualifier on relation paths   Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Gordon Yorke ---11/28/2012 05:32:05 AM---Hello All,     Based on suggestions from a colleague I would like to suggest some From: To: jsr338-experts@... Date: 11/28/2012 05:32 AM Subject: [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc... Hello All,   Based on suggestions from a colleague I would like to suggest some clarification and updates.   1 - with respect to @Version add to the proposal that version is always fetched and copied along with the PK.   2 - Add :    /**     * returns all named EntityGraphs that have been defined for the provided     * class type.     */   3 - Fix EntityManager.copy : Thank you, --Gordon   Hello all,   Please find below a proposal for EntityGraphs and their use to define entity graph fetching, loading, merging and copying.  I have been working with Mike Keith and Linda DeMichiel on this proposal and we feel it is time to present to the expert group.  Please have a look and provide feedback as the intention is to integrate this within the 2.1 spec. Thank you, Gordon ---- A common functional request has been to enable users to dynamically define how an EntityManager operation or query should apply to an entity graph.  This may more commonly be known as “merge groups” or “fetch plans”.  This functionality can be especially important when detached entities must be serialized and then merged into an active Persistence Context. An entity graph is a template defined in the form of metadata or an object created by the dynamic EntityGraph API that captures the path and boundaries for an operation or a query   For static metadata the NamedEntityGraph annotation defines the attribute nodes and boundaries of the grouping. @Retention(RUNTIME) @Retention(RUNTIME)    /**     * (Optional) The name of the sub-graph.     * Defaults to the entity name of the root entity.     */    /**     * (Required) list of attributes that are included in in this sub-graph.     */    /**     * (Optional) Lists all of the attributes of this Entity class to be listed in     * the NamedEntityGraph without the need to explicitly list them.  Included     * attributes can still be fully specified by an attribute node referencing a     * sub-graph.     */    /**     * (Optional) This is a list of sub-graphs that are included in the     * entity graph. They  are referenced by name from NamedAttributeNode     * definitions.     */    /**     * (Optional) This is a list of sub-graphs that will add additional attributes     * for subclasses of the entity represented by this NamedEntityGraph to this     * entity graph.  Specified attributes from superclasses are included in     * subclasses.     */ The NamedEntityGraph annotation is applied to an Entity class that is defined as the root of a group of attributes, relationships and related entity classes.  The NamedAttributeNode annotations are embedded within the NamedEntityGraph metadata and define the   attributes of the entity that are included in the grouping.  When a NamedAttributeNode represents an Entity or an Embedded  (including a collection-valued relationship or an element collection of embeddables), the referenced type’s attributes can be included as a NamedSubGraph entry of the entity graph and referenced by name, creating a multi-level entity graph. If the sub-graph represents an entity with inheritance multiple NamedSubGraph entries can be made with the same name to extend the sub-graph for specific subclasses.  The class type that is being extended is required in the NamedSubGraph. /** * A NamedAttributeNode is a member attribute of a NamedEntityGraph. * * @see javax.persistence.NamedEntityGraph * @see javax.persistence.NamedSubGraph */ @Retention(RUNTIME)    /**     * (Required) the name of the attribute that must be in the sub-graph.     */    /**     * (Optional) if this attribute references a managed type that has its own     * AttributeNodes then this refers to that NamedSubGraph definition.  If the     * target type has inheritance than multiple sub graphs can be specified.     * These additional sub-graphs are intended to add additional attributes from     * subclasses.  Superclass sub-graph entries will be merged into subclasses     * sub-graphs.     */   /**    * (Optional) if the attribute references a Map type this can be used to specify    * a subGraph for the Key in the case of an Entity key type.  A    * keySubGraph can not be specified without the Map attribute also being    * specified.  If the target type has inheritance than multiple sub-graphs can    * be specified.  These additional sub-graphs are intended to add additional    * attributes from subclasses. Superclass sub-graph entries will be merged    * into subclasses sub-graphs.    */ /** * A NamedSubGraph is a member of a NamedEntityGraph that represents a managed * type. The NamedSubGraph is only referenced from within a NamedEntityGraph and * can not be referenced on its own. It is referenced by name from a * NamedAttributeNode of the NamedEntityGraph. * * @see javax.persistence.NamedEntityGraph * @see javax.persistence.NamedAttributeNode */ @Retention(RUNTIME)    /**     * (Required) the name of the sub-graph as referenced from a     * NamedAttributeNode.     */    /**     * (Optional) the type represented by this subgraph.  Required when this     * subgraph is extending a subclass’ definition.     */    /** (Required)     * if this NamedAttributeNode references a managed type then this list is     * the attributes of that type that must be included.     */ For example: @Entity  @Id  @GeneratedValue  @Basic  @Basic  @OneToMany()  @OneToMany()  @OneToMany()  ... @Entity @Inheritance    @Id    @GeneratedValue    @OneToOne(fetch=FetchType.EAGER)   ... @Entity    @OneToOne(fetch=FetchType.LAZY)   ... @Entity    @Id   ... @Entity    @Id    @Lob    @OneToOne(fetch=FetchType.LAZY)    protected Approval approval   ... @NamedEntityGraph(        @NamedAttributeNode(            value="projects",            subGraph=”projects”        ),        @NamedttributeNode(name = "phoneNumbers"),        @NamedSubGraph(            name=”projects”,                @NamedAttributeNode("requirements")        ),        @NamedSubGraph(            name=”projects”,            type=LargeProject.class,                @NamedAttributeNode("approver")        ) ) @NamedEntityGraph(    name-”Project”,        @NamedAttributeNode("requirements")        @NamedSubGraph(            type=LargeProject.class,                @NamedAttributeNode("approver")        ) ) @NamedEntityGraph(    name="EmployeeProjectRequirements"        @NamedAttributeNode(            value="projects",            subGraph=”projects”        ),        @NamedttributeNode(name = "phoneNumbers"),        @NamedSubGraph(            name=”projects”,                @NamedAttributeNode(                     value ="requirements",                     subGraph=”requirements”                )        ),        @NamedSubGraph(            name=”requirements”,                @NamedAttributeNode("description"),                @NamedAttributeNode("approval")        ) ) The entity graph could also be created dynamically through the following APIs: /** * This type represents the root of an entity graph that will be used as a * template to define the attribute nodes and boundaries of a graph of entities and * entity relationships. The root must be an Entity type. * *            the type of the root entity. */    /**     * Returns the name of the static EntityGraph.  Will return null if the     * EntityGraph is not a named EntityGraph.     */    /*     * Add an AttributeNode attribute to the entity graph.     *     * @throws IllegalArgumentException if the attribute is not an attribute of     * this entity.     * @throws IllegalStateException if this EntityGraph has been statically     * defined     */    /*     * Add an AttributeNode attribute to the entity graph.     *     * @throws IllegalStateException if this EntityGraph has been statically     * defined     */    /*     * Used to add a node of the graph that corresponds to a managed type. This     * allows for construction of multi-node Entity graphs that include related     * managed types.     *     * @throws IllegalArgumentException if the attribute's target type is not a     * managed type     *     * @throws IllegalStateException if this EntityGraph has been statically     * defined     */    /**     * Used to add a node of the graph that corresponds to a managed type with     * inheritance.  This allows for multiple subclass sub-graphs to be defined     * for this node of the entity graph. Subclass sub-graphs will include the     * specified attributes of superclass sub-graphs     *     * @throws IllegalArgumentException     *             if the attribute's target type is not a managed type     * @throws IllegalStateException     *             if this EntityGraph has been statically defined     */    /*     * Used to add a node of the graph that corresponds to a managed type. This     * allows for construction of multi-node Entity graphs that include related     * managed types.     *     * @throws IllegalArgumentException if the attribute is not an attribute of     * this entity.     * @throws IllegalArgumentException if the attribute's target type is not a     * managed type     *     * @throws IllegalStateException if this EntityGraph has been statically     * defined     */    /**     * Used to add a node of the graph that corresponds to a managed type with     * inheritance.  This allows for multiple subclass sub-graphs to be defined     * for this node of the entity graph.  Subclass sub-graphs will include the     * specified attributes of superclass sub-graphs     *     * @throws IllegalArgumentException if the attribute is not an attribute of     * this managed type.     * @throws IllegalArgumentException     *             if the attribute's target type is not a managed type     * @throws IllegalStateException     *             if this EntityGraph has been statically defined     */    /*     * Used to add a node of the graph that corresponds to a map key that is a     * managed type. This allows for construction of multi-node Entity graphs that     * include related managed types.     *     * @throws IllegalArgumentException if the attribute's target type is not a     * managed type entity     *     * @throws IllegalStateException if this EntityGraph has been statically     * defined     */    /*     * Used to add a node of the graph that corresponds to a map key that is a     * managed type with inheritance. This allows for construction of multi-     * node Entity graphs that include related managed types.  Subclass sub-graphs     * will include the specified attributes of superclass sub-graphs     *     * @throws IllegalArgumentException if the attribute's target type is not a     * managed type entity     *     * @throws IllegalStateException if this EntityGraph has been statically     * defined     */    /*     * Used to add a node of the graph that corresponds to a map key that is a     * managed type. This allows for construction of multi-node Entity graphs that     * include related managed types.     *     * @throws IllegalArgumentException if the attribute is not an attribute of     * this entity.     * @throws IllegalArgumentException if the attribute's target type is not a     * managed type     *     * @throws IllegalStateException if this EntityGraph has been statically     * defined     */    /*     * Used to add a node of the graph that corresponds to a map key that is a     * managed type with inheritance. This allows for construction of multi-     * node Entity graphs that include related managed types.  Subclass sub-graphs     * will include the specified attributes of superclass sub-graphs     *     * @throws IllegalArgumentException if the attribute is not an attribute of     * this entity.     * @throws IllegalArgumentException if the attribute's target type is not a     * managed type     *     * @throws IllegalStateException if this EntityGraph has been statically     * defined     */    /*     * returns the attributes of this entity that are included in the entity     * graph     */ /** * Represents an AttributeNode of an entity graph. */    /*     * returns the Type of the AttributeNode.     */    /*     * returns the name of the referencing attribute.     */ /** * This type represents a AttributeNode of an EntityGraph that corresponds to a * Managed Type. Using this class an entity graph can be embedded within an * EntityGraph. * *            the Class type of the AttributeNode. */    /**     * Add an AttributeNode attribute to the entity graph.     *     * @throws IllegalArgumentException if the attribute is not an attribute of     * this managed type.     * @throws IllegalStateException     *             if this EntityGraph has been statically defined     */    /**     * Add an AttributeNode attribute to the entity graph.     *     * @throws IllegalStateException     *             if this EntityGraph has been statically defined     */    /**     * Used to add a node of the graph that corresponds to a managed type. This     * allows for construction of multi-node Entity graphs that include related     * managed types.     *     * @throws IllegalArgumentException     *             if the attribute's target type is not a managed type     * @throws IllegalStateException     *             if this EntityGraph has been statically defined     */    /**     * Used to add a node of the graph that corresponds to a managed type with     * inheritance.  This allows for multiple subclass sub-graphs to be defined     * for this node of the entity graph. Subclass sub-graphs will include the     * specified attributes of superclass sub-graphs     *     * @throws IllegalArgumentException     *             if the attribute's target type is not a managed type     * @throws IllegalStateException     *             if this EntityGraph has been statically defined     */    /**     * Used to add a node of the graph that corresponds to a managed type. This     * allows for construction of multi-node Entity graphs that include related     * managed types.     *     * @throws IllegalArgumentException if the attribute is not an attribute of     * this managed type.     * @throws IllegalArgumentException     *             if the attribute's target type is not a managed type     * @throws IllegalStateException     *             if this EntityGraph has been statically defined     */    /**     * Used to add a node of the graph that corresponds to a managed type with     * inheritance.  This allows for multiple subclass sub-graphs to be defined     * for this node of the entity graph. Subclass sub-graphs will include the     * specified attributes of superclass sub-graphs     *     * @throws IllegalArgumentException if the attribute is not an attribute of     * this managed type.     * @throws IllegalArgumentException     *             if the attribute's target type is not a managed type     * @throws IllegalStateException     *             if this EntityGraph has been statically defined     */    /*     * Used to add a node of the graph that corresponds to a map key that is a     * managed type. This allows for construction of multi-node Entity graphs that     * include related managed types.     *     * @throws IllegalArgumentException if the attribute's target type is not a     * managed type entity     *     * @throws IllegalStateException if this EntityGraph has been statically     * defined     */    /*     * Used to add a node of the graph that corresponds to a map key that is a     * managed type with inheritance. This allows for construction of multi-     * node Entity graphs that include related managed types.  Subclass sub-graphs     * will include the specified attributes of superclass sub-graphs     *     * @throws IllegalArgumentException if the attribute's target type is not a     * managed type entity     *     * @throws IllegalStateException if this EntityGraph has been statically     * defined     */    /*     * Used to add a node of the graph that corresponds to a map key that is a     * managed type. This allows for construction of multi-node Entity graphs that     * include related managed types.     *     * @throws IllegalArgumentException if the attribute is not an attribute of     * this entity.     * @throws IllegalArgumentException if the attribute's target type is not a     * managed type     *     * @throws IllegalStateException if this EntityGraph has been statically     * defined     */    /*     * Used to add a node of the graph that corresponds to a map key that is a     * managed type with inheritance. This allows for construction of multi-     * node Entity graphs that include related managed types.  Subclass sub-graphs     * will include the specified attributes of superclass sub-graphs     *     * @throws IllegalArgumentException if the attribute is not an attribute of     * this entity.     * @throws IllegalArgumentException if the attribute's target type is not a     * managed type     *     * @throws IllegalStateException if this EntityGraph has been statically     * defined     */    /**     * returns the attributes of this managed type that are included in the     * sub-graph     */    /**     * returns the attribute that references this sub-graph.     /    /**     * returns the type of this sub-graph if it was used to extend a superclass’     * sub-graph definition.      */     ...    /**     * returns any named EntityGraph that has been created through static     * metadata. Returned EntityGraphs should be considered immutable.     */    /**     * add a named copy of the EntityGraph to the EntityManagerFactory     */    /**     * returns a mutable EntityGraph that can be used to dynamically create an     * EntityGraph.     */    /**     * returns a mutable copy of the named EntityGraph     */ Example:               SubGraph requirements = employee.addSubGraph("projects") Once the EntityGraph is available or has been created it can be applied to operations on the EntityManager or used with queries to control eager loading of results. The standard property javax.persistence.fetchgraph  can be used with the find operation or as a query hint to specify a EntityGraph that defines additional FetchType semantics whereby  any AttributeNode of the EntityGraph is treated as FetchType.EAGER  and any attribute not specified in the EntityGraph is treated as   FetchType.LAZY .  The primary key of an entity is always retrieved even if not present in the fetch group.   The following characterizes the semantics of entity graphs that are used as fetch graphs. A fetch graph attribute node specified within an entity graph or sub-graph specifies how an attribute is to be fetched.  Attributes that are not specified are treated as FetchType.LAZY . The following rules apply, depending on attribute type. A primary key attribute never needs to be specified in an attribute node.  (This applies to composite primary keys as well, including embedded id primary keys.)  When an entity is fetched, its primary key is always fetched.  It is not incorrect, however, to specify primary key attributes. Attributes other than primary key attributes are not fetched unless the attribute is specified.  The following rules apply to the specification of attributes: The default fetch graph for an entity or embeddable consists of the transitive closure of all of its attributes that are specified as  FetchType.EAGER  (or defaulted as such). If the attribute is an embedded attribute, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the default fetch graph for the embeddable is fetched.  If a subgraph is specified for the attribute, the attributes of the embeddable are fetched according to their specification in the corresponding subgraph, and the rules of this section recursively apply. If the attribute is an element collection of basic type, and the attribute is specified in an attribute node, the element collection together with its basic elements is fetched. If the attribute is an element collection of embeddables, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the element collection together with the default fetch graph of its embeddable elements is fetched.  If a subgraph is specified for the attribute, the attributes of the embeddable elements will be fetched according to the corresponding subgraph specification, and the rules of this section recursively apply. If the attribute is a one-to-one or many-to-one relationship, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the default fetch graph of the target entity is fetched.  If a subgraph is specified for the attribute, the attributes of the target entity will be fetched according to the corresponding subgraph specification, and the rules of this section recursively apply. If the attribute is a one-to-many or many-to-many relationship, and the attribute is specified in an attribute node, but a subgraph is not specified, the collection will be fetched and the default fetch graphs of the referenced entities will be fetched.  If a subgraph is specified for the attribute, the entities in the collection will be fetched according to the corresponding subgraph specification, and the rules of this section recursively apply. If the key of a map which has been specified in an attribute node is a basic type, it will always be fetched and if the key of a map which has been specified in an attribute node is an embedded type the default fetch graph will be fetched for the embeddable; otherwise, if a map key subgraph is not specified for the attribute node, the map key will be fetched according to its default fetch graph.  If a key subgraph is specified for the map key attribute, the map key attribute will be fetched according to the map key subgraph specification, and the rules of this section recursively apply.   So using the following entity graph: @NamedEntityGraph( ) used to load a Phonenumber only the id and the FKs would be retrieved.   When using the following entity graph: @NamedEntityGraph(        @NamedAttributeNode("projects"), ) loading an Employee  the PKs and FKs would be retrieved and the “projects” attribute would be eagerly loaded.   The standard property javax.persistence.loadgraph  can be used to specify an entity graph that defines additional FetchType semantics with any attribute node of the entity graph becoming FetchType.EAGER  and any unspecified attributes remain as defined in the mapping. The following characterizes the semantics of entity graphs that are used as load graphs. A load graph attribute node specified within an entity graph or sub-graph specifies how an attribute is to be fetched.  Attributes that are not specified are treated according to their default FetchType. The following rules apply to the specification of attributes: A primary key attribute never needs to be specified in an attribute node.  (This applies to composite primary keys as well, including embedded id primary keys.)  When an entity is fetched, its primary key is always fetched.  It is not incorrect, however, to specify primary key attributes. If the attribute is an embedded attribute, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the default fetch graph for the embeddable is fetched. If a subgraph is specified for the attribute, any attributes of the embeddable that are not further specified within the subgraph are treated according to their fetch type; attributes that are specified by the subgraph are also fetched; and the rules of this section recursively apply. If the attribute is an element collection of basic type, and the attribute is specified in an attribute node, the element collection together with its basic elements is fetched.  If the attribute is an element collection of embeddables, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the element collection together with the default fetch graph of its embeddable elements is fetched.  If a subgraph is specified for the attribute, any attributes of the embeddables that are not further specified are treated according to their fetch type; attributes that are specified by the subgraph are also fetched; and the rules of this section recursively apply. If the attribute is a one-to-one or many-to-one relationship, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the default fetch graph of the target entity is fetched.  If a subgraph is specified for the attribute, any attributes of the target entity that are not further specified are treated according to their fetch type; attributes that are specified by the subgraph are also fetched; and the rules of this section recursively apply. If the attribute is a one-to-many or many-to-many relationship, and the attribute is specified in an attribute node, but a subgraph is not specified, the collection will be fetched and the default fetch graphs of the referenced entities will be fetched.  If a subgraph is specified for the attribute, any entities in the collection will be treated according to their fetch type; attributes that are specified by the subgraph are also fetched; and the rules of this section recursively apply. If a collection-valued attribute is a map, keys that are basic or embeddable types will be fetched when the map is fetched; entity map key attributes will be fetched according to their fetch type and, if a key subgraph is specified, additional entity attributes are fetched as specified in the subgraph; and the rules of this section recursively apply. So using the following entity graph: @NamedEntityGraph( ) used to load a Phonenumber only the id and the FKs would be retrieved. The requirements attribute would not be loaded but would be lazy.   When using the following entity graph: @NamedEntityGraph(        @NamedAttributeNode("projects"), ) loading an Employee  the PKs and FKs would be retrieved and the “projects” attribute would be eagerly loaded. EntityManager API    /**     * Copy the provided entity graph  using the EntityGraph as a template to specify     * the attributes that will be copied. Attributes not included in the entity graph will not be copied.     *     * @param entity     *            entity instance     * @param EntityGraph     *            the entity graph template for applying copy.     * @throws IllegalArgumentException     *             if the instance is not an entity or the EntityGraph root is     *             not the same class as the entity.     */    ...    /**     * Merge the state of the given entity into the current persistence context     * using the EntityGraph as a template for applying the merge operation.     *     *     * @param entity     *            entity instance     * @param EntityGraph     *            the entity graph template for applying merge.     * @return the managed instance that the state was merged to     * @throws IllegalArgumentException     *             if instance is not an entity, is a removed entity or the root     *             of the EntityGraph is not of the same type as the entity     * @throws TransactionRequiredException     *             if there is no transaction when invoked on a     *             container-managed entity manager of that is of type     */    ...etc Merge When using an entity graph  in combination with the “merge” operation any listed attribute node will be merged and any unlisted attribute node will not be merged, this includes @Basic  mappings.  Any attribute that references a Map is considered to include both the key and the value.  If the attribute node references a sub-graph or map key sub-graph for the relationship target then the merge will operate on that target object as well with only those attributes referenced in the sub-graph being merged and this will apply recursively. For example with the merge operation applied with the following entity graph : @NamedEntityGraph(        @NamedAttributeNode(“name”),        @NamedAttributeNode(            value="projects",            subGraph=”projects”        ),        @NamedAttributeNode("phoneNumbers"),        @NamedSubGraph(            name=”projects”,                @NamedAttributeNode("requirements")        ) ) Only the name, projects and phoneNumbers attributes of the Employee will be merged.  The merge will be  applied to the projects but only the requirements attribute of the Project will be merged and the merge will not be  applied to the Requirement target.  The PhoneNumber target will not be merged. Copy A new operation “copy” is being introduced.  This operation is intended to be used when a user wishes to disconnect a graph of entities from a larger graph.  When using an entity graph  in combination with the “copy” operation a copy of the provided entity is created and the attributes of the copied entity are populated based on the attributes listed in the entity graph.  If an attribute node for an  attribute is present then the attribute will be copied.  If the attribute node represents an attribute that has a managed type as the target, element or key type then that target instance is copied as well and the copy is set in the new tree.  If a sub-graph is defined for the value or key then any attributes corresponding to the attribute nodes within the sub-graph will be copied as above.  If no sub-graph is defined for the attribute node then no attributes of the target are copied. For example with the copy operation applied with the following entity graph : @NamedEntityGraph(        @NamedAttributeNode(“name”),        @NamedAttributeNode(            value="projects",            subGraph=”projects”        ),        @NamedAttributeNode("phoneNumbers"),        @NamedSubGraph(            name=”projects”,                @NamedAttributeNode("requirements")        ) ) When applied to an Employee instance a new copy of the Employee will be created and the “name” , “phoneNumbers” and “projects” attributes will be copied over.  For each PhoneNumber in the phoneNumbers list a new copy of the PhoneNumber will be created and used but no attributes will be copied.  For each Project in the projects list a new copy of the Project will be created and used and only the requirements attribute will be copied.  For each Requirement in the requirements list a new copy of the Requirement will be created but no attributes will be copied.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Thu Nov 29 14:08:51 CET 2012</date>
    <body>Hi Gordon, On Tue, Nov 27, 2012 at 10:20 PM, Gordon Yorke gordon.yorke@... Attributes other than primary key attributes are not fetched unless the attribute is specified.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Thu Nov 29 15:47:29 CET 2012</date>
    <body>+1 In all other areas the spec calls out "partial loading" as optional. When I read this new proposal I walked away with the same feeling of a seeming disconnect in that same regard; that the proposal was trying to make partial loading a mandate on providers.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Thu Nov 29 17:54:00 CET 2012</date>
    <body>Pinaki/all, On Wed, Nov 28, 2012 at 9:32 PM, Pinaki Poddar ppoddar@... Thank you for bringing Fetch Plan for discussion and an extensive proposal to realize it. Few comments: 1. The naming could be FetchPlan/FetchAttribute etc as they are more commonly used than NamedEntityGraph etc.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Thu Nov 29 19:29:22 CET 2012</date>
    <body>Hello Pinaki and others,     Comments inline: Thank you, --Gordon On 28/11/2012 4:32 PM, Pinaki Poddar Thank you for bringing Fetch Plan for discussion and an extensive proposal to realize it. Few comments: 1. The naming could be FetchPlan/FetchAttribute etc as they are more commonly used than NamedEntityGraph etc.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Thu Nov 29 19:31:00 CET 2012</date>
    <body>Hello Christian,    Yes the intention is to require only the FetchType.LAZY lazy semantics and FetchType.LAZY is  intentionally used in the definition of the behaviour.  We can update the language to make it clearer. --Gordon On 29/11/2012 9:08 AM, Christian von Hi Gordon, On Tue, Nov 27, 2012 at 10:20 PM, Gordon Yorke gordon.yorke@... &lt;blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"&gt; Attributes other than primary key attributes are not fetched unless the attribute is specified.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Thu Nov 29 19:36:10 CET 2012</date>
    <body>Hello Werner, This suggests, there will be XML override like in other areas, correct?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Fri Nov 30 16:02:10 CET 2012</date>
    <body>Hi Gordon, Pinaki, all Gordon Yorke, am 29 Nov 2012 hast Du um 14:29 zum Thema "[jsr338- experts] Re: Proposal for EntityGraphs, f"  geschrieben :  &amp;gt; Few comments:  &amp;gt; 1. The naming could be FetchPlan/FetchAttribute etc as they are more   &amp;gt; commonly used than NamedEntityGraph etc.  But the EntityGraph is used for more than just FetchPlans.  Naming it   FetchPlan would give the wrong focus. I have always been thinking of the artefact as a contract between  provider and consumer about the attributes that have to be loaded. You  may use these load contracts for find() and queries, but I could also  imagine them with refresh() and detach() or even as properties of a  persistence context. When you merge entities, you inform the jpa  provider about the load contract you have received them under.  &amp;gt; 2. The need for a SubGraph may not be required. If every   &amp;gt; FetchGroup/NamedEntityGraph defined/rooted in entity type X can   &amp;gt; include FetchGroup(s) defined in other entity type Y, then a separate   &amp;gt; definition of SubGraph would not be necessary.  With this pattern it is very difficult for users to track the contents   of an entity graph as the details would be spread out all over the   object model.  With the proposed pattern the information is in one   location and easy to maintain. This depends a bit on your perspective on the artefact, I would say.  You may either see it as an autonomous construct that overrules the  entity definition, or you could say, an entity offers views of itself  that contribute to different contracts. By the way, I have also been thinking whether these  EntityGraphs/FetchPlans/LoadContracts might be designed in a similar  way as bean validation is, such that you may annotate an entity  attribute with something like @Eager(contracts=AccountantView.class)  or @Load(contracts=AccountantView.class)  where AccountantView is a tagging interface. Avowedly, providing a  dynamic way to create such artefacts might become difficult... Concerning the copy operation, I think, I have not fully understood the  copied entity's state and identity. Besides, I find the coexistence of properties fetchgraph and loadgraph  a bit confusing and would prefer if a decision could be made for just  one of them. Best regards Rainer --- Rainer Schweigkoffer               SAP AG Walldorf Regulatory Compliance              TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, F.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...       fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Lars  Dalgaard, Luisa Deplazes Delgado, Bill McDermott (Co-CEO),  Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP  Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse  oder sonstige vertrauliche Informationen enthalten.  Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist  Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung  oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte  benachrichtigen Sie uns und vernichten Sie die empfangene  E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged,  undisclosed, or otherwise confidential information. If you  have received this e-mail in error, you are hereby  notified that any review, copying, or distribution of it  is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your  cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Fri Nov 30 21:52:01 CET 2012</date>
    <body>Hello Rainer,    Please find some inline comments below. --Gordon For operations like detach() the user may not want to force the data to be loaded but just detach the entity tree as is.  Passing a FetchGroup in that case would be counter intuitive.   We may also find uses for the entity graphs separate from EntityManager operations in which case FetchGroup would not be appropriate.  It is better to define a more flexible design now. My concern with this pattern is readability.  Users must scan through the entire object model to determine what is in an entity graph and what will be loaded or what is missing for their case. This pattern also can get quite complicated quickly when there are multiple "contracts" in place for a particular entity. What parts are unclear? a loadgraph is just a shortcut for users that allows them to specify what relationships should be loaded without requiring all mapping configured EAGER attributes to be specified.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Row count for criteria query</header>
    <date>Sun Dec 02 16:24:01 CET 2012</date>
    <body>Hi, I know I'm a little bit late to the game but unfortunately I only caught wind of this great effort two days ago. I hope there is still room for some small changes and additions. I've searched the mailing list and could not find any reference to having access to the row count a query generated. This metadata is crucial when performing pagination against a large resultset. The current workaround is usually implemented as two db queries- one for the data and one for the count. It seems to me that this metadata can reside on the Query interface as a getter. I'd really appreciate feedback on this issue. BTW, if I should have posted to the issues mailing list please update me and I'll post to there from now on. Two stackoverflow questions on the same subject: http://stackoverflow.com/questions/5349264/total-row-count-for-paginati on-using-jpa-criteria-api http://stackoverflow.com/questions/3997587/jpa-2-0-count-for-arbitrary- criteriaquery</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Mon Dec 03 18:21:46 CET 2012</date>
    <body>Hello Rainer et al,   The specification could be also viewed as a syntax of describing subgraphs of the entire graph of a domain model as described by the Java language itself. These subgraphs are then attached to the persistence operations such as query, merge or detach. In that sense they are an elaborate mechnics of how a typical persietence operation will cascde. In this view, I do think the proposal's usage of "EntityGraph" as a better moniker than "FetchPlan" - though it may be more common term as the concept originally appeared in the context of fecthing of objects and their relations.    The aspect that I am not much comfortable is the SubGraph concept. My view is each entity describes subgraphs that refer to its own properties and includes subgraphs that are rooted in other entities. Such a mechanics will be able to generate a large variety of combinations. The same capability will become too verbose if we tend to define these combinations in a single root entity through the SubGraph annotations as I understand of the proposal.    On a specifc issue of what are the extra qualifiers on the edges of such subgraphs:    a) Some of the qualifiers such as lock mode as these edges are traversed does not control topology of the subgraph but merely adds qualities to the persistent operatons that cascade along the edges. They are nice to have.    b) filters (as, say, in xpath) could be considered -- but may be at a later phase.    c) recursion depth as a limit on the number of times a edge can be traversed. This recursion depth seems to be essential. But let me understand the proposal on that aspect better through the following use case: Let us say we have two domian entities: File and Directory. Directory extends File. Every File (except the root) has a parent which is a Directory. A directory can have zero or more Files, some of which can be directories. Now a user wants to say: "Query for a directory named 'X' and get me all children 3 level down" How is that "3-level down" expressed in the proposal?   Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware "Rainer Kwesi Schweigkoffer" ---11/30/2012 07:15:59 AM---Hi Gordon, Pinaki, all Gordon Yorke, am 29 Nov 2012 hast Du um 14:29 zum Thema "[jsr338- From: To: Gordon Yorke &amp;lt;gordon.yorke@...&amp;gt;, jsr338-experts@..., Pinaki Poddar/Dallas/IBM@IBMUS Date: 11/30/2012 07:15 AM Subject: Re: [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc... Hi Gordon, Pinaki, all Gordon Yorke, am 29 Nov 2012 hast Du um 14:29 zum Thema "[jsr338- experts] Re: Proposal for EntityGraphs, f"  geschrieben : I have always been thinking of the artefact as a contract between provider and consumer about the attributes that have to be loaded. You may use these load contracts for find() and queries, but I could also imagine them with refresh() and detach() or even as properties of a persistence context. When you merge entities, you inform the jpa provider about the load contract you have received them under. This depends a bit on your perspective on the artefact, I would say. You may either see it as an autonomous construct that overrules the entity definition, or you could say, an entity offers views of itself that contribute to different contracts. By the way, I have also been thinking whether these EntityGraphs/FetchPlans/LoadContracts might be designed in a similar way as bean validation is, such that you may annotate an entity attribute with something like @Eager(contracts=AccountantView.class) or @Load(contracts=AccountantView.class) where AccountantView is a tagging interface. Avowedly, providing a dynamic way to create such artefacts might become difficult... Concerning the copy operation, I think, I have not fully understood the copied entity's state and identity. Besides, I find the coexistence of properties fetchgraph and loadgraph a bit confusing and would prefer if a decision could be made for just one of them. Best regards Rainer --- Rainer Schweigkoffer               SAP AG Walldorf Regulatory Compliance              TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, F.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...       fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Lars Dalgaard, Luisa Deplazes Delgado, Bill McDermott (Co-CEO), Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP Supervisory Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse oder sonstige vertrauliche Informationen enthalten. Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte benachrichtigen Sie uns und vernichten Sie die empfangene E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged, undisclosed, or otherwise confidential information. If you have received this e-mail in error, you are hereby notified that any review, copying, or distribution of it is strictly prohibited. Please inform us immediately and destroy the original transmittal. Thank you for your cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Mon Dec 03 19:37:47 CET 2012</date>
    <body>Hi Gordon, all, Gordon Yorke, am 30 Nov 2012 hast Du um 16:52 zum Thema "[jsr338- experts] Re: Proposal for EntityGraphs, f"  geschrieben :  &amp;gt; I have always been thinking of the artefact as a contract between  &amp;gt; provider and consumer about the attributes that have to be loaded. You  &amp;gt; may use these load contracts for find() and queries, but I could also  &amp;gt; imagine them with refresh() and detach() or even as properties of a  &amp;gt; persistence context. When you merge entities, you inform the jpa  &amp;gt; provider about the load contract you have received them under.  For operations like detach() the user may not want to force the data to   be loaded but just detach the entity tree as is.  Passing a FetchGroup   in that case would be counter intuitive.   We may also find uses for the   entity graphs separate from EntityManager operations in which case   FetchGroup would not be appropriate.  It is better to define a more   flexible design now. Well, I am not sure whether I am getting you right here. If a user wants to detach an entity tree just as it is, they may call  detach() without providing an entity graph. Once an entity or a set of entities has been detached, however, often  the question is what part of the entities may be trusted to have been  loaded. In JPA 2.0 we have worked out the isLoaded() utility as per  section 9.8.1 (then 9.7.1) for that. Being able to supply an entity  graph along with a detach() call may give the user a chance to declare  what is expected to have been loaded. One step further, setting entity graphs as properties of a persistence  context may enable the user to define in general what should have been  loaded when entities get detached from the persistence context (e.g.  due to end of life of the context).  &amp;gt; By the way, I have also been thinking whether these  &amp;gt; EntityGraphs/FetchPlans/LoadContracts might be designed in a similar  &amp;gt; way as bean validation is, such that you may annotate an entity  &amp;gt; attribute with something like  &amp;gt; @Eager(contracts=AccountantView.class)  &amp;gt; or  &amp;gt; @Load(contracts=AccountantView.class)  &amp;gt; where AccountantView is a tagging interface. Avowedly, providing a  &amp;gt; dynamic way to create such artefacts might become difficult...  My concern with this pattern is readability.  Users must scan through   the entire object model to determine what is in an entity graph and what   will be loaded or what is missing for their case. This pattern also can   get quite complicated quickly when there are multiple "contracts" in   place for a particular entity. Actually, this concern works both ways: With the given proposal, if a  user wants to understand what views are available of a particular  entity, all entity graphs throughout both the entire object model and  the coding must be scanned for.  &amp;gt; Concerning the copy operation, I think, I have not fully understood the  &amp;gt; copied entity's state and identity.  What parts are unclear? When e.g. an employee, as per the example in the proposal, gets copied,  what is identity and state of the resulting copy?  &amp;gt; Besides, I find the coexistence of properties fetchgraph and loadgraph  &amp;gt; a bit confusing and would prefer if a decision could be made for just  &amp;gt; one of them.  a loadgraph is just a shortcut for users that allows them to specify   what relationships should be loaded without requiring all mapping   configured EAGER attributes to be specified. Maybe, a more descriptive naming might be of some help here; fetchgraph  and loadgraph seem to me too much like synonyms (overridegraph vs.  modifygraph?). Best regards Rainer   --- Rainer Schweigkoffer               SAP AG Walldorf Regulatory Compliance              TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, F.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...       fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Lars  Dalgaard, Luisa Deplazes Delgado, Bill McDermott (Co-CEO),  Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP  Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse  oder sonstige vertrauliche Informationen enthalten.  Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist  Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung  oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte  benachrichtigen Sie uns und vernichten Sie die empfangene  E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged,  undisclosed, or otherwise confidential information. If you  have received this e-mail in error, you are hereby  notified that any review, copying, or distribution of it  is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your  cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Tue Dec 04 02:25:26 CET 2012</date>
    <body>Hello Pinaki,    Let us say we have two domian entities: File and Directory. Directory extends File. Every File (except the root) has a parent which is a Directory. A directory can have zero or more Files, some of which can be directories. Now a user wants to say: "Query for a directory named 'X' and get me all children 3 level down" How is that "3-level down" expressed in the proposal?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Tue Dec 04 02:39:38 CET 2012</date>
    <body>Hello Rainer and all,   thank you for the update, comments inline below. --Gordon Yes but what is detached is based on the statically defined mapping cascade types.  The purpose of the entity graph is to dynamically provide a template for the operation.  In the future we could easily use defined entity graphs to control the detach operation. Perhaps, but the user may want to setup a detach template without requiring that all of the data be loaded.  There's also the possibility to apply the load and detach operations separately but use the same entity graph. That is an option that is not precluded by the proposed entity graphs. it's just a matter of using the entity graph provided by the user as a default fetch graph for this persistence context. No because the entity graph is defined at the entity level and the entity graph only applies to the entity where it is defined.  So to view how queries for Address will load Address and related entities one only has to look at the Address entity.  There would be no need to search the related entities to see how the Address query will access that data. The copy has the persistence identity of the copied entity and it's version but only has the state that the entity graph template designated should be copied. yes, perhaps the names should be improved.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Tue Dec 04 14:25:01 CET 2012</date>
    <body>Hi Gordon, all, thanks for your quick response. Please find my comments inline. Gordon Yorke, am 3 Dec 2012 hast Du um 21:39 zum Thema "[jsr338- experts] Re: Proposal for EntityGraphs, f"  geschrieben :  &amp;gt;&amp;gt; My concern with this pattern is readability.  Users must scan through  &amp;gt;&amp;gt; the entire object model to determine what is in an entity graph and what  &amp;gt;&amp;gt; will be loaded or what is missing for their case. This pattern also can  &amp;gt;&amp;gt; get quite complicated quickly when there are multiple "contracts" in  &amp;gt;&amp;gt; place for a particular entity.  &amp;gt; Actually, this concern works both ways: With the given proposal, if a  &amp;gt; user wants to understand what views are available of a particular  &amp;gt; entity, all entity graphs throughout both the entire object model and  &amp;gt; the coding must be scanned for.  No because the entity graph is defined at the entity level and the   entity graph only applies to the entity where it is defined.  So to view   how queries for Address will load Address and related entities one only   has to look at the Address entity.  There would be no need to search the   related entities to see how the Address query will access that data. If it is Address you mention, one could imagine that Address might be  related to different entities such as e.g. Employee (home and work  address), Customer (billing and shipping address), Company and many  more. For each of them there might exist a named graph with the  respective entity as root, that may affect loading of Address instances  via one of its subgraphs. Even different occurrences of Address within  the same entity graph (e.g. home address and work address) might yield  different load strategies. Plus, there might be dynamically created  entity graphs affecting Address. That's what I meant.  &amp;gt;&amp;gt;&amp;gt; Concerning the copy operation, I think, I have not fully understood the  &amp;gt;&amp;gt;&amp;gt; copied entity's state and identity.  &amp;gt;&amp;gt; What parts are unclear?  &amp;gt; When e.g. an employee, as per the example in the proposal, gets copied,  &amp;gt; what is identity and state of the resulting copy?  The copy has the persistence identity of the copied entity and it's   version but only has the state that the entity graph template designated   should be copied. Actually, with state I was referring to detached vs. managed vs. new. Kind regards Rainer --- Rainer Schweigkoffer               SAP AG Walldorf Regulatory Compliance              TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, F.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...       fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Lars  Dalgaard, Luisa Deplazes Delgado, Bill McDermott (Co-CEO),  Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP  Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse  oder sonstige vertrauliche Informationen enthalten.  Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist  Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung  oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte  benachrichtigen Sie uns und vernichten Sie die empfangene  E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged,  undisclosed, or otherwise confidential information. If you  have received this e-mail in error, you are hereby  notified that any review, copying, or distribution of it  is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your  cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Tue Dec 04 16:48:16 CET 2012</date>
    <body>               &amp;gt; The depth could easily be restricted through the definition of a finite set of SubGraphs. "easily" it is not. Imagine if the use case said "10-level down"! The point is embedding all related entity graphs in a single entity is not an elegant solution. @SubGraph does not allow reuse and makes things unnecessarily verbose. For example, if Address is to be fetched with many other entities, then each entity has to annotate that subgraph. The elgant solution is to root every subgraph to its own entity + allow inclusion of subgraphs by name in other entities. Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Gordon Yorke ---12/03/2012 05:25:49 PM---Hello Pinaki, &amp;gt; Let us say we have two domian entities: File and Directory. Directory From: To: jsr338-experts@... Date: 12/03/2012 05:25 PM Subject: [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc... Hello Pinaki,   Let us say we have two domian entities: File and Directory. Directory extends File. Every File (except the root) has a parent which is a Directory. A directory can have zero or more Files, some of which can be directories. Now a user wants to say: "Query for a directory named 'X' and get me all children 3 level down" How is that "3-level down" expressed in the proposal? The depth could easily be restricted through the definition of a finite set of SubGraphs. @NamedEntityGraph(        @NamedSubGraph(             name="level1",         ),        @NamedSubGraph(             name="level2", )    The aspect that I am not much comfortable is the SubGraph concept. My view is each entity describes subgraphs that refer to its own properties and includes subgraphs that are rooted in other entities. Such a mechanics will be able to generate a large variety of combinations. The same capability will become too verbose if we tend to define these combinations in a single root entity through the SubGraph annotations as I understand of the proposal. The point of defining the entire entity graph in one spot is to  make the definition very clear while providing flexibility. --Gordon Hello Rainer et al,  The specification could be also viewed as a syntax of describing subgraphs of the entire graph of a domain model as described by the Java language itself. These subgraphs are then attached to the persistence operations such as query, merge or detach. In that sense they are an elaborate mechnics of how a typical persietence operation will cascde. In this view, I do think the proposal's usage of "EntityGraph" as a better moniker than "FetchPlan" - though it may be more common term as the concept originally appeared in the context of fecthing of objects and their relations.   The aspect that I am not much comfortable is the SubGraph concept. My view is each entity describes subgraphs that refer to its own properties and includes subgraphs that are rooted in other entities. Such a mechanics will be able to generate a large variety of combinations. The same capability will become too verbose if we tend to define these combinations in a single root entity through the SubGraph annotations as I understand of the proposal.   On a specifc issue of what are the extra qualifiers on the edges of such subgraphs:   a) Some of the qualifiers such as lock mode as these edges are traversed does not control topology of the subgraph but merely adds qualities to the persistent operatons that cascade along the edges. They are nice to have.   b) filters (as, say, in xpath) could be considered -- but may be at a later phase.   c) recursion depth as a limit on the number of times a edge can be traversed. This recursion depth seems to be essential. But let me understand the proposal on that aspect better through the following use case: Let us say we have two domian entities: File and Directory. Directory extends File. Every File (except the root) has a parent which is a Directory. A directory can have zero or more Files, some of which can be directories. Now a user wants to say: "Query for a directory named 'X' and get me all children 3 level down" How is that "3-level down" expressed in the proposal? Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware &lt;img src="" width="16" height="16" alt="Inactive ---11/30/2012 07:15:59 AM---Hi Gordon, Pinaki, all Gordon Yorke, am 29 N"/&gt; "Rainer Kwesi Schweigkoffer" ---11/30/2012 07:15:59 AM---Hi Gordon, Pinaki, all Gordon Yorke, am 29 Nov 2012 hast Du um 14:29 zum Thema "[jsr338- From: "Rainer Kwesi Schweigkoffer" To: Gordon Yorke , jsr338-experts@... , Pinaki Poddar/Dallas/IBM@IBMUS Date: 11/30/2012 07:15 AM Subject: Re: [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc... Hi Gordon, Pinaki, all Gordon Yorke, am 29 Nov 2012 hast Du um 14:29 zum Thema "[jsr338- experts] Re: Proposal for EntityGraphs, f"  geschrieben : I have always been thinking of the artefact as a contract between provider and consumer about the attributes that have to be loaded. You may use these load contracts for find() and queries, but I could also imagine them with refresh() and detach() or even as properties of a persistence context. When you merge entities, you inform the jpa provider about the load contract you have received them under. This depends a bit on your perspective on the artefact, I would say. You may either see it as an autonomous construct that overrules the entity definition, or you could say, an entity offers views of itself that contribute to different contracts. By the way, I have also been thinking whether these EntityGraphs/FetchPlans/LoadContracts might be designed in a similar way as bean validation is, such that you may annotate an entity attribute with something like @Eager(contracts=AccountantView.class) or @Load(contracts=AccountantView.class) where AccountantView is a tagging interface. Avowedly, providing a dynamic way to create such artefacts might become difficult... Concerning the copy operation, I think, I have not fully understood the copied entity's state and identity. Besides, I find the coexistence of properties fetchgraph and loadgraph a bit confusing and would prefer if a decision could be made for just one of them. Best regards Rainer --- Rainer Schweigkoffer               SAP AG Walldorf Regulatory Compliance              TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, F.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...        fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Lars Dalgaard, Luisa Deplazes Delgado, Bill McDermott (Co-CEO), Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP Supervisory Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse oder sonstige vertrauliche Informationen enthalten. Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte benachrichtigen Sie uns und vernichten Sie die empfangene E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged, undisclosed, or otherwise confidential information. If you have received this e-mail in error, you are hereby notified that any review, copying, or distribution of it is strictly prohibited. Please inform us immediately and destroy the original transmittal. Thank you for your cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Thu Dec 06 12:40:17 CET 2012</date>
    <body>Hi Gordon, all, Gordon Yorke, am 5 Dec 2012 hast Du um 10:14 zum Thema "[jsr338- experts] Re: Proposal for EntityGraphs, f"  geschrieben :  context.  There's no need for the developer to search out the elements   of the entity graph throughout the domain model and correlate   relationships to determine how an entity graph that is being used for a   particular operation will impact the result. Actually, to my understanding, with javax.persistence.loadgraph this  seems to be the case.  &amp;gt; Actually, with state I was referring to detached vs. managed vs. new.  The copied entity instances would be detached and possibly new if new   instances were copied. Might be worth a sentence in the spec. TX and best regards Rainer --- Rainer Schweigkoffer               SAP AG Walldorf Regulatory Compliance              TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, F.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...       fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Lars  Dalgaard, Luisa Deplazes Delgado, Bill McDermott (Co-CEO),  Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP  Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse  oder sonstige vertrauliche Informationen enthalten.  Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist  Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung  oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte  benachrichtigen Sie uns und vernichten Sie die empfangene  E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged,  undisclosed, or otherwise confidential information. If you  have received this e-mail in error, you are hereby  notified that any review, copying, or distribution of it  is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your  cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Thu Dec 06 14:02:31 CET 2012</date>
    <body>Hello Rainer and others,    A reply below, Thank you, --Gordon the load graph is a template to force load desired attributes so once a load graph has been used the developer has a reasonable expectation that the specified attributes were loaded but does not care about the state of the other attributes.  Also, the load state of all other attributes is unknown and based on application access and provider lazy support (a provider is always allowed to load more than specified by the user) so if load state must be know isLoaded() must be used and searching the domain model is of little help.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Thu Dec 06 19:13:31 CET 2012</date>
    <body>Hello everyone and many thanks for Linda and Gordon for this proposal. Here are my comments after a first pass on the proposal. ## API Have you considered a superinterface between SubGraph and EntityGraph to keep all the duplicated methods like addSubGraph and co? I am not keep on the graph / node analogy for the API names. Though I understand Gordon's reserves on the name Fetch Plan. I side with Gordon. Despite it's verbosity and a slight lack of flexibility I prefer the current approach of subgraph definition to a model where subgraph would simply be entity graphs defined in other entity classes.  Readability and immediate understanding of the closure is very important for me. Plus I have always found max depth settings counter intuitive when defining a closure. I like Oliver's proposal of String[] to list simple types. It makes the API and annotations much more palatable. Should we use interfaces as graph names? That's a good idea and I like it though I' biased :D But we should note that this is not the style of the JPA specification. That plus the problem of programmatic definition naming might be enough to refrain from using this approach. I'm neutral. I find the javax.persistence.fetchgraph vs javax.persistence.loadgraph very confusing. My understanding is that fetch will replace the fetch graph define statically whereas load will only increase the size of the fetch graph defined statically. Is that correct? ## Semantic It's implied but the spec could be specific in the fact that managed entities are returned. (I might have missed the comment though). I'd like to see the wording be clearer in allowing implementations to return more than what has been defined in the closure. Just like FetchType does. This is particularly useful for objects already present in the PC and already more complete. It seems to me that copy will potentially load previously unloaded parts of the graph. We might want to make that clearer. ## Use cases I would really like to get use cases associated to each of the new features proposed here. In particular: - it's unclear to me why you really need loadgraph and fetchgraph. - why is merge( fetchgraph ) required when you can achieve the same with   - find( fetchgraph )   - detach / serialize or whatever   - merge() regular method ## General feeling Generally speaking, I have mixed feelings with the proposal. While I can see some use cases where it is extremely convenient, I am really afraid of people abusing it because they believe they are optimizing their application by fine-tuning things. The consequence for them will be: - much more verbose and harder to read code - actually slower than without a custom fetch graph Emmanuel  Hello all,     Please find below a proposal for EntityGraphs and their use to  define entity graph fetching, loading, merging and copying.  I have  been working with Mike Keith and Linda DeMichiel on this proposal  and we feel it is time to present to the expert group.  Please have  a look and provide feedback as the intention is to integrate this  within the 2.1 spec.  Thank you,  Gordon    ----  A common functional request has been to enable users to dynamically  define how an EntityManager operation or query should apply to an  entity graph.  This may more commonly be known as "merge groups" or  "fetch plans".  This functionality can be especially important when  detached entities must be serialized and then merged into an active  Persistence Context.    An entity graph is a template defined in the form of metadata or an  object created by the dynamic EntityGraph API that captures the path  and boundaries for an operation or a query   For static metadata the  NamedEntityGraph annotation defines the attribute nodes and  boundaries of the grouping.    @Retention(RUNTIME)    @Retention(RUNTIME)      /**       * (Optional) The name of the sub-graph.       * Defaults to the entity name of the root entity.       */        /**       * (Required) list of attributes that are included in in this  sub-graph.       */        /**       * (Optional) Lists all of the attributes of this Entity class  to be listed in       * the NamedEntityGraph without the need to explicitly list  them.  Included       * attributes can still be fully specified by an attribute node  referencing a       * sub-graph.       */        /**       * (Optional) This is a list of sub-graphs that are included in the       * entity graph. They  are referenced by name from NamedAttributeNode       * definitions.       */          /**       * (Optional) This is a list of sub-graphs that will add  additional attributes       * for subclasses of the entity represented by this  NamedEntityGraph to this       * entity graph.  Specified attributes from superclasses are  included in       * subclasses.       */      The NamedEntityGraph annotation is applied to an Entity class that  is defined as the root of a group of attributes, relationships and  related entity classes.  The NamedAttributeNode annotations are  embedded within the NamedEntityGraph metadata and define the  attributes of the entity that are included in the grouping.  When a  NamedAttributeNode represents an Entity or an Embedded  (including a  collection-valued relationship or an element collection of  embeddables), the referenced type's attributes can be included as a  NamedSubGraph entry of the entity graph and referenced by name,  creating a multi-level entity graph. If the sub-graph represents an  entity with inheritance multiple NamedSubGraph entries can be made  with the same name to extend the sub-graph for specific subclasses.  The class type that is being extended is required in the  NamedSubGraph.    /**   * A NamedAttributeNode is a member attribute of a NamedEntityGraph.   *   * @see javax.persistence.NamedEntityGraph   * @see javax.persistence.NamedSubGraph    */  @Retention(RUNTIME)      /**       * (Required) the name of the attribute that must be in the sub-graph.       */        /**       * (Optional) if this attribute references a managed type that  has its own       * AttributeNodes then this refers to that NamedSubGraph  definition.  If the       * target type has inheritance than multiple sub graphs can be  specified.       * These additional sub-graphs are intended to add additional  attributes from       * subclasses.  Superclass sub-graph entries will be merged into  subclasses       * sub-graphs.       */       /**      * (Optional) if the attribute references a Map type this can be  used to specify      * a subGraph for the Key in the case of an Entity key type.  A      * keySubGraph can not be specified without the Map attribute also being      * specified.  If the target type has inheritance than multiple  sub-graphs can      * be specified.  These additional sub-graphs are intended to add  additional      * attributes from subclasses. Superclass sub-graph entries will  be merged      * into subclasses sub-graphs.      */      /**   * A NamedSubGraph is a member of a NamedEntityGraph that represents  a managed   * type. The NamedSubGraph is only referenced from within a  NamedEntityGraph and   * can not be referenced on its own. It is referenced by name from a   * NamedAttributeNode of the NamedEntityGraph.   *   * @see javax.persistence.NamedEntityGraph  * @see javax.persistence.NamedAttributeNode   */  @Retention(RUNTIME)      /**       * (Required) the name of the sub-graph as referenced from a       * NamedAttributeNode.       */        /**       * (Optional) the type represented by this subgraph. Required when this       * subgraph is extending a subclass' definition.       */        /** (Required)       * if this NamedAttributeNode references a managed type then  this list is       * the attributes of that type that must be included.       */      For example:  @Entity      @Id    @GeneratedValue      @Basic      @Basic      @OneToMany()      @OneToMany()      @OneToMany()    ...    @Entity  @Inheritance      @Id      @GeneratedValue        @OneToOne(fetch=FetchType.EAGER)     ...    @Entity        @OneToOne(fetch=FetchType.LAZY)     ...    @Entity      @Id       ...    @Entity      @Id        @Lob        @OneToOne(fetch=FetchType.LAZY)      protected Approval approval     ...    @NamedEntityGraph(          @NamedAttributeNode(              value="projects",              subGraph="projects"          ),          @NamedttributeNode(name = "phoneNumbers"),          @NamedSubGraph(              name="projects",                  @NamedAttributeNode("requirements")          ),          @NamedSubGraph(              name="projects",              type=LargeProject.class,                  @NamedAttributeNode("approver")          )  )    @NamedEntityGraph(      name-"Project",          @NamedAttributeNode("requirements")          @NamedSubGraph(              type=LargeProject.class,                  @NamedAttributeNode("approver")          )  )    @NamedEntityGraph(      name="EmployeeProjectRequirements"          @NamedAttributeNode(              value="projects",              subGraph="projects"          ),          @NamedttributeNode(name = "phoneNumbers"),          @NamedSubGraph(              name="projects",                  @NamedAttributeNode(                       value ="requirements",                       subGraph="requirements"                  )          ),          @NamedSubGraph(              name="requirements",                  @NamedAttributeNode("description"),                  @NamedAttributeNode("approval")          )  )    The entity graph could also be created dynamically through the  following APIs:    /**   * This type represents the root of an entity graph that will be used as a   * template to define the attribute nodes and boundaries of a graph  of entities and   * entity relationships. The root must be an Entity type.   *   *            the type of the root entity.   */          /**       * Returns the name of the static EntityGraph.  Will return null if the       * EntityGraph is not a named EntityGraph.       */        /*       * Add an AttributeNode attribute to the entity graph.       *       * @throws IllegalArgumentException if the attribute is not an  attribute of       * this entity.       * @throws IllegalStateException if this EntityGraph has been  statically       * defined       */        /*       * Add an AttributeNode attribute to the entity graph.       *       * @throws IllegalStateException if this EntityGraph has been  statically       * defined       */        /*       * Used to add a node of the graph that corresponds to a managed  type. This       * allows for construction of multi-node Entity graphs that  include related       * managed types.       *       * @throws IllegalArgumentException if the attribute's target  type is not a       * managed type       *       * @throws IllegalStateException if this EntityGraph has been  statically       * defined       */        /**       * Used to add a node of the graph that corresponds to a managed  type with       * inheritance.  This allows for multiple subclass sub-graphs to  be defined       * for this node of the entity graph. Subclass sub-graphs will  include the       * specified attributes of superclass sub-graphs       *       * @throws IllegalArgumentException       *             if the attribute's target type is not a managed type       * @throws IllegalStateException       *             if this EntityGraph has been statically defined       */        /*       * Used to add a node of the graph that corresponds to a managed  type. This       * allows for construction of multi-node Entity graphs that  include related       * managed types.       *       * @throws IllegalArgumentException if the attribute is not an  attribute of       * this entity.       * @throws IllegalArgumentException if the attribute's target  type is not a       * managed type       *       * @throws IllegalStateException if this EntityGraph has been  statically       * defined       */        /**       * Used to add a node of the graph that corresponds to a managed  type with       * inheritance.  This allows for multiple subclass sub-graphs to  be defined       * for this node of the entity graph.  Subclass sub-graphs will  include the       * specified attributes of superclass sub-graphs       *       * @throws IllegalArgumentException if the attribute is not an  attribute of       * this managed type.       * @throws IllegalArgumentException       *             if the attribute's target type is not a managed type       * @throws IllegalStateException       *             if this EntityGraph has been statically defined       */        /*       * Used to add a node of the graph that corresponds to a map key  that is a       * managed type. This allows for construction of multi-node  Entity graphs that       * include related managed types.       *       * @throws IllegalArgumentException if the attribute's target  type is not a       * managed type entity       *       * @throws IllegalStateException if this EntityGraph has been  statically       * defined       */        /*       * Used to add a node of the graph that corresponds to a map key  that is a       * managed type with inheritance. This allows for construction  of multi-       * node Entity graphs that include related managed types.  Subclass sub-graphs       * will include the specified attributes of superclass sub-graphs       *       * @throws IllegalArgumentException if the attribute's target  type is not a       * managed type entity         *       * @throws IllegalStateException if this EntityGraph has been  statically       * defined       */        /*       * Used to add a node of the graph that corresponds to a map key  that is a       * managed type. This allows for construction of multi-node  Entity graphs that       * include related managed types.       *       * @throws IllegalArgumentException if the attribute is not an  attribute of       * this entity.       * @throws IllegalArgumentException if the attribute's target  type is not a       * managed type       *       * @throws IllegalStateException if this EntityGraph has been  statically       * defined       */        /*       * Used to add a node of the graph that corresponds to a map key  that is a       * managed type with inheritance. This allows for construction  of multi-       * node Entity graphs that include related managed types.  Subclass sub-graphs       * will include the specified attributes of superclass sub-graphs       *       * @throws IllegalArgumentException if the attribute is not an  attribute of       * this entity.       * @throws IllegalArgumentException if the attribute's target  type is not a       * managed type       *       * @throws IllegalStateException if this EntityGraph has been  statically       * defined       */        /*       * returns the attributes of this entity that are included in  the entity       * graph       */      /**   * Represents an AttributeNode of an entity graph.   */        /*       * returns the Type of the AttributeNode.       */            /*       * returns the name of the referencing attribute.       */    /**   * This type represents a AttributeNode of an EntityGraph that  corresponds to a   * Managed Type. Using this class an entity graph can be embedded within an   * EntityGraph.   *   *            the Class type of the AttributeNode.   */          /**       * Add an AttributeNode attribute to the entity graph.       *       * @throws IllegalArgumentException if the attribute is not an  attribute of       * this managed type.       * @throws IllegalStateException       *             if this EntityGraph has been statically defined       */        /**       * Add an AttributeNode attribute to the entity graph.       *       * @throws IllegalStateException       *             if this EntityGraph has been statically defined       */        /**       * Used to add a node of the graph that corresponds to a managed  type. This       * allows for construction of multi-node Entity graphs that  include related       * managed types.         *       * @throws IllegalArgumentException       *             if the attribute's target type is not a managed type       * @throws IllegalStateException       *             if this EntityGraph has been statically defined       */        /**       * Used to add a node of the graph that corresponds to a managed  type with       * inheritance.  This allows for multiple subclass sub-graphs to  be defined       * for this node of the entity graph. Subclass sub-graphs will  include the       * specified attributes of superclass sub-graphs       *       * @throws IllegalArgumentException       *             if the attribute's target type is not a managed type       * @throws IllegalStateException       *             if this EntityGraph has been statically defined       */        /**       * Used to add a node of the graph that corresponds to a managed  type. This       * allows for construction of multi-node Entity graphs that  include related       * managed types.       *       * @throws IllegalArgumentException if the attribute is not an  attribute of       * this managed type.       * @throws IllegalArgumentException       *             if the attribute's target type is not a managed type       * @throws IllegalStateException       *             if this EntityGraph has been statically defined       */        /**       * Used to add a node of the graph that corresponds to a managed  type with       * inheritance.  This allows for multiple subclass sub-graphs to  be defined       * for this node of the entity graph. Subclass sub-graphs will  include the       * specified attributes of superclass sub-graphs       *       * @throws IllegalArgumentException if the attribute is not an  attribute of       * this managed type.       * @throws IllegalArgumentException       *             if the attribute's target type is not a managed type       * @throws IllegalStateException       *             if this EntityGraph has been statically defined       */        /*       * Used to add a node of the graph that corresponds to a map key  that is a       * managed type. This allows for construction of multi-node  Entity graphs that       * include related managed types.       *       * @throws IllegalArgumentException if the attribute's target  type is not a       * managed type entity         *       * @throws IllegalStateException if this EntityGraph has been  statically       * defined       */        /*       * Used to add a node of the graph that corresponds to a map key  that is a       * managed type with inheritance. This allows for construction  of multi-       * node Entity graphs that include related managed types.  Subclass sub-graphs       * will include the specified attributes of superclass sub-graphs       *       * @throws IllegalArgumentException if the attribute's target  type is not a       * managed type entity         *       * @throws IllegalStateException if this EntityGraph has been  statically       * defined       */        /*       * Used to add a node of the graph that corresponds to a map key  that is a       * managed type. This allows for construction of multi-node  Entity graphs that       * include related managed types.       *       * @throws IllegalArgumentException if the attribute is not an  attribute of       * this entity.       * @throws IllegalArgumentException if the attribute's target  type is not a       * managed type       *       * @throws IllegalStateException if this EntityGraph has been  statically       * defined       */        /*       * Used to add a node of the graph that corresponds to a map key  that is a       * managed type with inheritance. This allows for construction  of multi-       * node Entity graphs that include related managed types.  Subclass sub-graphs       * will include the specified attributes of superclass sub-graphs       *       * @throws IllegalArgumentException if the attribute is not an  attribute of       * this entity.       * @throws IllegalArgumentException if the attribute's target  type is not a       * managed type       *       * @throws IllegalStateException if this EntityGraph has been  statically       * defined       */        /**       * returns the attributes of this managed type that are included in the       * sub-graph       */        /**       * returns the attribute that references this sub-graph.       /        /**       * returns the type of this sub-graph if it was used to extend a  superclass'       * sub-graph definition.       */         ...      /**       * returns any named EntityGraph that has been created through static       * metadata. Returned EntityGraphs should be considered immutable.       */        /**       * add a named copy of the EntityGraph to the EntityManagerFactory       */      public void addNamedEntityGraph(String graphName, EntityGraph        /**       * returns a mutable EntityGraph that can be used to dynamically  create an       * EntityGraph.       */        /**       * returns a mutable copy of the named EntityGraph       */    Example:       SubGraph largeProjects = employee.addSubGraph("projects",       SubGraph requirements = employee.addSubGraph("projects")    Once the EntityGraph is available or has been created it can be  applied to operations on the EntityManager or used with queries to  control eager loading of results.    The standard property javax.persistence.fetchgraph can be used with  the find operation or as a query hint to specify a EntityGraph that  defines additional FetchType semantics whereby  any AttributeNode of  the EntityGraph is treated as FetchType.EAGER and any attribute not  specified in the EntityGraph is treated as FetchType.LAZY. The  primary key of an entity is always retrieved even if not present in  the fetch group.    The following characterizes the semantics of entity graphs that are  used as fetch graphs.    A fetch graph attribute node specified within an entity graph or  sub-graph specifies how an attribute is to be fetched.  Attributes  that are not specified are treated as FetchType.LAZY.    The following rules apply, depending on attribute type.    A primary key attribute never needs to be specified in an attribute  node.  (This applies to composite primary keys as well, including  embedded id primary keys.)  When an entity is fetched, its primary  key is always fetched.  It is not incorrect, however, to specify  primary key attributes.    Attributes other than primary key attributes are not fetched unless  the attribute is specified.  The following rules apply to the  specification of attributes:    The default fetch graph for an entity or embeddable consists of the  transitive closure of all of its attributes that are specified  asFetchType.EAGER (or defaulted as such).    If the attribute is an embedded attribute, and the attribute is  specified in an attribute node, but a subgraph is not specified for  the attribute, the default fetch graph for the embeddable is  fetched.  If a subgraph is specified for the attribute, the  attributes of the embeddable are fetched according to their  specification in the corresponding subgraph, and the rules of this  section recursively apply.    If the attribute is an element collection of basic type, and the  attribute is specified in an attribute node, the element collection  together with its basic elements is fetched.    If the attribute is an element collection of embeddables, and the  attribute is specified in an attribute node, but a subgraph is not  specified for the attribute, the element collection together with  the default fetch graph of its embeddable elements is fetched.  If a  subgraph is specified for the attribute, the attributes of the  embeddable elements will be fetched according to the corresponding  subgraph specification, and the rules of this section recursively  apply.    If the attribute is a one-to-one or many-to-one relationship, and  the attribute is specified in an attribute node, but a subgraph is  not specified for the attribute, the default fetch graph of the  target entity is fetched.  If a subgraph is specified for the  attribute, the attributes of the target entity will be fetched  according to the corresponding subgraph specification, and the rules  of this section recursively apply.    If the attribute is a one-to-many or many-to-many relationship, and  the attribute is specified in an attribute node, but a subgraph is  not specified, the collection will be fetched and the default fetch  graphs of the referenced entities will be fetched.  If a subgraph is  specified for the attribute, the entities in the collection will be  fetched according to the corresponding subgraph specification, and  the rules of this section recursively apply.    If the key of a map which has been specified in an attribute node is  a basic type, it will always be fetched and if the key of a map  which has been specified in an attribute node is an embedded type  otherwise, if a map key subgraph is not specified for the attribute  node, the map key will be fetched according to its default fetch  graph.  If a key subgraph is specified for the map key attribute,  the map key attribute will be fetched according to the map key  subgraph specification, and the rules of this section recursively  apply.    So using the following entity graph:    @NamedEntityGraph(  )    used to load a Phonenumber only the id and the FKs would be retrieved.    When using the following entity graph:  @NamedEntityGraph(          @NamedAttributeNode("projects"),  )    loading an Employee  the PKs and FKs would be retrieved and the  "projects" attribute would be eagerly loaded.    The standard property javax.persistence.loadgraph can be used to  specify an entity graph that defines additional FetchType semantics  with any attribute node of the entity graph becoming FetchType.EAGER  and any unspecified attributes remain as defined in the mapping.    The following characterizes the semantics of entity graphs that are  used as load graphs.    A load graph attribute node specified within an entity graph or  sub-graph specifies how an attribute is to be fetched.  Attributes  that are not specified are treated according to their default  FetchType.    The following rules apply to the specification of attributes:    A primary key attribute never needs to be specified in an attribute  node.  (This applies to composite primary keys as well, including  embedded id primary keys.)  When an entity is fetched, its primary  key is always fetched.  It is not incorrect, however, to specify  primary key attributes.    If the attribute is an embedded attribute, and the attribute is  specified in an attribute node, but a subgraph is not specified for  the attribute, the default fetch graph for the embeddable is  fetched. If a subgraph is specified for the attribute, any  attributes of the embeddable that are not further specified within  the subgraph are treated according to their fetch type; attributes  that are specified by the subgraph are also fetched; and the rules  of this section recursively apply.    If the attribute is an element collection of basic type, and the  attribute is specified in an attribute node, the element collection  together with its basic elements is fetched.  If the attribute is an  element collection of embeddables, and the attribute is specified in  an attribute node, but a subgraph is not specified for the  attribute, the element collection together with the default fetch  graph of its embeddable elements is fetched.  If a subgraph is  specified for the attribute, any attributes of the embeddables that  attributes that are specified by the subgraph are also fetched; and  the rules of this section recursively apply.    If the attribute is a one-to-one or many-to-one relationship, and  the attribute is specified in an attribute node, but a subgraph is  not specified for the attribute, the default fetch graph of the  target entity is fetched.  If a subgraph is specified for the  attribute, any attributes of the target entity that are not further  specified are treated according to their fetch type; attributes that  are specified by the subgraph are also fetched; and the rules of  this section recursively apply.    If the attribute is a one-to-many or many-to-many relationship, and  the attribute is specified in an attribute node, but a subgraph is  not specified, the collection will be fetched and the default fetch  graphs of the referenced entities will be fetched.  If a subgraph is  specified for the attribute, any entities in the collection will be  treated according to their fetch type; attributes that are specified  by the subgraph are also fetched; and the rules of this section  recursively apply.    If a collection-valued attribute is a map, keys that are basic or  embeddable types will be fetched when the map is fetched; entity map  key attributes will be fetched according to their fetch type and, if  a key subgraph is specified, additional entity attributes are  fetched as specified in the subgraph; and the rules of this section  recursively apply.    So using the following entity graph:    @NamedEntityGraph(  )    used to load a Phonenumber only the id and the FKs would be  retrieved. The requirements attribute would not be loaded but would  be lazy.    When using the following entity graph:  @NamedEntityGraph(          @NamedAttributeNode("projects"),  )    loading an Employee  the PKs and FKs would be retrieved and the  "projects" attribute would be eagerly loaded.    EntityManager API        /**       * Copy the provided entity graph  using the EntityGraph as a  template to  specify       * the attributes that will be copied. Attributes not included  in the entity graph will not be copied.       *       * @param entity       *            entity instance       * @param EntityGraph       *            the entity graph template for applying copy.       * @throws IllegalArgumentException       *             if the instance is not an entity or the  EntityGraph root is       *             not the same class as the entity.       */      public void copy(Object entity, EntityGraph entityGraph,      ...        /**       * Merge the state of the given entity into the current  persistence context       * using the EntityGraph as a template for applying the merge  operation.       *       *       * @param entity       *            entity instance       * @param EntityGraph       *            the entity graph template for applying merge.       * @return the managed instance that the state was merged to       * @throws IllegalArgumentException       *             if instance is not an entity, is a removed entity  or the root       *             of the EntityGraph is not of the same type as the entity       * @throws TransactionRequiredException       *             if there is no transaction when invoked on a       *             container-managed entity manager of that is of type       */      public &amp;lt;T&amp;gt; T merge(T entity, EntityGraph entityGraph,      ...etc    _Merge_  When using an entity graph  in combination with the "merge"  operation any listed attribute node will be merged and any unlisted  attribute node will not be merged, this includes @Basic mappings.  Any attribute that references a Map is considered to include both  the key and the value.  If the attribute node references a sub-graph  or map key sub-graph for the relationship target then the merge will  operate on that target object as well with only those attributes  referenced in the sub-graph being merged and this will apply  recursively.    For example with the merge operation applied with the following  entity graph :    @NamedEntityGraph(          @NamedAttributeNode("name"),          @NamedAttributeNode(              value="projects",              subGraph="projects"          ),          @NamedAttributeNode("phoneNumbers"),          @NamedSubGraph(              name="projects",                  @NamedAttributeNode("requirements")          )  )    Only the name, projects and phoneNumbers attributes of the Employee  will be merged.  The merge will be  applied to the projects but only  the requirements attribute of the Project will be merged and the  merge will not be  applied to the Requirement target.  The  PhoneNumber target will not be merged.    _Copy_  A new operation "copy" is being introduced.  This operation is  intended to be used when a user wishes to disconnect a graph of  entities from a larger graph.  When using an entity graph  in  combination with the "copy" operation a copy of the provided entity  is created and the attributes of the copied entity are populated  based on the attributes listed in the entity graph.  If an attribute  node for an  attribute is present then the attribute will be copied.  If the attribute node represents an attribute that has a managed  type as the target, element or key type then that target instance is  copied as well and the copy is set in the new tree.  If a sub-graph  is defined for the value or key then any attributes corresponding to  the attribute nodes within the sub-graph will be copied as above.  If no sub-graph is defined for the attribute node then no attributes  of the target are copied.    For example with the copy operation applied with the following  entity graph :    @NamedEntityGraph(          @NamedAttributeNode("name"),          @NamedAttributeNode(              value="projects",              subGraph="projects"          ),          @NamedAttributeNode("phoneNumbers"),          @NamedSubGraph(              name="projects",                  @NamedAttributeNode("requirements")          )  )    When applied to an Employee instance a new copy of the Employee will  be created and the "name" , "phoneNumbers" and "projects" attributes  will be copied over.  For each PhoneNumber in the phoneNumbers list  a new copy of the PhoneNumber will be created and used but no  attributes will be copied.  For each Project in the projects list a  new copy of the Project will be created and used and only the  requirements attribute will be copied.  For each Requirement in the  requirements list a new copy of the Requirement will be created but  no attributes will be copied.  </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Fri Dec 07 15:09:39 CET 2012</date>
    <body>Hello Pinaki and others,    @NamedSubgraph is reusable within the same NamedEntityGraph definition as references from the named attribute nodes are based on name.   However,  we will simply have to disagree on which pattern is easier to maintain and read. --Gordon On 04/12/2012 11:48 AM, Pinaki Poddar down" expressed in the proposal?                &amp;gt; The depth could easily be restricted through the definition of a finite set of SubGraphs. "easily" it is not. Imagine if the use case said "10-level down"! The point is embedding all related entity graphs in a single entity is not an elegant solution. @SubGraph does not allow reuse and makes things unnecessarily verbose. For example, if Address is to be fetched with many other entities, then each entity has to annotate that subgraph. The elgant solution is to root every subgraph to its own entity + allow inclusion of subgraphs by name in other entities. Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware &lt;img src="msg00010/gifDOwacx7hxD.gif" alt="Inactive hide details for Gordon Yorke ---12/03/2012 05:25:49 PM---Hello Pinaki, &gt; Let us say we have two domian entities: File" height="16" width="16" border="0"/&gt; Gordon Yorke ---12/03/2012 05:25:49 PM---Hello Pinaki, &amp;gt; Let us say we have two domian entities: File and Directory. Directory From: Gordon Yorke To: jsr338-experts@... Date: 12/03/2012 05:25 PM Subject: [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc... Hello Pinaki,   Let us say we have two domian entities: File and Directory. Directory extends File. Every File (except the root) has a parent which is a Directory. A directory can have zero or more Files, some of which can be directories. Now a user wants to say: "Query for a directory named 'X' and get me all children 3 level down" How is that "3-level down" expressed in the proposal? The depth could easily be restricted through the definition of a finite set of SubGraphs. @NamedEntityGraph(        @NamedSubGraph(             name="level1",         ),        @NamedSubGraph(             name="level2", )    The aspect that I am not much comfortable is the SubGraph concept. My view is each entity describes subgraphs that refer to its own properties and includes subgraphs that are rooted in other entities. Such a mechanics will be able to generate a large variety of combinations. The same capability will become too verbose if we tend to define these combinations in a single root entity through the SubGraph annotations as I understand of the proposal. The point of defining the entire entity graph in one spot is to  make the definition very clear while providing flexibility. --Gordon On 03/12/2012 1:21 PM, Pinaki Poddar Hello Rainer et al,  The specification could be also viewed as a syntax of describing subgraphs of the entire graph of a domain model as described by the Java language itself. These subgraphs are then attached to the persistence operations such as query, merge or detach. In that sense they are an elaborate mechnics of how a typical persietence operation will cascde. In this view, I do think the proposal's usage of "EntityGraph" as a better moniker than "FetchPlan" - though it may be more common term as the concept originally appeared in the context of fecthing of objects and their relations.   The aspect that I am not much comfortable is the SubGraph concept. My view is each entity describes subgraphs that refer to its own properties and includes subgraphs that are rooted in other entities. Such a mechanics will be able to generate a large variety of combinations. The same capability will become too verbose if we tend to define these combinations in a single root entity through the SubGraph annotations as I understand of the proposal.   On a specifc issue of what are the extra qualifiers on the edges of such subgraphs:   a) Some of the qualifiers such as lock mode as these edges are traversed does not control topology of the subgraph but merely adds qualities to the persistent operatons that cascade along the edges. They are nice to have.   b) filters (as, say, in xpath) could be considered -- but may be at a later phase.   c) recursion depth as a limit on the number of times a edge can be traversed. This recursion depth seems to be essential. But let me understand the proposal on that aspect better through the following use case: Let us say we have two domian entities: File and Directory. Directory extends File. Every File (except the root) has a parent which is a Directory. A directory can have zero or more Files, some of which can be directories. Now a user wants to say: "Query for a directory named 'X' and get me all children 3 level down" How is that "3-level down" expressed in the proposal? Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware &lt;img src="msg00010/gifDOwacx7hxD.gif" alt="Inactive hide details for &amp;quot;Rainer Kwesi Schweigkoffer&amp;quot; ---11/30/2012 07:15:59 AM---Hi Gordon, Pinaki, all Gordon Yorke, am 29 N" height="16" width="16"/&gt; "Rainer Kwesi Schweigkoffer" ---11/30/2012 07:15:59 AM---Hi Gordon, Pinaki, all Gordon Yorke, am 29 Nov 2012 hast Du um 14:29 zum Thema "[jsr338- From: "Rainer Kwesi Schweigkoffer" To: Gordon Yorke , jsr338-experts@... , Pinaki Poddar/Dallas/IBM@IBMUS Date: 11/30/2012 07:15 AM Subject: Re: [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc... Hi Gordon, Pinaki, all Gordon Yorke, am 29 Nov 2012 hast Du um 14:29 zum Thema "[jsr338- experts] Re: Proposal for EntityGraphs, f"  geschrieben : etc as they are more FetchPlans.  Naming it I have always been thinking of the artefact as a contract between provider and consumer about the attributes that have to be loaded. You may use these load contracts for find() and queries, but I could also imagine them with refresh() and detach() or even as properties of a persistence context. When you merge entities, you inform the jpa provider about the load contract you have received them under. every entity type X can Y, then a separate track the contents all over the information is in one This depends a bit on your perspective on the artefact, I would say. You may either see it as an autonomous construct that overrules the entity definition, or you could say, an entity offers views of itself that contribute to different contracts. By the way, I have also been thinking whether these EntityGraphs/FetchPlans/LoadContracts might be designed in a similar way as bean validation is, such that you may annotate an entity attribute with something like @Eager(contracts=AccountantView.class) or @Load(contracts=AccountantView.class) where AccountantView is a tagging interface. Avowedly, providing a dynamic way to create such artefacts might become difficult... Concerning the copy operation, I think, I have not fully understood the copied entity's state and identity. Besides, I find the coexistence of properties fetchgraph and loadgraph a bit confusing and would prefer if a decision could be made for just one of them. Best regards Rainer --- Rainer Schweigkoffer               SAP AG Walldorf Regulatory Compliance              TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, F.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...        fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Lars Dalgaard, Luisa Deplazes Delgado, Bill McDermott (Co-CEO), Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP Supervisory Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse oder sonstige vertrauliche Informationen enthalten. Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte benachrichtigen Sie uns und vernichten Sie die empfangene E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged, undisclosed, or otherwise confidential information. If you have received this e-mail in error, you are hereby notified that any review, copying, or distribution of it is strictly prohibited. Please inform us immediately and destroy the original transmittal. Thank you for your cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Fri Dec 07 15:20:48 CET 2012</date>
    <body>Hello Emmanuel and others,   some comments below, Thank you, --Gordon No but that is a good suggestion.  The original intention was to clearly distinguish between the two types. Yes, that was in an earlier version of the proposal. Entity graph names are created by the user and there are no predefined names.  Perhaps you are suggesting something else? Yes, the purpose of a load graph is to create a shortcut for the user when they simply want to add additional attributes to the default fetch group.  With fetch graphs alone the user would need to redefine the default fetch graph in the entity graph. The standard query rules still apply and managed entities are not always returned.  For instance, a query with a fetch graph executed when no transaction is present would return detached entities just as queries do currently. Yes, this should be updated. Yes. merge(EntityGraph) applies when the user is using copy(EntityGraph) to detach an entity tree for serialization.  If an attribute is not present in the copy then it must not be merged.  So, your example becomes:     - find (EntityGraph eg1)     - copy(EntityGraph eg1)     - serialize out/in     - merge(EntityGraph eg1)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Fri Dec 07 17:58:55 CET 2012</date>
    <body>Am 07.12.2012 um 15:20 schrieb Gordon Yorke &amp;lt;gordon.yorke@...&amp;gt;:  Yes, that was in an earlier version of the proposal. What exactly does that mean? Was it in there and has been abandoned? If so,  why? Cheers, Ollie --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Fri Dec 07 18:57:11 CET 2012</date>
    <body>It was removed from an earlier draft version to reduce the clutter in the NamedEntityManager annotation as NamedAttributeNode serves the same purpose.  It also eliminates any confusion about when to use the String[] vs the NamedAttributeNode[] and how they interact. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Fri Dec 07 19:29:54 CET 2012</date>
    <body>Am 07.12.2012 um 18:57 schrieb Gordon Yorke &amp;lt;gordon.yorke@...&amp;gt;:  It was removed from an earlier draft version to reduce the clutter in the   NamedEntityManager annotation as NamedAttributeNode serves the same   purpose.  It also eliminates any confusion about when to use the String[]   vs the NamedAttributeNode[] and how they interact.  --Gordon I guess you mean NamedEntityGraph, right? I don't think an additional simple  attribute is clutter actually. Also, I can't see any confusion as the  String[] can easily be evaluated like an @NamedAttributeNode with a plain  name attribute and would be added to other @NamedAttributeNode declarations.  Simple rule, no corner cases. Especially in scenarios where you'd simply list  a couple of Strings this would actually *reduce* clutter and complexity on  the definition side. Make the simple things simple, complex things possible. The examples you showed were referring to a few properties only and already  consumed quite a bit of space. Assuming real world scenarios, this overhead  would even grow beyond that. It's yet another annotation per property to  refer to. I'd actually would be interested in seeing an XML definition  example because it feels like the annotation based definition might actually  be more verbose than the XML equivalent. I strongly recommend to favor conciseness on the definition / user side of  things over a few more lines of implementation code. Parts of the spec  already do a suboptimal job in this regard and I think we do the users and  the spec a favor if we make it as usable as possible. Cheers, Ollie - Sent while on the run...  </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Fri Dec 07 21:01:19 CET 2012</date>
    <body>Hi Ollie, While I sympathize with your ease-of-use argument here,  I do think that  having 3 ways of specifying attributes in NamedEntityGraph would be a bit much. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Fri Dec 07 22:10:09 CET 2012</date>
    <body>Hi Rainer, all Yes, and I think this argues for the entity graph approach.  For example, as an employee, I don't want my home address exposed (or necessarily loaded), although exposing my work address is fine.  As a customer, I don't want my work address exposed/loaded.  Similarly for home/work/cell phone numbers. best regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Fri Dec 07 22:16:39 CET 2012</date>
    <body>Hi Pinaki, I disagree here.  I think that readability and clarity at the point of use  are more important here. I also think that in many cases only one graph for a particular type will not  be sufficient in this reuse scenario anyway, and that one will have to have multiple such graphs  anyway for the needed flexibility. In addition, if we did get strong feedback from the community that the  inclusion by name approach were needed, we could add it in a future release as an option to what we have  proposed. best regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Entity Graphs</header>
    <date>Mon Dec 10 00:04:55 CET 2012</date>
    <body>I've spec'd out a more detailed version of the proposal.  Please review and  comment. thanks, -Linda ------------------------------- Section: Entity Graphs An entity graph is a template that is defined in the form of metadata or an object created by the dynamic EntityGraph API and that captures the path and boundaries for an operation or query. Entity graphs are used in the specification of "fetch plans" for query or find operations and as specifications for the boundaries of merge or copy operations. Subsection:  Use of Entity Graphs in find and query operations An entity graph can be used with the find operation or as a query hint to override or augment FetchType semantics. The standard properties javax.persistence.fetchgraph and javax.persistence.loadgraph are used to specify such graphs to queries and find operations. The default fetch graph for an entity or embeddable is defined to consist of the transitive closure of all of its attributes that are specified as FetchType.EAGER (or defaulted as such). The persistence provider is permitted to fetch additional entity state beyond that specified by a fetch graph or load graph.  It is required, however, that the persistence provider fetch all state specified by the fetch or load graph. Subsectionsection:  Fetch graph semantics When the javax.persistence.fetchgraph property is used to specify an entity graph, attributes that are specified by attribute nodes of the entity graph are treated as FetchType.EAGER and attributes that are not specified are treated as FetchType.LAZY.  The primary key and version attributes of an entity are always retrieved, even if not specified by the fetch graph. The following rules apply, depending on attribute type.  The rules of this section are applied recursively. A primary key or version attribute never needs to be specified in an attribute node of a fetch graph.  (This applies to composite primary keys as well, including embedded id primary keys.)  When an entity is fetched, its primary key and version attributes are always fetched. It is not incorrect, however, to specify primary key attributes or version attributes. Attributes other than primary key and version attributes are assumed not to be fetched unless the attribute is specified.  The following rules apply to the specification of attributes: If the attribute is an embedded attribute, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the default fetch graph for the embeddable is fetched. If a subgraph is specified for the attribute, the attributes of the embeddable are fetched according to their specification in the corresponding subgraph. If the attribute is an element collection of basic type, and the attribute is specified in an attribute node, the element collection together with its basic elements is fetched. If the attribute is an element collection of embeddables, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the element collection together with the default fetch graph of its embeddable elements is fetched.  If a subgraph is specified for the attribute, the attributes of the embeddable elements will be fetched according to the corresponding subgraph specification. If the attribute is a one-to-one or many-to-one relationship, and the attribute is specified in an attribute node, but a subgraph is not specified for the attribute, the default fetch graph of the target entity is fetched.  If a subgraph is specified for the attribute, the attributes of the target entity will be fetched according to the corresponding subgraph specification. If the attribute is a one-to-many or many-to-many relationship, and the attribute is specified in an attribute node, but a subgraph is not specified, the collection will be fetched and the default fetch graphs of the referenced entities will be fetched.  If a subgraph is specified for the attribute, the entities in the collection will be fetched according to the corresponding subgraph specification. If the key of a map which has been specified in an attribute node is a basic type, it will always be fetched. If the key of a map which has been specified in an attribute node is an embedded type the default fetch graph will be fetched for the embeddable.  Otherwise, if the key of the map is an entity, and a map key subgraph is not specified for the attribute node, the map key will be fetched according to its default fetch graph.  If a key subgraph is specified for the map key attribute, the map key attribute will be fetched according to the map key subgraph specification. Example: @NamedEntityGraph @Entity     @Id    ... In this example, only the number attribute would be eagerly fetched. Example: @NamedEntityGraph(     @NamedAttributeNode("projects") ) @Entity   @Id   @GeneratedValue   @Basic   @Basic   @OneToMany()   @OneToMany()   @OneToMany()   ... @Entity @Inheritance     @Id     @GeneratedValue     @OneToOne(fetch=FetchType.EAGER)    ... @Entity     @OneToOne(fetch=FetchType.LAZY)    ... @Entity     @Id     @Lob     @OneToOne(fetch=FetchType.LAZY)     protected Approval approval    ... In the above example, the Employee entity's primary key will be fetched as well as the related project instances, whose default fetch graph (id, name, and doc attributes) will be fetched.  The related Requirements object will be fetched according to its default fetch graph. If the approver attribute of LargeProject were FetchType.EAGER, and if any of the projects were instances of LargeProject, their approver attributes would also be fetched. Since the type of the approver attribute is Employee, the approver's default fetch graph (id, name, and employeeNumber attributes) would also be fetched. Subsubsection:  Load graph semantics: When the javax.persistence.loadgraph property is used to specify an entity graph, attributes that are specified by attribute nodes of the entity graph are treated as FetchType.EAGER and attributes that are not specified are treated according to their specified or default FetchType.  The primary key and version attributes of an entity are always retrieved. The following rules apply.  The rules of this section are applied recursively. A primary key or version attribute never needs to be specified in an attribute node of a load graph.  (This applies to composite primary keys as well, including embedded id primary keys.)  When an entity is fetched, its primary key and version attributes are always fetched. It is not incorrect, however, to specify primary key attributes or version attributes. If the attribute is an embedded attribute, and the attribute is specified in an attribute node, the default fetch graph for the embeddable is fetched. If a subgraph is specified for the attribute, attributes that are specified by the subgraph are also fetched. If the attribute is an element collection of basic type, and the attribute is specified in an attribute node, the element collection together with its basic elements is fetched. If the attribute is an element collection of embeddables, and the attribute is specified in an attribute node, the element collection together with the default fetch graph of its embeddable elements is fetched.  If a subgraph is specified for the attribute, attributes that are specified by the subgraph are also fetched. If the attribute is a one-to-one or many-to-one relationship, and the attribute is specified in an attribute node, the default fetch graph of the target entity is fetched.  If a subgraph is specified for the attribute, attributes that are specified by the subgraph are also fetched. If the attribute is a one-to-many or many-to-many relationship, and the attribute is specified in an attribute node, the collection will be fetched and the default fetch graphs of the referenced entities will be fetched.  If a subgraph is specified for the attribute, attributes that are specified by the subgraph are also fetched. If a collection-valued attribute is a map, and the map-valued attribute is specified in an attribute node, keys that are basic or embeddable types will be fetched when the map is fetched; entity map key attributes will be fetched according to the default fetch graph and, if a key subgraph is specified, additional entity attributes are fetched as specified in the subgraph. Example: @NamedEntityGraph @Entity     @Id    ... In the above example, the number and type attributes are loaded. Example: @NamedEntityGraph(     @NamedAttributeNode("projects") ) @Entity   @Id   @GeneratedValue   @Basic   @Basic   @OneToMany()   @OneToMany()   @OneToMany()   ... @Entity @Inheritance     @Id     @GeneratedValue     @OneToOne(fetch=FetchType.EAGER)    ... @Entity     @OneToOne(fetch=FetchType.LAZY)    ... @Entity     @Id     @Lob     @OneToOne(fetch=FetchType.LAZY)     protected Approval approval    ... In the above example, the default fetch graph (id, name, and employeeNumber) of Employee is loaded.  The default fetch graphs of the related Project instances (id, name, and doc attributes) and their Requirements instances (id and description attributes) are also loaded. Subsection:  Merge graph semantics An entity graph may be used as a "merge graph" and passed as an argument to the merge method. The following semantics apply to entity graphs that are used as merge graphs. A merge graph attribute node specified within an entity graph or subgraph specifies how an attribute is to be merged.  Primary key and version attributes do not need to be specified in the merge graph.  If other attributes are not specified, they are not merged.  Note that cascade=MERGE specifications are ignored. The persistence provider must observe the scope and boundaries of a merge graph specification exactly. The following additional rules apply for attributes that are specified in attribute nodes.  These rules are applied recursively. If the attribute is an embedded attribute and a subgraph is not specified for the attribute, the embedded attribute is merged but the attributes of the embeddable are not merged.  If a subgraph is specified for the attribute, the attributes of the embeddable are merged according to their specification in the corresponding subgraph. If the attribute is an element collection of basic type, the element collection is merged.  The values in the element collection are replaced. If the attribute is an element collection of embeddables and a subgraph is not specified for the attribute, the element collection is merged.  The values in the element collection are replaced and all attributes of the embeddables are included.  If a subgraph is specified for the attribute, the values in the element collection are replaced and all attributes of the embeddables are included, and the attributes specified in the subgraph are processed according to the subgraph specification. If the attribute is a one-to-one or many-to-one relationship and a subgraph is not specified for the attribute, the attribute is merged, but the attributes of the target entity are not merged.  If a subgraph is specified for the attribute, the attributes of the target entity will be merged according to the corresponding subgraph specification. If the attribute is a one-to-many or many-to-many relationship and a subgraph is not specified for the attribute, the attribute is merged, but the attributes of the target entity are not merged.  If a subgraph is specified for the attribute, the entities in the collection will be merged according to the corresponding subgraph specification.   In both of the two relationship cases above, note that if a new   entity (entity in the "new" state) was added to the relationship and   only a subset of its attributes is specified in the subgraph, only   those specified attributes are copied. If the attribute is a map, the map key will be merged.  If the map key is an embeddable, all attributes of the embeddable are included.  If the map key is an entity, the attribute is merged, but the attributes of the target entity are not merged.  If a subgraph is specified for the attribute, the target entity is merged according to the corresponding subgraph specification. Example: @NamedEntityGraph(         @NamedAttributeNode("name"),         @NamedAttributeNode(             value="projects",             subGraph="projects"         ),         @NamedAttributeNode("phoneNumbers"),         @NamedSubGraph(             name="projects",                 @NamedAttributeNode("doc")         ) ) @Entity   @Id   @GeneratedValue   @Basic   @Basic   @OneToMany()   @OneToMany()   @OneToMany()   ... @Entity     @Id    ... @Entity @Inheritance     @Id     @GeneratedValue     @OneToOne(fetch=FetchType.EAGER)    ... In the above example,  the name attribute and the projects and phoneNumbers collections will be merged.  Within projects, only the doc attribute will be merged.  No attributes of phoneNumbers will be merged. Subsection:  Copy graph semantics An entity graph may be used as a "copy graph" and passed as an argument to the copy method. The copy method is intended for use in disconnecting a graph of entities from a larger graph.  When using an entity graph in combination with the copy operation, a copy of the entity is created and the attributes of the copied entity are populated based on copies of the attributes listed in the entity graph. The following semantics apply to entity graphs that are used as copy graphs. The persistence provider must observe the scope and boundaries of a copy graph specification exactly. OPEN ISSUE:  If a copy graph specifies an attribute that has not yet been loaded, should it be loaded? The following rules apply to the specification of attributes. The rules of this section are applied recursively. Primary key and version attributes are always copied. Attributes are otherwise not copied unless they are specified. If the attribute is an embedded attribute and an attribute node is specified for the attribute but a subgraph is not specified for the attribute, a new instance of the embeddable is inserted into the resulting copy of the entity graph, but no state is copied.  If a subgraph is specified for the attribute, the attributes of the embeddable are copied according to their specification in the corresponding subgraph. If the attribute is an element collection of basic type, the element collection and its contents are copied. If the attribute is an element collection of embeddables and an attribute node is specified for the attribute but a subgraph is not specified, a new collection is created and new embeddables instances are inserted into it, but no state is copied.  If a subgraph is specified for the attribute, the embeddables are copied according to their specification in the subgraph. If the attribute is a one-to-one or many-to-one relationship and an attribute node is specified for the attribute, but a subgraph is not specified, a copy of the entity is created and inserted.  Only the primary key and version attributes of the entity are copied.  If a subgraph is specified for the attribute, a copy of the entity is created and inserted and the attributes of the target entity are copied according to the corresponding subgraph specification along with the primary key and version attributes. If the attribute is a one-to-many or many-to-many relationship and an attribute node is specified for the attribute, but a subgraph is not specified, a new collection is created and inserted, and copies of the referenced entities are created and inserted into the collection. Only the primary key and version attributes of these entities are copied.  If a subgraph is specified for the attribute, the entities in the collection will be copied according to the corresponding subgraph specification along with the primary key and version attributes. If the attribute is a map and an attribute node has been specified for the attribute:    if the map key attribute is an embedded attribute, a new instance of the    if the map key attribute is an entity, a copy of the entity is created,        and only the primary key and version attributes of the entity are  copied.    If a subgraph is specified for the embeddable or entity, the attributes        of the target are copied according to the corresponding map key        subgraph specification. Example: @NamedEntityGraph(         @NamedAttributeNode("name"),         @NamedAttributeNode(             value="projects",             subGraph="projects"         ),         @NamedAttributeNode("phoneNumbers"),         @NamedSubGraph(             name="projects",                 @NamedAttributeNode("doc")         ) ) @Entity   @Id   @GeneratedValue   @Basic   @Basic   @OneToMany()   @OneToMany()   @OneToMany()   ... @Entity     @Id    ... @Entity @Inheritance     @Id     @GeneratedValue     @OneToOne(fetch=FetchType.EAGER)    ... In the above example, a new Employee instance will be created and the values of the id and name attributes copied.  The projects and phoneNumbers collections are recreated and populated in the copy.  For the entities within the new projects collection, the id attributes are copied and new Requirements objects created.  Only the id attribute of the Requirement entity is copied.  For the entities within the new phoneNumbers collection, only the number attribute is copied.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Mon Dec 10 00:06:39 CET 2012</date>
    <body>OK, I've been mulling this over some more and I have to admit I am really  torn about this issue in view of Ollie's verbosity argument. Can we have some more feedback on this point, please? Keep in mind that if we don't add such an element in now, we still have the  option of doing so in a future draft or future release. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Merging of referenced new objects w/o cascade-merge specification?</header>
    <date>Mon Dec 10 13:56:24 CET 2012</date>
    <body>Dear group, What is supposed to happen in this scenario: @Entity     //...     Address address;    //cascade-merge or cascade-all are not specified @Entity //... //what does m_p's "address" field contain? Or should merge throw an exception right away? Thank you! Christian -- Christian von Kutzleben Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cvkutzleben@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Merging of referenced new objects w/o cascade-merge specification?</header>
    <date>Mon Dec 10 21:21:48 CET 2012</date>
    <body>Hello Christian,    "m_p" would reference a new managed instance of the Address()  that would be fully copied from the original Address.   The Address is fully copied because it is a practical way to get a managed version of the Address. --Gordon On 10/12/2012 8:56 AM, Christian von Dear group, What is supposed to happen in this scenario: @Entity     //...     Address address;    //cascade-merge or cascade-all are not specified @Entity //... //what does m_p's "address" field contain? Or should merge throw an exception right away? Thank you! Christian -- Christian von Kutzleben Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cvkutzleben@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Merging of referenced new objects w/o cascade-merge specification?</header>
    <date>Tue Dec 11 09:37:04 CET 2012</date>
    <body>Hi Gordon On Mon, Dec 10, 2012 at 9:21 PM, Gordon Yorke gordon.yorke@... Hello Christian,    "m_p" would reference a new managed instance of the Address()  that would be fully copied from the original Address.   The Address is fully copied because it is a practical way to get a managed version of the Address. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Merging of referenced new objects w/o cascade-merge specification?</header>
    <date>Tue Dec 11 14:46:37 CET 2012</date>
    <body>Hi Christian, While I think/hope the book describes the spec as completely as possible it probably shouldn't be taken as the definitive word ;-). However, I obviously agree with you that the spec is not well-defined for this case and needs to be clarified. My gut feeling is that it should probably end up as an ISE when the object gets flushed since there was no directive to merge the address, i.e. cascade merge was *not* specified. -Mike Hi Gordon On Mon, Dec 10, 2012 at 9:21 PM, Gordon Yorke gordon.yorke@... &lt;blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"&gt; Hello Christian,    "m_p" would reference a new managed instance of the Address()  that would be fully copied from the original Address.   The Address is fully copied because it is a practical way to get a managed version of the Address. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Schema generation APIs</header>
    <date>Tue Dec 11 15:59:37 CET 2012</date>
    <body>After some review it seems to me that PersistenceProvider is missing the following method:     /**      * Create database schemas and/or tables and/or create DDL      * scripts as determined by the supplied properties.      * Called by the Persistence class when schema generation is to      * occur as a separate phase from creation of the entity      * manager factory.      * @param persistenceUnitName the name of the persistence unit      * @param map properties for schema generation;  these may      *             also contain provider-specific properties.  The      *             value of these properties override any values that      *             may have been configured elsewhere..      * @throws PersistenceException if insufficient or inconsistent      *         configuration information is provided of if schema      *         generation otherwise fails      *      * @since Java Persistence 2.1      */ as currently there is no support for the Persistence.generateSchema(String, Map) method. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Merging of referenced new objects w/o cascade-merge specification?</header>
    <date>Tue Dec 11 16:33:46 CET 2012</date>
    <body>Yes, it is not defined at all in the specification and I agree that it should be. --Gordon On 11/12/2012 4:37 AM, Christian von Hi Gordon On Mon, Dec 10, 2012 at 9:21 PM, Gordon Yorke gordon.yorke@... &lt;blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"&gt; Hello Christian,    "m_p" would reference a new managed instance of the Address()  that would be fully copied from the original Address.   The Address is fully copied because it is a practical way to get a managed version of the Address. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Merging of referenced new objects w/o cascade-merge specification?</header>
    <date>Tue Dec 11 16:43:11 CET 2012</date>
    <body>An ISE would have been the correct choice when JPA was introduced but if we introduce this now I suspect a lot of applications will stop working when someone upgrades.  At this point we either leave it undefined or the spec should be updated to cascade the merge to new objects. --Gordon On 11/12/2012 9:46 AM, michael keith Hi Christian, While I think/hope the book describes the spec as completely as possible it probably shouldn't be taken as the definitive word ;-). However, I obviously agree with you that the spec is not well-defined for this case and needs to be clarified. My gut feeling is that it should probably end up as an ISE when the object gets flushed since there was no directive to merge the address, i.e. cascade merge was *not* specified. -Mike Hi Gordon On Mon, Dec 10, 2012 at 9:21 PM, Gordon Yorke gordon.yorke@... &lt;blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"&gt; Hello Christian,    "m_p" would reference a new managed instance of the Address()  that would be fully copied from the original Address.   The Address is fully copied because it is a practical way to get a managed version of the Address. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Merging of referenced new objects w/o cascade-merge specification?</header>
    <date>Tue Dec 11 16:57:04 CET 2012</date>
    <body>Yeah, it's always a little tricky to catch the horse once it has left the barn.  Someone ends up getting kicked ;-) If we want to update the spec to define the behavior for this case then we would need to poll the different impls to see what everybody does. If they all do the same thing then we could just specify that to be the "right thing". If not, we either go ahead and specify it anyway and the vendors who made the "wrong" choice have to figure out how to best help their customers, or we decide that the cost of specifying is not worth it. An ISE would have been the correct choice when JPA was introduced but if we introduce this now I suspect a lot of applications will stop working when someone upgrades.  At this point we either leave it undefined or the spec should be updated to cascade the merge to new objects. --Gordon On 11/12/2012 9:46 AM, michael keith Hi Christian, While I think/hope the book describes the spec as completely as possible it probably shouldn't be taken as the definitive word ;-). However, I obviously agree with you that the spec is not well-defined for this case and needs to be clarified. My gut feeling is that it should probably end up as an ISE when the object gets flushed since there was no directive to merge the address, i.e. cascade merge was *not* specified. -Mike Hi Gordon On Mon, Dec 10, 2012 at 9:21 PM, Gordon Yorke gordon.yorke@... &lt;blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"&gt; Hello Christian,    "m_p" would reference a new managed instance of the Address()  that would be fully copied from the original Address.   The Address is fully copied because it is a practical way to get a managed version of the Address. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Merging of referenced new objects w/o cascade-merge specification?</header>
    <date>Tue Dec 11 18:43:00 CET 2012</date>
    <body>Yes, that is how we need to proceed. I'd like to hear from the experts on this group who represent JPA  implementations:   How do you all handle this case? If we have convergence on this, we should definitely clarify the spec.  If  not, I'm afraid we are stuck with "undefined". thanks, Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Schema generation APIs</header>
    <date>Tue Dec 11 18:54:09 CET 2012</date>
    <body>PersistenceProvider has the following method, which subsumes this  functionality: We don't need both, do we?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Schema generation APIs</header>
    <date>Tue Dec 11 18:55:41 CET 2012</date>
    <body>Yes, otherwise the javax.persistence.Persistence class will need access to an XML library to create the PersistenceUnitInfo from the persistence.xml --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Schema generation APIs</header>
    <date>Tue Dec 11 19:45:25 CET 2012</date>
    <body>OK thanks -- I will add. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Merging of referenced new objects w/o cascade-merge specification?</header>
    <date>Wed Dec 12 09:22:26 CET 2012</date>
    <body>Hi Linda, For Versant/JPA we are open to change our behavior for this case. Christian On Tue, Dec 11, 2012 at 6:43 PM, Linda DeMichiel linda.demichiel@... Yes, that is how we need to proceed. I'd like to hear from the experts on this group who represent JPA implementations:   How do you all handle this case? If we have convergence on this, we should definitely clarify the spec.  If not, I'm afraid we are stuck with "undefined". thanks, Linda Yeah, it's always a little tricky to catch the horse once it has left the barn. Someone ends up getting kicked ;-) If we want to update the spec to define the behavior for this case then we would need to poll the different impls to see what everybody does. If they all do the same thing then we could just specify that to be the "right thing". If not, we either go ahead and specify it anyway and the vendors who made the "wrong" choice have to figure out how to best help their customers, or we decide that the cost of specifying is not worth it. An ISE would have been the correct choice when JPA was introduced but if we introduce this now I suspect a lot of applications will stop working when someone upgrades. At this point we either leave it undefined or the spec should be updated to cascade the merge to new objects. --Gordon Hi Christian, While I think/hope the book describes the spec as completely as possible it probably shouldn't be taken as the definitive word ;-). However, I obviously agree with you that the spec is not well-defined for this case and needs to be clarified. My gut feeling is that it should probably end up as an ISE when the object gets flushed since there was no directive to merge the address, i.e. cascade merge was *not* specified. -Mike Hi Gordon gordon.yorke@... gordon.yorke@oracle. com     Hello Christian,     "m_p" would reference a new managed instance of the Address() that would be fully copied from the original     Address. The Address is fully copied because it is a practical way to get a managed version of the Address.     --Gordon While researching more on this issue, I did also find the following statement in "Pro JPA 2", p. 161 "Merging Detached Entities": "If the entity has a relationship to an object that has no persistent identity, the outcome of the merge operation is undefined." So I agree, that what you described is a useful behavior (and that is what we do, if cascade-merge is enabled), however this is not really defined in the spec, and IMO needs clarification. Thank you! Christian     Dear group,     What is supposed to happen in this scenario:     @Entity     //...     Address address; //cascade-merge or cascade-all are not specified     @Entity     //...     //what does m_p's "address" field contain? Or should merge throw an exception right away?     Thank you!     Christian     --     Christian von Kutzleben     Chief Engineer | Versant GmbH     (T) +49 40 60990-0     (F) +49 40 60990-113     (E) cvkutzleben@... cromberg@...     www.versant.com http://www.versant.com www.db4o.com http://www.db4o.com     --     Versant     GmbH is incorporated in Germany. Company registration number: HRB     54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359     Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John     CONFIDENTIALITY     NOTICE: This e-mail message, including any attachments, is for the sole     use of the intended recipient(s) and may contain confidential or     proprietary information. Any unauthorized review, use, disclosure or     distribution is prohibited. If you are not the intended recipient,     immediately contact the sender by reply e-mail and destroy all copies of     the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Merging of referenced new objects w/o cascade-merge specification?</header>
    <date>Wed Dec 12 21:06:28 CET 2012</date>
    <body>We cannot require an exception immediately on the call to merge since the spec allows the cascades (or lack thereof) to be processed at flush time.  That is what happens in Hibernate.  You'd end up here with a javax.persistence.RollbackException (caused by an IllegalStateException due to the transient, non-cascaded association).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] fetch joins with ON conditions</header>
    <date>Thu Dec 13 21:25:52 CET 2012</date>
    <body>We still have the open issue  http://java.net/jira/browse/JPA_SPEC-40 regarding problems with fetch joins and ON conditions. At this point, I am inclined to remove support for ON conditions with fetch joins, since with the proposal for the more general support for prefetching in terms of fetch graphs, fetch joins become less important in general. This would entail removal from the JPQL BNF as well as removal of methods on the Fetch interface. Can you please let me know whether you support this change or not. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Section 8.2.1.6.2 clarification?</header>
    <date>Thu Dec 13 22:11:34 CET 2012</date>
    <body>Section 8.2.1.6.2 of the JPA 2.0 specification (page 314) talks about entity mapping files ( orm.xml and its ilk). I am trying to understand whether the spec calls for META-INF/orm.xml to always be processed, or only when one of the following is true: META-INF/orm.xml is explicitly included in persistence.xml as a mapping-file element META-INF/orm.xml is not mentioned anywhere in persistence.xml Specifically, the wording that confused me was this: A [sic] object/relational mapping XML file named orm.xml may be specified in the META-INF directory in the root of the persistence unit or in the META-INF directory of any jar file referenced by the persistence.xml . Alternatively, or in addition , one or more mapping files may be referenced by the mapping-file elements of the persistence-unit element.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Section 8.2.1.6.2 clarification?</header>
    <date>Thu Dec 13 22:24:38 CET 2012</date>
    <body>Yes, it will be.  I thought the paragraph following the one you cite made  this clear, but will have another look. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Section 8.2.1.6.2 clarification?</header>
    <date>Thu Dec 13 22:38:45 CET 2012</date>
    <body>On Thu, Dec 13, 2012 at 1:24 PM, Linda DeMichiel linda.demichiel@... Yes, it will be.  I thought the paragraph following the one you cite made this clear, but will have another look.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] new spec draft (Public Draft candidate)</header>
    <date>Fri Dec 14 00:57:38 CET 2012</date>
    <body>I've just uploaded a new draft of the specification, which includes the entity group functionality we have been discussing.  I would like to submit this (or something very close to it) next week to the JCP for the Public Draft Review process and to expose it to the wider community for greater feedback. See  http://java.net/projects/jpa-spec/downloads Please let me know asap if you see any problems or think I should not. The changebars show the delta from the previous draft (EDR2). I plan to be uploading javadocs and updated orm.xsd files as well. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new spec draft (Public Draft candidate)</header>
    <date>Fri Dec 14 01:16:49 CET 2012</date>
    <body>I've just uploaded an updated orm.xsd.  Unfortunately java.net doesn't let me upload a file of the same name as one that is already there, so I had to either rename the earlier version or delete it (I chose to rename, so we have the history). Anyway, be sure to grab the "draft 3" version (it is dated December 11, 2012  inside). -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: fetch joins with ON conditions</header>
    <date>Fri Dec 14 12:47:07 CET 2012</date>
    <body>Linda DeMichiel, am 13 Dec 2012 hast Du um 12:25 zum Thema "[jsr338- experts] fetch joins with ON conditions"  geschrieben :  We still have the open issue  http://java.net/jira/browse/JPA_SPEC-40  regarding problems with fetch joins and ON conditions.    At this point, I am inclined to remove support for ON conditions  with fetch joins, since with the proposal for the more general support  for prefetching in terms of fetch graphs, fetch joins become less  important in general.    This would entail removal from the JPQL BNF as well as removal of  methods on the Fetch interface.    Can you please let me know whether you support this change or not. +1 --- Rainer Schweigkoffer               SAP AG Walldorf Regulatory Compliance              TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, F.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...       fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Lars  Dalgaard, Luisa Deplazes Delgado, Bill McDermott (Co-CEO),  Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP  Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse  oder sonstige vertrauliche Informationen enthalten.  Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist  Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung  oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte  benachrichtigen Sie uns und vernichten Sie die empfangene  E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged,  undisclosed, or otherwise confidential information. If you  have received this e-mail in error, you are hereby  notified that any review, copying, or distribution of it  is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your  cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new spec draft (Public Draft candidate)</header>
    <date>Fri Dec 14 18:58:27 CET 2012</date>
    <body>One of our folks pointed out to me my typos in appendix A.10 (change log for this draft).  I've just uploaded a corrected copy.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: fetch joins with ON conditions</header>
    <date>Fri Dec 14 22:27:59 CET 2012</date>
    <body>Thanks Rainer! Can I please hear from more of you on this issue? -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: fetch joins with ON conditions</header>
    <date>Fri Dec 14 23:03:15 CET 2012</date>
    <body>Linda, Wouldn't this break backwards compatibility? On Fri, Dec 14, 2012 at 4:27 PM, Linda DeMichiel linda.demichiel@... Thanks Rainer! Can I please hear from more of you on this issue? -Linda Linda DeMichiel, am 13 Dec 2012 hast Du um 12:25 zum Thema "[jsr338- experts] fetch joins with ON conditions"  geschrieben : We still have the open issue http://java.net/jira/browse/ JPA_SPEC-40 regarding problems with fetch joins and ON conditions. At this point, I am inclined to remove support for ON conditions with fetch joins, since with the proposal for the more general support for prefetching in terms of fetch graphs, fetch joins become less important in general. This would entail removal from the JPQL BNF as well as removal of methods on the Fetch interface. Can you please let me know whether you support this change or not.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: [jsr338-experts] Re: fetch joins with ON conditions</header>
    <date>Fri Dec 14 23:08:01 CET 2012</date>
    <body>No, this feature (ON-conditions) was only introduced in earlier drafts in  this JSR. I appreciate the question though, as backwards compatibility is something that we take very seriously. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: fetch joins with ON conditions</header>
    <date>Fri Dec 14 23:29:30 CET 2012</date>
    <body>I understand the danger, but still see benefit for advanced users.  If we want to remove it from the spec, I am fine with that; ultimately a vendor could still choose to support it as an extension.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: fetch joins with ON conditions</header>
    <date>Fri Dec 14 23:40:27 CET 2012</date>
    <body>They won't be able to support it via the Criteria APIs however.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: fetch joins with ON conditions</header>
    <date>Sat Dec 15 00:07:00 CET 2012</date>
    <body>They could offer their own extensions to the Criteria interfaces.  But overall, yes, I agree it would not be as transparent in the Criteria case as users would need to cast to those vendor-specific extension interfaces thereby creating a vendor dependency.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: fetch joins with ON conditions</header>
    <date>Sat Dec 15 09:43:14 CET 2012</date>
    <body>Steve Ebersole, am 14 Dec 2012 hast Du um 17:07 zum Thema "[jsr338- experts] Re: fetch joins with ON conditio"  geschrieben :  They could offer their own extensions to the Criteria interfaces.  But   overall, yes, I agree it would not be as transparent in the Criteria   case as users would need to cast to those vendor-specific extension   interfaces thereby creating a vendor dependency.   This is not necessarily a disadvantage. They would have a vendor  dependency anyway. In the JPQL case this would only be a functional  one; in the criteria case it would be documented formally. Best regards Rainer --- Rainer Schweigkoffer               SAP AG Walldorf Regulatory Compliance              TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, F.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...       fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Lars  Dalgaard, Luisa Deplazes Delgado, Bill McDermott (Co-CEO),  Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP  Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse  oder sonstige vertrauliche Informationen enthalten.  Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist  Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung  oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte  benachrichtigen Sie uns und vernichten Sie die empfangene  E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged,  undisclosed, or otherwise confidential information. If you  have received this e-mail in error, you are hereby  notified that any review, copying, or distribution of it  is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your  cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: fetch joins with ON conditions</header>
    <date>Sat Dec 15 18:46:27 CET 2012</date>
    <body>+1 linda.demichiel@... Thanks Rainer! Can I please hear from more of you on this issue? -Linda Linda DeMichiel, am 13 Dec 2012 hast Du um 12:25 zum Thema "[jsr338- experts] fetch joins with ON conditions"  geschrieben : We still have the open issue http://java.net/jira/browse/ JPA_SPEC-40 regarding problems with fetch joins and ON conditions. At this point, I am inclined to remove support for ON conditions with fetch joins, since with the proposal for the more general support for prefetching in terms of fetch graphs, fetch joins become less important in general. This would entail removal from the JPQL BNF as well as removal of methods on the Fetch interface. Can you please let me know whether you support this change or not.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Mon Dec 17 10:25:48 CET 2012</date>
    <body> &amp;gt;## Use cases  &amp;gt;I would really like to get use cases associated to each of the new  &amp;gt;features proposed here.  &amp;gt;In particular:  &amp;gt;- it's unclear to me why you really need loadgraph and fetchgraph.  &amp;gt;- why is merge( fetchgraph ) required when you can achieve the same with  &amp;gt;   - find( fetchgraph )  &amp;gt;   - detach / serialize or whatever  &amp;gt;   - merge() regular method  merge(EntityGraph) applies when the user is using copy(EntityGraph)  to detach an entity tree for serialization.  If an attribute is not  present in the copy then it must not be merged.  So, your example  becomes:      - find (EntityGraph eg1)      - copy(EntityGraph eg1)      - serialize out/in      - merge(EntityGraph eg1) Sure but the graph returned by copy does contain guards on uninitialized properties and associations right? Ie when a non copied property / association is traversed, some exception is raised. If that's not the case then what are you returning? null or the primitive default values? I would be strongly against that. So assuming the engine has those guards on uninitialized properties and associations, then the merge with an EntityGraph is not necessary as this information can be discovered (like we do for a regular merge). Emmanuel</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Mon Dec 17 14:40:53 CET 2012</date>
    <body>Hello Emmanuel, The goal of a copy graph is to produce a simple subgraph copy of the entity graph for environments where enhancement/proxies may not be available or where these "guards" can not be retained when then graph is serialized.  The developer has a template in the form of the entity graph which provides the details of which attributes have legitimate values and which ones should be ignored and this template is highly recommended for the merge.  There is no requirement for "guards" embedded within the copies.  In client/server environments where "guards" would be available there is no need for copy() at all as the same functionality is achieved serializing entities loaded with a fetch graph. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Mon Dec 17 18:09:14 CET 2012</date>
    <body>OK I get it now. To be honest I am quite uncomfortable with such a feature. Does everyone really think that this feature is a must have? I imagine that the use case you have in mind is to pass this copied object graph to another serialization or traversal framework that would otherwise traverse the JPA guards like there is no tomorrow. Our experience with Bean Validation has shown that handling guards is extremely easy and that the necessary contract to make a traversal / serialization framework aware of these is quite simple  https://github.com/beanvalidation/beanvalidation-api/blob/master/src/main/java/javax/validation/TraversableResolver.java The solution proposed here (the copy) forces to copy an object graph which leads to memory duplication just to get the data serialized in another form. Plus the user *must* use the same graph definition for both `copy` and `merge` or else data loss are going to happen. I find the programming model quite error prone and dangerous. Emmanuel  Hello Emmanuel,    The goal of a copy graph is to produce a simple subgraph copy of  the entity graph for environments where enhancement/proxies may not  be available or where these "guards" can not be retained when then  graph is serialized.  The developer has a template in the form of  the entity graph which provides the details of which attributes have  legitimate values and which ones should be ignored and this template  is highly recommended for the merge.  There is no requirement for  "guards" embedded within the copies.  In client/server environments  where "guards" would be available there is no need for copy() at all  as the same functionality is achieved serializing entities loaded  with a fetch graph.    --Gordon  &amp;gt;&amp;gt;&amp;gt;## Use cases  &amp;gt;&amp;gt;&amp;gt;I would really like to get use cases associated to each of the new  &amp;gt;&amp;gt;&amp;gt;features proposed here.  &amp;gt;&amp;gt;&amp;gt;In particular:  &amp;gt;&amp;gt;&amp;gt;- it's unclear to me why you really need loadgraph and fetchgraph.  &amp;gt;&amp;gt;&amp;gt;- why is merge( fetchgraph ) required when you can achieve the same with  &amp;gt;&amp;gt;&amp;gt;   - find( fetchgraph )  &amp;gt;&amp;gt;&amp;gt;   - detach / serialize or whatever  &amp;gt;&amp;gt;&amp;gt;   - merge() regular method  &amp;gt;&amp;gt;merge(EntityGraph) applies when the user is using copy(EntityGraph)  &amp;gt;&amp;gt;to detach an entity tree for serialization.  If an attribute is not  &amp;gt;&amp;gt;present in the copy then it must not be merged.  So, your example  &amp;gt;&amp;gt;becomes:  &amp;gt;&amp;gt;     - find (EntityGraph eg1)  &amp;gt;&amp;gt;     - copy(EntityGraph eg1)  &amp;gt;&amp;gt;     - serialize out/in  &amp;gt;&amp;gt;     - merge(EntityGraph eg1)  &amp;gt;Sure but the graph returned by copy does contain guards on uninitialized  &amp;gt;properties and associations right? Ie when a non copied property /  &amp;gt;association is traversed, some exception is raised.  &amp;gt;If that's not the case then what are you returning? null or the  &amp;gt;primitive default values? I would be strongly against that.  &amp;gt;So assuming the engine has those guards on uninitialized properties and  &amp;gt;associations, then the merge with an EntityGraph is not necessary as  &amp;gt;this information can be discovered (like we do for a regular merge).  &amp;gt;Emmanuel  </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: fetch joins with ON conditions</header>
    <date>Mon Dec 17 19:35:20 CET 2012</date>
    <body>OK -- on the basis of the feedback I've received, I plan to remove fetch joins with ON conditions for the public draft. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: fetch joins with ON conditions</header>
    <date>Mon Dec 17 20:07:01 CET 2012</date>
    <body>Hello,   &amp;gt;&amp;gt; Can you please let me know whether you support this change or not.   +1   I prefer an approach where a syntax (such as EntityGraph/Subgraph/FetchPlan or whatever whatever other moniker) describes a subest or view of the entire domain model and then the subset specification is applied on persistent operations such as find(), merge(), refresh() etc. Such cascaded behavior/concept already exists in the spec, we can make that behavior more sophisticated. This approach will provide an unified as well as extensible means to grow the spec, imo.   I do not prefer the approach where query takes the dual role of selection and cascading behavior for load via ON (or, for that matter, FETCH JOIN) keywords in JPQL. Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Linda DeMichiel ---12/14/2012 02:40:51 PM---On 12/14/2012 2:29 PM, Steve Ebersole wrote: &amp;gt; I understand the danger, but still see benefit for ad From: To: jsr338-experts@... Date: 12/14/2012 02:40 PM Subject: [jsr338-experts] Re: fetch joins with ON conditions They won't be able to support it via the Criteria APIs however. http://java.net/jira/browse/JPA_SPEC-40</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] generateSchema method contract for Java SE environments</header>
    <date>Mon Dec 17 20:30:56 CET 2012</date>
    <body>Gordon has pointed out to me that for the PersistenceProvider generateSchema  method that is used in Java SE environments, we need to make use of a bootstrap contract  much like the one for createEntityManagerFactory, as described in section 9.2. I therefore plan to add the following as a subsection to 9.2 to cover this. Please let me know if you see any problems. thanks, -Linda Schema Generation [inserted as a subsection of 9.2]   In Java SE environments, the Persistence.generateSchema method may be used by the application to cause schema generation to occur as a separate phase from entity manager factory creation.   In this case, the Persistence bootstap class must locate all of the persistence providers using the PersistenceProviderResolver mechanism described in section 9.3 and call generateSchema on them in turn until an appropriate backing provider returns true. A provider may deem itself as appropriate for the persistence unit if any of the following are true: Its implementation class has been specified in the provider element for that persistence unit in the persistence.xml file and has not been overridden by a different javax.persistence.provider property value included in the Map passed to the generateSchema method. The javax.persistence.provider property was included in the Map passed to generateSchema and the value of the property is the provider's implementation class. No provider was specified for the persistence unit in either the persistence.xml or the property map. If a provider does not qualify as the provider for the named persistence unit, it must return false when generateSchema is invoked on it.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] updated spec draft</header>
    <date>Mon Dec 17 22:59:43 CET 2012</date>
    <body>I've just uploaded an updated spec draft. Changes:   removal of on-conditions for fetch joins   clarification of bootstrap contracts for Persistence.generateSchema  invocation</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: generateSchema method contract for Java SE environments</header>
    <date>Tue Dec 18 11:25:02 CET 2012</date>
    <body>A bit late :) This looks good. It stresses the fact that deployments will be slower in JPA 2.1 vs JPA 2 if the container controls the database object creation. But if I am right, that's going to be a rare case. Emmanuel  Gordon has pointed out to me that for the PersistenceProvider   generateSchema method that  is used in Java SE environments, we need to make use of a bootstrap   contract much like  the one for createEntityManagerFactory, as described in section 9.2.    I therefore plan to add the following as a subsection to 9.2 to cover this.  Please let me know if you see any problems.    thanks,    -Linda      Schema Generation [inserted as a subsection of 9.2]      In Java SE environments, the Persistence.generateSchema method may  be used by the application to cause schema generation to occur as a  separate phase from entity manager factory creation.      In this case, the Persistence bootstap class must locate all of the  persistence providers using the PersistenceProviderResolver mechanism  described in section 9.3 and call generateSchema on them in turn until  an appropriate backing provider returns true. A provider may deem  itself as appropriate for the persistence unit if any of the following  are true:    Its implementation class has been specified in the provider element  for that persistence unit in the persistence.xml file and has not been  overridden by a different javax.persistence.provider property value  included in the Map passed to the generateSchema method.    The javax.persistence.provider property was included in the Map passed  to generateSchema and the value of the property is the provider's  implementation class.    No provider was specified for the persistence unit in either the  persistence.xml or the property map.    If a provider does not qualify as the provider for the named  persistence unit, it must return false when generateSchema is invoked  on it.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Tue Dec 18 19:15:44 CET 2012</date>
    <body>+1 A separate copy() function does seem to add redundant complexity. As such, creating copies often is the beginning of all sorts of anti-patterns. If the need for a copy() is strongly felt, then a) detach() with an option to leave the objects in the persistence context can be considered to achieve similar functionality. b) the need for a separate copy() should be articulated more clearly c) in any case, the proposal for a copy() should be tabled separately from EntityGraph specification.   Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Emmanuel Bernard ---12/17/2012 09:12:47 AM---OK I get it now. To be honest I am quite uncomfortable with such a feature. Does everyone From: To: jsr338-experts@... Date: 12/17/2012 09:12 AM Subject: [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc... OK I get it now. To be honest I am quite uncomfortable with such a feature. Does everyone really think that this feature is a must have? I imagine that the use case you have in mind is to pass this copied object graph to another serialization or traversal framework that would otherwise traverse the JPA guards like there is no tomorrow. Our experience with Bean Validation has shown that handling guards is extremely easy and that the necessary contract to make a traversal / serialization framework aware of these is quite simple https://github.com/beanvalidation/beanvalidation-api/blob/master/src/main/java/javax/validation/TraversableResolver.java The solution proposed here (the copy) forces to copy an object graph which leads to memory duplication just to get the data serialized in another form. Plus the user *must* use the same graph definition for both `copy` and `merge` or else data loss are going to happen. I find the programming model quite error prone and dangerous. Emmanuel</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Tue Dec 18 22:02:28 CET 2012</date>
    <body>There is no requirement for a user to use copy(), the specification already has support for partial loading of entities  and providers are currently required to support serialization so there are no barriers for a user to serialize a tree of entities and only the loaded data would be expected to be serialized.  The only purpose of copy() is to allow users to create a copy of a portion of an entity graph.  Many users have asked for this functionality and with EntityGraphs we now have a means to template the operation so why not add the functionality.  I see no need for "guards" as users are intentionally creating a truncated entity graph with a template that they have provided.  Also, "guards" add unnecessary complexity and require some sort of enhancement of the entity classes that has not yet and should not be required by the specification. With respect to merge(EntityGraph) the specification is introducing partial merges which introduce the prospect of users creating partially populated entities and merging them without or without copy(EntityGraph).  This merge() feature is introducing more versatile functionality and it is unfortunate that versatility can increase complexity but that is the usual cost. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Tue Dec 18 22:21:14 CET 2012</date>
    <body> Hello Pinaki and others, - detach(EntityGraph) is functionality we should introduce but the semantics are more complicated than copy() and should be deferred because of time constraints. - the need derives from user requests.  Users wish to create copies of a graph of entities for many different reasons and currently they must implement this functionality themselves.   - I believe we are discussing copy() separately from the discussions on EntityGraphs and copy() exists as its own functionality but copy() is enabled through EntityGraphs so the features are related and hence introduced at the same time. --Gordon  On 18/12/2012 2:15 PM, Pinaki Poddar error prone and dangerous. +1 A separate copy() function does seem to add redundant complexity. As such, creating copies often is the beginning of all sorts of anti-patterns. If the need for a copy() is strongly felt, then a) detach() with an option to leave the objects in the persistence context can be considered to achieve similar functionality. b) the need for a separate copy() should be articulated more clearly c) in any case, the proposal for a copy() should be tabled separately from EntityGraph specification.   Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware &lt;img src="msg00060/gifOkGyveCcAJ.gif" alt="Inactive hide details for Emmanuel Bernard ---12/17/2012 09:12:47 AM---OK I get it now. To be honest I am quite uncomfortable w" height="16" width="16" border="0"/&gt; Emmanuel Bernard ---12/17/2012 09:12:47 AM---OK I get it now. To be honest I am quite uncomfortable with such a feature. Does everyone From: Emmanuel Bernard To: jsr338-experts@... Date: 12/17/2012 09:12 AM Subject: [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc... OK I get it now. To be honest I am quite uncomfortable with such a feature. Does everyone really think that this feature is a must have? I imagine that the use case you have in mind is to pass this copied object graph to another serialization or traversal framework that would otherwise traverse the JPA guards like there is no tomorrow. Our experience with Bean Validation has shown that handling guards is extremely easy and that the necessary contract to make a traversal / serialization framework aware of these is quite simple https://github.com/beanvalidation/beanvalidation-api/blob/master/src/main/java/javax/validation/TraversableResolver.java The solution proposed here (the copy) forces to copy an object graph which leads to memory duplication just to get the data serialized in another form. Plus the user *must* use the same graph definition for both `copy` and `merge` or else data loss are going to happen. I find the programming model quite error prone and dangerous. Emmanuel subgraph copy of enhancement/proxies may not when then form of attributes have this template requirement for environments copy() at all entities loaded associated to each of the new loadgraph and fetchgraph. you can achieve the same with copy(EntityGraph) an attribute is not  So, your example guards on uninitialized copied property / null or the that. uninitialized properties and not necessary as regular merge).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Tue Dec 18 23:00:05 CET 2012</date>
    <body>Gordon,    As long as an industry-level specification is concerned, "time constraints" should never be a valid argument for introducing an alternative that is difficult to justify later. If time is a constraint, then it is better to hold back the feature for later releases.   The users need for a subgraph to another tier is adequately addressed within detach-modify-merge cycle of JPA programming model, which is a significant and useful departure from remotable EJB programming model of earlier days. From that viewpoint, introduction of a copy() function is a step backwards and, as mentioned in this thread, may promote a "dangerous" programming model.   Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Gordon Yorke ---12/18/2012 01:22:20 PM---  Hello Pinaki and others, - detach(EntityGraph) is functionality we should introduce but the From: To: jsr338-experts@... Date: 12/18/2012 01:22 PM Subject: [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...  Hello Pinaki and others, - detach(EntityGraph) is functionality we should introduce but the semantics are more complicated than copy() and should be deferred because of time constraints. - the need derives from user requests.  Users wish to create copies of a graph of entities for many different reasons and currently they must implement this functionality themselves.   - I believe we are discussing copy() separately from the discussions on EntityGraphs and copy() exists as its own functionality but copy() is enabled through EntityGraphs so the features are related and hence introduced at the same time. --Gordon   +1 A separate copy() function does seem to add redundant complexity. As such, creating copies often is the beginning of all sorts of anti-patterns. If the need for a copy() is strongly felt, then a) detach() with an option to leave the objects in the persistence context can be considered to achieve similar functionality. b) the need for a separate copy() should be articulated more clearly c) in any case, the proposal for a copy() should be tabled separately from EntityGraph specification.   Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware &lt;img src="" width="16" height="16" alt="Inactive hide details for Emmanuel Bernard ---12/17/2012 09:12:47 AM---OK I get it now. To be honest I am quite uncomfortable w"/&gt; Emmanuel Bernard ---12/17/2012 09:12:47 AM---OK I get it now. To be honest I am quite uncomfortable with such a feature. Does everyone From: Emmanuel Bernard To: jsr338-experts@... Date: 12/17/2012 09:12 AM Subject: [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc... OK I get it now. To be honest I am quite uncomfortable with such a feature. Does everyone really think that this feature is a must have? I imagine that the use case you have in mind is to pass this copied object graph to another serialization or traversal framework that would otherwise traverse the JPA guards like there is no tomorrow. Our experience with Bean Validation has shown that handling guards is extremely easy and that the necessary contract to make a traversal / serialization framework aware of these is quite simple https://github.com/beanvalidation/beanvalidation-api/blob/master/src/main/java/javax/validation/TraversableResolver.java The solution proposed here (the copy) forces to copy an object graph which leads to memory duplication just to get the data serialized in another form. Plus the user *must* use the same graph definition for both `copy` and `merge` or else data loss are going to happen. I find the programming model quite error prone and dangerous. Emmanuel</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Tue Dec 18 23:35:57 CET 2012</date>
    <body>In general, I agree with your point regarding the factor of time constraints. However, I do not see detach graphs as an alternative to copy graphs, since  the application may not necessarily want to detach those entities from the persistence context.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] copy and merge graphs [Re: Re: Proposal for EntityGraphs, fetch plans, etc...]</header>
    <date>Tue Dec 18 23:40:46 CET 2012</date>
    <body>While we can/should definitely continue to discuss the best way to achieve this or comparable functionality, I have to second Gordon's point that the ability to "prune off" part of an entity graph is something that we have been getting requests for for quite some time. Fetch graphs and detach alone don't provide this functionality, insofar as they may result in exposing more data than the application wishes to expose (particularly since our rules for FetchType.LAZY allow the implementation to bring in more than has been requested), thus entailing the need for "manual" pruning. In any case, I would like to get this functionality out in the Public Draft for wider exposure (and hopefully wider feedback). Depending on this results of that, we can decide how best to proceed. In any event, I plan to add a note to the specification to the effect that the copy and merge graph options are still under discussion and require further feedback. Sound reasonable?? thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Wed Dec 19 00:03:08 CET 2012</date>
    <body>a simple solution is to add an option to detach that leaves the original entities in the persistence context. Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Linda DeMichiel ---12/18/2012 02:37:28 PM---On 12/18/2012 2:00 PM, Pinaki Poddar wrote: &amp;gt; Gordon, From: To: jsr338-experts@... Date: 12/18/2012 02:37 PM Subject: [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc... In general, I agree with your point regarding the factor of time constraints. However, I do not see detach graphs as an alternative to copy graphs, since the application may not necessarily want to detach those entities from the persistence context. http://openjpa.apache.org/ mailto:ebernard@... mailto:jsr338-experts@...</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Wed Dec 19 00:17:26 CET 2012</date>
    <body>Hi Pinaki, I'm afraid I'm not following you -- doesn't that also create a copy? regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Wed Dec 19 01:18:33 CET 2012</date>
    <body>Copy() functionality is not an alternative to detach().   Copy() is legitimate functionality in its own right and the two features are not mutually exclusive nor does one build upon the other so I do not agree with your suggestion that copy() is somehow not applicable for an "industry standard specification".  If detach-modify-merge was suffecient then users would not be requesting copy functionality. Copy() is not comparable to remotable Entity Beans at all so that reference makes little sense.  I would also argue that the inclusion of this functionality does not promote any specific model but respects users requests for functionality they want. --Gordon Gordon,    As long as an industry-level specification is concerned, "time constraints" should never be a valid argument for introducing an alternative that is difficult to justify later. If time is a constraint, then it is better to hold back the feature for later releases.   The users need for a subgraph to another tier is adequately addressed within detach-modify-merge cycle of JPA programming model, which is a significant and useful departure from remotable EJB programming model of earlier days. From that viewpoint, introduction of a copy() function is a step backwards and, as mentioned in this thread, may promote a "dangerous" programming model.   Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Gordon Yorke ---12/18/2012 01:22:20 PM---  Hello Pinaki and others, - detach(EntityGraph) is functionality we should introduce but the From: To: jsr338-experts@... Date: 12/18/2012 01:22 PM Subject: [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...  Hello Pinaki and others, - detach(EntityGraph) is functionality we should introduce but the semantics are more complicated than copy() and should be deferred because of time constraints. - the need derives from user requests.  Users wish to create copies of a graph of entities for many different reasons and currently they must implement this functionality themselves.   - I believe we are discussing copy() separately from the discussions on EntityGraphs and copy() exists as its own functionality but copy() is enabled through EntityGraphs so the features are related and hence introduced at the same time. --Gordon   +1 A separate copy() function does seem to add redundant complexity. As such, creating copies often is the beginning of all sorts of anti-patterns. If the need for a copy() is strongly felt, then a) detach() with an option to leave the objects in the persistence context can be considered to achieve similar functionality. b) the need for a separate copy() should be articulated more clearly c) in any case, the proposal for a copy() should be tabled separately from EntityGraph specification.   Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware &lt;img src="content://com.kaitenmail.attachmentprovider/521cff60-3941-47bd-b17f-7bb1eaf87f3e/5478/RAW" width="16" height="16" alt="Inactive hide details for Emmanuel Bernard ---12/17/2012 09:12:47 AM---OK I get it now. To be honest I am quite uncomfortable w"/&gt; Emmanuel Bernard ---12/17/2012 09:12:47 AM---OK I get it now. To be honest I am quite uncomfortable with such a feature. Does everyone From: Emmanuel Bernard To: jsr338-experts@... Date: 12/17/2012 09:12 AM Subject: [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc... OK I get it now. To be honest I am quite uncomfortable with such a feature. Does everyone really think that this feature is a must have? I imagine that the use case you have in mind is to pass this copied object graph to another serialization or traversal framework that would otherwise traverse the JPA guards like there is no tomorrow. Our experience with Bean Validation has shown that handling guards is extremely easy and that the necessary contract to make a traversal / serialization framework aware of these is quite simple https://github.com/beanvalidation/beanvalidation-api/blob/master/src/main/java/javax/validation/TraversableResolver.java The solution proposed here (the copy) forces to copy an object graph which leads to memory duplication just to get the data serialized in another form. Plus the user *must* use the same graph definition for both `copy` and `merge` or else data loss are going to happen. I find the programming model quite error prone and dangerous. Emmanuel</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Merging of referenced new objects w/o cascade-merge specification?</header>
    <date>Wed Dec 19 09:50:08 CET 2012</date>
    <body>Hi Linda, Linda DeMichiel, am 11 Dec 2012 hast Du um 9:43 zum Thema "[jsr338- experts] Re: [jpa-spec users] Re: Merging"  geschrieben :  Yes, that is how we need to proceed.    I'd like to hear from the experts on this group who represent JPA   implementations:       How do you all handle this case?    If we have convergence on this, we should definitely clarify the spec.  If   not, I'm  afraid we are stuck with "undefined". JPA 1.0 implementation of SAP NetWeaver AS Java throws ISE upon flush  to the database. Best regards Rainer --- Rainer Schweigkoffer               SAP AG Walldorf Regulatory Compliance              TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, F.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...       fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Lars  Dalgaard, Luisa Deplazes Delgado, Bill McDermott (Co-CEO),  Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP  Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse  oder sonstige vertrauliche Informationen enthalten.  Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist  Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung  oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte  benachrichtigen Sie uns und vernichten Sie die empfangene  E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged,  undisclosed, or otherwise confidential information. If you  have received this e-mail in error, you are hereby  notified that any review, copying, or distribution of it  is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your  cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Wed Dec 19 12:21:52 CET 2012</date>
    <body>Hi Linda, all, Linda DeMichiel, am 9 Dec 2012 hast Du um 15:06 zum Thema "[jsr338- experts] Re: Proposal for EntityGraphs, f"  geschrieben :      &amp;gt; Hi Ollie,  &amp;gt;&amp;gt; Am 07.12.2012 um 18:57 schrieb Gordon Yorke&amp;lt;gordon.yorke@...&amp;gt;:  &amp;gt;&amp;gt;&amp;gt; It was removed from an earlier draft version to reduce the clutter in   &amp;gt;&amp;gt;&amp;gt; the NamedEntityManager annotation as  &amp;gt;&amp;gt;&amp;gt; NamedAttributeNode serves the same purpose. It also eliminates any   &amp;gt;&amp;gt;&amp;gt; confusion about when to use the String[] vs the  &amp;gt;&amp;gt;&amp;gt; NamedAttributeNode[] and how they interact.  &amp;gt;&amp;gt;&amp;gt; --Gordon  &amp;gt;&amp;gt; I guess you mean NamedEntityGraph, right? I don't think an additional   &amp;gt;&amp;gt; simple attribute is clutter actually. Also, I  &amp;gt;&amp;gt; can't see any confusion as the String[] can easily be evaluated like an   &amp;gt;&amp;gt; @NamedAttributeNode with a plain name  &amp;gt;&amp;gt; attribute and would be added to other @NamedAttributeNode declarations.   &amp;gt;&amp;gt; Simple rule, no corner cases. Especially in  &amp;gt;&amp;gt; scenarios where you'd simply list a couple of Strings this would   &amp;gt;&amp;gt; actually *reduce* clutter and complexity on the  &amp;gt;&amp;gt; definition side. Make the simple things simple, complex things possible.  &amp;gt;&amp;gt; The examples you showed were referring to a few properties only and   &amp;gt;&amp;gt; already consumed quite a bit of space. Assuming  &amp;gt;&amp;gt; real world scenarios, this overhead would even grow beyond that. It's   &amp;gt;&amp;gt; yet another annotation per property to refer to.  &amp;gt;&amp;gt; I'd actually would be interested in seeing an XML definition example   &amp;gt;&amp;gt; because it feels like the annotation based  &amp;gt;&amp;gt; definition might actually be more verbose than the XML equivalent.  &amp;gt;&amp;gt; I strongly recommend to favor conciseness on the definition / user side   &amp;gt;&amp;gt; of things over a few more lines of  &amp;gt;&amp;gt; implementation code. Parts of the spec already do a suboptimal job in   &amp;gt;&amp;gt; this regard and I think we do the users and the  &amp;gt;&amp;gt; spec a favor if we make it as usable as possible.  &amp;gt; While I sympathize with your ease-of-use argument here, I do think that   &amp;gt; having 3 ways of specifying  &amp;gt; attributes in NamedEntityGraph would be a bit much.    OK, I've been mulling this over some more and I have to admit I am really   torn about this issue in  view of Ollie's verbosity argument.    Can we have some more feedback on this point, please? I could imagine that Ollie's proposed string attributes might become as  useful as the string-based methods in the Criteria API. Just my Euro 0.02 Rainer --- Rainer Schweigkoffer               SAP AG Walldorf Regulatory Compliance              TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, F.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...       fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Lars  Dalgaard, Luisa Deplazes Delgado, Bill McDermott (Co-CEO),  Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP  Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse  oder sonstige vertrauliche Informationen enthalten.  Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist  Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung  oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte  benachrichtigen Sie uns und vernichten Sie die empfangene  E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged,  undisclosed, or otherwise confidential information. If you  have received this e-mail in error, you are hereby  notified that any review, copying, or distribution of it  is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your  cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] short question on Cacheable(false) and shared-cache-mode == ALL</header>
    <date>Wed Dec 19 15:36:15 CET 2012</date>
    <body>Dear group, Is our understanding correct, that instances of entity classes, which are marked with Cacheable(false), will be cached, if the shared-cache-mode is set to ALL? Thank you! Christian -- Christian von Kutzleben Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cvkutzleben@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: short question on Cacheable(false) and shared-cache-mode == ALL</header>
    <date>Wed Dec 19 16:09:42 CET 2012</date>
    <body>According the the spec, if the user specifies shared-cache-mode == ALL caching is enabled across all entities regardless of what individual entity classes specify for @Cacheable (includnhg whether they have that annotation or not).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Wed Dec 19 18:28:37 CET 2012</date>
    <body> There is no requirement for a user to use copy(), the specification  already has support for partial loading of entities  and providers  are currently required to support serialization so there are no  barriers for a user to serialize a tree of entities and only the  loaded data would be expected to be serialized.  The only purpose of  copy() is to allow users to create a copy of a portion of an entity  graph.  Many users have asked for this functionality and with  EntityGraphs we now have a means to template the operation so why  not add the functionality.   I see no need for "guards" as users are  intentionally creating a truncated entity graph with a template that  they have provided.  Also, "guards" add unnecessary complexity and  require some sort of enhancement of the entity classes that has not  yet and should not be required by the specification. Hum. When I load an entity today and serialize / deserialize this object graph, then the object graph "remembers" what association has been loaded and what has not. Same for properties though these ones require indeed bytecode enhancement. If it was not, merge would erase lazy associations with null or some other random value. So we have that feature today. What I am claiming is that we can rely on the same mechanisms we have today. The semantic of copy which would replace lazy associations with null or null collection - it's not defined by the way AFAIR - is really something I am strongly against unless we have a very valid use case in front of us. I have explained that passing the graph to a serialization or traversal framework is not one of those use case as we have demonstrated with Bean Validation. Emmanuel PS: I'm officially on vacations tonight and till early next year so I might not be following this thread too closely in the next few days. Happy holidays everyone.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Proposal for EntityGraphs, fetch plans, etc...</header>
    <date>Wed Dec 19 19:30:32 CET 2012</date>
    <body>Lazy loading support is optional.  In an environment where enhancement is not available all attributes except for XToMany and ElementCollections will be loaded so there is no unloaded attributes for these relationships  and nothing for merge to get wrong.  In other words there is no "remembering" when there is no enhancement and the spec is written to handle that. It is not the serialization framework that is the concern.  Any EE framework could be updated to support the already existing JPA isLoaded().  It is when the client is not EE that the guards/markers are lost and there is no way to determine the difference between unloaded data and the client having set the attribute to null.  With a template the user indicates what attributes should be ignored because they were not loaded before. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] copy and merge graphs (gone)</header>
    <date>Wed Dec 19 19:35:59 CET 2012</date>
    <body>Since hope of convergence on these any time soon appears to be dim, I am going to remove copy and merge graphs from the Public Draft. While we can certainly continue to discuss the issues and/or alternatives, this makes it unlikely that we would be able to include such functionality in JPA 2.1. The use of entity graphs as fetch and load graphs will remain. I'll try to post an updated spec and javadocs by later today. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] updated spec and javadocs (Public Draft)</header>
    <date>Wed Dec 19 21:48:18 CET 2012</date>
    <body>I've uploaded an updated spec and javadocs.  These reflect the removal of copy and merge graphs. I plan to submit these to the JCP later today. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] move JSR 338  to JCP 2.9</header>
    <date>Mon Dec 24 19:20:13 CET 2012</date>
    <body>Hi all, When I submitted the JPA spec to the JCP staff for the public review, they  asked if we wanted to move to JCP 2.9.  (JPA 2.1 is currently at JCP 2.8). My understanding is that there is no process change from our side, and that the only change in 2.9 is the merging of two ECs into one (which seems a fait accompli). Please let me know if you see any reason we should not make this change, otherwise I will tell them to proceed. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Schema generation feedback</header>
    <date>Wed Jan 02 16:51:36 CET 2013</date>
    <body>Hello everyone and happy new year to all, I have been discussing the schema generation feature with Max in our tooling team. We have some minor feedback We probably should offer a default value for create-script-target /  drop-script-target. Also being able to specify relative paths would be useful to not have to change these from one environment to another. What kind of URLs are accepted for destinations? Only file based URLs or any thing? Not sure we want to be forced to write support for every URL protocol on the planet. Does create-database-schemas also cover the (non) creation of catalogs for completeness (CREATE CATALOG command)? Is the *-script-source considered mutual exclusive from the generated script execution or is it considered something that is run in *addition* to these  scripts ? Should generateSchema be renamed prepareSchema to better reflect what actually happens? It's not necessary just schema generation but can involve creation / update / drop etc. Emmanuel</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Schema generation feedback</header>
    <date>Tue Jan 08 00:47:30 CET 2013</date>
    <body>Hi Emmanuel, Thanks for the feedback.  Some comments and further questions for you and the  group below.... -Linda What did you have in mind here? I'm assuming we should support file-based URLs.  I'd like to hear from the  group as to what other protocols should be supported. If catalog is functionally equivalent to schema, then yes.  If catalog were functionally equivalent to database, then I would say not. What do you think? I'm not sure I'm understanding the question, but I'll give it a try :-) ... The way the schema generation section is currently written (although perhaps  not explicitly enough), it is assumed that schema-generation-target and schema-generation-action must be specified.  *-script-source wouldn't be mutually exclusive then, but rather complementary. So, does your question then reduce to whether specification of the  *-script-source properties, by themselves, should be sufficient to cause tables to be  created, dropped, or both? I'm generally not too wedded to particular names, but "schema generation" as a generic term seems to have become fairly established.   If we were to change  it though, what did you have in mind for the related property names?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Clarification of PersistenceUnitUtils.getIdentifier(…)</header>
    <date>Thu Jan 10 10:18:26 CET 2013</date>
    <body>Hi all, the JavaDoc of  PersistenceUnitUtils.getIdentifier(…) states the following:  Return the id of the entity.  A generated id is not guaranteed to be available until after  the database insert has occurred.  Returns null if the entity does not yet have an id. I had a discussion with Romain Manni-Bucau (involved with the Apache  Foundation and OpenJPA, AFAIK) about the detailed semantics of this in case  you are using primitive identifiers like this: @Entity Romain argued that the spec defines that getIdentifier(…) would/should return  null for a new Person(), as it theoretically did not get any ID assigned yet  but effectively has a value. Personally, I didn't read this meaning into the  specification of the method. This impression seems to be backed by the fact  that I discovered different persistence providers handling this differently: - OpenJPA 2.2.0 -&amp;gt; returns null for new Person(), but a non-null value once I  called em.persist(…) - Hibernate 3.10.0 -&amp;gt; return a non-null value even for new Person() What is the actual intent of the method? As it is right now, one cannot use  the method to determine whether the entity already had been persisted  reliably across persistence providers. What do the TCK tests of this method  look like? Regards, Ollie --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification of PersistenceUnitUtils.getIdentifier(…)</header>
    <date>Fri Jan 11 01:16:27 CET 2013</date>
    <body>Yes, this is a hole in the spec from JPA 2.0. I'd like to get opinions from the group as to how we should close it,  especially if you have knowledge of developers depending on the current behaviors of  your respective implementations. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification of PersistenceUnitUtils.getIdentifier(…)</header>
    <date>Fri Jan 11 17:23:53 CET 2013</date>
    <body>Granted that this is an "implementation detail", but the problem here is that returning null in this case effectively requires bytecode enhancement.  As Hibernate does not work on bytecode enhancement we have to simply return the entity instance's current attribute state. So as I said, I realize that the above is an "implementation detail", but given that the spec specifically does not require bytecode enhancement I don't see how we can then effectively require bytecode enhancement</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Mon Jan 14 09:01:55 CET 2013</date>
    <body>Hi, After using Hibernate for a while, I had assumed that insertable = false and  updatable = false where useful for supporting reuse of columns across fields.   But I got into an argument this week end with someone saying that the JPA  spec does not define how/if this is mandated. Example: @Entity     @Id     @GeneratedValue(strategy = GenerationType.IDENTITY)     @OneToMany(cascade = CascadeType.ALL, mappedBy = "company") [..] @Entity     @Id     @GeneratedValue(strategy = GenerationType.IDENTITY)     @ManyToOne()     @JoinColumn(name = "COMPANY_ID")      // Reuse of the field for the FK id     @Column(name = "COMPANY_ID", insertable = false, updatable = false) I went through the JPA 2.1 draft, can could not find anything specific to  this topic. Should this be clarified? Or was it discussed before I joined? Best regards, Nicolas Seyvet</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Clarification: BindableType</header>
    <date>Thu Jan 17 13:07:44 CET 2013</date>
    <body>Hi all, the metamodel API exposes a Bindable interface which in turn exposes a  BindableType. Unfortunately it doesn't seem to be defined in which cases  which BindableType should be returned. E.g. I have the following types: @Entity @Entity   … Using the Criteria API I now try to find out that a Path instance is starting  at Person is pointing to an entity effectively: I was expecting to get ENTITY_TYPE returned but I get SINGULAR_ATTRIBUTE from  a recent Hibernate. Trying to come up with a bug report for Hibernate I tried  to find out what the specified behavior is but couldn't find anything except  the rather brief JavaDoc. Cheers, Ollie --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification: BindableType</header>
    <date>Thu Jan 17 13:19:56 CET 2013</date>
    <body>As a follow up: I am now falling back on checking whether BindableType is a  SingluarAttribute and invoking ….isAssociation() on it. If that fails, I  inspect the persistentAttributeType for being one of the association  annotations. Quite a wild ride actually but there doesn't seem to be an easier way without  having direct access to the Metamodel. Cheers, Ollie Am 17.01.2013 um 13:07 schrieb Oliver Gierke &amp;lt;ogierke@...&amp;gt;:  Hi all,    the metamodel API exposes a Bindable interface which in turn exposes a   BindableType. Unfortunately it doesn't seem to be defined in which cases   which BindableType should be returned. E.g. I have the following types:    @Entity    @Entity   …    Using the Criteria API I now try to find out that a Path instance is   starting at Person is pointing to an entity effectively:      I was expecting to get ENTITY_TYPE returned but I get SINGULAR_ATTRIBUTE   from a recent Hibernate. Trying to come up with a bug report for Hibernate   I tried to find out what the specified behavior is but couldn't find   anything except the rather brief JavaDoc.    Cheers,  Ollie  --   /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */ --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification: BindableType</header>
    <date>Thu Jan 17 14:16:20 CET 2013</date>
    <body>The path in your example is a SINGULAR_ATTRIBUTE, the only time you should expect an ENTITY_TYPE is if the Path&amp;lt;&amp;gt; represents an actual entity like: from.getModel().getBinableType() == ENTITY_TYPE I do see how this question arises as the specification does not clearly call out when each type should be returned.  One has to search the interfaces to see what interfaces extend from Bindable then it becomes clearer. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Clarification: BindableType</header>
    <date>Thu Jan 17 14:46:46 CET 2013</date>
    <body>Hm, from reading the spec naively, would have expected a SingluarAttribute  instance for from.get(…).getModel() which then returns a BindableType of  ENTITY_TYPE as the path is pointing to an entity. I was under the impression that the separation between Attribute interfaces  and BindableType actually exists to allow a flexible mapping. Otherwise I  could just do instanceof checks on the actual Attribute type. Also, according to chapter 2.9 of the Spec an attribute with the …ToOne or  …ToMany annotations is considered an association. Thus, am I wrong  considering the call to ….isAssociation() on my Path pointing to "address"  returning false to be a bug then? Cheers, Ollie   Am 17.01.2013 um 14:16 schrieb Gordon Yorke &amp;lt;gordon.yorke@...&amp;gt;:  The path in your example is a SINGULAR_ATTRIBUTE, the only time you should   expect an ENTITY_TYPE is if the Path&amp;lt;&amp;gt; represents an actual entity like:    from.getModel().getBinableType() == ENTITY_TYPE    I do see how this question arises as the specification does not clearly   call out when each type should be returned.  One has to search the   interfaces to see what interfaces extend from Bindable then it becomes   clearer.    --Gordon --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Clarification: BindableType</header>
    <date>Thu Jan 17 15:10:56 CET 2013</date>
    <body>The issue is that the BindableType is the actual type of the Bindable and has nothing to do with state of the attribute/EntityType. "getBindableJavaType" on Bindable likely extends the confusion and should not be there or renamed to something like getTargetJavaType() but it was added as a shortcut to get the target type without requiring a cast to PluralAttribute. in your example : ((Attribute)path.getModel()).isAssociation() should return true.  You cannot always cast to Attribute without checking instanceof or BindableType first though. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Clarification: BindableType</header>
    <date>Thu Jan 17 15:16:31 CET 2013</date>
    <body>I am not sure I get the special semantics of Bindable. I digged a bit deeper  into what different implementations return for the calls, here's what I found  out:             | BindableType       | ….isAssociation() | ------------+--------------------+-------------------+ Hibernate   | SINGULAR_ATTRIBUTE | false             | OpenJpa     | ENTITY_TYPE        | true              | EclipseLink | SINGULAR_ATTRIBUTE | true              | ------------+--------------------+-------------------+ This quite a mixed bag (if you wanted to be negative you could call it a  mess). At least I think I can reasonably argue that ….isAssociation() is not  implemented correctly given the definition of an association in 2.9 of the  spec. I'll refrain from looking into BindableType at all then and just shoot for  ….isAssociation() falling back on the persistence attribute type. Cheers, Ollie Am 17.01.2013 um 15:10 schrieb Gordon Yorke &amp;lt;gordon.yorke@...&amp;gt;:  The issue is that the BindableType is the actual type of the Bindable and   has nothing to do with state of the attribute/EntityType.    "getBindableJavaType" on Bindable likely extends the confusion and should   not be there or renamed to something like getTargetJavaType() but it was   added as a shortcut to get the target type without requiring a cast to   PluralAttribute.    in your example : ((Attribute)path.getModel()).isAssociation() should   return true.  You cannot always cast to Attribute without checking   instanceof or BindableType first though.    --Gordon   --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Feature request: allow type level annotations to be used as meta-annotations</header>
    <date>Thu Jan 17 16:50:57 CET 2013</date>
    <body>Hi all, I repeatedly find myself annotating my JPA entities with the very same set of  annotations: @Entity @EntityListeners(AuditingEntityListener.class) If both @Entity and @EntityListener were allowed to be used as  meta-annotations I could collapse them into: @Target(TYPE) @Retention(RUNTIME) @Entity @EntityListeners(AuditingEntityListener.class) Resulting in: @AuditedEntity This is in line with the meta-annotation handling CDI exposes to introduce  annotation with richer semantics in annotation code [0]. The following  changes would be required. - Add ElementType.ANNOTATION_TYPE to the relevant annotations - Specify that persistence providers have to evaluate the annotations from  the meta-level as well using the first one found, so that locally defined  annotations would be considered first. Cheers, Ollie [0]  https://github.com/dblevins/metatypes/ --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Feature request: allow type level annotations to be used as meta-annotations</header>
    <date>Thu Jan 17 20:53:33 CET 2013</date>
    <body>Hi Oliver, I agree with your suggestion.  We tried to bring in a  Metatype/Stereotype-like functionality into Java EE 7, but ran out of runway for this release.  It is definitely on  our roadmap for Java EE 8 however.  You can view the discussions on the  javaee-spec.java.net project archives. Could you please log this in the JPA JIRA as an RFE for JPA.next? thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] EntityGraph's AttributeNode and Type</header>
    <date>Thu Jan 17 21:06:18 CET 2013</date>
    <body>Building on Ollie's feedback with respect to BindableType I was reviewing AttributeNode.getType() and see this method as having very little value and unnecessarily crowds the interface.  Given the stage we are at with delivering the specifiation I suggest we simply remove this method for now and provide other methods that add value to the interface in the future.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EntityGraph's AttributeNode and Type</header>
    <date>Thu Jan 17 21:14:44 CET 2013</date>
    <body>Hi Gordon, Would it be useful to simply return the Java type rather than Type?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Feature request: allow type level annotations to be used as meta-annotations</header>
    <date>Thu Jan 17 21:15:11 CET 2013</date>
    <body>There you go:  http://java.net/jira/browse/JPA_SPEC-43 : ) Am 17.01.2013 um 20:53 schrieb Linda DeMichiel &amp;lt;linda.demichiel@...&amp;gt;:  Hi Oliver,    I agree with your suggestion.  We tried to bring in a   Metatype/Stereotype-like functionality  into Java EE 7, but ran out of runway for this release.  It is definitely   on our roadmap  for Java EE 8 however.  You can view the discussions on the   javaee-spec.java.net project archives.    Could you please log this in the JPA JIRA as an RFE for JPA.next?    thanks,    -Linda     https://github.com/dblevins/metatypes/ --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Thu Jan 17 21:21:40 CET 2013</date>
    <body>Hi Nicolas, This was discussed back in the JPA 1.0 days.  The semantics of insertable /  updatable are defined in the Column annotation and are consistent with your use of them  in the example above. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EntityGraph's AttributeNode and Type</header>
    <date>Thu Jan 17 21:44:36 CET 2013</date>
    <body>Perhaps but they are practically the same thing in this case and I do not see a clear use case for this information especially since there is no way to inspect the related subgraphs of the attribute node.  It would be far more helpful to add getSubgraphs() getKeySubgraphs() as a starting point for EntityGraph inspection. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Query.setLockMode</header>
    <date>Thu Jan 17 23:29:05 CET 2013</date>
    <body>Query.setLockMode currently states: "@throws IllegalStateException if the query is found not to be * a Java Persistence query language SELECT query * or a Criteria API query" To be consistent with the JPQL semantics, this should be restated as "@throws IllegalStateException if the query is found not to be * a Java Persistence query language SELECT query * or a CriteriaQuery query" I.e., we disallow setting query lock modes for update and delete "queries" I plan to make this change, so yell now if you disagree. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EntityGraph's AttributeNode and Type</header>
    <date>Thu Jan 17 23:43:54 CET 2013</date>
    <body>Thanks, Gordon. Folks, I plan to remove AttributeNode.getType().  Yell now if you disagree. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Clarification: FETCH JOINs and ORDER BY clauses</header>
    <date>Sun Jan 20 10:07:18 CET 2013</date>
    <body>Hi all, I have a user asking why a query he builds using a fetch clause gets rejected  as soon as he's referring to the association declared in the fetch clause: @Entity @Entity select p from Person p left join fetch p.address where p.firstname =  :firstname works, whereas select p from Person p left join fetch p.address where p.firstname =  :firstname order by p.address.city fails on Hibernate with the following error message: "query specified join fetching, but the owner of the fetched association was  not present in the select list" This definitely makes some sense to me as Section 4.4.5.3 of the spec  explicitly states:  The association referenced by the right side of the FETCH JOIN clause must   be an association or element collection that is referenced from an entity   or embeddable that is returned as a result of the query. It is not   permitted to specify an identification variable for the objects referenced   by the right side of the FETCH JOIN clause, and hence references to the   implicitly fetched entities or elements *cannot appear elsewhere in the   query.* I browsed the spec intensively but couldn't find any discussion of how to  combine order by clauses with fetch joins as one obviously has to violate the  latter of the two statements if one wants to sort by a property referenced in  the fetch clause. What's the intended way of combining ORDER BY clauses and FETCH JOINs? Cheers, Ollie --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Typo in spec</header>
    <date>Mon Jan 21 08:37:30 CET 2013</date>
    <body>Typo Section 3.3 Container-managed persistence contexts are described further in section 7.6. Persistence contexts managed by the application are described futher in section 7.7.   futher -&amp;gt; further.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Limited mapping for Enum Types</header>
    <date>Mon Jan 21 09:15:18 CET 2013</date>
    <body>One key issue that is to my feeling is not addressed sufficiently is how we map an Enum. 1./ An enum is not allowed to have a Persistent Counterpart 2./ An enum has the limited choice of  - @Enumerated(EnumType.STRING)  - @Enumerated(EnumType.ORDINAL) Which is in real case scenario's too limiting.  The limitations seem to originate from the idea that "enums that have state are not supported". I explain both. Consider the Following Table representing a Male/Female Choice in the DB Table: SEX SEX_ID  SEX_NAME M       Male F       Female N/A     Not Established Two Issues come up, matching with the earlier listed limitations.  1./ Sex has also a Descriptive Field, the Name. It is common to have a fixed set of Categories defined on the DB, and have additional attributes associated with them. Although Constant, those attributes can be specific to the environment. One might decide on the Database end to just rename 'Not Established' to 'Unknown' without affecting the integrity of the Database, because the PK is based on SEX_ID, not SEX_NAME. It would be much more flexible to have an Enum, as a READ-ONLY entity do a one-time SYNC/FETCH to sync up additional Attributes once the ID is established (the Enum constants are defined). 2./ Even though on the Database the SEX_ID is keyed by M/F, Most likely, one would like this to be defined as MALE / FEMALE Enum Constants. In Addition, The Syntax of an Enum does not allow for Special Characters (forward slashes, spaces, etc). So 'N/A' cannot be represented by an Enum Constant with the same name. Also instead of using a Alphabetic ID, one might decide on the Database end to key to the Enum Constants based on a numeric Key. A numeric Key can also not be represented as an Enum Constant, and the ORDINAL will rarely match the database generated keys. Or even a worse scenario for 2./ but typical: Everywhere the ENUM is being keyed using a numeric value, but the value of the key is different from Environment to Environment (Production versus QA). So the Key associated with the ENUM Constants first need to be fetched/synced from Database. Kind of a combination of 1./ and 2./ together. These two issues makes it extremely difficult to use Enum's in a pre-existing Database, with a schema that was not designed for use with JPA specifically. This is how the Enum could look like with such features enabled: @Enumerated(EnumType.CUSTOM) @OneTimeSync  MALE,  FEMALE,  @Id  @Column('sex_id')    @MapEntity('M',Sex.MALE),    @MapEntity('F',Sex.FEMALE),    @MapEntity('N/A',Sex.UNKNOWN)  @ImmutableAttribute  @Column(sex_name) or for a Numerically Keyed value, but keys fetched from Database: @Enumerated(EnumType.CUSTOM) @OneTimeSync  MALE,  FEMALE,  @SyncId  // Id/Key to be used for a one-time-sync.  @Column('sex_id')    @MapEntity('M',Sex.MALE),    @MapEntity('F',Sex.FEMALE),    @MapEntity('N/A',Sex.UNKNOWN)  @Id  @ImmutableAttribute  @Column('sex_key')  @ImmutableAttribute  @Column(sex_name)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Mon Jan 21 11:02:51 CET 2013</date>
    <body>Ok that is good. But this is a typical instance of a table field being re-used within an  entity. Let's say, there is a table A with three columns id, col1 and col2.    A | id |col1 | col2| Is the following legal according to the spec? @Entity      @Id      @GeneratedValue(strategy = GenerationType.IDENTITY)   @Column(name = "col1 ")     @Column(name = "col1 ", insertable = false, updatable = false)         [..] If not, should it be covered in the spec? -----Original Message----- From: Linda DeMichiel [ mailto:linda.demichiel@...]  Sent: Thursday, January 17, 2013 9:22 PM To: jsr338-experts@... Subject: [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field  re-use Hi Nicolas,  Hi,  After using Hibernate for a while, I had assumed that insertable = false   and updatable = false where useful for supporting reuse of columns across   fields.  But I got into an argument this week end with someone saying that   the JPA spec does not define how/if this is mandated.  Example:  @Entity       @Id       @GeneratedValue(strategy = GenerationType.IDENTITY)       @OneToMany(cascade = CascadeType.ALL, mappedBy = "company")  [..]  @Entity       @Id       @GeneratedValue(strategy = GenerationType.IDENTITY)       @ManyToOne()       @JoinColumn(name = "COMPANY_ID")        // Reuse of the field for the FK id       @Column(name = "COMPANY_ID", insertable = false, updatable = false)  I went through the JPA 2.1 draft, can could not find anything specific to   this topic.  Should this be clarified? Or was it discussed before I joined? This was discussed back in the JPA 1.0 days.  The semantics of insertable /  updatable are defined in the Column annotation and are consistent with your  use of them in the example above. -Linda  Best regards,  Nicolas Seyvet</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Mon Jan 21 14:28:48 CET 2013</date>
    <body>In JPA 1.0 we did not get around to specifying a read-only mapping (in fact we still haven't). Because of that, using the insertable=false, updatable=false combination in the @Column mapping became the de facto way of setting a mapping to be read-only. It is not ideal since it requires setting multiple physical column/join column attributes when a logical read-only option would be the more appropriate option, but for the most part it has stuck and is what most vendors support since it at least uses standard options instead of a vendor-specific read-only annotation. -Mike</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Tue Jan 22 13:06:36 CET 2013</date>
    <body>" and is what most vendors support since it at least uses standard options  instead of a vendor-specific read-only annotation." Ok, but this is still not a specification. I guess the point is that this specification will not clarify the re-use of a  table column within the same entity. Is that correct? /Nicolas -----Original Message----- From: michael keith [ mailto:michael.keith@...]  Sent: Monday, January 21, 2013 2:29 PM To: jsr338-experts@... Cc: Linda DeMichiel Subject: [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field  re-use In JPA 1.0 we did not get around to specifying a read-only mapping (in fact  we still haven't). Because of that, using the insertable=false, updatable=false combination in  the @Column mapping became the de facto way of setting a mapping to be  read-only. It is not ideal since it requires setting multiple physical  column/join column attributes when a logical read-only option would be the  more appropriate option, but for the most part it has stuck and is what most  vendors support since it at least uses standard options instead of a  vendor-specific read-only annotation. -Mike  Hi Nicolas,  This was discussed back in the JPA 1.0 days.  The semantics of   insertable / updatable are defined in the Column annotation and are   consistent with your use of them in the example above.  -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Tue Jan 22 14:23:59 CET 2013</date>
    <body>No, I never said the spec "will not clarify the re-use of a table column". I just filled in some of the history and how we got to the current state. There is nothing in the spec that disallows reusing a column in this way, and this is how the vendors support it, so you shouldn't encounter any problems in your application. -Mike</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Tue Jan 22 14:50:32 CET 2013</date>
    <body>I mean to say that it was not the intent of the spec to detail this.  Let me  try to ask this question differently: Would it be good to standardize how the  vendors support this as part of jsr-338 scope? /Nicolas -----Original Message----- From: michael keith [ mailto:michael.keith@...]  Sent: Tuesday, January 22, 2013 2:24 PM To: jsr338-experts@... Cc: Nicolas Seyvet Subject: Re: [jsr338-experts] Re: insertable = false, updatable = false &amp;amp;  field re-use No, I never said the spec "will not clarify the re-use of a table column". I  just filled in some of the history and how we got to the current state. There  is nothing in the spec that disallows reusing a column in this way, and this  is how the vendors support it, so you shouldn't encounter any problems in  your application. -Mike  " and is what most vendors support since it at least uses standard options   instead of a vendor-specific read-only annotation."  Ok, but this is still not a specification.  I guess the point is that this specification will not clarify the re-use of   a table column within the same entity. Is that correct?  /Nicolas  -----Original Message-----  From: michael keith [ mailto:michael.keith@ ...]  Sent: Monday, January 21, 2013 2:29 PM  To: jsr338-experts@...  Cc: Linda DeMichiel  Subject: [jsr338-experts] Re: insertable = false, updatable = false&amp;amp;    field re-use  In JPA 1.0 we did not get around to specifying a read-only mapping (in fact   we still haven't).  Because of that, using the insertable=false, updatable=false combination in   the @Column mapping became the de facto way of setting a mapping to be   read-only. It is not ideal since it requires setting multiple physical   column/join column attributes when a logical read-only option would be the   more appropriate option, but for the most part it has stuck and is what   most vendors support since it at least uses standard options instead of a   vendor-specific read-only annotation.  -Mike</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Tue Jan 22 16:00:24 CET 2013</date>
    <body>Ideally we would have some kind of annotation that just pointed to the other attribute to signify that the column was defined by it and that only one of the mappings should write to the column or contribute to schema gen (a bit like the @MapsId situation, but in the opposite direction). Since in practice something already works, though, and there's not really any time left to add new features to the spec in jsr 338 it won't likely happen in this round. Maybe in the future if enough people ask for it?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Typo in spec</header>
    <date>Tue Jan 22 19:11:14 CET 2013</date>
    <body>Thanks!!</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: insertable = false, updatable = false &amp;amp; field re-use</header>
    <date>Tue Jan 22 19:17:49 CET 2013</date>
    <body>Right. BTW, IIRC, in past we ran up against the issue that vendors differed in their  use of this technique for overlapping PK/FK mappings -- which was a problem that we solved with the  introduction of @MapsId.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification: FETCH JOINs and ORDER BY clauses</header>
    <date>Tue Jan 22 19:34:44 CET 2013</date>
    <body>Hi Oliver, I guess I don't understand your question.  Are you asking how you can order  query results by the target of the join fetch?  If so, the intent of the spec is that you can not do that. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification: FETCH JOINs and ORDER BY clauses</header>
    <date>Tue Jan 22 20:04:20 CET 2013</date>
    <body>Okay, what's the reason to forbid that? -- Sent while on the run... Am 22.01.2013 um 19:34 schrieb Linda DeMichiel &amp;lt;linda.demichiel@...&amp;gt;:  Hi Oliver,      I guess I don't understand your question.  Are you asking how you can order   query results by the target  of the join fetch?  If so, the intent of the spec is that you can not do   that.    regards,    -Linda      </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification: FETCH JOINs and ORDER BY clauses</header>
    <date>Tue Jan 22 20:34:36 CET 2013</date>
    <body>The orderby rules were written to allow direct mapping to SQL.  If an  implementation were to support ordering by something that were incidentally fetched but not  in the SELECT list that would be one thing -- but I don't think we should require  support for this.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] criteria api missing where(List&#xD;
                          &lt;predicate&gt;&#xD;
                            )&#xD;
                          &lt;/predicate&gt;</header>
    <date>Wed Jan 23 07:47:14 CET 2013</date>
    <body>Hello, I opened JPA_SPEC-44, which was closed ( http://java.net/jira/browse/JPA_SPEC-44 ). It was about the criteria query API missing a where() method with a List parameter: Although groupBy, having, orderBy etc. all have both array (varargs) and List parameter versions. The issue was closed with this comment: "It was felt that CriteriaQuery&amp;lt;T&amp;gt; where(List&amp;lt;Predicate&amp;gt; restrictions) was not needed, as unlike in the other cases, there are operations to combine expressions using the logical operators." However, I think it would be a good idea to include it. I'm using the criteria API like this: I then have to do this: I think it would be easier, and the API would be more consistent, if we could just do this: What do you think? Best regards, Yannick Majoros</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: criteria api missing where(List&#xD;
                          &lt;predicate&gt;&#xD;
                            )&#xD;
                          &lt;/predicate&gt;</header>
    <date>Wed Jan 23 13:23:34 CET 2013</date>
    <body>+1 from me as well.  This is often how I build my where clauses in criteria api. On Wed, Jan 23, 2013 at 1:47 AM, Yannick Majoros yannick.majoros@... Hello, I opened JPA_SPEC-44, which was closed ( http://java.net/jira/browse/ JPA_SPEC-44 ). It was about the criteria query API missing a where() method with a List parameter: Although groupBy, having, orderBy etc. all have both array (varargs) and List parameter versions. The issue was closed with this comment: "It was felt that CriteriaQuery&amp;lt;T&amp;gt; where(List&amp;lt;Predicate&amp;gt; restrictions) was not needed, as unlike in the other cases, there are operations to combine expressions using the logical operators." However, I think it would be a good idea to include it. I'm using the criteria API like this: ClientStatus searchedClientStatus = clientSearch.getClientStatus()     predicates.add( I then have to do this:         clientQuery.where( I think it would be easier, and the API would be more consistent, if we could just do this: What do you think? Best regards, Yannick Majoros</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: criteria api missing where(List&#xD;
                          &lt;predicate&gt;&#xD;
                            )&#xD;
                          &lt;/predicate&gt;</header>
    <date>Wed Jan 23 17:52:17 CET 2013</date>
    <body>Have you considered using the CriteriaBuilder and method to additively build  up your predicates? e.g. ...</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: criteria api missing where(List&#xD;
                          &lt;predicate&gt;&#xD;
                            )&#xD;
                          &lt;/predicate&gt;</header>
    <date>Wed Jan 23 18:00:07 CET 2013</date>
    <body>Linda, Atleast in my case, that assumes that you have one predicate already defined.  In my setup I have no predicates guaranteed defined.  This would end up happening: p = cb.and(p,cityPredicate) Should that work? On Wed, Jan 23, 2013 at 11:52 AM, Linda DeMichiel linda.demichiel@... Hello, I opened JPA_SPEC-44, which was closed ( http://java.net/jira/browse/ JPA_SPEC-44 ). It was about the criteria query API missing a where() method with a List parameter: Although groupBy, having, orderBy etc. all have both array (varargs) and List parameter versions. The issue was closed with this comment: "It was felt that CriteriaQuery&amp;lt;T&amp;gt; where(List&amp;lt;Predicate&amp;gt; restrictions) was not needed, as unlike in the other cases, there are operations to combine expressions using the logical operators." However, I think it would be a good idea to include it. I'm using the criteria API like this: ClientStatus searchedClientStatus = clientSearch.getClientStatus() predicates.add( I then have to do this: clientQuery.where( I think it would be easier, and the API would be more consistent, if we could just do this: What do you think?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Early access to the TCK</header>
    <date>Thu Jan 24 20:49:05 CET 2013</date>
    <body>A few months ago there was a discussion about early access to the TCK as being a generally good thing and there seemed to be a general consensus for doing that in any later revisions of the spec.  The Cliff Notes (tm) version of the pros were mainly: 1) the ability to identify as early as possible problems in the TCK itself 2) to help providers get started on certifying 3) allow early feedback from the group as to missing portability coverage I was just curious whether we could possibly start getting access to the 2.1 TCK, especially now that the spec is pretty solidified.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Early access to the TCK</header>
    <date>Thu Jan 24 21:13:02 CET 2013</date>
    <body>+1 I am not a lawyer, but I assume, as EC Member, I'd be among "Qualified Individuals" to have access to the TCK without a license fee For Qualified Not-for-Profits and Qualified Individuals: $0. Werner On Thu, Jan 24, 2013 at 8:49 PM, Steve Ebersole steve.ebersole@... A few months ago there was a discussion about early access to the TCK as being a generally good thing and there seemed to be a general consensus for doing that in any later revisions of the spec.  The Cliff Notes (tm) version of the pros were mainly: 1) the ability to identify as early as possible problems in the TCK itself 2) to help providers get started on certifying 3) allow early feedback from the group as to missing portability coverage I was just curious whether we could possibly start getting access to the 2.1 TCK, especially now that the spec is pretty solidified.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Early access to the TCK</header>
    <date>Thu Jan 24 21:39:28 CET 2013</date>
    <body>Hi Steve, The team tells me that we are still a number of weeks away on the TCK being  ready for this. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Early access to the TCK</header>
    <date>Thu Jan 24 22:09:37 CET 2013</date>
    <body>To be honest, that reads like: we write the tests right before we ship the  software. How can the expert group make sure the test we will see eventually  are in line with what the spec defines? This essentially leaves us as author  of a spec that we will potentially see implementations of that do/can not  follow the spec because the TCK we have no influence on didn't catch serious  glitches. Where's the value in voting about a spec where the actual rules  that govern implementations are not under the control of those who vote? Beside Steve's issue with implementing the new features of the spec what is  the plan for the general work on the TCK going forward? To be honest I think  it's close to ridiculous to ask members of the EG to hand in ideas and  suggestions for improvements to the TCK which will then be implemented  however by whoever somewhere. Especially in the light of the ambiguities discovered recently I think one of  the top priorities going forward has to be that we (the EG) can make sure  we'll see the the new features and behavior tested adequately. This not only  will help us creating a good/better spec as ambiguities will be found before  it is to late but also a much better usability as the lack of defined *and  enforced* semantics prevents functionality from being rendered useless as  pretty much every persistence provider behaves slightly different as they  essentially have no real means to test their implementations. So I suggest that going forward... 1. the TCK has to developed alongside the spec by the EG. Newly defined API  and functionality has to be backed by test cases on submission of the feature  to the spec. 2. This requires EG members having access to the TCK during spec development  (requires appropriate licensing of the spec) 3. If I conclude wishful thinking here I wonder why even the community  shouldn't have access to the TCK to maybe help out discovering potential  issues. @Werner: does any of the current JCP versions already define TCK rules like  this? The closest JSR I can remember is CDI that always developed an Apache  licensed TCK fully in the open IIRC. Or is there any general rules within the  JCP/Oracle that should prevent us from going down that route? Cheers, Ollie -- Sent while on the run... Am 24.01.2013 um 21:39 schrieb Linda DeMichiel &amp;lt;linda.demichiel@...&amp;gt;:  Hi Steve,    The team tells me that we are still a number of weeks away on the TCK being   ready for this.    thanks,    -Linda    </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Early access to the TCK</header>
    <date>Thu Jan 24 22:23:51 CET 2013</date>
    <body>Oli/all, In an ideal world, that's how TDD works  And based on the session Patrick and others from the PMO held a while ago on TCKs, a TCK must not be tailored towards one implementation or the RI only, so if a Spec Lead company is large enough like in this case, ideally the part that writes the TCK should not necessarily be the ones who write the Spec or RI, though in some situations of course they may interact. Regarding the license, I believe the Spec page  http://jcp.org/en/jsr/detail?id=338 section 2.18 makes it very clear, that neither RI nor TCK for 338 are licensed under Apache or any other Open Source license. Nor are JSON or WebSockets, just to mention 2 other parts of the Java EE 7 Umbrella. In those cases the Spec isn't even in a public repository, while RI is, though certain licenses may also apply for commercial purposes beyond R&amp;amp;D, other Open Source Projects that go as Non-Profits or those "Qualified Individuals".  JCP.next is not about "Make everything Open Source", it is about a transparent way of designing and developing the JSRs with as little "deals behind closed doors" as possible, or none HTH, Werner On Thu, Jan 24, 2013 at 10:09 PM, Oliver Gierke ogierke@... To be honest, that reads like: we write the tests right before we ship the software. How can the expert group make sure the test we will see eventually are in line with what the spec defines? This essentially leaves us as author of a spec that we will potentially see implementations of that do/can not follow the spec because the TCK we have no influence on didn't catch serious glitches. Where's the value in voting about a spec where the actual rules that govern implementations are not under the control of those who vote? Beside Steve's issue with implementing the new features of the spec what is the plan for the general work on the TCK going forward? To be honest I think it's close to ridiculous to ask members of the EG to hand in ideas and suggestions for improvements to the TCK which will then be implemented however by whoever somewhere. Especially in the light of the ambiguities discovered recently I think one of the top priorities going forward has to be that we (the EG) can make sure we'll see the the new features and behavior tested adequately. This not only will help us creating a good/better spec as ambiguities will be found before it is to late but also a much better usability as the lack of defined *and enforced* semantics prevents functionality from being rendered useless as pretty much every persistence provider behaves slightly different as they essentially have no real means to test their implementations. So I suggest that going forward... 1. the TCK has to developed alongside the spec by the EG. Newly defined API and functionality has to be backed by test cases on submission of the feature to the spec. 2. This requires EG members having access to the TCK during spec development (requires appropriate licensing of the spec) 3. If I conclude wishful thinking here I wonder why even the community shouldn't have access to the TCK to maybe help out discovering potential issues. @Werner: does any of the current JCP versions already define TCK rules like this? The closest JSR I can remember is CDI that always developed an Apache licensed TCK fully in the open IIRC. Or is there any general rules within the JCP/Oracle that should prevent us from going down that route? Cheers, Ollie -- Sent while on the run... linda.demichiel@...</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Early access to the TCK</header>
    <date>Thu Jan 24 23:24:05 CET 2013</date>
    <body>Ollie, Thanks for your enthusiasm in offering to contribute here. To put things into perspective, I need to note that in over 10 years of creating TCKs we've found essentially no one who was interested in reviewing our TCKs, including our TCK licensees.  Because of this, our processes and practices are working on the assumption that such review will not happen, and we're still in the transition period of changing over to a model that better supports such review. Note also that we don't have a dedicated team for the JPA TCK so our TCK engineers may have been working on the TCKs for other technologies before the TCK for JPA, thus increasing the lead time here. Our team is doing their best to provide something as soon as possible. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification: FETCH JOINs and ORDER BY clauses</header>
    <date>Thu Jan 24 20:53:45 CET 2013</date>
    <body>What do you mean by "incidentally fetched". It's explicitly fetched I'd argue. So just to make sure I get this right: 1. If you want to order on something you want to make sure it gets loaded  eagerly, you have to equip the SELECT clause appropriately. 2. Only if you're not ordering on something you want to load eagerly you can  use ….fetch(…). Correct? Cheers, Ollie Am 22.01.2013 um 20:34 schrieb Linda DeMichiel &amp;lt;linda.demichiel@...&amp;gt;:        The orderby rules were written to allow direct mapping to SQL.  If an   implementation  were to support ordering by something that were incidentally fetched but   not in the  SELECT list that would be one thing -- but I don't think we should require   support for this.       --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Early access to the TCK</header>
    <date>Fri Jan 25 15:30:11 CET 2013</date>
    <body>To chime in on this, Oliver never said Open Source.  Obviously those of us that work in that realm believe it to be  superior way to develop software, and I personally at least would love to see the JPA TCK get to that point as many other TCKs are doing.  However, the point Oliver made was about making it *open*.  If I read Oliver correctly, I took his use of "open" to mean transparent as in open to within this group which you then elaborated on beautifully. Just to add to Oliver's list of reasons.. Over the years I have submitted *many* accepted challenges to the JPA TCK.  But the problem there is that providers who have already certified do not have to re-certify with the updated TCK, even though they passed flawed tests (perhaps even passed *because of* flawed tests).  Overall it brings into question the integrity of being "certified" at least to some extent, in my opinion.  It also directly leads to a fragmented JPA community with major concerns over provider migration.  Granted there will always be provider migration concerns, but I do not think that should happen over spec mandated (and certified!) features/behavior. On 01/24/2013 03:23 PM, Werner Keil Oli/all, In an ideal world, that's how TDD works &lt;img src="msg00046/gifWc5PRAti5g.gif" style="margin: 0px 0.2ex; vertical-align: middle;" goomoji="329"/&gt;  And based on the session Patrick and others from the PMO held a while ago on TCKs, a TCK must not be tailored towards one implementation or the RI only, so if a Spec Lead company is large enough like in this case, ideally the part that writes the TCK should not necessarily be the ones who write the Spec or RI, though in some situations of course they may interact. Regarding the license, I believe the Spec page  http://jcp.org/en/jsr/detail?id=338 section 2.18 makes it very clear, that neither RI nor TCK for 338 are licensed under Apache or any other Open Source license. Nor are JSON or WebSockets, just to mention 2 other parts of the Java EE 7 Umbrella. In those cases the Spec isn't even in a public repository, while RI is, though certain licenses may also apply for commercial purposes beyond R&amp;amp;D, other Open Source Projects that go as Non-Profits or those "Qualified Individuals".  JCP.next is not about "Make everything Open Source", it is about a transparent way of designing and developing the JSRs with as little "deals behind closed doors" as possible, or none &lt;img src="msg00046/gifqSE2s99eym.gif" style="margin: 0px 0.2ex; vertical-align: middle;" goomoji="347"/&gt; HTH, Werner On Thu, Jan 24, 2013 at 10:09 PM, Oliver Gierke ogierke@... &lt;blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"&gt; To be honest, that reads like: we write the tests right before we ship the software. How can the expert group make sure the test we will see eventually are in line with what the spec defines? This essentially leaves us as author of a spec that we will potentially see implementations of that do/can not follow the spec because the TCK we have no influence on didn't catch serious glitches. Where's the value in voting about a spec where the actual rules that govern implementations are not under the control of those who vote? Beside Steve's issue with implementing the new features of the spec what is the plan for the general work on the TCK going forward? To be honest I think it's close to ridiculous to ask members of the EG to hand in ideas and suggestions for improvements to the TCK which will then be implemented however by whoever somewhere. Especially in the light of the ambiguities discovered recently I think one of the top priorities going forward has to be that we (the EG) can make sure we'll see the the new features and behavior tested adequately. This not only will help us creating a good/better spec as ambiguities will be found before it is to late but also a much better usability as the lack of defined *and enforced* semantics prevents functionality from being rendered useless as pretty much every persistence provider behaves slightly different as they essentially have no real means to test their implementations. So I suggest that going forward... 1. the TCK has to developed alongside the spec by the EG. Newly defined API and functionality has to be backed by test cases on submission of the feature to the spec. 2. This requires EG members having access to the TCK during spec development (requires appropriate licensing of the spec) 3. If I conclude wishful thinking here I wonder why even the community shouldn't have access to the TCK to maybe help out discovering potential issues. @Werner: does any of the current JCP versions already define TCK rules like this? The closest JSR I can remember is CDI that always developed an Apache licensed TCK fully in the open IIRC. Or is there any general rules within the JCP/Oracle that should prevent us from going down that route? Cheers, Ollie -- Sent while on the run... linda.demichiel@... weeks away on the TCK being ready for this. early access to the TCK as being a generally good thing and there seemed later revisions of the spec. The Cliff Notes (tm) version of the pros problems in the TCK itself missing portability coverage start getting access to the 2.1 TCK, especially now that the spec is pretty</body>
  </mail>
  <mail>
    <header>[jpa-spec users] persistence XSD version in persistence.xml</header>
    <date>Sun Jan 27 17:52:08 CET 2013</date>
    <body>Hi All, In persistence.xml , version attribute and XSD version should still be specified as 2.0, else I am getting error when I deployed in teh glassfish server version glassfish-4.0-b72. In public draft document, I find below "The container must validate the persistence.xml file against the persistence_2_1.xsd, persistence_2_0.xsd, or persistence_1_0.xsd schema in accordance with the version specified by the persistence.xml file and report any validation errors." Hence , as per it , shouldnt version 2.1 version value be allowed in the persistence.xml, or am I missing something? The exception Caused by: org.xml.sax.SAXParseException; lineNumber: 4; columnNumber: 121; cvc-complex-type.3.1: Value '2.1' of attribute 'version' of element 'persistence' is not valid with respect to the corresponding attribute use. Attribute 'version' has a fixed value of '2.0'.     at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)     at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:134) |SEVERE|glassfish 4.0|javax.enterprise.system.core|_ThreadID=45;_ThreadName=AutoDeployer;_TimeMillis=1359302785077;_LevelValue=1000;_MessageID=NCLS-CORE-0026;|Exception during lifecycle processing java.io.IOException: org.xml.sax.SAXParseExceptionpublicId: http://www.oracle.com/technetwork/java/index.html    var g_HttpRelativeWebRoot = "/ocom/";'. Thanks, Nivedita</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: persistence XSD version in persistence.xml</header>
    <date>Sun Jan 27 21:04:38 CET 2013</date>
    <body>Hi Nivedita, The persistence_2_1.xsd is not available yet.  Hopefully it will be shortly. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Clarification: FETCH JOINs and ORDER BY clauses</header>
    <date>Mon Jan 28 22:45:52 CET 2013</date>
    <body>Yes. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] schema generation open issues</header>
    <date>Tue Jan 29 00:14:46 CET 2013</date>
    <body>There are currently a few open issues in schema generation that need clarification.  These pertain to sections 9.4 ("Schema generation") and 8.2.1.9 ("properties") in the spec. I've made some clarifications and proposed some additions to the metadata for schema generation as described below.  The draft I've just uploaded reflects these changes  ( http://java.net/projects/jpa-spec/downloads/download/JavaPersistence01282013.pdf ). Please review these sections and post any feedback.   Changes are summarized  below. ---- Section 9.4 This section isn't sufficiently clear about requirements for the specification of schema-generation-action and schema-generation-target. I've clarified that schema-generation-action must be specified or no actions will be taken.  This was an assumed default in the current public draft, but IMO not sufficiently explicit. Given the clarification to schema-generation-action, I think it now makes sense for schema-generation-target to have default values. That is, if script targets are specified by the properties for scripts, then generation actions should default into scripts.  If not, actions should be directly into the database. The current public draft assumes that if script sources are specified, generation should occur from scripts exclusively; and conversely, if script sources are not specified, generation should occur exclusively from the ORM metadata.   I've gotten feedback that this wasn't sufficiently flexible, and that we should support a combination of ORM and script sources.  (For example, consider the case where generation should occur from scripts, but it is desirable to add stored procedures to the database.) I therefore propose that we add a schema-generation-source property that will support such combination.  Again, if script sources are specified by the properties for scripts, then generation will default to use of only script sources; if script sources aren't specified, then generation will default from the ORM metadata. I've also received feedback (from our JPA/EE integration engineer) that we need to specify that in Java EE environments that strings corresponding to file URLs must use absolute (and not relative) paths.  I've added that clarification as well. Section 8.2.1.9 In the current public draft, this section lacks information as to how script locations are specified.  I've added a clarification that scripts packaged as part of the application must be specified relative to the root of the persistence unit, as we do for other packaged artifacts.  I've also added a clarification about the expectations for use of file URLs in Java EE environments as above. Finally, when I wrote this up initially, I was reluctant to add metadata for schema generation actions (including targets) because I believed that such metadata should not be specific to the persistence unit definition itself.  It seems however that people expect that the use of these properties (as specified in section 9.4) will be available anyway, whether they are specified in the spec document or not, and that some vendors will support these anyway as well. Further, this additional metadata may facilitate ease of use in prototyping/development environments.  I've therefore added these properties into 8.2.1.9.  If you disagree, please speak up. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 10:56:16 CET 2013</date>
    <body>Dear JPA experts, Could you please clarify, whether it is allowed for stateful session bean having container-managed extended persistence context to return  its EntityManager to clients (and for clients to perform operations on this EntityManager), or is it forbidden? Currently, GlassFish allows returning the extended EntityManager to outside (e.g.  http://java.net/jira/browse/GLASSFISH-11805 ), Apache OpenEJB does not, and there is currently ongoing discussion about issue https://issues.apache.org/jira/browse/TOMEE-509 .  JPA 2.0 specification had following text (stars added by me): ----------------------------------------------- 3.3 Persistence Context Lifetime ... When an extended persistence context is used, the extended persistence context exists from the time the EntityManager instance is created until it is closed. This persistence context might span multiple transactions and non-transactional invocations of the EntityManager. A container-managed extended persistence context is enlisted in the current transaction when *the EntityManager is invoked in the scope of that transaction* or when the stateful session bean to which the extended persistence context is bound is invoked in the scope of that transaction. ----------------------------------------------- The part between “*” seems to suggest, that EntityManager could be accessed by some external entity directly, not necessarily through  stateful session bean method.  I cannot find this text in JPA 2.1 public draft though.   Question: can reference to extended EntityManager be provided to outside, and methods called on this reference? Regards, Donatas</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 11:02:48 CET 2013</date>
    <body>Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb "donatas.ciuksys@..." unter Dear JPA experts, Could you please clarify, whether it is allowed for stateful session bean having container-managed extended persistence context to return its EntityManager to clients (and for clients to perform operations on this EntityManager), or is it forbidden? Currently, GlassFish allows returning the extended EntityManager to outside (e.g.  http://java.net/jira/browse/GLASSFISH-11805 ), Apache OpenEJB does not, and there is currently ongoing discussion about issue https://issues.apache.org/jira/browse/TOMEE-509  ;. JPA 2.0 specification had following text (stars added by me): ----------------------------------------------- 3.3 Persistence Context Lifetime ... When an extended persistence context is used, the extended persistence context exists from the time the EntityManager instance is created until it is closed. This persistence context might span multiple transactions and non-transactional invocations of the EntityManager. A container-managed extended persistence context is enlisted in the current transaction when *the EntityManager is invoked in the scope of that transaction* or when the stateful session bean to which the extended persistence context is bound is invoked in the scope of that transaction. ----------------------------------------------- The part between ³*² seems to suggest, that EntityManager could be accessed by some external entity directly, not necessarily through stateful session bean method. I cannot find this text in JPA 2.1 public draft though.   Question: can reference to extended EntityManager be provided to outside, and methods called on this reference? Regards, Donatas</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 12:49:58 CET 2013</date>
    <body>Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers On Tue, Jan 29, 2013 at 11:02 AM, Arne Limburg arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 12:54:40 CET 2013</date>
    <body>Hi Christian, I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans. Regards, Arne Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients? Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers On Tue, Jan 29, 2013 at 11:02 AM, Arne Limburg arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 13:08:40 CET 2013</date>
    <body>Hi Arne, For a container-managed persistence context, it is expected that the container injects a proxy EntityManager instance. This proxy might contain application-server specific logic (e.g. keeping track of the "close" events, if various proxies target the same real EM instance). Because this is not forbidden by the EJB or JPA spec and up to the app-server vendor, I would presume, that this is of course not supported, because manipulating the proxy outside the app-server could break the logic assumed by the app-server. Christian On Tue, Jan 29, 2013 at 12:54 PM, Arne Limburg arne.limburg@... Hi Christian, I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans. Regards, Arne Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients? Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers On Tue, Jan 29, 2013 at 11:02 AM, Arne Limburg arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 13:57:56 CET 2013</date>
    <body>Hi Christian,   Original question is about single VM, single EJB container, multiple EJB beans implementing singe use-case.   Imagine, we’d like to implement somewhat complex use-case (maybe even conversation alike), and single EJB component is too coarse-grained (low cohesion). We would naturally like to split it to several components. One of them is SFSB with extended persistence context, others just want to access this entity manager. Of course, there is a way to implement this using persistence context propagation. But the limitation is that this SFSB must always be the first in the call chain, so it must contain all the methods of use-case, though most of them will just delegate to other components. This does not sound lean. CDI allows us to inject “things”, and provides opportunity to simplify this call chain. If it would be legal for SFSB to have a CDI producer returning extended EntityManager, use-case implementation could be simplified. JSF page could directly invoke stateless business components. Glassfish supports such a producers, the questions is, is it legal per EJB/JPA specification, or is it explicitly forbidden.   Just for comparison: it seems that transactional EntityManager can be provided to other EJB/CDI components (CDI spec even has examples), and all EJB implementations support it. Why extended EntityManager is different?   Donatas   From: Christian von Kutzleben [mailto:cvkutzleben@...] Sent: Tuesday, January 29, 2013 2:09 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne, For a container-managed persistence context, it is expected that the container injects a proxy EntityManager instance. This proxy might contain application-server specific logic (e.g. keeping track of the "close" events, if various proxies target the same real EM instance). Because this is not forbidden by the EJB or JPA spec and up to the app-server vendor, I would presume, that this is of course not supported, because manipulating the proxy outside the app-server could break the logic assumed by the app-server. Christian arne.limburg@... Hi Christian,   I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans.   Regards, Arne   Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 14:21:31 CET 2013</date>
    <body>Hi Donatas, Although I can not actually answer your question, because I'm just looking through the JPA and not the EJB or CDI goggles, I have the feeling - maybe I'm wrong - that there is a slight misconception about the nature of "extended" vs. "non-extended" persistence contexts: As a JPA/EJB user you might easily get the impression, that those are two different things, but actually, they are not: It is how the container handles a persistence context, which makes the difference, it's not inherent in the actually created EntityManager instance of the JPA provider. This means, we, as the JPA vendor, don't know at all whether a persistence context we create and which is used by the container and the application is used as extended or non-extended persistence context. This in turn means, there is no custom way to create an extended persistence context, except through the container itself. (Not sure, whether this would be a requirement for the CDI producer you mentioned or not.) Christian On Tue, Jan 29, 2013 at 1:57 PM, Donatas Čiukšys donatas.ciuksys@... Hi Christian,   Original question is about single VM, single EJB container, multiple EJB beans implementing singe use-case.   Imagine, we’d like to implement somewhat complex use-case (maybe even conversation alike), and single EJB component is too coarse-grained (low cohesion). We would naturally like to split it to several components. One of them is SFSB with extended persistence context, others just want to access this entity manager. Of course, there is a way to implement this using persistence context propagation. But the limitation is that this SFSB must always be the first in the call chain, so it must contain all the methods of use-case, though most of them will just delegate to other components. This does not sound lean. CDI allows us to inject “things”, and provides opportunity to simplify this call chain. If it would be legal for SFSB to have a CDI producer returning extended EntityManager, use-case implementation could be simplified. JSF page could directly invoke stateless business components. Glassfish supports such a producers, the questions is, is it legal per EJB/JPA specification, or is it explicitly forbidden.   Just for comparison: it seems that transactional EntityManager can be provided to other EJB/CDI components (CDI spec even has examples), and all EJB implementations support it. Why extended EntityManager is different?   Donatas   From: Christian von Kutzleben [mailto: cvkutzleben@... ] Sent: Tuesday, January 29, 2013 2:09 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne, For a container-managed persistence context, it is expected that the container injects a proxy EntityManager instance. This proxy might contain application-server specific logic (e.g. keeping track of the "close" events, if various proxies target the same real EM instance). Because this is not forbidden by the EJB or JPA spec and up to the app-server vendor, I would presume, that this is of course not supported, because manipulating the proxy outside the app-server could break the logic assumed by the app-server. Christian arne.limburg@... Hi Christian,   I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans.   Regards, Arne   Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 16:12:56 CET 2013</date>
    <body>Hi Christian, What you write is not right: You write that "there is no custom way to create an extended persistence context, except through the container itself." But Chapter 7.7 states about application-managed persistence context that they are "extended in scope". So in fact EVERY EntityManager we create manually is extended. But that is not the point of Donatas and me: Donatas point is: You can always take an EntityManager that was injected by the Container and pass it around (to another EJB or even outside an EJB). The question is: Is it supposed to work outside that EJB it was injected into? And my additional question is: Is lazy loading of entities from an extended EntityManager supposed to work outside that EJB. Regards, Arne Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 14:21 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients? Hi Donatas, Although I can not actually answer your question, because I'm just looking through the JPA and not the EJB or CDI goggles, I have the feeling - maybe I'm wrong - that there is a slight misconception about the nature of "extended" vs. "non-extended" persistence contexts: As a JPA/EJB user you might easily get the impression, that those are two different things, but actually, they are not: It is how the container handles a persistence context, which makes the difference, it's not inherent in the actually created EntityManager instance of the JPA provider. This means, we, as the JPA vendor, don't know at all whether a persistence context we create and which is used by the container and the application is used as extended or non-extended persistence context. This in turn means, there is no custom way to create an extended persistence context, except through the container itself. (Not sure, whether this would be a requirement for the CDI producer you mentioned or not.) Christian On Tue, Jan 29, 2013 at 1:57 PM, Donatas Čiukšys donatas.ciuksys@... Hi Christian,   Original question is about single VM, single EJB container, multiple EJB beans implementing singe use-case.   Imagine, we’d like to implement somewhat complex use-case (maybe even conversation alike), and single EJB component is too coarse-grained (low cohesion). We would naturally like to split it to several components. One of them is SFSB with extended persistence context, others just want to access this entity manager. Of course, there is a way to implement this using persistence context propagation. But the limitation is that this SFSB must always be the first in the call chain, so it must contain all the methods of use-case, though most of them will just delegate to other components. This does not sound lean. CDI allows us to inject “things”, and provides opportunity to simplify this call chain. If it would be legal for SFSB to have a CDI producer returning extended EntityManager, use-case implementation could be simplified. JSF page could directly invoke stateless business components. Glassfish supports such a producers, the questions is, is it legal per EJB/JPA specification, or is it explicitly forbidden.   Just for comparison: it seems that transactional EntityManager can be provided to other EJB/CDI components (CDI spec even has examples), and all EJB implementations support it. Why extended EntityManager is different?   Donatas   From: Christian von Kutzleben [mailto: cvkutzleben@... ] Sent: Tuesday, January 29, 2013 2:09 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne, For a container-managed persistence context, it is expected that the container injects a proxy EntityManager instance. This proxy might contain application-server specific logic (e.g. keeping track of the "close" events, if various proxies target the same real EM instance). Because this is not forbidden by the EJB or JPA spec and up to the app-server vendor, I would presume, that this is of course not supported, because manipulating the proxy outside the app-server could break the logic assumed by the app-server. Christian arne.limburg@... Hi Christian,   I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans.   Regards, Arne   Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 16:16:37 CET 2013</date>
    <body>Ø   As a JPA/EJB user you might easily get the impression, that those are two different things, but actually, they are not   Well I wish they would be the same :) I actually encountered a case when EJB container treats them differently. My EJB component (with @PersistenceContext annotation) has method: getEntityManager(). Other EJB components use this method successfully unless the PersistenceContext is of extended type (Apache OpenEJB case).   The question could be put this way: must SFSB component (governing extended EM) be present in call stack if that EntityManager is accessed? Could the answer make it to JPA specificaition?   Donatas   From: Christian von Kutzleben [mailto:cvkutzleben@...] Sent: Tuesday, January 29, 2013 3:22 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Donatas, Although I can not actually answer your question, because I'm just looking through the JPA and not the EJB or CDI goggles, I have the feeling - maybe I'm wrong - that there is a slight misconception about the nature of "extended" vs. "non-extended" persistence contexts: As a JPA/EJB user you might easily get the impression, that those are two different things, but actually, they are not: It is how the container handles a persistence context, which makes the difference, it's not inherent in the actually created EntityManager instance of the JPA provider. This means, we, as the JPA vendor, don't know at all whether a persistence context we create and which is used by the container and the application is used as extended or non-extended persistence context. This in turn means, there is no custom way to create an extended persistence context, except through the container itself. (Not sure, whether this would be a requirement for the CDI producer you mentioned or not.) Christian donatas.ciuksys@... Hi Christian,   Original question is about single VM, single EJB container, multiple EJB beans implementing singe use-case.   Imagine, we’d like to implement somewhat complex use-case (maybe even conversation alike), and single EJB component is too coarse-grained (low cohesion). We would naturally like to split it to several components. One of them is SFSB with extended persistence context, others just want to access this entity manager. Of course, there is a way to implement this using persistence context propagation. But the limitation is that this SFSB must always be the first in the call chain, so it must contain all the methods of use-case, though most of them will just delegate to other components. This does not sound lean. CDI allows us to inject “things”, and provides opportunity to simplify this call chain. If it would be legal for SFSB to have a CDI producer returning extended EntityManager, use-case implementation could be simplified. JSF page could directly invoke stateless business components. Glassfish supports such a producers, the questions is, is it legal per EJB/JPA specification, or is it explicitly forbidden.   Just for comparison: it seems that transactional EntityManager can be provided to other EJB/CDI components (CDI spec even has examples), and all EJB implementations support it. Why extended EntityManager is different?   Donatas   From: Christian von Kutzleben [mailto: cvkutzleben@... ] Sent: Tuesday, January 29, 2013 2:09 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne, For a container-managed persistence context, it is expected that the container injects a proxy EntityManager instance. This proxy might contain application-server specific logic (e.g. keeping track of the "close" events, if various proxies target the same real EM instance). Because this is not forbidden by the EJB or JPA spec and up to the app-server vendor, I would presume, that this is of course not supported, because manipulating the proxy outside the app-server could break the logic assumed by the app-server. Christian arne.limburg@... Hi Christian,   I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans.   Regards, Arne   Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 16:21:31 CET 2013</date>
    <body>Hi Arne, He was asking about "container-managed" persistence contexts, not "application-managed" persistence contexts. it was injected into? For a portable application I would not assume that, but I can't give a definite answer on that. Assuming your same-VM scenario: of course this should work, why shouldn't work? Christian On Tue, Jan 29, 2013 at 4:12 PM, Arne Limburg arne.limburg@... Hi Christian, What you write is not right: You write that "there is no custom way to create an extended persistence context, except through the container itself." But Chapter 7.7 states about application-managed persistence context that they are "extended in scope". So in fact EVERY EntityManager we create manually is extended. But that is not the point of Donatas and me: Donatas point is: You can always take an EntityManager that was injected by the Container and pass it around (to another EJB or even outside an EJB). The question is: Is it supposed to work outside that EJB it was injected into? And my additional question is: Is lazy loading of entities from an extended EntityManager supposed to work outside that EJB. Regards, Arne Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 14:21 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients? Hi Donatas, Although I can not actually answer your question, because I'm just looking through the JPA and not the EJB or CDI goggles, I have the feeling - maybe I'm wrong - that there is a slight misconception about the nature of "extended" vs. "non-extended" persistence contexts: As a JPA/EJB user you might easily get the impression, that those are two different things, but actually, they are not: It is how the container handles a persistence context, which makes the difference, it's not inherent in the actually created EntityManager instance of the JPA provider. This means, we, as the JPA vendor, don't know at all whether a persistence context we create and which is used by the container and the application is used as extended or non-extended persistence context. This in turn means, there is no custom way to create an extended persistence context, except through the container itself. (Not sure, whether this would be a requirement for the CDI producer you mentioned or not.) Christian On Tue, Jan 29, 2013 at 1:57 PM, Donatas Čiukšys donatas.ciuksys@... Hi Christian,   Original question is about single VM, single EJB container, multiple EJB beans implementing singe use-case.   Imagine, we’d like to implement somewhat complex use-case (maybe even conversation alike), and single EJB component is too coarse-grained (low cohesion). We would naturally like to split it to several components. One of them is SFSB with extended persistence context, others just want to access this entity manager. Of course, there is a way to implement this using persistence context propagation. But the limitation is that this SFSB must always be the first in the call chain, so it must contain all the methods of use-case, though most of them will just delegate to other components. This does not sound lean. CDI allows us to inject “things”, and provides opportunity to simplify this call chain. If it would be legal for SFSB to have a CDI producer returning extended EntityManager, use-case implementation could be simplified. JSF page could directly invoke stateless business components. Glassfish supports such a producers, the questions is, is it legal per EJB/JPA specification, or is it explicitly forbidden.   Just for comparison: it seems that transactional EntityManager can be provided to other EJB/CDI components (CDI spec even has examples), and all EJB implementations support it. Why extended EntityManager is different?   Donatas   From: Christian von Kutzleben [mailto: cvkutzleben@... ] Sent: Tuesday, January 29, 2013 2:09 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne, For a container-managed persistence context, it is expected that the container injects a proxy EntityManager instance. This proxy might contain application-server specific logic (e.g. keeping track of the "close" events, if various proxies target the same real EM instance). Because this is not forbidden by the EJB or JPA spec and up to the app-server vendor, I would presume, that this is of course not supported, because manipulating the proxy outside the app-server could break the logic assumed by the app-server. Christian arne.limburg@... Hi Christian,   I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans.   Regards, Arne   Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 16:26:36 CET 2013</date>
    <body>Hi Christian, OK, I agree with you that one cannot create a container-managed persistence context, except through the container itself ;-) About the lazy-loading: I expected that to work, too, but I had a scenario where it didn't work in JBoss AS 7 with Hibernate… Btw. Both points (Donatas and mine) should be clarified by the spec (at least there should be a sentence that it is not portable). Regards, Arne Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 16:21 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients? Hi Arne, He was asking about "container-managed" persistence contexts, not "application-managed" persistence contexts. For a portable application I would not assume that, but I can't give a definite answer on that. Assuming your same-VM scenario: of course this should work, why shouldn't work? Christian On Tue, Jan 29, 2013 at 4:12 PM, Arne Limburg arne.limburg@... Hi Christian, What you write is not right: You write that "there is no custom way to create an extended persistence context, except through the container itself." But Chapter 7.7 states about application-managed persistence context that they are "extended in scope". So in fact EVERY EntityManager we create manually is extended. But that is not the point of Donatas and me: Donatas point is: You can always take an EntityManager that was injected by the Container and pass it around (to another EJB or even outside an EJB). The question is: Is it supposed to work outside that EJB it was injected into? And my additional question is: Is lazy loading of entities from an extended EntityManager supposed to work outside that EJB. Regards, Arne Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 14:21 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients? Hi Donatas, Although I can not actually answer your question, because I'm just looking through the JPA and not the EJB or CDI goggles, I have the feeling - maybe I'm wrong - that there is a slight misconception about the nature of "extended" vs. "non-extended" persistence contexts: As a JPA/EJB user you might easily get the impression, that those are two different things, but actually, they are not: It is how the container handles a persistence context, which makes the difference, it's not inherent in the actually created EntityManager instance of the JPA provider. This means, we, as the JPA vendor, don't know at all whether a persistence context we create and which is used by the container and the application is used as extended or non-extended persistence context. This in turn means, there is no custom way to create an extended persistence context, except through the container itself. (Not sure, whether this would be a requirement for the CDI producer you mentioned or not.) Christian On Tue, Jan 29, 2013 at 1:57 PM, Donatas Čiukšys donatas.ciuksys@... Hi Christian,   Original question is about single VM, single EJB container, multiple EJB beans implementing singe use-case.   Imagine, we’d like to implement somewhat complex use-case (maybe even conversation alike), and single EJB component is too coarse-grained (low cohesion). We would naturally like to split it to several components. One of them is SFSB with extended persistence context, others just want to access this entity manager. Of course, there is a way to implement this using persistence context propagation. But the limitation is that this SFSB must always be the first in the call chain, so it must contain all the methods of use-case, though most of them will just delegate to other components. This does not sound lean. CDI allows us to inject “things”, and provides opportunity to simplify this call chain. If it would be legal for SFSB to have a CDI producer returning extended EntityManager, use-case implementation could be simplified. JSF page could directly invoke stateless business components. Glassfish supports such a producers, the questions is, is it legal per EJB/JPA specification, or is it explicitly forbidden.   Just for comparison: it seems that transactional EntityManager can be provided to other EJB/CDI components (CDI spec even has examples), and all EJB implementations support it. Why extended EntityManager is different?   Donatas   From: Christian von Kutzleben [mailto: cvkutzleben@... ] Sent: Tuesday, January 29, 2013 2:09 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne, For a container-managed persistence context, it is expected that the container injects a proxy EntityManager instance. This proxy might contain application-server specific logic (e.g. keeping track of the "close" events, if various proxies target the same real EM instance). Because this is not forbidden by the EJB or JPA spec and up to the app-server vendor, I would presume, that this is of course not supported, because manipulating the proxy outside the app-server could break the logic assumed by the app-server. Christian arne.limburg@... Hi Christian,   I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans.   Regards, Arne   Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 16:28:43 CET 2013</date>
    <body>Hi Donatas Well I wish they would be the same :) I actually encountered a case when EJB container treats them differently. My EJB component (with @PersistenceContext annotation) has method: getEntityManager(). Other EJB components use this method successfully unless the PersistenceContext is of extended type (Apache OpenEJB case).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 16:41:45 CET 2013</date>
    <body>Hi Arne, It suprises me, that it did not work: the JPA entities naturally should only know their persistence context and not any proxies. I'm not sure, whether JBoss disables anything for the (Hibernate) persistence context after the bean invocation. If you use JBoss AS 7 with our JPA implementation (or probably any other 3rd party JPA implementation), then JBoss can only use JPA API/SPI to communicate with us. And there is no way to disable anything regarding lazy loading directly. Did you get a specific exception? Christian On Tue, Jan 29, 2013 at 4:26 PM, Arne Limburg arne.limburg@... Hi Christian, OK, I agree with you that one cannot create a container-managed persistence context, except through the container itself ;-) About the lazy-loading: I expected that to work, too, but I had a scenario where it didn't work in JBoss AS 7 with Hibernate… Btw. Both points (Donatas and mine) should be clarified by the spec (at least there should be a sentence that it is not portable). Regards, Arne Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 16:21 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients? Hi Arne, He was asking about "container-managed" persistence contexts, not "application-managed" persistence contexts. For a portable application I would not assume that, but I can't give a definite answer on that. Assuming your same-VM scenario: of course this should work, why shouldn't work? Christian On Tue, Jan 29, 2013 at 4:12 PM, Arne Limburg arne.limburg@... Hi Christian, What you write is not right: You write that "there is no custom way to create an extended persistence context, except through the container itself." But Chapter 7.7 states about application-managed persistence context that they are "extended in scope". So in fact EVERY EntityManager we create manually is extended. But that is not the point of Donatas and me: Donatas point is: You can always take an EntityManager that was injected by the Container and pass it around (to another EJB or even outside an EJB). The question is: Is it supposed to work outside that EJB it was injected into? And my additional question is: Is lazy loading of entities from an extended EntityManager supposed to work outside that EJB. Regards, Arne Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 14:21 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients? Hi Donatas, Although I can not actually answer your question, because I'm just looking through the JPA and not the EJB or CDI goggles, I have the feeling - maybe I'm wrong - that there is a slight misconception about the nature of "extended" vs. "non-extended" persistence contexts: As a JPA/EJB user you might easily get the impression, that those are two different things, but actually, they are not: It is how the container handles a persistence context, which makes the difference, it's not inherent in the actually created EntityManager instance of the JPA provider. This means, we, as the JPA vendor, don't know at all whether a persistence context we create and which is used by the container and the application is used as extended or non-extended persistence context. This in turn means, there is no custom way to create an extended persistence context, except through the container itself. (Not sure, whether this would be a requirement for the CDI producer you mentioned or not.) Christian On Tue, Jan 29, 2013 at 1:57 PM, Donatas Čiukšys donatas.ciuksys@... Hi Christian,   Original question is about single VM, single EJB container, multiple EJB beans implementing singe use-case.   Imagine, we’d like to implement somewhat complex use-case (maybe even conversation alike), and single EJB component is too coarse-grained (low cohesion). We would naturally like to split it to several components. One of them is SFSB with extended persistence context, others just want to access this entity manager. Of course, there is a way to implement this using persistence context propagation. But the limitation is that this SFSB must always be the first in the call chain, so it must contain all the methods of use-case, though most of them will just delegate to other components. This does not sound lean. CDI allows us to inject “things”, and provides opportunity to simplify this call chain. If it would be legal for SFSB to have a CDI producer returning extended EntityManager, use-case implementation could be simplified. JSF page could directly invoke stateless business components. Glassfish supports such a producers, the questions is, is it legal per EJB/JPA specification, or is it explicitly forbidden.   Just for comparison: it seems that transactional EntityManager can be provided to other EJB/CDI components (CDI spec even has examples), and all EJB implementations support it. Why extended EntityManager is different?   Donatas   From: Christian von Kutzleben [mailto: cvkutzleben@... ] Sent: Tuesday, January 29, 2013 2:09 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne, For a container-managed persistence context, it is expected that the container injects a proxy EntityManager instance. This proxy might contain application-server specific logic (e.g. keeping track of the "close" events, if various proxies target the same real EM instance). Because this is not forbidden by the EJB or JPA spec and up to the app-server vendor, I would presume, that this is of course not supported, because manipulating the proxy outside the app-server could break the logic assumed by the app-server. Christian arne.limburg@... Hi Christian,   I am not talking about clients in another VM, of course this is not supported since the EntityManager is not defined to be Serializable. We are just talking about local clients here, like JSF managed beans or CDI beans.   Regards, Arne   Von: cvkutzleben@... Antworten an: " users@... users@... Datum: Dienstag, 29. Januar 2013 12:49 An: " users@... users@... Betreff: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Arne and Donatas, Just my 2 cents regardings this: The general case is, that the client is in another VM. This in turn would required remoting concepts wrt. to the EntityManager, as e.g. an RMI proxy instance would be needed, so that the actual requests really happen in the same context. It seems not to be viable, to actually serialize the whole context to any client, because the context would exist twice then, and some resources hold by the context might not be transferable at all. (e.g. a JDBC connection). This leaves a remote proxy as only workable approach, and giving the lack of any discussion of this aspect in the spec, I doubt whether this scenario was ever intended. Personally I do also dislike this approach because it tends to break the layering of an application. Christian P.S. in our JDO product we had a feature once - which was dropped later - to have remote PersistenceManagers arne.limburg@... Hi all, Good issue from Donatas. I stumbled over this, too, some time ago and I would like to extend the question: When an entity was loaded from an extended persistence context that is bound to a stateful session bean. Is lazy loading supposed to work outside of the call stack of that stateful session bean, i.e. in a JSF managed bean or a cdi bean that is directly accessed via EL? I think it was Jboss AS 7 with Hibernate where this did not work. Regards, Arne Am 29.01.13 10:56 schrieb " donatas.ciuksys@... " unter donatas.ciuksys@... http://java.net/jira/browse/GLASSFISH-11805 ), Apache https://issues.apache.org/jira/browse/TOMEE-509 .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 17:47:19 CET 2013</date>
    <body>Hi Christian, Arne,   Small code example to illustrate the issue:   ------------------------------------------------------------ @Stateful       @Stateless         @PostConstruct             entityManagerHolder.getEntityManager().persist(...); // &amp;lt;-- WORKING OK         em.persist(...); // &amp;lt;-- NOT WORKING     ------------------------------------------------------------   As noted in method testEMaccess() comments, one line is working, other is not (in OpenEJB; in GlassFish both lines are working).   Note that both lines are very similar, and most programmers would be tended to get rid of repeated “entityManagerHolder.getEntityManager()” calls by storing em in class field (as shown in the second sentence).   Donatas   From: Christian von Kutzleben [mailto:cvkutzleben@...] Sent: Tuesday, January 29, 2013 5:29 PM To: users@... Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?   Hi Donatas Well I wish they would be the same :) I actually encountered a case when EJB container treats them differently. My EJB component (with @PersistenceContext annotation) has method: getEntityManager(). Other EJB components use this method successfully unless the PersistenceContext is of extended type (Apache OpenEJB case).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Tue Jan 29 18:48:11 CET 2013</date>
    <body>Is class A stateful in your app? Which version of AS7 exactly are you using? Have you tried to recreate against the latest JBoss AS nightly build http://community.jboss.org/thread/167590 ? Also, general questions about JBoss AS can also be raised on https://community.jboss.org/en/jbossas7 . Regards, Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Tue Jan 29 23:14:13 CET 2013</date>
    <body>What is the proposed behavior of EntityGraph or Subgraph when addAttributeNodes is called with non-basic attribute types (a ManyToOne for example)?  Is that considered an exception condition?  And if so, what exception (IllegalArgumentException)?  Or do providers simply interpret it as a call to addSubgraph? Also, a minor edit to report in the spec, at least as of the Public Review Draft.  In the definition of the NamedEntityGraph annotation, the type of subclassSubgraphs is defined as NamedSubGraph[] rather than NamedSubgraph[]</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Tue Jan 29 23:42:54 CET 2013</date>
    <body>Also related... Do EntityGraph.getAttributeNodes / Subgraph.getAttributeNodes return Subgraphs in the AttributeNode list?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EntityGraph's AttributeNode and Type</header>
    <date>Wed Jan 30 00:13:04 CET 2013</date>
    <body>Ok, I think I was missing this part in my earlier questions. So AttributeNode.getType() is going away; got it.  Will there be separate getSubgraphs() and getKeySubgraphs() methods (in addition to getAttributeNodes()) as mentioned by Gordon? Still does not clear up my overall confusion of addAttributeNodes when passed non-basic types in terms of spec-defined behavior.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EntityGraph's AttributeNode and Type</header>
    <date>Wed Jan 30 02:27:25 CET 2013</date>
    <body>Not in this release, but perhaps later if needed.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 02:27:55 CET 2013</date>
    <body>It just adds the attibute node for the attribute, but doesn't give the  opportunity to expand on it as a subgraph. BTW, the documentation for EntityGraph is messed up.  The last paragraph of  the getName method docs should really have been applied to EntityGraph instead (i.e., at top level). thanks!</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 02:30:58 CET 2013</date>
    <body>Yes</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 03:34:04 CET 2013</date>
    <body>Is adding the same attribute (by name) using both addAttributeNode and addSubgraph allowed? ...</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Is it allowed for SFSB to return extended EntityManager to clients?</header>
    <date>Wed Jan 30 07:00:56 CET 2013</date>
    <body>Hi Scott, Class A is annotated with @Stateless in the example, though I do not see any  difference it being @Stateful. This is not the case of persistence context  propagation/inheritance where it would matter. Real life implementation would  of course use CDI scopes like @ConversationScoped (on EntityManagerHolder)  and @RequestScoped (on class A). Since this is not CDI specific (but JPA/EJB  integration), I skipped CDI annotations. Also this is not the question whether Hibernate/JBoss support this scenario  (I stumbled upon OpenEJB not supporting this), but question whether this is  not portable per JPA specification, and where in specification it is told so. Regards, Donatas -----Original Message----- From: Scott Marlow [ mailto:smarlow@...]  Sent: Tuesday, January 29, 2013 7:48 PM To: users@... Cc: Donatas Čiukšys Subject: [jpa-spec users] Re: Is it allowed for SFSB to return extended  EntityManager to clients?  Hi Christian, Arne,  Small code example to illustrate the issue:  ------------------------------------------------------------  @Stateful       @PersistenceContext(type=PersistenceContextType.EXTENDED) private Is class A stateful in your app? Which version of AS7 exactly are you using? Have you tried to recreate against the latest JBoss AS nightly build  http://community.jboss.org/thread/167590 ? Also, general questions about JBoss AS can also be raised on  https://community.jboss.org/en/jbossas7 . Regards, Scott  @Stateless       @PostConstruct           entityManagerHolder.getEntityManager().persist(...); // &amp;lt;--  WORKING OK           em.persist(...); // &amp;lt;-- NOT WORKING  ------------------------------------------------------------  As noted in method testEMaccess() comments, one line is working, other  is not (in OpenEJB; in GlassFish both lines are working).  Note that both lines are very similar, and most programmers would be  tended to get rid of repeated “entityManagerHolder.getEntityManager()”  calls by storing em in class field (as shown in the second sentence).  Donatas  *From:*Christian von Kutzleben [ mailto:cvkutzleben@ ...]  *Sent:* Tuesday, January 29, 2013 5:29 PM  *To:* users@...  *Subject:* [jpa-spec users] Re: Is it allowed for SFSB to return  extended EntityManager to clients?  Hi Donatas      Well I wish they would be the same :) I actually encountered a case      when EJB container treats them differently. My EJB component (with      @PersistenceContext annotation) has method: getEntityManager().      Other EJB components use this method successfully unless the      PersistenceContext is of extended type (Apache OpenEJB case).  The injected proxy is not exactly the same as the real persistence  context; the injected proxy might be different (that's up to the  container), but  eventually the persistence context implementation is the same.      The question could be put this way: must SFSB component (governing      extended EM) be present in call stack if that EntityManager is      accessed? Could the answer make it to JPA specificaition?  I would assume so for a portable application.  Christian      Donatas      *From:*Christian von Kutzleben [ mailto:cvkutzleben@ ... mailto:cvkutzleben@ ...&amp;gt;]      *Sent:* Tuesday, January 29, 2013 3:22 PM mailto:users@      *Subject:* [jpa-spec users] Re: Is it allowed for SFSB to return      extended EntityManager to clients?      Hi Donatas,      Although I can not actually answer your question, because I'm just      looking through the JPA and not the EJB or CDI goggles,      I have the feeling - maybe I'm wrong - that there is a slight      misconception about the nature of "extended" vs. "non-extended"      persistence contexts:      As a JPA/EJB user you might easily get the impression, that those      are two different things, but actually, they are not:      It is how the container handles a persistence context, which makes      the difference, it's not inherent in the actually created      EntityManager instance of the JPA provider. This means, we, as the      JPA vendor, don't know at all whether a persistence      context we create and which is used by the container and the      application is used as extended or non-extended persistence context.      This in turn means, there is no custom way to create an extended      persistence context, except through the container itself.      (Not sure, whether this would be a requirement for the CDI producer      you mentioned or not.)      Christian      On Tue, Jan 29, 2013 at 1:57 PM, Donatas Čiukšys mailto:donatas.ciuksys@          Hi Christian,          Original question is about single VM, single EJB container,          multiple EJB beans implementing singe use-case.          Imagine, we’d like to implement somewhat complex use-case (maybe          even conversation alike), and single EJB component is too          coarse-grained (low cohesion). We would naturally like to split          it to several components. One of them is SFSB with extended          persistence context, others just want to access this entity          manager.          Of course, there is a way to implement this using persistence          context propagation. But the limitation is that this SFSB must          always be the first in the call chain, so it must contain all          the methods of use-case, though most of them will just delegate          to other components. This does not sound lean.          CDI allows us to inject “things”, and provides opportunity to          simplify this call chain. If it would be legal for SFSB to have          a CDI producer returning extended EntityManager, use-case          implementation could be simplified. JSF page could directly          invoke stateless business components. Glassfish supports such a          producers, the questions is, is it legal per EJB/JPA          specification, or is it explicitly forbidden.          Just for comparison: it seems that transactional EntityManager          can be provided to other EJB/CDI components (CDI spec even has          examples), and all EJB implementations support it. Why extended          EntityManager is different?          Donatas          *From:*Christian von Kutzleben [ mailto:cvkutzleben@ ... mailto:cvkutzleben@ ...&amp;gt;]          *Sent:* Tuesday, January 29, 2013 2:09 PM mailto:users@          *Subject:* [jpa-spec users] Re: Is it allowed for SFSB to return          extended EntityManager to clients?          Hi Arne,          For a container-managed persistence context, it is expected that          the container injects a proxy EntityManager instance.          This proxy might contain application-server specific logic (e.g.          keeping track of the "close" events, if various          proxies target the same real EM instance).          Because this is not forbidden by the EJB or JPA spec and up to          the app-server vendor,          I would presume, that this is of course not supported, because          manipulating the proxy outside          the app-server could break the logic assumed by the app-server.          Christian          On Tue, Jan 29, 2013 at 12:54 PM, Arne Limburg          &amp;lt;arne.limburg@... mailto:arne.limburg@              Hi Christian,              I am not talking about clients in another VM, of course this              is not supported since the EntityManager is not defined to              be Serializable.              We are just talking about local clients here, like JSF              managed beans or CDI beans.              Regards,              Arne              *Von: *Christian von Kutzleben &amp;lt;cvkutzleben@... mailto:cvkutzleben@              *Antworten an: *"users@... mailto:users@ ...&amp;gt;" &amp;lt;users@... mailto:users@              *Datum: *Dienstag, 29. Januar 2013 12:49              *An: *"users@... mailto:users@ ...&amp;gt;" &amp;lt;users@... mailto:users@              *Betreff: *[jpa-spec users] Re: Is it allowed for SFSB to              return extended EntityManager to clients?              Hi Arne and Donatas,              Just my 2 cents regardings this:              The general case is, that the client is in another VM.              This in turn would required remoting concepts wrt. to the              EntityManager, as e.g. an RMI proxy instance would be needed,              so that the actual requests really happen in the same context.              It seems not to be viable, to actually serialize the whole              context to any client,              because the context would exist twice then, and some              resources hold by the context might not be transferable at all.              (e.g. a JDBC connection). This leaves a remote proxy as only              workable approach, and giving the lack of              any discussion of this aspect in the spec, I doubt whether              this scenario was ever intended.              Personally I do also dislike this approach because it tends              to break the layering of an application.              Christian              P.S. in our JDO product we had a feature once - which was              dropped later - to have remote PersistenceManagers              On Tue, Jan 29, 2013 at 11:02 AM, Arne Limburg              &amp;lt;arne.limburg@... mailto:arne.limburg@                  Hi all,                  Good issue from Donatas. I stumbled over this, too, some                  time ago and I                  would like to extend the question:                  When an entity was loaded from an extended persistence                  context that is                  bound to a stateful session bean. Is lazy loading                  supposed to work outside                  of the call stack of that stateful session bean, i.e. in                  a JSF managed                  bean or a cdi bean that is directly accessed via EL?                  I think it was Jboss AS 7 with Hibernate where this did                  not work.                  Regards,                  Arne                  Am 29.01.13 10:56 schrieb "donatas.ciuksys@... mailto:donatas.ciuksys@ ...&amp;gt;" unter                  &amp;lt;donatas.ciuksys@... mailto:donatas.ciuksys@ ...&amp;gt;&amp;gt;:                  &amp;gt;Dear JPA experts,                  &amp;gt;Could you please clarify, whether it is allowed for   stateful session                  &amp;gt;bean having container-managed extended persistence context   to return                  &amp;gt;its EntityManager to clients (and for clients to perform   operations on                  &amp;gt;this EntityManager), or is it forbidden?                  &amp;gt;Currently, GlassFish allows returning the extended   EntityManager to                  &amp;gt;outside (e.g. http://java.net/jira/browse/GLASSFISH-11805 ),   Apache                  &amp;gt;OpenEJB does not, and there is currently ongoing   discussion about issue https://issues.apache.org/jira/browse/TOMEE-509  ;.                  &amp;gt;JPA 2.0 specification had following text (stars added by   me):                  &amp;gt;-----------------------------------------------                  &amp;gt;3.3 Persistence Context Lifetime                  &amp;gt;...                  &amp;gt;When an extended persistence context is used, the extended   persistence                  &amp;gt;context exists from the time the                  &amp;gt;EntityManager instance is created until it is closed. This   persistence                  &amp;gt;context might span multiple transactions                  &amp;gt;and non-transactional invocations of the EntityManager. A                  &amp;gt;container-managed extended persistence                  &amp;gt;context is enlisted in the current transaction when *the   EntityManager                  &amp;gt;is invoked in the scope of                  &amp;gt;that transaction* or when the stateful session bean to   which the                  &amp;gt;extended persistence context is bound is                  &amp;gt;invoked in the scope of that transaction.                  &amp;gt;-----------------------------------------------                  &amp;gt;The part between ³*² seems to suggest, that EntityManager   could be                  &amp;gt;accessed by some external entity directly, not necessarily   through                  &amp;gt;stateful session bean method.                  &amp;gt;I cannot find this text in JPA 2.1 public draft though.                  &amp;gt;Question: can reference to extended EntityManager be   provided to                  &amp;gt;outside, and methods called on this reference?                  &amp;gt;Regards,                  &amp;gt;Donatas              --              Christian von Kutzleben              Chief Engineer | Versant GmbH mailto:cromberg@ http://www.versant.com http://www.db4o.com              --              Versant              GmbH is incorporated in Germany. Company registration              number: HRB              54723, Amtsgericht Hamburg. Registered Office: Halenreie 42,              22359              Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John              CONFIDENTIALITY              NOTICE: This e-mail message, including any attachments, is              for the sole              use of the intended recipient(s) and may contain confidential or              proprietary information. Any unauthorized review, use,              disclosure or              distribution is prohibited. If you are not the intended              recipient,              immediately contact the sender by reply e-mail and destroy              all copies of              the original message.          --          Christian von Kutzleben          Chief Engineer | Versant GmbH mailto:cromberg@ http://www.versant.com http://www.db4o.com          --          Versant          GmbH is incorporated in Germany. Company registration number: HRB          54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359          Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John          CONFIDENTIALITY          NOTICE: This e-mail message, including any attachments, is for          the sole          use of the intended recipient(s) and may contain confidential or          proprietary information. Any unauthorized review, use, disclosure or          distribution is prohibited. If you are not the intended recipient,          immediately contact the sender by reply e-mail and destroy all          copies of          the original message.      --      Christian von Kutzleben      Chief Engineer | Versant GmbH mailto:cromberg@ http://www.versant.com http://www.db4o.com      --      Versant      GmbH is incorporated in Germany. Company registration number: HRB      54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359      Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John      CONFIDENTIALITY      NOTICE: This e-mail message, including any attachments, is for the sole      use of the intended recipient(s) and may contain confidential or      proprietary information. Any unauthorized review, use, disclosure or      distribution is prohibited. If you are not the intended recipient,      immediately contact the sender by reply e-mail and destroy all copies of      the original message.  --  Christian von Kutzleben  Chief Engineer | Versant GmbH  (T) +49 40 60990-0  (F) +49 40 60990-113 mailto:cromberg@ http://www.versant.com http://www.db4o.com  --  Versant  GmbH is incorporated in Germany. Company registration number: HRB  54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359  Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John  CONFIDENTIALITY  NOTICE: This e-mail message, including any attachments, is for the sole  use of the intended recipient(s) and may contain confidential or  proprietary information. Any unauthorized review, use, disclosure or  distribution is prohibited. If you are not the intended recipient,  immediately contact the sender by reply e-mail and destroy all copies of  the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 15:21:01 CET 2013</date>
    <body>Hello Steve, Calling addAttributeNodes with a non-basic type is not an exception condition.  It covers the simple case where a user wishes to include the attribute in the entity graph but has no requirement to further specify a subgraph for the attribute target class.  The operations specify behaviour for this case specifically for instance with fetch graph the default fetch graph is used for the target entity when no subgraph is specified otherwise the user would need to recreate the default fetch graph for all referenced attributes. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 15:29:13 CET 2013</date>
    <body>Yes, they should although it may be easier to support more inspection in the future if AttributeNode.getSubgraps() AttributeNode.getKeySubgraphs() were to exist or planned to exist. The alternate pattern that will be needed of isKeySubgraph() is not a great pattern.  I think we should add this simple methods to this release if possible. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 15:32:04 CET 2013</date>
    <body>Yes it should be allowed, the subsequent addition would replace the existing definition.  Otherwise there is no way for users to modify existing entity graphs. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 15:46:05 CET 2013</date>
    <body>Based on on your responses then, I think its is much more feasible to include the 2 new methods (getSubgraphs and getKeySubgraphs) in addition to getAttributeNodes in this release then.  Otherwise the API, IMHO is very confusing in this regard. Also, I'd like to suggest that this notion of "key subgraphs" be renamed to "map key subgraphs".  I see key here and continually think of database keys (as in maybe references to composite primary key embeddables).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 15:51:11 CET 2013</date>
    <body>In regards to getAttributeNodes returning a List, why a List exactly? What is the intending ordering of this List?  Insertion order? The reason I ask is that obviously the behavior you lay out (replacing) would be most easily served by usnig a Map.  But using a Map for the storage makes it awkward for returning Lists. And conversely using Lists for the storage makes it awkward to test for the replacement condition.  True, this is a implementation detail/difficulty, but its caused by using a List in the API when I am not sure that is necessary.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Partial rollback of entities/transactions</header>
    <date>Wed Jan 30 16:00:48 CET 2013</date>
    <body>Hello everyone, I stumbled upon a problem that I think deserves a standard solution in JPA. Imagine an application that allows a user to update an entity (or a set of entities) in several successive steps (e.g. in separate dialogs) before committing to the database. Within each step the entity can be updated (one or several times) and the user has to see its current state. The user can choose to cancel a step (dialog) at any time and proceed with another, or restart the same step again. As far as I know (maybe I'm wrong) there no way to cancel only selected changes made to an uncommitted entity. So far I heard/read about two workarounds: 1. Before each step (dialog) you create a copy of the entity and work with it. If the step is finished gracefully the state of the copy has to be written to the entity. If the step is canceled the entity remains unchanged. 2. One needs to track all the changes within a step and manually undo the changes on the entity. In my opinion this use case should be quite common and deserves an easy high-level API. There are two similar mechanism I know about: A) Savepoints: JDBC offers the concept of savepoints within an Transaction to do a partial rollbacks.  B) Nested transactions: Allows to create sub-transactions which can be rolled back separately. A similar construct in JPA would help a lot to avoid cumbersome, handmade, solutions. - Sebastian</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 17:42:51 CET 2013</date>
    <body>The list order is undefined.   Having a Map as the return type adds the complication of specifying a key as users of the untyped String API are not going to expect or want the keys to be Attribute types and vice versa for other users. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 17:45:23 CET 2013</date>
    <body>I never said to have a Map as the return type.   I am talking about internally (impl detail) using a Map.  Honestly, to me the return ought</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 18:35:15 CET 2013</date>
    <body>Hi Gordon, Could you propose the specific methods that you think should be added in this  release? thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 20:29:17 CET 2013</date>
    <body>The original plan was to simplify the interfaces and eliminate the AttributeNode as an artifact the user needed, however as the structure of the Subgraph became more complex to support the more complex scenarios I no longer think this is an option as we should support inspection in the future.  I am proposing that Subgraph interface no longer extends from AttributeNode and that AttributeNode would be updated to be :     /**      * Return the name of the attribute corresponding to the      * attribute node.      * @return name of the attribute      */     /** * Return the Map&amp;lt;Class, Subgraph&amp;gt; of Subgraphs associated with this AttributeNode      * @return Map of Subgraphs associated with this AttribureNode      */     /** * Return the Map&amp;lt;Class, Subgraph&amp;gt; of Subgraphs associated with this AttributeNode's map key * @return Map of Subgraphs associated with this AttribureNode's map key      */ This would allow for easier inspection of the EntityGraph and easily allow the user to differentiate between map key subgraphs and element subgraphs. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 21:02:29 CET 2013</date>
    <body>Oh, I misunderstood.  I thought you were suggesting that getSubgraphs and getKeySubgraphs would become part of EntityGraph and Subgraph. Putting them on AttributeNode does not make sense to me.   At least not as I was understanding the intention/breakdown between AttributeNode and Subgraph from your and Linda's replies from yesterday.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 21:51:46 CET 2013</date>
    <body>The issue is multiple key subgraphs and multiple subgraphs can exist for any attribute.  If we do not organize the subgraphs under an AttributeNode just like the annotations are structured it gets very difficult to convey the contents of an entity graph to a user in a manageable way.  if we were to add getKeySubgraphs to EntityManager the return would have to be List&amp;lt;Map&amp;lt;Class,Subgraph&amp;gt;&amp;gt; at the least.  Any other structure(ie. List&amp;lt;Subgraph&amp;gt;) and the user would need to organize the results to make the information meaningful.  Also if get*Subgraphs methods were added to the EntityGraph the user would need to check all three collections to determine what attributes were present in the entity graph. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Wed Jan 30 22:38:14 CET 2013</date>
    <body>I think this is an improvement. Unless there is objection, I will plan to make this change. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Early access to the TCK</header>
    <date>Wed Jan 30 17:56:01 CET 2013</date>
    <body>If that can help someone to vent frustrations, you can come and help Hardy, Gunnar and I write the Bean Validation TCK :D We can definitively use spare hands as the Feb 20th deadline is approaching fast ;) Emmanuel  To be honest, that reads like: we write the tests right before we ship the   software. How can the expert group make sure the test we will see   eventually are in line with what the spec defines? This essentially leaves   us as author of a spec that we will potentially see implementations of that   do/can not follow the spec because the TCK we have no influence on didn't   catch serious glitches. Where's the value in voting about a spec where the   actual rules that govern implementations are not under the control of those   who vote?    Beside Steve's issue with implementing the new features of the spec what is   the plan for the general work on the TCK going forward? To be honest I   think it's close to ridiculous to ask members of the EG to hand in ideas   and suggestions for improvements to the TCK which will then be implemented   however by whoever somewhere.    Especially in the light of the ambiguities discovered recently I think one   of the top priorities going forward has to be that we (the EG) can make   sure we'll see the the new features and behavior tested adequately. This   not only will help us creating a good/better spec as ambiguities will be   found before it is to late but also a much better usability as the lack of   defined *and enforced* semantics prevents functionality from being rendered   useless as pretty much every persistence provider behaves slightly   different as they essentially have no real means to test their   implementations.    So I suggest that going forward...    1. the TCK has to developed alongside the spec by the EG. Newly defined API   and functionality has to be backed by test cases on submission of the   feature to the spec.  2. This requires EG members having access to the TCK during spec   development (requires appropriate licensing of the spec)  3. If I conclude wishful thinking here I wonder why even the community   shouldn't have access to the TCK to maybe help out discovering potential   issues.    @Werner: does any of the current JCP versions already define TCK rules like   this? The closest JSR I can remember is CDI that always developed an Apache   licensed TCK fully in the open IIRC. Or is there any general rules within   the JCP/Oracle that should prevent us from going down that route?    Cheers,  Ollie    --  Sent while on the run...    Am 24.01.2013 um 21:39 schrieb Linda DeMichiel &amp;lt;linda.demichiel@...&amp;gt;:    &amp;gt; Hi Steve,  &amp;gt;   &amp;gt; The team tells me that we are still a number of weeks away on the TCK   &amp;gt; being ready for this.  &amp;gt;   &amp;gt; thanks,  &amp;gt;   &amp;gt; -Linda  &amp;gt;   &amp;gt;   &amp;gt;&amp;gt; A few months ago there was a discussion about early access to the TCK as   &amp;gt;&amp;gt; being a generally good thing and there seemed  &amp;gt;&amp;gt; to be a general consensus for doing that in any later revisions of the   &amp;gt;&amp;gt; spec. The Cliff Notes (tm) version of the pros  &amp;gt;&amp;gt; were mainly:  &amp;gt;&amp;gt; 1) the ability to identify as early as possible problems in the TCK   &amp;gt;&amp;gt; itself  &amp;gt;&amp;gt; 2) to help providers get started on certifying  &amp;gt;&amp;gt; 3) allow early feedback from the group as to missing portability coverage  &amp;gt;&amp;gt;   &amp;gt;&amp;gt; I was just curious whether we could possibly start getting access to the   &amp;gt;&amp;gt; 2.1 TCK, especially now that the spec is pretty  &amp;gt;&amp;gt; solidified</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Thu Jan 31 15:10:43 CET 2013</date>
    <body>Sorry for being dense about this (but honestly I have asked others in the group and they are just as confused by this graph stuff)... Can you explain the "multiple key subgraphs and multiple subgraphs can exist for any attribute" part? An attribute would be something like the "customer" attribute on an Account entity.  So we'd have EntityGraph&amp;lt;Account&amp;gt;, with AttributeNode&amp;lt;Customer&amp;gt; as part of that EntityGraph&amp;lt;Account&amp;gt;'s getAttributeNodes() collection.  The mere presence of AttributeNode&amp;lt;Customer&amp;gt; would indicate that Account.customer is supposed to be fetched, yes? As I understood it from yesterday's replies from yourself and Linda the intent in the above case would be that Account.customer could be depending on whether the user intended to further qualify the fetch graph "below" Customer.  Your new suggestion seems to indicate a fundamentally different view of this.  Now, an attribute reference is aways going to be an AttributeNode.  Under that we'd just have further possible qualifications of that fetch subgraph.  Yes?  If so, OK I can buy that as an improvement (the old deal was VERY counter-intuitive IMHO). Going back to the quote "multiple key subgraphs and multiple subgraphs can exist for any attribute", especially confusing to me is the "multiple key subgraphs" portion.  How can an attribute (a representation of the Map attribute) have multiple subgraphs *at that level* representing the map key?  There is something fundamental I am missing here...</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Thu Jan 31 18:36:03 CET 2013</date>
    <body>Hi Steve, I don't know exactly what Gordon had in mind when he wrote this, but here is the problem that I see. Since an entity graph may involve many entities, there may be multiple entities that have the same attribute (i.e., attribute name/type combination).  If you just retrieve all the subgraphs in an entity graph, that doesn't give you enough information as to what subgraph corresponds to which entity's attribute. Traversing the graph downwards (and recursively) from the root, via the attribute nodes for any given entity allows the structure of the graph to be discerned. HTH, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Thu Jan 31 19:11:27 CET 2013</date>
    <body>I get that part.   I would assume you can only get the AttributeNodes relative to the Graph that contains it. So if I have:    ...    ...    ... If I wanted to get a graph of Account+Customer+(billing)Address you would do: EntityGraph&amp;lt;Account&amp;gt; accountGraph = em.createEntityGraph( Account.class Subgraph&amp;lt;Customer&amp;gt; customerGraph = accountGraph.addSubgraph( // though as I understand it calling addAttributeNodes( Customer_.billngAddress ) would be the same :/ So the question is all in the proposed API for understanding an EntityGraph.  In the current proposal (public review draft), as I understood it you'd end up with EntityGraph&amp;lt;Account&amp;gt; returning 2 AttributeNodes from getAttributeNodes(): 1) AttributeNode&amp;lt;String&amp;gt;(referenceNumber) 2) Subgraph&amp;lt;Customer&amp;gt;(customer) In turn, Subgraph&amp;lt;Customer&amp;gt;(customer) would return 2 AtributeNodes from its getAttributeNodes(): 1) AttributeNode&amp;lt;String&amp;gt;(name) 2) Subgraph&amp;lt;Address&amp;gt;(billingAddress) I understand that EntityGraph&amp;lt;Account&amp;gt;.getAttributeNodes() above returning *all* AttributeNodes for the entire graph would be badness. So now in the new proposal, if I understand correctly, we'd instead have EntityGraph&amp;lt;Account&amp;gt; returning 2 AttributeNodes from getAttributeNodes() like so: 1) AttributeNode&amp;lt;String&amp;gt;(referenceNumber) 1.a) getSubgraphs() -&amp;gt; none 1.b) getKeySubgraphs() -&amp;gt; none 2) AttributeNode&amp;lt;Customer&amp;gt;(customer) 2.a) getSubgraphs() -&amp;gt; [Subgraph&amp;lt;Customer&amp;gt;] 2.b) getKeySubgraphs() -&amp;gt; none and so on. Tbh, its six-in-one to me.  They each give you the same information. Yes, in one you need to do an explicit type check which is always nice to avoid. And still, I am not getting how you possibly end up with multiple "key subgraphs" for a given AttributeNode.  That part makes no sense to me. In my understanding the Subgraph there would refer to the Map key of the Map-valued collection represented by the containing AttributeNode. How can a Map have multiple keys?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Thu Jan 31 19:22:17 CET 2013</date>
    <body>You wouldn't.  I had assumed the confusion was over whether EntityGraph would  have methods getSubgraphs and getKeySubgraphs (which would result in the badness I  described in my last msg) In my understanding the Subgraph there would refer to the Map key of the  Map-valued collection</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Thu Jan 31 19:24:42 CET 2013</date>
    <body>Hello Steve,   Comments below: --Gordon Because for a polymorphic relationship there may be additional subgraphs defined for a particular attribute to include attributes of the target's subclass types.  This functionality is the reason "type" is on Subgraph. Yes Yes, in order to keep the interface model as simple as possible. Yes, the interfaces need to be expanded to capture the complicated nature of the AttributeNode. See first comment.  It is because entity graphs support subclass specific definitions for each type within a polymorphic attribute.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Thu Jan 31 19:26:35 CET 2013</date>
    <body>I understand that EntityGraph&amp;lt;Account&amp;gt;.getAttributeNodes() above returning * all * AttributeNodes for the entire graph would be badness.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Thu Jan 31 19:32:37 CET 2013</date>
    <body>It would  because entity graphs support subclass specific definitions for each type within a polymorphic attribute. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Thu Jan 31 19:49:12 CET 2013</date>
    <body>These look great.  Thanks Linda.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Thu Jan 31 20:13:12 CET 2013</date>
    <body>One quick question. For 'javax.persistence.schema-generation-source' values of metadata-then-scripts and scripts-then-metadata is the order the same for both create and drop? I can envision a situation, for example, with metadata-then-scripts where the metadata creates a table and then the script creates a procedure.  I am not sure that all databases will allow you to drop the table if other database objects refer to it.  Would it be better to say that the defined order (x-then-y) applies to create, but the reverse order (y-then-x) applies to drop?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Thu Jan 31 20:50:59 CET 2013</date>
    <body>Good point. Actually, I'm wondering whether this will be sufficient. Perhaps a safer approach is that we define a schema-drop-source property (that looks exactly like schema-generation-source, with the same defaults) to cover this. What do you all think? -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Thu Jan 31 20:52:12 CET 2013</date>
    <body>I also am not sure about this passage in discussing 'javax.persistence.ddl-create-script-target' and 'javax.persistence.ddl-drop-script-target': The persistence provider must produce both create and drop scripts if the corresponding DDL targets are specified. This is independent of whether a drop action is included in the value passed for the javax.persistence.schema-generation-action property. To me, this sounds like you are saying that the following config will generate both create and drop scripts: javax.persistence.schema-generation-action=create even though the config explicitly specified just create scripts.  Is that your intent? Same for: javax.persistence.schema-generation-action=drop that is supposed to generate both create and drop scripts? Perhaps the reasoning is that javax.persistence.schema-generation-action is really just geared towards javax.persistence.schema-generation-action=database.  But if that is the case, maybe we ought to rename the setting to be more explicit of that fact or at least document it in the discussion of 'javax.persistence.schema-generation-action'</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Thu Jan 31 21:13:20 CET 2013</date>
    <body>yes yes partially.  also it was intended to control whether any action would be taken. I'm thinking that all this might be clearer if we required  schema-generation-target to be specified. But if that is the case, maybe we ought to rename the setting to be</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Thu Jan 31 22:03:43 CET 2013</date>
    <body>Well the confusion IMO as a user is that I specifically asked for only creation scripts to be generated, but got both creation and drop scripts.  Thats not going to change by making schema-generation-target required.  Even this is still confusing: javax.persistence.schema-generation-action=create javax.persistence.schema-generation-target=scripts because i would not expect to get a drop script.  But if I read the other parts of the spec correctly, the following actually results in an exception: javax.persistence.schema-generation-action=create javax.persistence.schema-generation-target=scripts The exception is because 'javax.persistence.ddl-drop-script-target' is not specified, again even though i just said I only want create scripts generated.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Fri Feb 01 16:23:07 CET 2013</date>
    <body>I guess what I was getting at yesterday is that it *seems* as if the intent is that 'javax.persistence.ddl-create-script-target' and 'javax.persistence.ddl-drop-script-target': 1) need to be specified either both together or neither 2) Control: 2.a) whether scripts are generated 2.b) what types of scripts are generated. So if I have: javax.persistence.schema-generation-action=create javax.persistence.schema-generation-target=database-and-scripts the "action=create" bit really only describes the database target side of things.  On the script target side, both create and drop are created. To me at least that is not the most intuitive set up.  If I say "action=create", why are the drop scripts being generated?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Fri Feb 01 17:08:26 CET 2013</date>
    <body>Another option is another boolean property (javax.persistence.schema-generation-source-reverse-drop?) that says to reverse the order for drop.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Fri Feb 01 20:17:53 CET 2013</date>
    <body>Since there seems to be agreement on this, I have proceeded to update the  spec. I'll be posting the updated javadocs shortly. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Fri Feb 01 20:32:08 CET 2013</date>
    <body>Available now on the project Downloads area.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Non-entity class: legal as second argument to createNamedQuery()?</header>
    <date>Sat Feb 02 00:07:48 CET 2013</date>
    <body>Is it legal to use a non-entity class as the second argument to EntityManager#createNamedQuery(String, Class) ? For example, I'd like to do:</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Sat Feb 02 17:13:26 CET 2013</date>
    <body>WRT various source scripts (DDL create/drop sources, import script sources) what are we saying is the standard for delimiters?  Or is this up to each provider? In Hibernate we actually hide this behind an interface so that it is pluggable.  The interface, as you would imagine, is very simple.  Is that something we want to explore for the spec?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Arbitrary precsion value in JPA</header>
    <date>Mon Feb 04 19:16:51 CET 2013</date>
    <body>Hello, The spec should mention some more details about the value of a persistent property of arbitrary-precision numbers such as BigDecimal type. For example, if the application defines a BigDecimal field with scale = 2 and sets a value of 1234.56789, then a) will the value be stored in database as 1234.56 (i.e. losing precision)? b) If the field were declared without a scale parameter, then would the column value be the same as the in-memory value of 1234.56789? c) If the provider is responsible for rounding the in-memory value, then what is the rounding mode? I have few more questions, but as a starter if some clarifications are available for the above questions, it will be helpful. Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Mon Feb 04 22:49:36 CET 2013</date>
    <body>Steve, all, You have raised good questions.  To clarify all this up, I am proposing that  we refactor the metadata a bit. To summarize, I am proposing that we do the following: 1) modify javax.persistence.schema-generation-action to be javax.persistence.schema-generation.database-action.  The property values and semantics are the same, with the exception that if this property is not specified, no schema generation actions must be taken *on the database*. (2) remove javax.persistence.schema-generation-target and replace with javax.persistence.schema-generation.script-action.  The values of this property and semantics would be the same as javax.persistence.schema-generation.database-action, except that this pertains to scripts only.  If this property is not specified, no scripts will be generated. (3) To accommodate the feedback about drop execution order, I think we should add another property, javax.persistence.schema-generation.drop-source that is just like javax.persistence.schema-generation-source, but allows control over dropping the database artifacts and the order in which this is to occur. (4) make some minor naming changes to make things more consistent/clearer. More formally, the resulting spec section would look like this: Section 8.2.1.9  would be updated similarly, as appropriate. [Section 9.4] *  javax.persistence.schema-generation.database-action The javax.persistence.schema-generation.database-action property specifies the action to be taken by the persistence provider with regard to the database artifacts.  The values for this property are "none", "create", "drop-and-create", "drop". If the javax.persistence.schema-generation.database-action property is not specified, no schema generation actions must be taken on the database. *  javax.persistence.schema-generation.script-action The javax.persistence.schema-generation.script-action property specifies which scripts are to be generated by the persistence provider. The values for this property are "none", "create", "drop-and-create", "drop". Scripts will only be generated if script targets are specified. If this property is not specified, no scripts will be generated. *  javax.persistence.schema-generation.database-source The javax.persistence.schema-generation.database-source property specifies whether the creation of database artifacts is to occur on the basis of the object/relational mapping metadata, DDL script, or a combination of the two. The values for this property are "metadata", "script", "metadata-then-script", "script-then-metadata".  If this property is not specified, and a script is specified by the javax.persistence.schema-generation.create-script-source property, the script (only) will be used for schema generation; otherwise if this property is not specified, schema generation will occur on the basis of the object/relational mapping metadata (only). The "metadata-then-script" and "script-then-metadata" values specify that a combination of metadata and script is to be used and the order in which this use is to occur. If either of these values is specified and the resulting database actions are not disjoint, the results are undefined and schema generation may fail. *  javax.persistence.schema-generation.drop-source The javax.persistence.schema-generation.drop-source property specifies whether the dropping of database artifacts is to occur on the basis of the object/relational mapping metadata, DDL script, or a combination of the two. The values for this property are "metadata", "script", "metadata-then-script", "script-then-metadata".  If this property is not specified, and a script is specified by the javax.persistence.schema-generation.drop-script-source property, otherwise if this property is not specified, the dropping of database artifacts will occur on the basis of the object/relational mapping metadata (only). The "metadata-then-script" and "script-then-metadata" values specify that a combination of metadata and script is to be used and the order in which this use is to occur. If either of these values is specified and the resulting database actions are not disjoint, the results are undefined and the dropping of database artifacts may fail. *  javax.persistence.schema-generation.create-database-schemas In Java EE environments, it is anticipated that the Java EE platform provider may wish to control the creation of database schemas rather than delegate this task to the persistence provider.  The javax.persistence.schema-generation.create-database-schemas property specifies whether the persistence provider is to create the database schema(s) in addition to creating database objects such as tables, sequences, constraints, etc. The value of this boolean property should be set to true if the persistence provider is to create schemas in the database or to generate DDL that contains "CREATE SCHEMA" commands. If this property is not supplied, the provider should not attempt to create database schemas. This property may also be specified in Java SE environments. *  javax.persistence.schema-generation.create-script-target,    javax.persistence.schema-generation.drop-script-target If scripts are to be generated, the target locations for the writing of these scripts must be specified. The javax.persistence.schema-generation.create-script-target property specifies a java.IO.Writer configured for use by the persistence provider for output of the DDL script or a string specifying the file URL for the DDL script. This property should only be specified if scripts are to be generated. The javax.persistence.schema-generation.drop-script-target property specifies a java.IO.Writer configured for use by the persistence provider for output of the DDL script or a string specifying the file URL for the DDL script. This property should only be specified if scripts are to be generated. *  javax.persistence.database-product-name,    javax.persistence.database-major-version,    javax.persistence.database-minor-version If scripts are to be generated by the persistence provider and a connection to the target database is not supplied, the javax.persistence.database-product-name property must be specified. The value of this property should be the value returned for the target database by the JDBC DatabaseMetaData method getDatabaseProductName. If sufficient database version information is not included in the result of this method, the javax.persistence. database-major-version and javax.persistence.database-minor-version properties should be specified as needed. These should contain the values returned by the JDBC getDatabaseMajorVersion and getDatabaseMinorVersion methods respectively. *  javax.persistence.schema-generation.create-script-source, *  javax.persistence.schema-generation.drop-script-source The javax.persistence.schema-generation.create-script-source and javax.persistence.schema-generation.drop-script-source properties are used for script execution. In Java EE container environments, it is generally expected that the container will be responsible for executing DDL scripts, although the container is permitted to delegate this task to the persistence provider. If DDL scripts are to be used in Java SE environments or if the Java EE container delegates the execution of scripts to the persistence provider, these properties must be specified.  The  javax.persistence.schema-generation.create-script-source property specifies a java.IO.Reader configured for reading of the DDL script  or a string designating a file URL for the DDL script. The javax.persistence.schema-generation.drop-script-source property specifies a java.IO.Reader configured for reading of the DDL script or a string designating a file URL for the DDL script. *  javax.persistence.schema-generation.connection The javax.persistence.schema-generation.connection property specifies the JDBC connection to be used for schema generation. This is intended for use in Java EE environments, where the platform provider may want to control the database privileges that are available to the persistence provider. This connection is provided by the container, and should be closed by the container when the schema generation request or entity manager factory creation completes. The connection provided must have credentials sufficient for the persistence provider to carry out the requested actions. If this property is not specified, the persistence provider should use the DataSource that has otherwise been provided. [subsection 9.4.1] Data Loading Data loading, by means of the use of SQL scripts, may occur as part of the schema generation process after the creation of the database artifacts or independently of schema generation. The specification of the javax.persistence.sql-load-script-source controls whether data loading will occur. *  javax.persistence.sql-load-script-source In Java EE container environments, it is generally expected that the container will be responsible for executing data load scripts, although the container is permitted to delegate this task to the persistence provider. If a load script is to be used in Java SE environments or if the Java EE container delegates the execution of the load script to the persistence provider, this property must be specified.  The javax.persistence.sql-load-script-source property specifies a java.IO.Reader configured for reading of the SQL load script for database initialization or a string designating a file URL for the script.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 18:13:06 CET 2013</date>
    <body>1) I think the split into database-action and script-action plays much nicer.  Nice thought. 2) You wrote '* javax.persistence.schema-generation.database-source' in the updated spec section, as a replacement for javax.persistence.schema-generation-source.  IMO this should be named 'javax.persistence.schema-generation.create-source' instead to pair with the new 'javax.persistence.schema-generation.drop-source' 3) I wonder if a better consistent naming scheme would be to use javax.persistence.schema-generation.database.&amp;lt;setting&amp;gt; and javax.persistence.schema-generation.scripts.&amp;lt;setting&amp;gt;?  To me that plays much nicer: javax.persistence.database-product-name javax.persistence.database-major-version javax.persistence.database-minor-version javax.persistence.schema-generation.create-source javax.persistence.schema-generation.drop-source javax.persistence.schema-generation.create-script-source javax.persistence.schema-generation.drop-script-source javax.persistence.schema-generation.connection javax.persistence.schema-generation.create-database-schemas* javax.persistence.schema-generation.database.action javax.persistence.schema-generation.scripts.action javax.persistence.schema-generation.scripts.create-target javax.persistence.schema-generation.scripts.drop-target * - I was not clear whether create-database-schemas is meant to describe just database target or scripts target or both.. 4) WRT "data loading", the question of delimiter is still open.  I think this ought to be spelled out in the spec, otherwise provider portability will be a major problem.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 20:20:21 CET 2013</date>
    <body>thanks, Steve I am fine with this.  See the below for minor tweaks/questions. Otherwise, I plan to commit these changes to the spec. This would be javax.persistence.schema-generation.scripts.create-source and  drop-source, right? Should the preceding be  javax.persistence.schema-generation.database.create-source and drop-source ? Or is that obvious? Should cover both</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 20:22:00 CET 2013</date>
    <body>[snip] [I want to separate this item out, since the rest of the changes seem stable.] I'm not sure I understand what you mean by this?  Are you referring to a  property to cover the SQL terminator character in scripts?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 20:55:33 CET 2013</date>
    <body>In the initial suggestion, create-source and drop-source are meant to identify the metadata/scripts/metadata-then-scripts/scripts-then-metadata selections.  create-script-source and drop-script-source are meant to identify the location (Reader/file-url) of the input scripts There are multiple axises, if you will, at play here.  My suggestion was to focus the setting naming based on what used to be called targets: scripts, database.  So really for target-specific settings we have: For non-target-specific settings, we have just: As I read it initially, I had thought that create-script-source and drop-script-source were both meant for database and scripts targets.  Is that accurate?  If not, then by all means javax.persistence.schema-generation.scripts.create-source and drop-source make more sense.  But in that case I think we need to clarify the wording about when settings are supposed to bear on just database target or just scripts target.  The renaming I suggested is a step in that direction. To be honest, I did not like the multiple meanings for the term "source".  It gets confusing trying to explain the settings.  But, as I had no better sugestion, I let it go.  "source" can mean either: 1) what "sources" should we consider to obtain create/drop DDL commands 2) for the scripts source (see 1), what are the source/location/reader for that script.  My gut tells me this is the usage that needs a new term, I just dont know what that term is.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 21:07:05 CET 2013</date>
    <body>Well I just mean that commands in a script file might be delimited in a number of ways (eol-delimited, semicolon-delimited, keyword-delimited, etc). It is usually not feasible to handle the entire contents of such a "data loading script" as one big string, especially for sending to the database directly via JDBC.  Typically you want to be able to handle command-by-command.  Hence the command delimiter. As an exmaple, if Provider-A assumes end-of-line as the command delimiters and Provider-B assumes semicolon users will not be able to feed the same script to both providers. I just think it makes sense to be up front about expectations in this regard in the spec.  Are providers expected to parse the import script in the same fashion?  Are we saying this is just totally provider defined (and therefore completely non-portable)? This is where I was saying that Hibernate actually uses an interface to basically it takes a Reader and breaks it down into String[] of the commands.  That is obviously extremely flexible.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 21:10:46 CET 2013</date>
    <body>I had assumed that the appropriate database SQL delimiters would be used.  By  Provider-A and B are you referring to persistence providers or database providers?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 21:30:03 CET 2013</date>
    <body>I think it would be confusing to call these javax.persistence.schema-generation.scripts.create-source when they reference the actual scripts and are not actions.  We could call them javax.persistence.schema-generation.database.create-script-source if you need to qualify them but I believe they are fine as they are.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 21:37:39 CET 2013</date>
    <body>Yes, persistence providers.  I also don't personally like requiring DB specific delimiters.  Another tenant of JPA is database portability.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 21:44:11 CET 2013</date>
    <body>However, if this isn't database delimiters, then why aren't newline  characters enough?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 21:51:46 CET 2013</date>
    <body>Because experience tells us (Hibernate team) that users do not like writing scripts that are line delimited.  Also, our interface allows handling of comments, etc that might need to be ignored. I guess as long as the language we use does not exclude persistence providers from using more user-friendly options in parsing the import script, its fine.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: schema generation open issues</header>
    <date>Tue Feb 05 22:06:15 CET 2013</date>
    <body>OK, thanks.  Will keep as in Steve's initial proposal.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] updated spec draft and xsds</header>
    <date>Wed Feb 06 01:48:12 CET 2013</date>
    <body>I've posted a new spec draft with the updated schema generation metadata to the project Downloads area.   This draft also contains the updated persistence and orm schemas.  I have also uploaded these xsd files to the Downloads area. Notice that the namespace in the schemas has changed, as we will no longer be using java.sun.com for new work.   There are no other modifications to the schema for the persistence.xml, but since we have changed the orm schema, I figured it would be less confusing if both xsd files were updated. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Extensibility of schema generation</header>
    <date>Thu Feb 07 17:26:08 CET 2013</date>
    <body>Hello all, first let me emphasize that I'm really happy to see that JPA 2.1 will support database schema generation. In my opinion this is a very important and useful feature for everyone working with JPA. According to the current draft of the spec, JPA will support automatic creation of the database schema from mapping metadata and custom DDL scripts. The problem I'm seeing here is that these methods typically only work well if there is no initial database at all. But this is rarely the case. A very frequent case is that the database schema is not created but migrated . New versions of existing applications often require the database schema to be modified. In these cases the schema creation methods described in the current draft are not really useful. Of cause the JPA provider is able to automatically create missing tables and columns, but typically developers need more control over the migration process. Especially when doing real schema refactorings like column data type changes, modified constraints, etc. There are very good tools out there that focus on database schema migration like Liquibase [1] or Flyway [2]. But unfortunately there is no really easy portable way to integrate these tools into a JEE application.  So I was thinking that it may be a good addition to the spec to provide an extension point for the schema creation process. This would be a great feature because it would allow users to customize the schema creation in any way they want. They could integrate 3rd party tools specialized in database schema migration or they could build their own way of working with schema versions and their migration. I could think of an API like this:   void generateSchema(java.sql.Connection connection, java.util.Properties map) One option to specify an implementation for custom schema generation would be to set a property for the persistent unit like this: javax.persistence.schema-generation.provider-class=com.example.myapp.MySchemaGenerationProvider Another option would be to use the standard JDK ServiceLoader mechanism. What do you think? Does it make sense to provided portable extensibility for the schema generation process? Is it worth adding it to the spec? I would love to get feedback on this. Best regards Christian Kaltepoth [1]  http://www.liquibase.org/ [2]  http://flywaydb.org/ -- Christian Kaltepoth Blog: http://blog.kaltepoth.de/ Twitter: http://twitter.com/chkal GitHub: https://github.com/chkal</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft and xsds</header>
    <date>Fri Feb 08 18:07:44 CET 2013</date>
    <body>There is a discrepency between orm.xsd and the spec. orm.xsd defines that join-table lists that foreign-key should be a sub-element, however @JoinTable does not define a foreignKeys attribute.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: updated spec draft and xsds</header>
    <date>Fri Feb 08 18:33:20 CET 2013</date>
    <body>Sigh.  This is a bug (and with the other @XXXTable mappings that allow  joinColumns to be specified).  I intended compound FKs to be handled in @JoinColumns, but these  mapping types don't have a @JoinColumns subelement by rather a JoinColumn[] element. Will need to reevaluate. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Partial rollback of entities/transactions</header>
    <date>Sat Feb 09 08:29:02 CET 2013</date>
    <body>Hey Sebastian, in my opinion a JPA entity is a one-to-one representation of data in the database. Therefore you should only modify properties of the entity, when you want to change the values in the database. Therefore it is IMHO not a very good practice to modify properties of an attached entity although the user is still able to press a "cancel" button. So in my opinion you should really go with "workaround #1" and simply create copy of object to work on. That's a much cleaner solution and you have more control over how changes are handled when the user either clicks "OK" or "Cancel". Just my 2 cents on this. :) Christian 2013/1/30 sheba.public+javanet@... Hello everyone, I stumbled upon a problem that I think deserves a standard solution in JPA. Imagine an application that allows a user to update an entity (or a set of entities) in several successive steps (e.g. in separate dialogs) before committing to the database. Within each step the entity can be updated (one or several times) and the user has to see its current state. The user can choose to cancel a step (dialog) at any time and proceed with another, or restart the same step again. As far as I know (maybe I'm wrong) there no way to cancel only selected changes made to an uncommitted entity. So far I heard/read about two workarounds: 1. Before each step (dialog) you create a copy of the entity and work with it. If the step is finished gracefully the state of the copy has to be written to the entity. If the step is canceled the entity remains unchanged. 2. One needs to track all the changes within a step and manually undo the changes on the entity. In my opinion this use case should be quite common and deserves an easy high-level API. There are two similar mechanism I know about: A) Savepoints: JDBC offers the concept of savepoints within an Transaction to do a partial rollbacks. B) Nested transactions: Allows to create sub-transactions which can be rolled back separately. A similar construct in JPA would help a lot to avoid cumbersome, handmade, solutions. - Sebastian</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] ForeignKey annotation [Re: Re: updated spec draft and xsds]</header>
    <date>Mon Feb 11 20:48:31 CET 2013</date>
    <body>Here's my assessment of the issue. Ideally, I think foreign-key metadata should be kept in the JoinColumn(s) annotations.   However, for compound primary keys we have JoinColumn[] elements (or related elements) in the JoinTable, CollectionTable, and SecondaryTable annotations, which do not support this cleanly. I see several options (none unfortunately ideal): 1)  Move @ForeignKey to top level.  I think this gets messy because     it allows for a lot of overlap in where it might be specified. 2)  Add @ForeignKey elements to JoinTable, CollectionTable, and SecondaryTable     and clarify that they are to be used for compound FKs. 3)  Clarify that for compound foreign keys, in JoinTable, CollectionTable,     and SecondaryTable that at most one of the joinColumn elements should     be used to hold the FK metadata for the compound FK. Am I missing anything? Do any of you see a better way? So far, I am leaning toward #3 as the least objectionable. Opinions?? thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Thu Feb 14 23:17:46 CET 2013</date>
    <body>Quick clarification question... In regards to entity graphs the spec says that an entity's identifier and version attributes are automatically considered to be part of the fetch graph.  Does that mean, specifically, that: em.createEntityGraph( SomeEntity.class ).getAttributeNodes() should contain AttributeNodes for SomeEntity's identifier and version (if any) attributes?  Or is the "automatically" more considered implicit?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Fri Feb 15 15:32:35 CET 2013</date>
    <body>I guess the larger question here is about the intent of getAttributeNodes().  Is the expectation for it to return just the attribute nodes explicitly added by the user, or nodes for all the attributes that the persistence provider will actually be fetching? That distinction goes beyond just id and version attributes.  As the spec says, a "persistence provider is permitted to fetch additional entity state beyond that specified by a fetch graph or load graph" as long as the provider at the least fetches any explicitly added attribute nodes.  So given a call like: em.createEntityGraph( SomeEntity.class ) a provider is free to fetch all attributes of SomeEntity. So which truth should getAttributeNodes() return?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Entity Graphs</header>
    <date>Fri Feb 15 15:55:31 CET 2013</date>
    <body>getAttributeNodes() should only return the nodes explicitly added by the user. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] PagSeguro; Pagamento em analise..</header>
    <date>Fri Feb 15 16:03:25 CET 2013</date>
    <body>Title: Nova pagina 1 Ola, Seu pagamento de R$ 143,70 para   Groupon   esta em analise. Status:   Em analise Código:   340C1-AG79-4BC1-9AA1- 093DSAG Site:   http://www.groupon.com.br E-mail:   groupon@... Telefone:   11 3233-4012       Comprovante da transacao.   ITENS DO PEDIDO QUANTIDADE VALOR (R$) TOTAL (R$) Desconto imperdivel em delicioso Rodizio na Vento Aragano! 3 47,90 143,70 Total geral: R$ 143,70 Voce recebera um e-mail assim que o pagamento for confirmado. Importante:   Este pagamento sera registrado na fatura do seu cartao de credito como PagSeguro Panfleteria .   Facilite sua vida: o PagSeguro e a melhor maneira de fazer pagamentos e receber valores na internet com a seguranca dos seus dados garantida.   Aproveite para criar sua conta usando o seu email. . Atenciosamente, Equipe PagSeguro. PagSeguro.   Sua compra protegida. DUVIDAS? Acesse   http://www.pagseguro.com.br/ atendimento Este e um e-mail automatico disparado pelo sistema. Favor nao responde-lo, pois esta conta nao e monitorada.   Código: GCQ09438</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] ForeignKey annotation [Re: Re: updated spec draft and xsds]</header>
    <date>Fri Feb 15 19:21:06 CET 2013</date>
    <body>After weighing the options here, I've concluded that to accommodate the ability to specify composite foreign keys, we should add a ForeignKey element to the CollectionTable, SecondaryTable, and AssociationOverride annotations, and a pair of ForeignKey elements to the JoinTable annotation. If both such a ForeignKey element and the foreignKey element within the JoinColumn element are specified in one of these annotations, the behavior would be undefined. I also think that we need to make more intuitive what specifying @ForeignKey() as a value of such an element means--i.e., specifying @ForeignKey() when all of its member values are defaulted-- since the user's expectation will probably be that this means that foreign key constraints will be generated when schema generation is in effect.  To clarify this, we should replace the current disableForeignKey with a 3-valued enum (PROVIDER_DEFAULT, CONSTRAINT, NO_CONSTRAINT).  By default, if @ForeignKey is not specified, the default provider behavior will apply and @ForeignKey() will be equivalent to @ForeignKey(CONSTRAINT). I've uploaded javadocs that capture all of this. ( http://java.net/projects/jpa-spec/downloads/download/JPAjavadocs02142013.zip ) See especially the following: ForeignKey, ConstraintMode, JoinColumn, JoinTable, CollectionTable, SecondaryTable. Please let me know if you find anything amiss. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] new spec draft (PFD candidate) and orm.xsd</header>
    <date>Mon Feb 18 22:59:57 CET 2013</date>
    <body>I've just uploaded a new spec draft and orm.xsd file to our project downloads area:  http://java.net/projects/jpa-spec/downloads These reflect the recent updates to the foreign key metadata. I would like to submit this draft (or something very close to it) to the JCP at the end of the week as the JPA 2.1 Proposed Final Draft. Please let me know if you find any issues. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new spec draft (PFD candidate) and orm.xsd</header>
    <date>Wed Feb 20 14:25:53 CET 2013</date>
    <body>Hi Linda, Linda DeMichiel, am 18 Feb 2013 hast Du um 13:59 zum Thema "[jsr338- experts] new spec draft (PFD candidate) a"  geschrieben :  I've just uploaded a new spec draft and orm.xsd file to our project  downloads area:  http://java.net/projects/jpa-spec/downloads    These reflect the recent updates to the foreign key metadata.    I would like to submit this draft (or something very close to it)  to the JCP at the end of the week as the JPA 2.1 Proposed Final Draft.    Please let me know if you find any issues. thanks a lot for the tremendous work that you have put into this draft.  Please allow me for some minor comments: 3.7 3rd paragraph p. 108 construst -&amp;gt; construct 3.7.1 javadoc of addKeySubgraph 1st paragraph p. 110 "Use of ... to the graph". Given the general remark in the 2nd  paragraph of the javadoc of the interface on p. 109, and in consistence  with the other javadocs of addKeySubgraph this sentence should be  removed, shouldn't it? 3.7.3 javadoc of getClassType p.116 Would expect something like "Return the java class of the managed type  for which ...". 3.7.4.2 pp. 119 - 121 Would it make sense to consistently replace "fetch" by "load"  throughout this section? 3.10.2 javadoc of setLockMode throws clause p. 138 Criteria API query -&amp;gt; CriteriaQuery query 7.4 javadoc of addNamedEntityGraph p. 338 A remark on the (non-)effect on already created mutable copies (similar  to the addNamedQuery javadoc) might be helpful. 9.4 p. 384 2nd bullet last sentence I believe to remember a discussion on whether both script targets need  to be set even if the action (either create or drop) triggers only one  script to be created, but the current wording of the spec is not clear  enough to me to understand what the outcome of the discussion has been. 11.2.1.1 2nd paragraph 2nd sentence p. 491 in -&amp;gt; is   Best regards Rainer --- Rainer Schweigkoffer               SAP AG Walldorf Regulatory Compliance              TIP Core JI Core Java Infrastructure           Dietmar-Hopp-Allee 16 Technology &amp;amp; Innovation Platform   D-69190 Walldorf Building 3, F.3.14                 phone: +49 6227 7 45305 rainer.schweigkoffer@...       fax:   +49 6227 7 821177 Sitz der Gesellschaft/Registered Office: Walldorf, Germany Vorstand/SAP Executive Board: Werner Brandt, Lars  Dalgaard, Luisa Deplazes Delgado, Bill McDermott (Co-CEO),  Gerhard Oswald, Vishal Sikka, Jim Hagemann Snabe (Co-CEO) Vorsitzender des Aufsichtsrats/Chairperson of the SAP  Supervisory  Board: Hasso Plattner Registergericht/Commercial Register Mannheim No HRB 350269 Diese E-Mail kann Betriebs- oder Geschaeftsgeheimnisse  oder sonstige vertrauliche Informationen enthalten.  Sollten Sie diese E-Mail irrtuemlich erhalten haben, ist  Ihnen eine Verwertung des Inhalts, eine Vervielfaeltigung  oder Weitergabe der E-Mail ausdruecklich untersagt. Bitte  benachrichtigen Sie uns und vernichten Sie die empfangene  E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged,  undisclosed, or otherwise confidential information. If you  have received this e-mail in error, you are hereby  notified that any review, copying, or distribution of it  is strictly prohibited. Please inform us immediately and  destroy the original transmittal. Thank you for your  cooperation.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new spec draft (PFD candidate) and orm.xsd</header>
    <date>Wed Feb 20 22:35:07 CET 2013</date>
    <body>Hi Rainer, Thanks for the helpful comments.  More inline below.... fixed yes -- gone fixed fetch is used generically here, so I changed consistently to that. fixed I was going to add something here, but decided not to, as I think the existing docs are sufficiently clear about the (im)mutability of what is stored/returned Changed to: "A script will only be generated if the script target is  specified." Which begs the question of what happens if drop-and-create is specified, but only one script supplied.  I've left that as undefined.  It might result in a deployment error, for example. fixed Thanks again best regards -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] JPA TCK</header>
    <date>Sat Feb 23 01:18:31 CET 2013</date>
    <body>We expect to have a preliminary version of the JPA 2.1 TCK available for expert group members by the end of next week.  Since the TCK is too large to be emailed, it will be made available on our JLE drop site. If you are not a licensee, a temporary account will need to be set up for you.  In this case, if you need access to the TCK, you should send me a note requesting such access and providing the following contact information, which I will then pass on to the people managing the site to have you set up:     Expert name     Company Name (if not an individual)     Address     Phone Number     Email Address -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: JPA TCK</header>
    <date>Sat Feb 23 12:00:33 CET 2013</date>
    <body>That's great news, Linda. Thank you very much. I'll send you the details  off-list. Cheers, Ollie Am 23.02.2013 um 01:18 schrieb Linda DeMichiel &amp;lt;linda.demichiel@...&amp;gt;:  We expect to have a preliminary version of the JPA 2.1 TCK available  for expert group members by the end of next week.  Since the TCK is  too large to be emailed, it will be made available on our JLE drop  site.    If you are not a licensee, a temporary account will need to be set up  for you.  In this case, if you need access to the TCK, you should send  me a note requesting such access and providing the following contact  information, which I will then pass on to the people managing the site  to have you set up:     Expert name     Company Name (if not an individual)     Address     Phone Number     Email Address    -Linda --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-351-30929001  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] JPA 2.1 Proposed Final Draft</header>
    <date>Mon Feb 25 21:24:49 CET 2013</date>
    <body>I've just submitted the JPA 2.1 PFD to the JCP.  This very closely matches the draft I uploaded a week ago, and incorporates the editorial feedback I received since then. The only changes to the javadocs were to the comments for the following  methods: TypedQuery.setLockMode EntityGraph.addKeySubgraph Subgraph.getClassType The updated files are available on  http://java.net/projects/jpa-spec/downloads -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Wed Mar 06 23:16:18 CET 2013</date>
    <body>[1] requires that at Transaction completion, the container closes (or returns to cache), the transaction-scoped persistence context.  What is supposed to happen when the JTA transaction completes in a different thread than the application thread?  For example, if a background thread calls the Synchronization.afterCompletion() because the tx timeout period has been exceeded (as some Transaction Managers may do), its not exactly thread-safe to call EntityManager.close() (see [2]). Specifically, the application could be in the middle of a persist or some other EntityManager method, when EntityManager.close() is called. Related to the above, if a JTA transaction rollback occurs in a background thread [3], how are the managed entities expected to be detached without violating the EntityManager thread-safety [2]? There may be vendor specific solutions but shouldn't we (JPA spec eg) account for the interaction of thread-unsafe persistence contexts and the JTA Synchronization.afterCompletion that may be invoked in non-application (background) threads? Scott [1] 7.9.1 Container Responsibilities - After the JTA transaction has completed (either by transaction commit or rollback), the container closes the entity manager calling EntityManager.close. [2] 7.2 Obtaining an EntityManager - An entity manager must not be shared among multiple concurrently executing threads, as the entity manager and persistence context are not required to be threadsafe. Entity managers must only be accessed in a single-threaded manner. [3] 7.9.2 Provider Responsibilities - When the JTA transaction rolls back, the provider must detach all managed entities if the persistence context is of type SynchronizationType.SYNCHRONIZED or has otherwise been joined to the transaction.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 07 23:41:57 CET 2013</date>
    <body>The team here tells me that this should not be happening, and that the  transaction managers they are familiar with will just mark the transaction for rollback rather than rolling  it back at the point of timeout. Nevertheless, if the container were working with a TM where a timeout did  result in immediate rollback and invocation of afterCompletion, the container should note this,  and at the point at which the transaction would normally be completed then do the actual close as  it normally would. What do your transaction manager and container do?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Fri Mar 08 01:48:43 CET 2013</date>
    <body>Currently, we roll the transaction back from the background timer thread.  The JTA spec [4] does allow different threads to start/end the transaction. Should we include a form of the above text in the JPA 2.1 spec (section 7.9.1 [1])? How would we word what the provider side has to do when detaching entities after rollback [3]?  I'm not sure that the persistence provider will have the same chance to make a note for the container side to take action on (if there is an EE container involved).  There is also the expectation that any JPA provider will work, with any EE container to consider. [4] JTA 1.1 spec 3.4.3 Thread of Control: " The X/Open XA interface specifies that the transaction association related xa calls must be invoked from the same thread context. This thread-of-control requirement is not applicable to the object-oriented component-based application run-time environment, in which application threads are dispatched dynamically at method invocation time.  Different Java threads may be using the same connection resource to access the resource manager if the connection spans multiple method invocation. Depending on the implementation of the application server, different Java threads may be involved with the same XAResource object. The resource context and the transaction context may be operated independent of thread context. This means, for example, that it’s possible for different threads to be invoking the XAResource.start and XAResource.end methods. If the application server allows multiple threads to use a single XAResource object and the associated connection to the resource manager, it is the responsibility of the application server to ensure that there is only one transaction context associated with the resource at any point of time. Thus the XAResource interface specified in this document requires that the resource managers be able to support the two-phase commit protocol from any thread context. " Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Sat Mar 09 02:31:13 CET 2013</date>
    <body>Yes, I realize this is permitted. Unfortunately, I don't think this may always work, because the container may be relying on synchronization notifications at the normally expected tx end to know when it should be calling close (i.e., it may not know when the tx was started).  If EJB CMT were used, the container would know when a tx was started and could use a business method boundary as the interpositioning point.  If a container wrapped UserTransaction, I suppose it could use that point as well, but it is not obvious to me how this would be handled otherwise. How does your implementation handle this? I'd also like to hear from the other implementations here as to what they do and how their transaction manager implementations handle timeout.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Regularize sua divida.</header>
    <date>Sat Mar 09 15:28:40 CET 2013</date>
    <body>___________________________________________________________________________________________     Prezado(a) Cliente:   Temos uma otima oportunidade para Voce!  </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Mon Mar 11 17:32:02 CET 2013</date>
    <body>Yes, it would be easier if containers could queue (or hand-off) the action (closing EntityManager or returning it to a EM pool) to happen at the tx boundary as you suggest (if known). One concern that I have is if we were to hand-off the closing of the EntityManager to the top level component, that would probably be too late (e.g. we could exhaust memory/resources before that is reached). I'm not saying that we would, just pointing out that the action needs to be taken as close as possible to the transaction ending. How deeply do we want to go up the EE stack to deal with this?  Is there a non-JPA mechanism that we need to help with the container and provider side? At a minimum, I think that we should add a requirement for all providers to deal with this situation.  And, text about how containers should also deal with this if needed. On the container side we close the EntityManager in the background thread and the application thread cleans up when the container level deals with the transaction getting cancelled.  Depending on where this discussion goes, we will follow the recommendations made here.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Sua ocorr�ncia de FURTO(OUTROS) foi analisada..</header>
    <date>Tue Mar 12 15:01:51 CET 2013</date>
    <body>Title: Nova pagina 1 Sua ocorrência de FURTO(OUTROS) foi analisada. Caso não consiga visualizar o Boletim de Ocorrência (B.O.) em anexo, acesse o seguinte endereço: http://www.delegaciaeletronica.sp.gov.br/bo/Visualizacao.do?numProtocolo=09f8dg70ffs8d7f6s8 Protocolo:  3840293480242 Senha:  RT1 94803945803912R Mantenha em mãos o seu CPF, o número do protocolo e a senha, pois será preciso informá-los para visualização do B.O. Ao imprimir o B.O. assine-o no espaço "Responsável pela Informação". Para maiores informações, você deve entrar em contato com a Delegacia Eletrônica Grato por usar nossos serviços, Delegacia Eletrônica Polícia Civil do Estado de São Paulo. Secretaria da Segurança Pública e Defesa Social</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Current standalone TCK drop</header>
    <date>Wed Mar 13 16:16:39 CET 2013</date>
    <body>As I understand it the current TCK drop is not supposed to be final, but I have found serious problems with it.  To the point where it is essentially unusable. First, the signature tests are wrong.  Running against the JPA API jar I created for Hibernate testing using the latest Proposed Final Draft I get errors that "javax.persistence.criteria.CommonAbstractQuery" is missing from my jar.  But javax.persistence.criteria.CommonAbstractQuery was removed from the spec way back in Draft 6. Second, I am completely unable to run any of the actual persistence tests.  Every single one of them fails in perfoming set up.  The error is always the same: 03-13-2013 01:22:19:  ERROR: Map returned from emf.getProperties() was null,this should never occur However, it is impossible for the Hibernate EMF to return a null Map there.  So, since I can't actually debug the tests (or please I'd love to have details on how to actually accomplish that) I fell back to caveman debugging and put throwing exceptions as the first thing I do inside (1) the Hibernate PersistenceProvider#createEntityManagerFactory call and the constructor to the Hibernate EMF impl.  Neither gets triggered.  So that tells me that the TCK is not even attempting to call out to the configured "alternate provider".</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 16:43:00 CET 2013</date>
    <body>This is a bug in our APIs.  It looks like I neglected to remove it after our  having gone back and forth on the factorization so many times.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 16:55:09 CET 2013</date>
    <body>I haven't looked as deeply into the code as Steve yet, but I'd like to get a  general idea of how the feedback process is expected to work. Assuming I'd  like to add additional test cases to the TCK or have comments on existing  tests, what's the preferred way to communicate those? For the latter emailing  the EG mailing list seems to be fine. The former - through Git patches maybe? Cheers, Ollie Am 13.03.2013 um 16:16 schrieb Steve Ebersole &amp;lt;steve.ebersole@...&amp;gt;:  As I understand it the current TCK drop is not supposed to be final, but I   have found serious problems with it.  To the point where it is essentially   unusable.    First, the signature tests are wrong.  Running against the JPA API jar I   created for Hibernate testing using the latest Proposed Final Draft I get   errors that "javax.persistence.criteria.CommonAbstractQuery" is missing   from my jar.  But javax.persistence.criteria.CommonAbstractQuery was   removed from the spec way back in Draft 6.    Second, I am completely unable to run any of the actual persistence tests.    Every single one of them fails in perfoming set up.  The error is always   the same:    03-13-2013 01:22:19:  ERROR: Map returned from emf.getProperties() was   null,this should never occur    However, it is impossible for the Hibernate EMF to return a null Map there.    So, since I can't actually debug the tests (or please I'd love to have   details on how to actually accomplish that) I fell back to caveman   debugging and put throwing exceptions as the first thing I do inside (1)   the Hibernate PersistenceProvider#createEntityManagerFactory call and the   constructor to the Hibernate EMF impl.  Neither gets triggered.  So that   tells me that the TCK is not even attempting to call out to the configured   "alternate provider" --  /**  * @author Oliver Gierke - Senior Member Technical Staff  *  * @param email ogierke@...  * @param phone +49-151-50465477  * @param fax   +49-351-418898439  * @param skype einsdreizehn  * @see  http://www.olivergierke.de  */</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 17:03:37 CET 2013</date>
    <body>Any idea on the second issue?  Its by far the more obstructive of the 2.  Hard to test a provider if you cant tell the TCK to use said provider :)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 17:16:01 CET 2013</date>
    <body>This sounds to me like a configuration issue.  I'll let our TCK folks try to  help....</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 17:22:47 CET 2013</date>
    <body>Well I did follow all documented instructions... ;)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 18:09:33 CET 2013</date>
    <body>Are the "TCK folks" on this list?  Or how do I proceed with that?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 18:26:27 CET 2013</date>
    <body>Yes</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 18:59:41 CET 2013</date>
    <body>Hi Ollie, You should send your suggestions to me. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Current standalone TCK drop</header>
    <date>Wed Mar 13 19:01:15 CET 2013</date>
    <body>Hi Steve, Since you are a licensee, I recommend that you use your direct licensee  channels for this. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Wed Mar 13 19:04:33 CET 2013</date>
    <body>Are others responding privately perhaps?  :) At a minimum, I would like to state that the JTA transaction could be rolled back from an external thread in the following sections: The current wording is: " 7.9.1 Container Responsibilities ... *  After the JTA transaction has completed (either by transaction commit or rollback), the container closes the entity manager calling EntityManager.close. ... 7.9.2 Provider Responsibilities ... *  When the JTA transaction rolls back, the provider must detach all managed entities if the persistence context is of type SynchronizationType.SYNCHRONIZED or has otherwise been joined to the transaction. ... "</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Wed Mar 13 19:15:19 CET 2013</date>
    <body>Hi Scott, No.  I wish they were responding in *any* manner! As you point out, the JTA spec already allows this, so if that is all we were  to do, I'm not sure I see the point.   In case I am being dense though, can you tell me what words  you would like to see added to the spec. To me, the real issue seems to be whether we can/should provide any guidance  as to how to handle such situations.   I'd like to get the benefit of hearing from the vendors here as  to what their implementations do. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Updated javadocs</header>
    <date>Wed Mar 13 19:47:11 CET 2013</date>
    <body>I've uploaded new javadocs which reflect this fix.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Wed Mar 13 20:10:10 CET 2013</date>
    <body>For 7.9.1, how about something like: " After the JTA transaction has completed (either by transaction commit or rollback), the container closes the entity manager by calling EntityManager.close.  The JTA transaction may rollback in a background thread (e.g. transaction timeout), in which case, the container should arrange for the entity manager to be closed but the EntityManager.close() should not be concurrently called while the application is in an EntityManager invocation. " The 7.9.2 wording can be similar I think: " When the JTA transaction rolls back, the provider must detach all managed entities if the persistence context is of type SynchronizationType.SYNCHRONIZED or has otherwise been joined to the transaction.  The JTA transaction may rollback in a background thread (e.g. transaction timeout), in which case, the provider should arrange for the managed entities to be detached from the persistence context but not concurrently while the application is in an EntityManager invocation. "</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Wed Mar 13 20:53:47 CET 2013</date>
    <body>If anyone is interested, the background and discussions for this can be found at: https://hibernate.onjira.com/browse/HHH-7910 and https://github.com/hibernate/hibernate-orm/pull/476 As for what various providers do, we take an approach in Hibernate where: 1) While handling a sync callback, we check to see if the thread of execution is the same as the last known "main line" call into the EM. If it is not, we set a flag and stop processing the sync callback (for the SUCCESS case we just let the execution proceed regardless because for Hibernate there is no danger in that case). 2) Upon entry and exit from every public facing EM method we check that flag and process the "after completion" processing if needed. Why entry and exit?  Because we like to be through :)  The exit checks explicitly handle *concurrent calls* to the EM through the sync (while a call is in flight within the EM back on the application thread); the entry calls handle a slightly different, though related, case mainly in in-container execution.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Wed Mar 13 22:55:00 CET 2013</date>
    <body>I think this is reasonable.   Do any of you object to such language being  added to the spec?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 01:57:04 CET 2013</date>
    <body>Hi Evan, I understand your point about throwing the PersistenceException.  However,  when do you expect the container to call EntityManager.close()?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 02:13:44 CET 2013</date>
    <body>I do not object as this is an improvement on what we have today. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 02:14:04 CET 2013</date>
    <body>There is nothing in the spec now preventing a container from closing an EntityManager before the end of the bean method invocation or user transaction commit so IllegalStateExceptions could happen now. If preventing the close() is needed then that would need additional language likely something similar to what you propose. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 02:22:07 CET 2013</date>
    <body>Well, FWIW that was my initial thought.  However, this imposes more  complexity on the container implementation, so I'm not sure that it should be made a requirement.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 02:32:57 CET 2013</date>
    <body>Yes.  Interestingly enough, however, this particularly issue hasn't been  raised here in the 7 years since the release of JPA 1.0 :-) Given that the Java EE 7 release is set to close very shortly, I don't think  we can realistically impose this requirement on containers at this point.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 02:39:08 CET 2013</date>
    <body>At the point at which close is called, or other?  I.e., by the container or  the persistence provider?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 15:38:23 CET 2013</date>
    <body>Are you proposing that the JPA spec be changed, to throw a PersistenceException subclass instead of the IllegalStateException during all calls that can currently throw an IllegalStateException (as per javadoc comments)?  I didn't understand what you meant last night when I read your message but today *I think* that I am getting it better. I don't think that we can prevent IllegalStateException exceptions from being thrown but as to your proposal to use a wrapped exception for the documented known cases, I'm not against changing that at the right time. Rather than throw a PersistenceException, I would rather throw something like a IllegalPersistenceStateException or PersistenceIllegalStateException that is a subclass of IllegalStateException, so that application code doesn't need to change for this improvement. If this is a long term change for next time, I'd like to do a sweep of other exceptions as well (EJBException is one that I'd like to replace with a PersistenceException someday). Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 16:27:45 CET 2013</date>
    <body>This was my first thought also (you can see some of that earlier discussion in the jira that Steve Ebersole mentioned earlier). The short answer, is there is no current way to allow that to happen in a standard way. Currently, as a user, if you want the EntityManager.close to be called from the application thread (your same time requirement), the transaction should also end from the application thread as well.  The fact that your application depends on the EntityManager.close happening in the application thread, means you should use a transaction manager that handles transaction timeout lazily via Transaction.setRollbackOnly. I'm not sure why an application would/could have a dependency on the EntityManager.close occurring in the application thread though.  Could you give an example of how applications could be dependent on that exactly?  Please mention how that application will suffer, if the EntityManager.close occurs in a different thread but not concurrently while the application is using it. Currently, with the proposed spec change, I don't see how we are causing additional pain for either way of handling transaction timeout (immediate tx cancellation from background thread, tx.setRollbackOnly).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new sub-thread about handing "deadlock detected" error from the database...</header>
    <date>Thu Mar 14 16:47:36 CET 2013</date>
    <body>When the DBMS forces rollback, there is no concurrency problem today that I know of.  I'm not sure how clear I was before, probably not enough.  The concurrency issue is about a background thread calling EntityManager.close() while the application thread is in the middle of another EntityManager.close(). We all know that EnitityManager is not required to be thread safe, although some implementations offer thread safety as an option at the persistence unit definition level (which could slow application performance if enabled from the javadoc comments :). My "stop the presses" cry about this situation, is to bring awareness about this potential problem.  I didn't know if others were aware or not.  Now that we are all more aware of the issue, I think (everyone's) customers will expect us to fix the concurrency issue in our implementations whether we come to an agreement or not, to how we can avoid concurrency errors. Having said that, I'm very interested in hearing about the pain that your users are seeing and how we might be able to help when the DBMS returns a "deadlock detected error". If I understand your proposal, the fix would be to throw a specific error instead of the IllegalStateException.  Do you have a link pointing to a case that is accessible to us, that shows an example of an application getting a "deadlock detected" error and the exception call stack that goes with it? Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Stored Procedure Query updates</header>
    <date>Thu Mar 14 21:40:57 CET 2013</date>
    <body>Hello All,    Working through the TCK testing has indicated that the behaviour of the stored procedure queries is really not clear and could use some clarifications.  I propose the following updates to section 3.10.17.3 and the related JavaDocs (attached) on StoredProcedureQuery.java.    Below removed text has been styles with strikethrough and new text is in red.  For those with plain text email clients I have attached a ODF document as well The setParameter methods are used to set the values of all required IN and INOUT parameters. It is not required to set the values of stored procedure parameters for  which default values have been defined by the stored procedure. The case where there is only a single result set (or a single result) plus any results passed back via INOUT and OUT parameters is supported using the getResultList and getSingleResult methods.     When calling getResultList and getSingleResult on a stored procedure query the provider will call execute() on an unexecuted stored procedure query before processing the getResultList and getSingleResult. The case where there is only an update count plus any results passed back via INOUT and OUT parameters is supported using the executeUpdate method.     When calling executeUpdate on a stored procedure query the provider will call execute() on an unexecuted stored procedure query followed by a getUpdateCount.  The results of an executeUpdate will be those of getUpdateCount. The getOutputParameterValue methods are used to retrieve the values passed back from the procedure through INOUT and OUT parameters. The execute method supports both the simple case where scalar results are passed back only via INOUT and OUT parameters as well as the most general case (multiple result sets and/or update counts, possibly also in combination with output parameter values). The execute method returns true if the first result is a result set, and false if it is an update count or there are no results other than through INOUT and OUT parameters, if any. If the execute method returns true, the pending result set can be obtained by calling getResultList and getSingleResult .  The hasMoreResults method can then be used to test for further results. If execute or hasMoreResults returns false, the getUpdateCount method can be called to obtain the pending result if it is an update count. The getUpdateCount method will return either the update count (zero or greater) or -1 if there is no update count (i.e., either the next result is a result set or there is no next update count ). For portability, results that correspond to JDBC result sets and update counts need to be processed before the values of any INOUT or OUT parameters are extracted. After results returned through getResultList and getUpdateCount have been exhausted, results returned through INOUT and OUT parameters can be retrieved. The getOutputParameterValue methods are used to retrieve the values passed back from the procedure through INOUT and OUT parameters. For portability, results that correspond to JDBC result sets and update counts need to be processed before the values of any INOUT or OUT parameters are extracted. When using REF_CURSOR parameters for results sets the updates counts should be exhausted before calling getResultList to retrieve the result set.  Alternatively the REF_CURSOR result set can be retrieved through  getOutputParameterValue.  Result set mappings will be applied to REF_CURSOR results in the order the REF_CURSOR parameters were registered with the query In the simplest case, where results are returned only via INOUT and OUT parameters, execute can be followed immediately by calls to getOutputParameterValue. Attachment: storedprocwriteup.odt Description: application/vnd.oasis.opendocument.text Attachment: StoredProcedureQuery.pdf Description: Adobe PDF document Attachment: StoredProcedureQuery.java Description: Text document</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: transaction-scoped persistence context being closed at JTA transaction completion time from non-application thread ...</header>
    <date>Thu Mar 14 22:03:57 CET 2013</date>
    <body>I have added the proposed language to the spec.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] new spec draft (Final Release candidate)</header>
    <date>Tue Mar 19 20:40:20 CET 2013</date>
    <body>I've just uploaded a new spec draft and javadocs to our project downloads area:  http://java.net/projects/jpa-spec/downloads These reflect the recent updates to the StoredProcedureQuery interface, clarifications with regard to transaction rollback on a background thread, and minor clarification to the result of the EMF.getCache method.  Changebars reflect changes since the Proposed Final Draft. We are closing in on the Java EE 7 target release date. Please therefore let me know if you find any issues with this draft, or any reason that it should not be submitted for the JPA 2.1 Final Release. thanks, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new spec draft (Final Release candidate)</header>
    <date>Tue Mar 19 20:53:24 CET 2013</date>
    <body>Hi Linda, In 7.9.2, we should change "container" to "provider": " When the JTA transaction rolls back, the provider must detach all managed entities if the persistence context is of type SynchronizationType.SYNCHRONIZED or has otherwise been joined to the transaction. Note that the JTA transaction may rollback in a background thread (e.g., as a result of transaction timeout), in which case the *provider* should arrange for the managed entities to be detached from the persistence context but not concurrently while the application is in an EntityManager invocation. " Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new spec draft (Final Release candidate)</header>
    <date>Tue Mar 19 22:36:31 CET 2013</date>
    <body>Hi Scott, Yes,  I will make the change. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: new sub-thread about handing "deadlock detected" error from the database...</header>
    <date>Wed Mar 20 13:53:27 CET 2013</date>
    <body>Evan, From my point of view, there are multiple aspects to this discussion. Some of the aspects could be handled by non-JPA technologies that may not exist yet.  I think that the problem of avoiding concurrent EntityManager invocations has been around for a long time but just didn't become a priority until now. To meet the new JPA 2.1 requirements, the thread-unsafe EntityManager needs to be protected from concurrent access, however that can be accomplished by the container/provider respectfully. I think that some of the related aspects are: 1.  Informing the application that the tx timed out so that the *state of the world* may be collected (this is more of a management aspect than JPA IMO).  Currently, this is already possible (in vendor specific ways) and I'm not sure if we would want to standardize the notification of this event occurring somewhere (in some EE.future version).  In a previous (pre EE) Java application server that I'm familiar with, I liked how application code could receive notifications of events like this (e.g. applications could be notified if a database connection went bad so that the application could help notify someone to restart the db server). 2.  Having a consistent way for the application thread to learn that a tx timeout occurred.  I think that this was the point of your use case below but I don't believe the application will always see an IllegalStateException, although it could see an ISE.  The application will eventually see that the transaction ended but that might not happen until the CMT method returns (or BMT logic tries to end the TX that no longer exists).  Previous to the new 2.1 requirement for handling concurrency, applications could of seen various IllegalState like exceptions (e.g. NullPointerException).  Can we depend on the application *eventually* seeing that the tx disappeared when the application tries to end the TX?  Do we need more than that (in some EE.future version?) Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] minor bug in spec</header>
    <date>Wed Mar 20 19:50:33 CET 2013</date>
    <body>When I was reviewing the spec last night, I noticed the following problem. The restriction against propagating persistence contexts of type SynchronizationType.UNSYNCHRONIZED into PCs of type SYNCHRONIZED currently states: "If there is a persistence context of type SynchronizationType.UNSYNCHRONIZED associated with the JTA transaction and the target component specifies a persistence context of type SynchronizationType.SYNCHRONIZED, an EJBException is thrown by the container." This is inappropriate, since the components involved may not be EJBs. I plan to change it to state that the IllegalStateException is thrown. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] EMF.getProperties() and EM.getProperties()</header>
    <date>Thu Mar 21 14:02:44 CET 2013</date>
    <body>Working with the new TCK I ran across 2 situations I'd like to discuss with the group. First, the new TCK is now including assertions that EntityManager.getProperties() contains properties used to build the EntityManagerFactory that was used to create the EntityManager.  In my opinion the spec does not say anywhere that this should be the case. Can anyone point me to where it does if I am wrong (or else the TCK needs to be fixed for that)? Secondly, the types of properties the TCK is checking for got me thinking.  It checks for some sensitive information like JDBC url and user and password.  These are things we expose and are kind of forced to expose at the moment.  I'd like to suggest that we update the spec to explicitly state that some of these properties are not available period, at the very least javax.persistence.jdbc.password though I'd argue for javax.persistence.jdbc.url and javax.persistence.jdbc.user as well.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EMF.getProperties() and EM.getProperties()</header>
    <date>Thu Mar 21 17:06:07 CET 2013</date>
    <body>I don't believe it does.  This sounds like a bug. Are you referring to EMF.getProperties?  I wouldn't expect these to be  available from EM.getProperties. I think it is reasonable that an environment be able to restrict the  availability of such information. I think that adding something like the following to the discussion of the EMF  interface in the spec would be reasonable:    Note that the policies of the installation environment may restrict some  information from being made available through the getProperties method (for example,  JDBC user,  password, URL).</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EMF.getProperties() and EM.getProperties()</header>
    <date>Thu Mar 21 18:59:28 CET 2013</date>
    <body>Thanks! Personally I'd like to filter those out in Hibernate implementation, but nervous about the TCK continuing to test for that information if this is not discussed in the spec.  So that would be great.  Thanks!</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] updated spec draft (Final Release candidate)</header>
    <date>Fri Mar 22 23:59:49 CET 2013</date>
    <body>This reflects the outcome of the discussion of the past week, including the changes to the exception raised by illegal propagation of persistence contexts.  I've also made a number of what I believe to be purely editorial changes.  The change bars flag the changes since the last draft (i.e., from the draft of March 19). The updated javadocs contain only changes to the documentation. The modifications were the following:   EntityManager.close:  added StoredProcedureQuery to the list   EntityManagerFactory.addNamedQuery: added references to  createNamedStoredProcedureQuery   SqlResultSetMapping:  name element:  added reference to StoredProcedureQuery JoinColumns, MapKeyJoinColumns, PrimaryKeyJoinColumn, PrimaryKeyJoinColumns:   foreignKey element: made documentation more uniform</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Schema generation with existing EMF</header>
    <date>Sun Mar 24 08:49:34 CET 2013</date>
    <body>I've been running into a conceptual issue with schema generation in JPA 2.1  and how unit tests are typically organized. Let's say you want a clean database for every unit test method. I prefer to  create the schema in the database before every test method, and then drop it  after the test method completes. I have a single EntityManagerFactory for the whole test class, it can be  shared by every test method. This is possible with the Hibernate API, the  schema generator uses an existing EMF configuration to produce the  create/drop scripts. With the static Persistence.generateSchema() in 2.1 this approach doesn't  work. Calling generateSchema() internally builds a new EMF every time. One of the problems is that you get new automatically generated foreign key  names every time you call generateSchema(). The "drop" action therefore  always fails, unless you define all foreign key names in your metadata. There  could be other automatically generated artifact names, so the "drop" action  in general, if called from a static context, is not useful during development. The current solution then is to specify "drop-and-create" and to build and  close an EMF for every unit test method. This means starting and stopping the  persistence provider for every test method, slowing down test runs  significantly. A better solution would be an additional Persistence.generateSchema(EMF,  properties) method that accepts an existing EMF and some "override"  properties.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Schema generation with existing EMF</header>
    <date>Mon Mar 25 19:23:04 CET 2013</date>
    <body>Hi Christian, This is an interesting proposal, but too much to consider in the remaining  couple of days that we have left in this release cycle. Could you please log this as a JIRA item RFE so we can track for future. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Why no AutoCloseable for EntityManager?</header>
    <date>Fri Mar 29 23:59:18 CET 2013</date>
    <body>Has the possibility of EntityManager extending AutoCloseable been considered? Surely EntityManager instances *should* be able to work with try-with-resources.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] AUTO: RICK CURTIS is out of the office (returning 04/08/2013)</header>
    <date>Sat Mar 30 04:02:27 CET 2013</date>
    <body>I am out of the office until 04/08/2013. While I am out of the office please contact my manager Nate Ziemann with any urgent issues. For technical issues contact Kevin Sutter. Note: This is an automated response to your message  "[jpa-spec users] Why no AutoCloseable for EntityManager?" sent on 03/29/2013 5:59:18 PM. This is the only notification you will receive while this person is away.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] modifying fields of a removed instance</header>
    <date>Tue Apr 02 17:42:30 CEST 2013</date>
    <body>Dear group, Am I right in the assumption, that reading or writing fields/properties of a removed entity (prior to commit) is allowed? I.e. the following sequence is valid: Thank you! Christian -- Christian von Kutzleben Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cvkutzleben@... www.versant.com | www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: modifying fields of a removed instance</header>
    <date>Tue Apr 02 20:00:54 CEST 2013</date>
    <body>Hi Christian, Correct.  That is allowed.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: modifying fields of a removed instance</header>
    <date>Tue Apr 02 20:23:16 CEST 2013</date>
    <body>Well, more correctly... the spec is silent on the matter.  It does not say it is allowed, nor does it say it is disallowed.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: modifying fields of a removed instance</header>
    <date>Wed Apr 03 08:58:04 CEST 2013</date>
    <body>Hi Steve, Thanks for your reply! That was also our interpretation. We assumed, that this particular case should be explicitly disallowed in the spec, if the intention was, that this is not supported. So we do support this. Christian On Tue, Apr 2, 2013 at 8:23 PM, Steve Ebersole steve.ebersole@... Well, more correctly... the spec is silent on the matter.  It does not say it is allowed, nor does it say it is disallowed. Hi Christian, Correct.  That is allowed. Dear group, Am I right in the assumption, that reading or writing fields/properties of a removed entity (prior to commit) is allowed? I.e. the following sequence is valid: Thank you! Christian -- Christian von Kutzleben Chief Engineer | Versant GmbH (T) +49 40 60990-0 (F) +49 40 60990-113 (E) cvkutzleben@... cromberg@... www.versant.com http://www.versant.com www.db4o.com http://www.db4o.com -- Versant GmbH is incorporated in Germany. Company registration number: HRB 54723, Amtsgericht Hamburg. Registered Office: Halenreie 42, 22359 Hamburg, Germany. Geschäftsführer: Bernhard Wöbker, Volker John CONFIDENTIALITY NOTICE: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential or proprietary information. Any unauthorized review, use, disclosure or distribution is prohibited. If you are not the intended recipient, immediately contact the sender by reply e-mail and destroy all copies of the original message.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] updated javadocs</header>
    <date>Thu Apr 04 03:23:49 CEST 2013</date>
    <body>We caught a small glitch in the use of generics in the result of EntityManager.getEntityGraph. Instead of the signature of this method should be I've just uploaded new javadocs that reflect this to  http://java.net/projects/jpa-spec/downloads . There are no other changes to APIs or spec from what I uploaded on March 22. I will be submitting all required materials to the JCP shortly, in preparation for the final approval ballot. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Why no AutoCloseable for EntityManager?</header>
    <date>Thu Apr 04 14:25:06 CEST 2013</date>
    <body>Sorry to insist about this, and I know it's very late in the day for this version, but unless I've failed to see something this does look like a stupid oversight that is very easily fixed. EntityManager contains a close() method which has to be called on anything returned by emf.createEntityManager().  If the words "implements AutoCloseable" were added to the declaration of interface EntityManager then, without making any other changes, the try-with-resources statement could be used to ensure closure in all circumstances, thus:   // use em This is now the preferred idiom for anything that requires a close call. The only down side I can see to this change is that it would tie JPA to java 1.7 or later, since java.lang.AutoCloseable was only introduced in this version, but that doesn't seem much of a problem. Is there a good reason this hasn't been done?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Why no AutoCloseable for EntityManager?</header>
    <date>Thu Apr 04 14:52:35 CEST 2013</date>
    <body>Hi, Just my 2 cents regarding this, why I don't see this as useful: - EntityManager.close() has a special semantics, if a transaction is active, it does not free resources then - our implementation keeps resources while a transaction is active, so having a "transaction-scope" with a try could make sense*, but for "close" it does not bring us any advantage (might be different for other vendors) * also see the .NET System.Transactions API with using and the TransactionScope Regards, Christian On Thu, Apr 4, 2013 at 2:25 PM, jonty_lawson@... Sorry to insist about this, and I know it's very late in the day for this version, but unless I've failed to see something this does look like a stupid oversight that is very easily fixed. EntityManager contains a close() method which has to be called on anything returned by emf.createEntityManager(). If the words "implements AutoCloseable" were added to the declaration of interface EntityManager then, without making any other changes, the try-with-resources statement could be used to ensure closure in all circumstances, thus:   // use em This is now the preferred idiom for anything that requires a close call. The only down side I can see to this change is that it would tie JPA to java 1.7 or later, since java.lang.AutoCloseable was only introduced in this version, but that doesn't seem much of a problem. Is there a good reason this hasn't been done?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Why no AutoCloseable for EntityManager?</header>
    <date>Fri Apr 05 15:37:32 CEST 2013</date>
    <body> Just my 2 cents regarding this, why I don't see this as useful: Wow. So are you saying that calling close() on the EM is not important (at least for your implementation)? If this is the case in general then you are right and adding AutoCloseable to EntityManager would be a mistake. The javadoc for AutoCloseable states: "A resource that must be closed when it is no longer needed", and if this doesn't apply to EM instances then it's appropriate that it not be implemented. However, good citizen that I am, I had assumed that it was important that the close method be called on instances of EM, and that use of a try/finally was a good way of achieving this. This belief was fostered by: 1) The general usage of close methods within the java libraries.      Usually these require that close be called to free system resources      and it is effectively a programming error to fail to do so (eg in java.io,      java.net, java.sql, java.nio, javax.management and elsewhere). 2) The examples in the JPA spec that show close being called on an EM      in a number of places. Admittedly the JPA spec is silent on the *necessity* of calling close on application-managed EMs. I summary, I would say that either the JPA spec should be updated to state that calling close on an EM is optional (and explain when it should be called as is done for, eg, javax.naming.NamingEnumeration), or it should be changed to indicate that it is necessary and EM should implement AutoCloseable.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Why no AutoCloseable for EntityManager?</header>
    <date>Fri Apr 05 16:05:13 CEST 2013</date>
    <body>I summary, I would say that either the JPA spec should be updated to state that calling close on an EM is optional (and explain when it should be called as is done for, eg, javax.naming.NamingEnumeration), or it should be changed to indicate that it is necessary and EM should implement AutoCloseable.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] @Index annotation</header>
    <date>Sat Apr 06 07:20:20 CEST 2013</date>
    <body>I'm sure this has been covered before, but it's hard to search the mailing list archives for a word as generic as "index". I currently see no way to add an index for a column provided by a @MappedSuperclass (the @Table annotation cannot be used with @MappedSuperclass). Is this correct? And if so, what is the reason behind this restriction? Is there a way to specify indexes in a superclass without resorting to using @Entity with an inheritance strategy? MappedSuperclass lets you have your cake and eat it, too--it basically emulates "table per class" inheritance while you can still set the strategy to something else. Thanks, Alvin</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] EntityManagerFactory.addNamedQuery() stores a template?</header>
    <date>Tue Apr 16 00:22:15 CEST 2013</date>
    <body>Hello Linda,     From the JavaDoc of the newly introduced EntityManagerFactory.addNamedQuery(String name, Query q), I am assuming that the supplied query q is treated as a template as opposed to an instance as far as binding parameters are concerned. Let us consider a simple case  // we create a query      // and configure it     // and bind its parameter to a specific value      // let us declare this query to the persistence unit     // after a while we recall it         EntityManager em2 = ...;      // Then according to the JavaDoc the following assertions should succeed        assertTrue(query != query2); // the query we declared is a template and hence what is recalled is essentially a new one        assertEquals(51, query2.getFirstResult());    // but the new one  retains the first result position and other configuration of the template           // but the binding parameters are not memorized i.e   Will you please confirm if my reading of the spec in this regard is correct or not?         Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] scope of EntityGraphs</header>
    <date>Tue Apr 16 00:57:31 CEST 2013</date>
    <body>The scope of EntityGraph should be persistence unit, not persistence context. The method such as EntityManager.createEntityGraph(String name) etc. looks non-intuitive. If an application wants to create a graph at runtime,  it should call EntityManagerFactory.createEntityGraph(String name). As the API currently stands, it seems to allow EntityGraph of the same name be defined by two separate persistence contexts -- opening up a pandora's box of confusion. It is much safer to define these graphs scoped at EntityManagerFactory level and imposing uniqueness on the name of the graphs. Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: scope of EntityGraphs</header>
    <date>Tue Apr 16 01:37:51 CEST 2013</date>
    <body>It is.  However, an EntityGraph that is a mutable copy of a named entity  graph can be retrieved via the createEntityGraph method, modified, and then added back  to the EMF to replace the original one.   Given the use cases for entity graphs, it  seemed reasonable that they should be able to be retrieved via the EntityManager.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: EntityManagerFactory.addNamedQuery() stores a template?</header>
    <date>Tue Apr 16 01:41:57 CEST 2013</date>
    <body>Yes</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: scope of EntityGraphs</header>
    <date>Tue Apr 16 04:24:17 CEST 2013</date>
    <body>Hello Linda,   Thanks for explaining the usage. But I am afraid it looks confusing/non-intuitive to me. 1. If the graphs are in EMF scope, why the following method is on EM?   /** * Return all named EntityGraphs that have been defined for the provided class type. * @param entityClass entity class * @return list of all entity graphs defined for the entity * @throws IllegalArgumentException if the class is not an entity */ 2. If two entitymanagers separately call createEntityGraph(String egName) but with the same input argument, they get two separate mutable copies -- right? Now if they mutate their respective copies differently and replace back to the EMF, which copy of the original graph wins? The other entitymanagers later when gets the graph by the same name will now receive a graph with new structure -- right?   So then what does immutability will signify in such case? 3. And above all, why all these needless complexity? Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Linda DeMichiel ---04/15/2013 04:38:18 PM---On 4/15/2013 3:57 PM, Pinaki Poddar wrote: &amp;gt; The scope of EntityGraph should be persistence unit, no From: To: jsr338-experts@... Date: 04/15/2013 04:38 PM Subject: [jsr338-experts] Re: scope of EntityGraphs It is.  However, an EntityGraph that is a mutable copy of a named entity graph can be retrieved via the createEntityGraph method, modified, and then added back to the EMF to replace the original one.   Given the use cases for entity graphs, it seemed reasonable that they should be able to be retrieved via the EntityManager. http://openjpa.apache.org/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Method signature in EntityGraph.addSubclassSubgraph()</header>
    <date>Wed Apr 24 21:04:40 CEST 2013</date>
    <body>Following method signature is defined in EntityGraph&amp;lt;T&amp;gt; interface     /**      * Add additional attributes to this entity graph that      * correspond to attributes of subclasses of this EntityGraph's      * entity type.  ...     **/     Should the correct signature be    public  &amp;lt;S extends Otherwise, the current signature &amp;lt;T&amp;gt; will hide Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Uniqueness of names of NamedQuery/StoredProcedure</header>
    <date>Wed May 01 22:09:33 CEST 2013</date>
    <body>The spec API uses name as the identifying moniker for named queries and stored procedures as in       public or               public While the name of a @NamedQuery is scoped within a persistence unit (Section 3.10.14, pp 150), does this scope also include the named stored procedures?   For example, can we define a NamedQuery and a StoredProedure with the same name? Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Uniqueness of names of NamedQuery/StoredProcedure</header>
    <date>Wed May 01 22:42:39 CEST 2013</date>
    <body>Is JPA still subject to change after the Final Draft? Werner On Wed, May 1, 2013 at 10:09 PM, Pinaki Poddar ppoddar@... The spec API uses name as the identifying moniker for named queries and stored procedures as in       public or               public While the name of a @NamedQuery is scoped within a persistence unit (Section 3.10.14, pp 150), does this scope also include the named stored procedures?   For example, can we define a NamedQuery and a StoredProedure with the same name? Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Uniqueness of names of NamedQuery/StoredProcedure</header>
    <date>Wed May 01 23:48:52 CEST 2013</date>
    <body>The spec doesn't say so explicitly (although it should), but I would not  expect this to work (or at any rate be portable). We'll need to clarify in future. What do you think the behavior should be?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Uniqueness of names of NamedQuery/StoredProcedure</header>
    <date>Wed May 01 23:49:54 CEST 2013</date>
    <body>Certainly!  But that would be in an MR or JPA 2.2 :-)</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Uniqueness of names of NamedQuery/StoredProcedure</header>
    <date>Thu May 02 00:33:59 CEST 2013</date>
    <body>Hello Linda, Each of the query variants that can be recalled by a name must have a name that is unique in a persistence unit scope.   Effectively, my answer to the earlier question: "can we define a NamedQuery and a StoredProedure with the same name?" is "no". Nor can we define a NamedQuery(name="findAll") in two entity classes A and B. But spec already prohibits that. It is a common (or at least good) practice to qualify such common names by their class scope e.g. "Person.findAll" and "Order.findAll". So extending the uniqueness across all type of queries will not be difficult for the developers and will not invite unnecessary complexity for the providers. Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware Linda DeMichiel ---05/01/2013 02:51:25 PM---On 5/1/2013 1:09 PM, Pinaki Poddar wrote: &amp;gt; The spec API uses name as the identifying moniker for na From: To: jsr338-experts@... Date: 05/01/2013 02:51 PM Subject: [jsr338-experts] Re: Uniqueness of names of NamedQuery/StoredProcedure The spec doesn't say so explicitly (although it should), but I would not expect this to work (or at any rate be portable). We'll need to clarify in future. What do you think the behavior should be? http://openjpa.apache.org/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] find and removed objects</header>
    <date>Wed May 08 16:30:14 CEST 2013</date>
    <body>Dear group, Is "find" supposed to be consistent with "contains", i.e. removed objects won't be found, even if the context is not yet synchronized to the database? Thank you! Christian</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Faces Flow Definition</header>
    <date>Mon May 13 21:10:49 CEST 2013</date>
    <body>I have been following the JSF 2.2 specification closely, nice work.  I have a question regarding the final implementation of Faces Flow.  It seems that the flow definition may be added to the faces-config.xml file, within the &amp;lt;flow-definition&amp;gt; elements as follows: .... This works well under the current builds.  However, as mentioned in some previous articles and within the spec itself, the flow definition can also reside within a file that is named /WEB-INF/&amp;lt;flowName&amp;gt;-flow.xml OR /&amp;lt;flowName&amp;gt;/&amp;lt;flowName&amp;gt;-flow.xml, where the flow definition XML file still follow the following format under the final release of the specification? xmlns=" http://www.w3.org/1999/xhtml"       xmlns:h=" http://xmlns.jcp.org/jsf/html"       xmlns:f=" http://xmlns.jcp.org/jsf/core"       xmlns:j=" http://xmlns.jcp.org/jsf/flow" ;&amp;gt;                 &amp;lt;j:flow-return id="returnFromB"&amp;gt;                .... If so, I am unable to get my examples working when specifying a flow definition within an XML file named after the flow, such as exampleFlow2-flow.xml.  Also, the latest builds of Netbeans 7.3 for EE 7 do not seem to recognize the  http://xmlns.jcp.org/jsf/flow URI . Thanks in advance for your help.  I am trying to document the proper usage of the faces flow feature and I want to ensure that I am not incorrect. Best</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Faces Flow Definition</header>
    <date>Mon May 13 21:30:17 CEST 2013</date>
    <body>Hi there, I think you meant to send this to the JSF users group. regards, -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Re: Faces Flow Definition</header>
    <date>Mon May 13 22:28:06 CEST 2013</date>
    <body>Hi Linda, Indeed, a case of having too many tabs open.  Sorry for posting to the  incorrect group. Keep up the good work on JPA. Best Regards Josh Juneau http://jj-blogger.blogspot.com https://www.apress.com/index.php/author/author/view/id/1866  Hi there,    I think you meant to send this to the JSF users group.    regards,    -Linda       http://www.w3.org/1999/xhtml" http://xmlns.jcp.org/jsf/html" http://xmlns.jcp.org/jsf/core" http://xmlns.jcp.org/jsf/flow" ;&amp;gt;     http://xmlns.jcp.org/jsf/flow URI .</body>
  </mail>
  <mail>
    <header>[jpa-spec users] Participe!</header>
    <date>Tue May 28 20:34:56 CEST 2013</date>
    <body />
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: find and removed objects</header>
    <date>Sat Jun 01 00:44:57 CEST 2013</date>
    <body>Hi Christian, Yes -- that is correct. -Linda</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Fri Jun 07 17:40:57 CEST 2013</date>
    <body>Perhaps we should start a new thread if there is more interest in switching to a two phase approach to creating the container entity manager factory (for post 2.1).  I'm just now learning of a conflict between implicit CDI bean manager support and using ClassFileTransformers to enhance/rewrite entity classes.  I'd like to allow both but the features seem to conflict with each other. Knowing whether there is an explicit CDI bean manager, is easy to detect (beans.xml is found) but for the implicit support, we assume there is a bean manager if there is an EJB module.  When we create the container entity manager factory, if we pass in an implicit CDI bean manager, the CDI bean manager creation will scan application classes, which means ClassFileTransformers will register to late. Thoughts?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Tue Jun 11 16:18:13 CEST 2013</date>
    <body>I would like to introduce the least pain, when/if we switch to allowing a two phase bootstream of the container EMF.  I propose that we introduce a TwoPhaseBootstrap interface, if the persistence provider implements the TwoPhaseBootstrap interface, the methods can be used by the caller (e.g. EE container looking to create the container entity manager factory). Bootstrap IMO, this is a good way to allow full JPA support (ClassFileTransformers) and CDI implicit/explicit support (bean manager can be passed into the persistence provider).  Without this change, I'm afraid that all deployments with EJB modules, will need to choose between ClassFileTransformers working or use of CDI with entity listeners (performance vs functionality). Does anyone agree or disagree with making a change like the above asap? Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Availability of API jar files</header>
    <date>Thu Jun 13 19:51:03 CEST 2013</date>
    <body>Hi, I was just notified of the following JIRAs created by Matthew Adams: https://java.net/jira/browse/JPA_SPEC-19 (for JPA 2.0) https://java.net/jira/browse/JPA_SPEC-60 (for JPA 2.1) I also looked at the maven repo on java.net and found most of the other JSR API jar files:   https://maven.java.net/index.html#nexus-search;quick~javax Is there any reason why the API jar files are not posted for JPA?  I know the RI is EclipseLink, but it would be great to have all of the Java EE technologies available in a single repo... ---------------------------------------------------------------------------------------------------------------------------------------------------------------- Kevin Sutter, Java EE and Java Persistence API (JPA) architect mail:   sutter@..., Kevin Sutter/Rochester/IBM           http://webspherepersistence.blogspot.com/ phone:  tl-553-3620 (office), 507-253-3620 (office)                       http://openjpa.apache.org/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Availability of API jar files</header>
    <date>Sat Jun 15 05:00:24 CEST 2013</date>
    <body>Interesting, Kevin.  I wasn't even aware of that.  Thanks! All the more reason to get these API jars out there. On Thu, Jun 13, 2013 at 12:51 PM, Kevin Sutter sutter@... Hi, I was just notified of the following JIRAs created by Matthew Adams: https://java.net/jira/browse/JPA_SPEC-19 (for JPA 2.0) https://java.net/jira/browse/JPA_SPEC-60 (for JPA 2.1) I also looked at the maven repo on java.net and found most of the other JSR API jar files:   https://maven.java.net/index.html#nexus-search;quick~javax Is there any reason why the API jar files are not posted for JPA?  I know the RI is EclipseLink, but it would be great to have all of the Java EE technologies available in a single repo... ---------------------------------------------------------------------------------------------------------------------------------------------------------------- Kevin Sutter, Java EE and Java Persistence API (JPA) architect mail:   sutter@... , Kevin Sutter/Rochester/IBM           http://webspherepersistence.blogspot.com/ phone:  tl-553-3620 (office), 507-253-3620 (office)                       http://openjpa.apache.org/</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Mon Jun 17 15:56:38 CEST 2013</date>
    <body>A few minor changes to the below proposed interfaces are here (they just need a package change to javax.persistence :-) https://github.com/scottmarlow/jipijapa/commit/db861170076cc9dbc4c07b6fb2facb30c0bc0e8e#L3R30 + https://github.com/scottmarlow/jipijapa/commit/db861170076cc9dbc4c07b6fb2facb30c0bc0e8e#L2R27</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Mon Jun 17 16:18:25 CEST 2013</date>
    <body>Having a two step deployment of the persistence unit is a good idea.  It would be a lot simpler to add a single "preDeploy" or "preCreate" method to the current SPI though. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Mon Jun 17 16:51:00 CEST 2013</date>
    <body>Why is adding a method "simpler"?  "Simpler" in what sense?  From a user/integration perspective I don't think it makes any difference. From a provider perspective, I think the proposed solution is far more conceptually correct and therefore simpler.  And I think these "conceptual" matches tend to work better from a user perspective as well (when an API and the concept it tries to expose/model match up...). I assume you mean adding preDeploy/preCreate to PersistenceProvider. Here is what I mean about "conceptuality": How does the provider track the preDeploy call above as relative to the eventual createContainerEntityManagerFactory call?  Meaning, imagine: Unless we say that a PersistenceProvider instance needs to now conceptually model a single "pu".  But thats a different set of assumptions than what we have today.  A separate contract makes that explicitly clear: There is also the fact that simply adding a method does not allow opting in or opting out of this 2-phaseness.  I guess that depends on how and if this group decides to accept this proposal in general.  And if so, specifically, whether we expect all providers to provide a 2-phase bootstrap mechanism.  Because if its not a requirement (aka, its optional) then having an optional contract is the best way to expose this period. As an additional suggestion, in the Hibernate version of this I actually added methods to "Bootstrap" to handle schema-generation as well.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Mon Jun 17 18:46:06 CEST 2013</date>
    <body>agreed. only the lifecycle hooks could occur as pair of preCreate()/postCreate(). Regards -- Pinaki Poddar                           Chair, Apache OpenJPA Project           http://openjpa.apache.org/ JPA Expert Group Member Application &amp;amp; Integration Middleware gordon yorke ---06/17/2013 07:29:16 AM---Having a two step deployment of the persistence unit is a good idea.  It  would be a lot simpler to From: To: jsr338-experts@... Date: 06/17/2013 07:29 AM Subject: [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes Having a two step deployment of the persistence unit is a good idea.  It would be a lot simpler to add a single "preDeploy" or "preCreate" method to the current SPI though. --Gordon https://github.com/scottmarlow/jipijapa/commit/db861170076cc9dbc4c07b6fb2facb30c0bc0e8e#L3R30   https://github.com/scottmarlow/jipijapa/commit/db861170076cc9dbc4c07b6fb2facb30c0bc0e8e#L2R27  </body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Mon Jun 17 19:07:52 CEST 2013</date>
    <body>Hello All, The PersistenceProvider instance should remain scoped to the provider implementation and continue to manage multiple persistence units.  Any persistence provider can easily track the preDeploy call with the unique persistence unit name. Creating an additional interface to represent an instance of the persistence unit creates possible multiple instances of the same persistence unit which adds unneeded complexity.  Also, the additional interfaces do not provide any functional value and complicate the interface forcing containers to perform "instance of" checks or perform calls and check the result to determine if the new interfaces are supported.   Having the PersistenceProvider manage the persistence units allows the provider to ignore pre-deploy if it is not supported and allows containers to code to a single interface and not require rewriting to now manage an additional artifact should a provider support pre-deploy.  With the interfaces as proposed a container would need to have two code paths, one that interfaces with PersistenceProvider and another for the "bootstrap" interface. Alternatively,  if an artifact is needed an EntityManagerFactory could be returned in a pre-deployed state from createContainerEntityManagerFactory via a new parameter or property and activated through new API.  Although, some of the language in the specification on when certain artifacts are initialized (ie canonical metamodel) would need to change. --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Wed Jun 19 00:38:18 CEST 2013</date>
    <body>Doesn't the container need two code paths anyway?  One for older JPA provider versions that don't support the two step deployment of the persistence provider and another path for the providers that do.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Picking the right persistence unit from multiple choices when no unitName is specified</header>
    <date>Wed Jun 19 13:50:07 CEST 2013</date>
    <body>Should application deployment fail if there are multiple persistence units defined and no unitName is specified?  If not, which persistence unit should be picked? For example, given the below persistence.xml contents: " " And a persistence context: " " Should "rightpu1" be used or "otherrightpu2"? Should we have a standard pu property ("javax.persistence.default") that allows the default pu to be specified for this case? If the default pu property is not specified, should application deployment fail for this case?  Or should the application deployment continue with whichever persistence unit the deployer happens to pick? Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Picking the right persistence unit from multiple choices when no unitName is specified</header>
    <date>Wed Jun 19 17:07:13 CEST 2013</date>
    <body>Perhaps a Java system property called "javax.persistence.default-unit" would work here instead, since you're talking about a concept that spans multiple persistence units? On Wed, Jun 19, 2013 at 6:50 AM, Scott Marlow smarlow@... Should application deployment fail if there are multiple persistence units defined and no unitName is specified?  If not, which persistence unit should be picked? For example, given the below persistence.xml contents: " " And a persistence context: " " Should "rightpu1" be used or "otherrightpu2"? Should we have a standard pu property ("javax.persistence.default") that allows the default pu to be specified for this case? If the default pu property is not specified, should application deployment fail for this case?  Or should the application deployment continue with whichever persistence unit the deployer happens to pick? Scott</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Picking the right persistence unit from multiple choices when no unitName is specified</header>
    <date>Wed Jun 19 17:54:35 CEST 2013</date>
    <body>If all containers supported a global default-unit name, that would also help application portability (doesn't matter where the global setting is stored). Or we could all agree that the container should fail the deployment for the below (ambiguous) case.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: Re: Picking the right persistence unit from multiple choices when no unitName is specified</header>
    <date>Wed Jun 19 21:19:55 CEST 2013</date>
    <body>Hi, I like the idea of specifying a default persistence unit name (e.g in the form of a system property or any other global mechanism). Without this option adding a new persistence-unit to the persistence.xml would suddenly invalidate a PersistenceContext annotation of the form   @PersistenceContext EntityManager em Regards Michael On Perhaps a Java system property called "javax.persistence.default-unit" would work here instead, since you're talking about a concept that spans multiple persistence units?</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Fri Jun 21 15:51:16 CEST 2013</date>
    <body>Not when interacting with the provider for createEntityManagerFactory(). --Gordon</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Fri Jun 21 15:57:23 CEST 2013</date>
    <body>How would it require instanceof checks? And I am sorry, but I utterly disagree with you.  Adding these new methods to PersistenceProvider essentially forces them to use retarded static maps to track stuff which is a recipe for disaster (leaks, etc). The integration knows *much better* how to track and scope this information. As for the 2 code paths, I am not following.  I guess it depends on whether the 2 phase approach becomes the required norm, or whether it is viewed as optional.  But, regardless, this "2 code paths" argument does not hold water; containers would need 2 code paths anyway even in your suggestion.</body>
  </mail>
  <mail>
    <header>[jpa-spec users] [jsr338-experts] Re: two phase approach to creating the EMF, implicit CDI bean manager support ... was :schema generation proposed changes</header>
    <date>Fri Jun 21 16:33:06 CEST 2013</date>
    <body>In your proposal, how does the container know that it can call the PersistenceProvider preDeploy/preCreate method?  Would the application have to set the right version in the persistence.xml in order to use the two phase approach with createContainerEntityManagerFactory?  Or would the container be expected to always call preDeploy/preCreate and ignore the error when using older providers?</body>
  </mail>
</mails>

